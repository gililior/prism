[
  {
    "paper_id": "105_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_105_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7526999999999999,
      "max_similarity": 0.7719,
      "avg_coverage": 0.3411666666666667,
      "max_coverage": 0.5263
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 293,
      "avg_human_length": 584.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 4,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "ujTXTmXUzI",
        "similarity": 0.7283,
        "coverage": 0.5263,
        "human_length": 368,
        "human_text": "paper_topic_and_main_contributions: The authors introduce LLM4CS and conversation query rewriting and response aggregation framework, which improves the performance of conversational information retrieval by a wide margin. \nThe authors perform an extensive and systematic evaluation on three widely used conversational search benchmarks, CAsT-19, CAsT-20, and CAsT-21, demonstrating how their methodology consistently outperforms previous SOTA by a wide margin,  The method is simple yet effective and has an LLM that generates Rewriting (REW), Rewriting-Then-Response (RTR), and Rewriting And-Response (RAR) which when paired with an ANCE-based retriever, leads to impressive improvements in retrieval accuracy.\n\nreasons_to_accept: 1. LLM4CS works incredibly well. It outperforms all prior CQR and CDR methods on a wide variety of benchmarks and by a significant margin. \n2. LLM4CS has three methods REW RTR RAR, all of which work better than prior SOTA even before they are aggregated.\n\nreasons_to_reject: 1. There is no discussion about the impact of using LLM ( gpt3.5-turbo-16k) for all the LLM4CS prompts before retrieval. While the performance improvements are significant, it is difficult to understand the tradeoffs when it is unclear how slow the LLM4CS system is nor how well it works with a nonclosed source model. Ideally, I would like to see both information on the efficiency of LM4CS relative to other methods and some validation on how model size or scale impacts the performance of LLM4CS 2. There is no discussion on how the LLM4CS method works with any other forms of retrieval, such as traditional keyword retrieval (e.g. BM25)\n\nquestions_for_the_authors: 1. The anonymized github link is not functional please fix.\n\nmissing_references: 1. Missing MSMARCO  citation @article{Campos2016MSMA,   title={MS MARCO: A Human Generated MAchine Reading COmprehension Dataset},   author={Daniel Fernando Campos and Tri Nguyen and Mir Rosenberg and Xia Song and Jianfeng Gao and Saurabh Tiwary and Rangan Majumder and Li Deng and Bhaskar Mitra},   journal={ArXiv},   year={2016},   volume={abs/1611.09268},   url={https://api.semanticscholar.org/CorpusID:1289517} } 2.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "JHl0WR8vDK",
        "similarity": 0.7579,
        "coverage": 0.275,
        "human_length": 556,
        "human_text": "paper_topic_and_main_contributions: This paper aims to enhance the performance of conversational search by employing a Large Language Model (LLM) to provide additional information in the form of rewritten queries and responses.\nThe main contributions of this paper lie in the domain of NLP engineering experiments. The proposed framework, LLM4CS, demonstrates significant improvements in search performance compared to baseline methods. Notably, LLM4CS outperforms baselines on the CAsT-20 and CAsT-21 datasets. However, it is important to acknowledge that the generalizability and robustness of the framework are somewhat limited due to the use of only one LLM in the experiments and the relatively small dataset size.\nOverall, while the contributions are valuable, they may be more suitable for a shorter paper rather than a regular long paper.\n\nreasons_to_accept: 1. This paper presents a straightforward yet effective prompting framework, LLM4CS, which leverages a Large Language Model for conversational search. The method combines Conversational Query Rewriting (CQR) and Conversational Dense Retrieval (CDR) techniques, rewriting queries to obtain additional information and then encoding it into dense vectors. \n2. LLM4CS demonstrates remarkable performance on three CAsT datasets, particularly excelling on the CAsT-20 and CAsT-21 datasets. \n3. The paper conducts extensive experiments to analyze the impact of different prompting methods and content aggregation techniques.\n\nreasons_to_reject: 1. The novelty of the proposed framework is somewhat limited. Utilizing pre-trained LLMs to provide additional information may intuitively yield useful results, but the paper lacks novel methods or unexpected findings. It is not entirely clear how the content generated by LLMs leads to performance improvement. \n2. The experiments conducted in the paper may not sufficiently verify the generalizability of the framework. Only one LLM, gpt-3.5-turbo, was utilized. One concern is whether the LLM used (gpt-3.5-turbo) might have seen the dataset used for the experiment, potentially leading to data leakage and affecting the generalizability of the results. It is recommended to conduct experiments with at least two different open-source LLMs with varying parameter scales. Moreover, there should be an analysis of the impact of different text encoders. \n3. The proposed framework seems to be a combination of \"CQR\" and \"CDR,\" i.e. rewriting and then encoding dense vectors. The experiments center around 3 \"CQR\" methods and 3 \"CDR\" methods. However, there is no result of the baseline method that combines CQR and CDR. As far as I know, there is a hybrid version of the baseline method called CRDR, which achieves a result of 0.837 in MRR on the CAsT-19 dataset. I wonder why the results of the hybrid search of CRDR are missing in Table 2.\n\nquestions_for_the_authors: 1. Why are the results for CRDR and ConvGQR on the CAsT-21 dataset not shown in Table 2? \n2. Could you explain why LLM4CS achieves a larger lift on the CAsT-20 dataset compared to the CAsT-19 dataset? \n3. How does the framework proposed in this paper differ from other frameworks that utilize prompts to generate and encode content using LLMs for various tasks?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "XbfIs95COf",
        "similarity": 0.7719,
        "coverage": 0.2222,
        "human_length": 830,
        "human_text": "paper_topic_and_main_contributions: The main contribution of this paper lies in enhancing the effectiveness of Conversational Search through the use of a novel framework based on Language Model Leveraging (LLM). The framework is designed to robustly represent user intents by generating multiple queries and hypothetical responses. Additionally, the paper introduces several approaches for prompting the Language Model Leveraging and aggregating the generated contents in experimental settings.\nThe proposed framework is evaluated using TREC CAsT benchmarks, showcasing the efficacy of the methods presented. Overall, the paper's major contribution lies in its empirical applications of employing LLM to enhance the searching capabilities in Conversational AI.\n\nreasons_to_accept: The paper explored how LLM can provide fine-grained content/knowledge (e.g., rewritten queries, hypothetical responses) and further validated the effectiveness through benchmark datasets. The author conducted a few experiments with various empirical settings for using prompts; in addition, they conducted a few simple qualitative analyses with human evaluations on LLM-generated content.\n\nreasons_to_reject: The paper's proposed methods lack fair and comprehensive comparisons. There is a notable absence of insights and takeaways concerning LLM-generated content, such as qualitative and empirical analyses or correlations between the generated content and improved effectiveness. Moreover, the paper fails to adequately justify the motivation behind using LLM to generate rewrites or responses. The absence of evidence demonstrating how the LLM-generated content outperforms previous rewriting approaches like T5QR is concerning.\nAdditionally, the approach of prompting-LM generation appears impractical for Conversational Search, and alternative strategies, such as reranking models, could be explored to improve retrieval effectiveness. For efficiency, supervised sparse retrieval methods could be investigated as well. Furthermore, the paper might benefit from considering how LLM can address more knowledge-intensive tasks like Conversational QA rather than focusing solely on Conversational Search.\nIn summary, the paper requires significant improvements in terms of comparative analysis, insightful findings regarding LLM-generated content, and justifications for its approach. Exploring alternative strategies for retrieval effectiveness and efficiency could enhance the paper's contributions to the field. Additionally, expanding the scope of application to knowledge-intensive tasks may lead to more impactful outcomes.\n\nquestions_for_the_authors: [Sec. 2] It is essential to clarify the distinction between \"Generative IR\" and \"Text generation for IR\" in the paper. I recommend referencing \"Gen-IR at SIGIR'23\" [https://arxiv.org/pdf/2306.02887.pdf] for more detailed insights on this particular research topic.\n[Sec. 3.3.1] The paper mentions that the RAR method produces \"M\" hypothetical responses. However, it does not elaborate on how the response vectors (r_11, r12, ..r1M) are aggregated into a single new vector \"s.\" Providing further explanation in this section would enhance the clarity of the method.\n[Sec. 4.5] The comparison between RAR and RTR methods is limited since the RAR experiment seems to produce only one hypothetical response. To make the conclusion more robust, I suggest conducting experiments with the following settings: (i) N=1 vs. N=5(REW) and (ii) N=5, M=1(RTR) vs. N=5, M=5.\n[Sec. 5] It is crucial to outline the experimental setup for T5QR, especially regarding its consideration of canonical responses. To ensure fairness, T5QR should be evaluated while taking into account responses in previous turns.\n[Overall] Despite the declared limitation on computational costs and efficiency, it would be insightful to provide information about model parameter sizes or approximate numbers (e.g., times, FLOPs, model size, etc.). This information would help readers understand the trade-offs between different methods.\n[Overall] To strengthen the paper's contributions, it would be valuable to clarify what the prompting-LLM approach can achieve compared to previous fine-tuned methods. Highlighting the advantages and disadvantages of this approach would offer a better understanding of its practical implications and potential in the field.\n\nmissing_references: Conversational query reformulation (CQR)  \u2022 [Can You Unpack That? Learning to Rewrite Questions-in-Context] (Elgohary et al., EMNLP-IJCNLP  2019)   \u2022 [Query and Answer Expansion from Conversation History.] ( Yang et al., TREC 2019) Conversational Search  \u2022 [Open-Retrieval Conversational Question Answering] (Qu et al., SIGIR 2020)  \u2022 [Making Information Seeking Easier: An Improved Pipeline for Conversational Search] (Kumar &  Callan, EMNLP 2020)  \u2022 [Improving Conversational Passage Re-ranking with View Ensemble] (Ju et al., SIGIR 2023)\n\ntypos_grammar_style_and_presentation_improvements: [Section 3.3] To enhance clarity and ease of understanding, I recommend rearranging the three aggregation methods as follows: \"Max\" (3.3.1) should come first, followed by \"Mean\" (3.3.3), and finally \"Self-consistency\" (3.3.2). This arrangement in an easy-to-hard order might improve the fluency of the section.\n[Table 2; Sec. 4.2] It is crucial to ensure consistency in reporting. Many previous works mentioned in the related works section are not reflected in Table 2. To provide a comprehensive comparison, all relevant previous works should be included in Table 2, aligning with the scope and focus of the paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "ujTXTmXUzI",
        "length": 368,
        "human_text": "paper_topic_and_main_contributions: The authors introduce LLM4CS and conversation query rewriting and response aggregation framework, which improves the performance of conversational information retrieval by a wide margin. \nThe authors perform an extensive and systematic evaluation on three widely used conversational search benchmarks, CAsT-19, CAsT-20, and CAsT-21, demonstrating how their methodology consistently outperforms previous SOTA by a wide margin,  The method is simple yet effective and has an LLM that generates Rewriting (REW), Rewriting-Then-Response (RTR), and Rewriting And-Response (RAR) which when paired with an ANCE-based retriever, leads to impressive improvements in retrieval accuracy.\n\nreasons_to_accept: 1. LLM4CS works incredibly well. It outperforms all prior CQR and CDR methods on a wide variety of benchmarks and by a significant margin. \n2. LLM4CS has three methods REW RTR RAR, all of which work better than prior SOTA even before they are aggregated.\n\nreasons_to_reject: 1. There is no discussion about the impact of using LLM ( gpt3.5-turbo-16k) for all the LLM4CS prompts before retrieval. While the performance improvements are significant, it is difficult to understand the tradeoffs when it is unclear how slow the LLM4CS system is nor how well it works with a nonclosed source model. Ideally, I would like to see both information on the efficiency of LM4CS relative to other methods and some validation on how model size or scale impacts the performance of LLM4CS 2. There is no discussion on how the LLM4CS method works with any other forms of retrieval, such as traditional keyword retrieval (e.g. BM25)\n\nquestions_for_the_authors: 1. The anonymized github link is not functional please fix.\n\nmissing_references: 1. Missing MSMARCO  citation @article{Campos2016MSMA,   title={MS MARCO: A Human Generated MAchine Reading COmprehension Dataset},   author={Daniel Fernando Campos and Tri Nguyen and Mir Rosenberg and Xia Song and Jianfeng Gao and Saurabh Tiwary and Rangan Majumder and Li Deng and Bhaskar Mitra},   journal={ArXiv},   year={2016},   volume={abs/1611.09268},   url={https://api.semanticscholar.org/CorpusID:1289517} } 2.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "JHl0WR8vDK",
        "length": 556,
        "human_text": "paper_topic_and_main_contributions: This paper aims to enhance the performance of conversational search by employing a Large Language Model (LLM) to provide additional information in the form of rewritten queries and responses.\nThe main contributions of this paper lie in the domain of NLP engineering experiments. The proposed framework, LLM4CS, demonstrates significant improvements in search performance compared to baseline methods. Notably, LLM4CS outperforms baselines on the CAsT-20 and CAsT-21 datasets. However, it is important to acknowledge that the generalizability and robustness of the framework are somewhat limited due to the use of only one LLM in the experiments and the relatively small dataset size.\nOverall, while the contributions are valuable, they may be more suitable for a shorter paper rather than a regular long paper.\n\nreasons_to_accept: 1. This paper presents a straightforward yet effective prompting framework, LLM4CS, which leverages a Large Language Model for conversational search. The method combines Conversational Query Rewriting (CQR) and Conversational Dense Retrieval (CDR) techniques, rewriting queries to obtain additional information and then encoding it into dense vectors. \n2. LLM4CS demonstrates remarkable performance on three CAsT datasets, particularly excelling on the CAsT-20 and CAsT-21 datasets. \n3. The paper conducts extensive experiments to analyze the impact of different prompting methods and content aggregation techniques.\n\nreasons_to_reject: 1. The novelty of the proposed framework is somewhat limited. Utilizing pre-trained LLMs to provide additional information may intuitively yield useful results, but the paper lacks novel methods or unexpected findings. It is not entirely clear how the content generated by LLMs leads to performance improvement. \n2. The experiments conducted in the paper may not sufficiently verify the generalizability of the framework. Only one LLM, gpt-3.5-turbo, was utilized. One concern is whether the LLM used (gpt-3.5-turbo) might have seen the dataset used for the experiment, potentially leading to data leakage and affecting the generalizability of the results. It is recommended to conduct experiments with at least two different open-source LLMs with varying parameter scales. Moreover, there should be an analysis of the impact of different text encoders. \n3. The proposed framework seems to be a combination of \"CQR\" and \"CDR,\" i.e. rewriting and then encoding dense vectors. The experiments center around 3 \"CQR\" methods and 3 \"CDR\" methods. However, there is no result of the baseline method that combines CQR and CDR. As far as I know, there is a hybrid version of the baseline method called CRDR, which achieves a result of 0.837 in MRR on the CAsT-19 dataset. I wonder why the results of the hybrid search of CRDR are missing in Table 2.\n\nquestions_for_the_authors: 1. Why are the results for CRDR and ConvGQR on the CAsT-21 dataset not shown in Table 2? \n2. Could you explain why LLM4CS achieves a larger lift on the CAsT-20 dataset compared to the CAsT-19 dataset? \n3. How does the framework proposed in this paper differ from other frameworks that utilize prompts to generate and encode content using LLMs for various tasks?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "XbfIs95COf",
        "length": 830,
        "human_text": "paper_topic_and_main_contributions: The main contribution of this paper lies in enhancing the effectiveness of Conversational Search through the use of a novel framework based on Language Model Leveraging (LLM). The framework is designed to robustly represent user intents by generating multiple queries and hypothetical responses. Additionally, the paper introduces several approaches for prompting the Language Model Leveraging and aggregating the generated contents in experimental settings.\nThe proposed framework is evaluated using TREC CAsT benchmarks, showcasing the efficacy of the methods presented. Overall, the paper's major contribution lies in its empirical applications of employing LLM to enhance the searching capabilities in Conversational AI.\n\nreasons_to_accept: The paper explored how LLM can provide fine-grained content/knowledge (e.g., rewritten queries, hypothetical responses) and further validated the effectiveness through benchmark datasets. The author conducted a few experiments with various empirical settings for using prompts; in addition, they conducted a few simple qualitative analyses with human evaluations on LLM-generated content.\n\nreasons_to_reject: The paper's proposed methods lack fair and comprehensive comparisons. There is a notable absence of insights and takeaways concerning LLM-generated content, such as qualitative and empirical analyses or correlations between the generated content and improved effectiveness. Moreover, the paper fails to adequately justify the motivation behind using LLM to generate rewrites or responses. The absence of evidence demonstrating how the LLM-generated content outperforms previous rewriting approaches like T5QR is concerning.\nAdditionally, the approach of prompting-LM generation appears impractical for Conversational Search, and alternative strategies, such as reranking models, could be explored to improve retrieval effectiveness. For efficiency, supervised sparse retrieval methods could be investigated as well. Furthermore, the paper might benefit from considering how LLM can address more knowledge-intensive tasks like Conversational QA rather than focusing solely on Conversational Search.\nIn summary, the paper requires significant improvements in terms of comparative analysis, insightful findings regarding LLM-generated content, and justifications for its approach. Exploring alternative strategies for retrieval effectiveness and efficiency could enhance the paper's contributions to the field. Additionally, expanding the scope of application to knowledge-intensive tasks may lead to more impactful outcomes.\n\nquestions_for_the_authors: [Sec. 2] It is essential to clarify the distinction between \"Generative IR\" and \"Text generation for IR\" in the paper. I recommend referencing \"Gen-IR at SIGIR'23\" [https://arxiv.org/pdf/2306.02887.pdf] for more detailed insights on this particular research topic.\n[Sec. 3.3.1] The paper mentions that the RAR method produces \"M\" hypothetical responses. However, it does not elaborate on how the response vectors (r_11, r12, ..r1M) are aggregated into a single new vector \"s.\" Providing further explanation in this section would enhance the clarity of the method.\n[Sec. 4.5] The comparison between RAR and RTR methods is limited since the RAR experiment seems to produce only one hypothetical response. To make the conclusion more robust, I suggest conducting experiments with the following settings: (i) N=1 vs. N=5(REW) and (ii) N=5, M=1(RTR) vs. N=5, M=5.\n[Sec. 5] It is crucial to outline the experimental setup for T5QR, especially regarding its consideration of canonical responses. To ensure fairness, T5QR should be evaluated while taking into account responses in previous turns.\n[Overall] Despite the declared limitation on computational costs and efficiency, it would be insightful to provide information about model parameter sizes or approximate numbers (e.g., times, FLOPs, model size, etc.). This information would help readers understand the trade-offs between different methods.\n[Overall] To strengthen the paper's contributions, it would be valuable to clarify what the prompting-LLM approach can achieve compared to previous fine-tuned methods. Highlighting the advantages and disadvantages of this approach would offer a better understanding of its practical implications and potential in the field.\n\nmissing_references: Conversational query reformulation (CQR)  \u2022 [Can You Unpack That? Learning to Rewrite Questions-in-Context] (Elgohary et al., EMNLP-IJCNLP  2019)   \u2022 [Query and Answer Expansion from Conversation History.] ( Yang et al., TREC 2019) Conversational Search  \u2022 [Open-Retrieval Conversational Question Answering] (Qu et al., SIGIR 2020)  \u2022 [Making Information Seeking Easier: An Improved Pipeline for Conversational Search] (Kumar &  Callan, EMNLP 2020)  \u2022 [Improving Conversational Passage Re-ranking with View Ensemble] (Ju et al., SIGIR 2023)\n\ntypos_grammar_style_and_presentation_improvements: [Section 3.3] To enhance clarity and ease of understanding, I recommend rearranging the three aggregation methods as follows: \"Max\" (3.3.1) should come first, followed by \"Mean\" (3.3.3), and finally \"Self-consistency\" (3.3.2). This arrangement in an easy-to-hard order might improve the fluency of the section.\n[Table 2; Sec. 4.2] It is crucial to ensure consistency in reporting. Many previous works mentioned in the related works section are not reflected in Table 2. To provide a comprehensive comparison, all relevant previous works should be included in Table 2, aligning with the scope and focus of the paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "105_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_105_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7392666666666668,
      "max_similarity": 0.7596,
      "avg_coverage": 0.27463333333333334,
      "max_coverage": 0.4211
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 200,
      "avg_human_length": 584.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 3,
      "suggestions_count": 1
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "ujTXTmXUzI",
        "similarity": 0.7056,
        "coverage": 0.4211,
        "human_length": 368,
        "human_text": "paper_topic_and_main_contributions: The authors introduce LLM4CS and conversation query rewriting and response aggregation framework, which improves the performance of conversational information retrieval by a wide margin. \nThe authors perform an extensive and systematic evaluation on three widely used conversational search benchmarks, CAsT-19, CAsT-20, and CAsT-21, demonstrating how their methodology consistently outperforms previous SOTA by a wide margin,  The method is simple yet effective and has an LLM that generates Rewriting (REW), Rewriting-Then-Response (RTR), and Rewriting And-Response (RAR) which when paired with an ANCE-based retriever, leads to impressive improvements in retrieval accuracy.\n\nreasons_to_accept: 1. LLM4CS works incredibly well. It outperforms all prior CQR and CDR methods on a wide variety of benchmarks and by a significant margin. \n2. LLM4CS has three methods REW RTR RAR, all of which work better than prior SOTA even before they are aggregated.\n\nreasons_to_reject: 1. There is no discussion about the impact of using LLM ( gpt3.5-turbo-16k) for all the LLM4CS prompts before retrieval. While the performance improvements are significant, it is difficult to understand the tradeoffs when it is unclear how slow the LLM4CS system is nor how well it works with a nonclosed source model. Ideally, I would like to see both information on the efficiency of LM4CS relative to other methods and some validation on how model size or scale impacts the performance of LLM4CS 2. There is no discussion on how the LLM4CS method works with any other forms of retrieval, such as traditional keyword retrieval (e.g. BM25)\n\nquestions_for_the_authors: 1. The anonymized github link is not functional please fix.\n\nmissing_references: 1. Missing MSMARCO  citation @article{Campos2016MSMA,   title={MS MARCO: A Human Generated MAchine Reading COmprehension Dataset},   author={Daniel Fernando Campos and Tri Nguyen and Mir Rosenberg and Xia Song and Jianfeng Gao and Saurabh Tiwary and Rangan Majumder and Li Deng and Bhaskar Mitra},   journal={ArXiv},   year={2016},   volume={abs/1611.09268},   url={https://api.semanticscholar.org/CorpusID:1289517} } 2.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "JHl0WR8vDK",
        "similarity": 0.7526,
        "coverage": 0.225,
        "human_length": 556,
        "human_text": "paper_topic_and_main_contributions: This paper aims to enhance the performance of conversational search by employing a Large Language Model (LLM) to provide additional information in the form of rewritten queries and responses.\nThe main contributions of this paper lie in the domain of NLP engineering experiments. The proposed framework, LLM4CS, demonstrates significant improvements in search performance compared to baseline methods. Notably, LLM4CS outperforms baselines on the CAsT-20 and CAsT-21 datasets. However, it is important to acknowledge that the generalizability and robustness of the framework are somewhat limited due to the use of only one LLM in the experiments and the relatively small dataset size.\nOverall, while the contributions are valuable, they may be more suitable for a shorter paper rather than a regular long paper.\n\nreasons_to_accept: 1. This paper presents a straightforward yet effective prompting framework, LLM4CS, which leverages a Large Language Model for conversational search. The method combines Conversational Query Rewriting (CQR) and Conversational Dense Retrieval (CDR) techniques, rewriting queries to obtain additional information and then encoding it into dense vectors. \n2. LLM4CS demonstrates remarkable performance on three CAsT datasets, particularly excelling on the CAsT-20 and CAsT-21 datasets. \n3. The paper conducts extensive experiments to analyze the impact of different prompting methods and content aggregation techniques.\n\nreasons_to_reject: 1. The novelty of the proposed framework is somewhat limited. Utilizing pre-trained LLMs to provide additional information may intuitively yield useful results, but the paper lacks novel methods or unexpected findings. It is not entirely clear how the content generated by LLMs leads to performance improvement. \n2. The experiments conducted in the paper may not sufficiently verify the generalizability of the framework. Only one LLM, gpt-3.5-turbo, was utilized. One concern is whether the LLM used (gpt-3.5-turbo) might have seen the dataset used for the experiment, potentially leading to data leakage and affecting the generalizability of the results. It is recommended to conduct experiments with at least two different open-source LLMs with varying parameter scales. Moreover, there should be an analysis of the impact of different text encoders. \n3. The proposed framework seems to be a combination of \"CQR\" and \"CDR,\" i.e. rewriting and then encoding dense vectors. The experiments center around 3 \"CQR\" methods and 3 \"CDR\" methods. However, there is no result of the baseline method that combines CQR and CDR. As far as I know, there is a hybrid version of the baseline method called CRDR, which achieves a result of 0.837 in MRR on the CAsT-19 dataset. I wonder why the results of the hybrid search of CRDR are missing in Table 2.\n\nquestions_for_the_authors: 1. Why are the results for CRDR and ConvGQR on the CAsT-21 dataset not shown in Table 2? \n2. Could you explain why LLM4CS achieves a larger lift on the CAsT-20 dataset compared to the CAsT-19 dataset? \n3. How does the framework proposed in this paper differ from other frameworks that utilize prompts to generate and encode content using LLMs for various tasks?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "XbfIs95COf",
        "similarity": 0.7596,
        "coverage": 0.1778,
        "human_length": 830,
        "human_text": "paper_topic_and_main_contributions: The main contribution of this paper lies in enhancing the effectiveness of Conversational Search through the use of a novel framework based on Language Model Leveraging (LLM). The framework is designed to robustly represent user intents by generating multiple queries and hypothetical responses. Additionally, the paper introduces several approaches for prompting the Language Model Leveraging and aggregating the generated contents in experimental settings.\nThe proposed framework is evaluated using TREC CAsT benchmarks, showcasing the efficacy of the methods presented. Overall, the paper's major contribution lies in its empirical applications of employing LLM to enhance the searching capabilities in Conversational AI.\n\nreasons_to_accept: The paper explored how LLM can provide fine-grained content/knowledge (e.g., rewritten queries, hypothetical responses) and further validated the effectiveness through benchmark datasets. The author conducted a few experiments with various empirical settings for using prompts; in addition, they conducted a few simple qualitative analyses with human evaluations on LLM-generated content.\n\nreasons_to_reject: The paper's proposed methods lack fair and comprehensive comparisons. There is a notable absence of insights and takeaways concerning LLM-generated content, such as qualitative and empirical analyses or correlations between the generated content and improved effectiveness. Moreover, the paper fails to adequately justify the motivation behind using LLM to generate rewrites or responses. The absence of evidence demonstrating how the LLM-generated content outperforms previous rewriting approaches like T5QR is concerning.\nAdditionally, the approach of prompting-LM generation appears impractical for Conversational Search, and alternative strategies, such as reranking models, could be explored to improve retrieval effectiveness. For efficiency, supervised sparse retrieval methods could be investigated as well. Furthermore, the paper might benefit from considering how LLM can address more knowledge-intensive tasks like Conversational QA rather than focusing solely on Conversational Search.\nIn summary, the paper requires significant improvements in terms of comparative analysis, insightful findings regarding LLM-generated content, and justifications for its approach. Exploring alternative strategies for retrieval effectiveness and efficiency could enhance the paper's contributions to the field. Additionally, expanding the scope of application to knowledge-intensive tasks may lead to more impactful outcomes.\n\nquestions_for_the_authors: [Sec. 2] It is essential to clarify the distinction between \"Generative IR\" and \"Text generation for IR\" in the paper. I recommend referencing \"Gen-IR at SIGIR'23\" [https://arxiv.org/pdf/2306.02887.pdf] for more detailed insights on this particular research topic.\n[Sec. 3.3.1] The paper mentions that the RAR method produces \"M\" hypothetical responses. However, it does not elaborate on how the response vectors (r_11, r12, ..r1M) are aggregated into a single new vector \"s.\" Providing further explanation in this section would enhance the clarity of the method.\n[Sec. 4.5] The comparison between RAR and RTR methods is limited since the RAR experiment seems to produce only one hypothetical response. To make the conclusion more robust, I suggest conducting experiments with the following settings: (i) N=1 vs. N=5(REW) and (ii) N=5, M=1(RTR) vs. N=5, M=5.\n[Sec. 5] It is crucial to outline the experimental setup for T5QR, especially regarding its consideration of canonical responses. To ensure fairness, T5QR should be evaluated while taking into account responses in previous turns.\n[Overall] Despite the declared limitation on computational costs and efficiency, it would be insightful to provide information about model parameter sizes or approximate numbers (e.g., times, FLOPs, model size, etc.). This information would help readers understand the trade-offs between different methods.\n[Overall] To strengthen the paper's contributions, it would be valuable to clarify what the prompting-LLM approach can achieve compared to previous fine-tuned methods. Highlighting the advantages and disadvantages of this approach would offer a better understanding of its practical implications and potential in the field.\n\nmissing_references: Conversational query reformulation (CQR)  \u2022 [Can You Unpack That? Learning to Rewrite Questions-in-Context] (Elgohary et al., EMNLP-IJCNLP  2019)   \u2022 [Query and Answer Expansion from Conversation History.] ( Yang et al., TREC 2019) Conversational Search  \u2022 [Open-Retrieval Conversational Question Answering] (Qu et al., SIGIR 2020)  \u2022 [Making Information Seeking Easier: An Improved Pipeline for Conversational Search] (Kumar &  Callan, EMNLP 2020)  \u2022 [Improving Conversational Passage Re-ranking with View Ensemble] (Ju et al., SIGIR 2023)\n\ntypos_grammar_style_and_presentation_improvements: [Section 3.3] To enhance clarity and ease of understanding, I recommend rearranging the three aggregation methods as follows: \"Max\" (3.3.1) should come first, followed by \"Mean\" (3.3.3), and finally \"Self-consistency\" (3.3.2). This arrangement in an easy-to-hard order might improve the fluency of the section.\n[Table 2; Sec. 4.2] It is crucial to ensure consistency in reporting. Many previous works mentioned in the related works section are not reflected in Table 2. To provide a comprehensive comparison, all relevant previous works should be included in Table 2, aligning with the scope and focus of the paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "ujTXTmXUzI",
        "length": 368,
        "human_text": "paper_topic_and_main_contributions: The authors introduce LLM4CS and conversation query rewriting and response aggregation framework, which improves the performance of conversational information retrieval by a wide margin. \nThe authors perform an extensive and systematic evaluation on three widely used conversational search benchmarks, CAsT-19, CAsT-20, and CAsT-21, demonstrating how their methodology consistently outperforms previous SOTA by a wide margin,  The method is simple yet effective and has an LLM that generates Rewriting (REW), Rewriting-Then-Response (RTR), and Rewriting And-Response (RAR) which when paired with an ANCE-based retriever, leads to impressive improvements in retrieval accuracy.\n\nreasons_to_accept: 1. LLM4CS works incredibly well. It outperforms all prior CQR and CDR methods on a wide variety of benchmarks and by a significant margin. \n2. LLM4CS has three methods REW RTR RAR, all of which work better than prior SOTA even before they are aggregated.\n\nreasons_to_reject: 1. There is no discussion about the impact of using LLM ( gpt3.5-turbo-16k) for all the LLM4CS prompts before retrieval. While the performance improvements are significant, it is difficult to understand the tradeoffs when it is unclear how slow the LLM4CS system is nor how well it works with a nonclosed source model. Ideally, I would like to see both information on the efficiency of LM4CS relative to other methods and some validation on how model size or scale impacts the performance of LLM4CS 2. There is no discussion on how the LLM4CS method works with any other forms of retrieval, such as traditional keyword retrieval (e.g. BM25)\n\nquestions_for_the_authors: 1. The anonymized github link is not functional please fix.\n\nmissing_references: 1. Missing MSMARCO  citation @article{Campos2016MSMA,   title={MS MARCO: A Human Generated MAchine Reading COmprehension Dataset},   author={Daniel Fernando Campos and Tri Nguyen and Mir Rosenberg and Xia Song and Jianfeng Gao and Saurabh Tiwary and Rangan Majumder and Li Deng and Bhaskar Mitra},   journal={ArXiv},   year={2016},   volume={abs/1611.09268},   url={https://api.semanticscholar.org/CorpusID:1289517} } 2.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "JHl0WR8vDK",
        "length": 556,
        "human_text": "paper_topic_and_main_contributions: This paper aims to enhance the performance of conversational search by employing a Large Language Model (LLM) to provide additional information in the form of rewritten queries and responses.\nThe main contributions of this paper lie in the domain of NLP engineering experiments. The proposed framework, LLM4CS, demonstrates significant improvements in search performance compared to baseline methods. Notably, LLM4CS outperforms baselines on the CAsT-20 and CAsT-21 datasets. However, it is important to acknowledge that the generalizability and robustness of the framework are somewhat limited due to the use of only one LLM in the experiments and the relatively small dataset size.\nOverall, while the contributions are valuable, they may be more suitable for a shorter paper rather than a regular long paper.\n\nreasons_to_accept: 1. This paper presents a straightforward yet effective prompting framework, LLM4CS, which leverages a Large Language Model for conversational search. The method combines Conversational Query Rewriting (CQR) and Conversational Dense Retrieval (CDR) techniques, rewriting queries to obtain additional information and then encoding it into dense vectors. \n2. LLM4CS demonstrates remarkable performance on three CAsT datasets, particularly excelling on the CAsT-20 and CAsT-21 datasets. \n3. The paper conducts extensive experiments to analyze the impact of different prompting methods and content aggregation techniques.\n\nreasons_to_reject: 1. The novelty of the proposed framework is somewhat limited. Utilizing pre-trained LLMs to provide additional information may intuitively yield useful results, but the paper lacks novel methods or unexpected findings. It is not entirely clear how the content generated by LLMs leads to performance improvement. \n2. The experiments conducted in the paper may not sufficiently verify the generalizability of the framework. Only one LLM, gpt-3.5-turbo, was utilized. One concern is whether the LLM used (gpt-3.5-turbo) might have seen the dataset used for the experiment, potentially leading to data leakage and affecting the generalizability of the results. It is recommended to conduct experiments with at least two different open-source LLMs with varying parameter scales. Moreover, there should be an analysis of the impact of different text encoders. \n3. The proposed framework seems to be a combination of \"CQR\" and \"CDR,\" i.e. rewriting and then encoding dense vectors. The experiments center around 3 \"CQR\" methods and 3 \"CDR\" methods. However, there is no result of the baseline method that combines CQR and CDR. As far as I know, there is a hybrid version of the baseline method called CRDR, which achieves a result of 0.837 in MRR on the CAsT-19 dataset. I wonder why the results of the hybrid search of CRDR are missing in Table 2.\n\nquestions_for_the_authors: 1. Why are the results for CRDR and ConvGQR on the CAsT-21 dataset not shown in Table 2? \n2. Could you explain why LLM4CS achieves a larger lift on the CAsT-20 dataset compared to the CAsT-19 dataset? \n3. How does the framework proposed in this paper differ from other frameworks that utilize prompts to generate and encode content using LLMs for various tasks?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "XbfIs95COf",
        "length": 830,
        "human_text": "paper_topic_and_main_contributions: The main contribution of this paper lies in enhancing the effectiveness of Conversational Search through the use of a novel framework based on Language Model Leveraging (LLM). The framework is designed to robustly represent user intents by generating multiple queries and hypothetical responses. Additionally, the paper introduces several approaches for prompting the Language Model Leveraging and aggregating the generated contents in experimental settings.\nThe proposed framework is evaluated using TREC CAsT benchmarks, showcasing the efficacy of the methods presented. Overall, the paper's major contribution lies in its empirical applications of employing LLM to enhance the searching capabilities in Conversational AI.\n\nreasons_to_accept: The paper explored how LLM can provide fine-grained content/knowledge (e.g., rewritten queries, hypothetical responses) and further validated the effectiveness through benchmark datasets. The author conducted a few experiments with various empirical settings for using prompts; in addition, they conducted a few simple qualitative analyses with human evaluations on LLM-generated content.\n\nreasons_to_reject: The paper's proposed methods lack fair and comprehensive comparisons. There is a notable absence of insights and takeaways concerning LLM-generated content, such as qualitative and empirical analyses or correlations between the generated content and improved effectiveness. Moreover, the paper fails to adequately justify the motivation behind using LLM to generate rewrites or responses. The absence of evidence demonstrating how the LLM-generated content outperforms previous rewriting approaches like T5QR is concerning.\nAdditionally, the approach of prompting-LM generation appears impractical for Conversational Search, and alternative strategies, such as reranking models, could be explored to improve retrieval effectiveness. For efficiency, supervised sparse retrieval methods could be investigated as well. Furthermore, the paper might benefit from considering how LLM can address more knowledge-intensive tasks like Conversational QA rather than focusing solely on Conversational Search.\nIn summary, the paper requires significant improvements in terms of comparative analysis, insightful findings regarding LLM-generated content, and justifications for its approach. Exploring alternative strategies for retrieval effectiveness and efficiency could enhance the paper's contributions to the field. Additionally, expanding the scope of application to knowledge-intensive tasks may lead to more impactful outcomes.\n\nquestions_for_the_authors: [Sec. 2] It is essential to clarify the distinction between \"Generative IR\" and \"Text generation for IR\" in the paper. I recommend referencing \"Gen-IR at SIGIR'23\" [https://arxiv.org/pdf/2306.02887.pdf] for more detailed insights on this particular research topic.\n[Sec. 3.3.1] The paper mentions that the RAR method produces \"M\" hypothetical responses. However, it does not elaborate on how the response vectors (r_11, r12, ..r1M) are aggregated into a single new vector \"s.\" Providing further explanation in this section would enhance the clarity of the method.\n[Sec. 4.5] The comparison between RAR and RTR methods is limited since the RAR experiment seems to produce only one hypothetical response. To make the conclusion more robust, I suggest conducting experiments with the following settings: (i) N=1 vs. N=5(REW) and (ii) N=5, M=1(RTR) vs. N=5, M=5.\n[Sec. 5] It is crucial to outline the experimental setup for T5QR, especially regarding its consideration of canonical responses. To ensure fairness, T5QR should be evaluated while taking into account responses in previous turns.\n[Overall] Despite the declared limitation on computational costs and efficiency, it would be insightful to provide information about model parameter sizes or approximate numbers (e.g., times, FLOPs, model size, etc.). This information would help readers understand the trade-offs between different methods.\n[Overall] To strengthen the paper's contributions, it would be valuable to clarify what the prompting-LLM approach can achieve compared to previous fine-tuned methods. Highlighting the advantages and disadvantages of this approach would offer a better understanding of its practical implications and potential in the field.\n\nmissing_references: Conversational query reformulation (CQR)  \u2022 [Can You Unpack That? Learning to Rewrite Questions-in-Context] (Elgohary et al., EMNLP-IJCNLP  2019)   \u2022 [Query and Answer Expansion from Conversation History.] ( Yang et al., TREC 2019) Conversational Search  \u2022 [Open-Retrieval Conversational Question Answering] (Qu et al., SIGIR 2020)  \u2022 [Making Information Seeking Easier: An Improved Pipeline for Conversational Search] (Kumar &  Callan, EMNLP 2020)  \u2022 [Improving Conversational Passage Re-ranking with View Ensemble] (Ju et al., SIGIR 2023)\n\ntypos_grammar_style_and_presentation_improvements: [Section 3.3] To enhance clarity and ease of understanding, I recommend rearranging the three aggregation methods as follows: \"Max\" (3.3.1) should come first, followed by \"Mean\" (3.3.3), and finally \"Self-consistency\" (3.3.2). This arrangement in an easy-to-hard order might improve the fluency of the section.\n[Table 2; Sec. 4.2] It is crucial to ensure consistency in reporting. Many previous works mentioned in the related works section are not reflected in Table 2. To provide a comprehensive comparison, all relevant previous works should be included in Table 2, aligning with the scope and focus of the paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "30_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_30_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7077666666666667,
      "max_similarity": 0.7158,
      "avg_coverage": 0.7631666666666667,
      "max_coverage": 0.8333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 866,
      "avg_human_length": 305.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 8,
      "weaknesses_count": 12,
      "suggestions_count": 15
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "N890jVPGLf",
        "similarity": 0.7125,
        "coverage": 0.8333,
        "human_length": 315,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a recursive visual explanation method for step-by-step vision-language explanation generation. The proposed method appears to build upon the powerful BLIP-2 model. Experiments on three datasets demonstrate the effectiveness of the proposed method.\n\nreasons_to_accept: - The paper proposes a multi-step explanation generation method, and the experimental results indicate that the model's performance improves with more steps.\n- The paper proposes a few-shot self-training method, enabling training with generated pseudo-explanations.\n\nreasons_to_reject: - Lack of detailed description of the first contribution. 5% of the human-annotated explanations are insufficient to understand the significance of this aspect. The paper should provide more comprehensive information regarding the comparison methods, experimental settings, and detailed analyses to establish a clear contribution.\n- It seems that the novelty of the proposed recursive method is relatively incremental, as the iterative computation of visual features conditioned on text input is a common technique in the multimodal domain, as seen in previous work (e.g., Paper [1]). The paper should address how the proposed method extends beyond existing techniques to justify its contribution.\n- The paper should include a thorough analysis of the significant performance gap observed between the proposed method and state-of-the-art methods, which are essential for a comprehensive evaluation of the proposed method's effectiveness.\nReferences:   [1] Improving one-stage visual grounding by recursive sub-query construction. ECCV, 2020.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Z5Gq2d9i43",
        "similarity": 0.7158,
        "coverage": 0.6667,
        "human_length": 293,
        "human_text": "paper_topic_and_main_contributions: This paper proposes ReVise, a method that recursively generate explanations for vision-language tasks by recomputing visual features. The method is data efficient and achieves on-par performance with current methods for VL-NLE. They also showed the effectiveness of ReVise in self-training by generating pseudo ground truth annotations for explanations and self-train on these generated explanations.\n\nreasons_to_accept: - Paper is written with clarity and easy to follow.\n- Extensive experiments to show the effectiveness of ReVise against previous methods and also its use in self-training.\n- Method is data efficient, compared to existing approaches.\n- This method also allows for few-shot self-training, further addresses the issue of scarce annotations.\n\nreasons_to_reject: - I had a hard time understanding the results because of the various metrics. While the different metrics show ReVise performs the best, it would be nice if the authors can give a better idea of what metrics to look at and how to interpret them.  - Unclear evaluation: not sure whether the evaluation is conducted on answers or explanations.\n\nquestions_for_the_authors: A. Are the reported metrics evaluated on the answer or the explanation?\nB. Have you looked into how explanation quality varies in the iteration process?\n\ntypos_grammar_style_and_presentation_improvements: - Too many tables / figures (especially in page 6, 7, and 8.) Some can be moved to appendix.\n- Some citations should use \\citet\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "dhBphAA2OD",
        "similarity": 0.695,
        "coverage": 0.7895,
        "human_length": 309,
        "human_text": "paper_topic_and_main_contributions: In this manuscript, the authors address the visual question answering problem, aiming to provide insightful explanations together with the answers. The proposed approach, namely the Recursive Visual Explanation algorithm (ReVisE), recursively generates an answer, together with an explanation, based on the encoded image. The generated answer and explanation will be used for the next step in the recursion. The motivation is to allow the model to improve the answer and explanation step by step in the recursion until it converges. The authors also proposed a few-shot self-training mechanism to train the model with low annotated explanation resources. Experimental results are shown to support the proposed method.\n\nreasons_to_accept: Recursive revising the answer and explanation is a promising direction towards explanation AI. \nThe self-training mechanism is interesting. \nThe manuscript is well organized, and the writing is easy to follow.\n\nreasons_to_reject: The convergence of the algorithm is questionable. \nThe improvement of most metrics is somewhat marginal, given times of recursion as the price.\n\nquestions_for_the_authors: My main concern is the convergence of such recursion. First, the answer and explanation generation involves a  sampling, with which $A_n = A_{n-1}$ doesn't necessarily lead to convergence. Second, why would the model tend to converge? There is no source of \"improvement\" at each step. Providing clear proof might be challenging given the complexity of LLM, but it would be better to give a persuasive motivation for the possible step-wise improvement that leads to convergence.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "N890jVPGLf",
        "length": 315,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a recursive visual explanation method for step-by-step vision-language explanation generation. The proposed method appears to build upon the powerful BLIP-2 model. Experiments on three datasets demonstrate the effectiveness of the proposed method.\n\nreasons_to_accept: - The paper proposes a multi-step explanation generation method, and the experimental results indicate that the model's performance improves with more steps.\n- The paper proposes a few-shot self-training method, enabling training with generated pseudo-explanations.\n\nreasons_to_reject: - Lack of detailed description of the first contribution. 5% of the human-annotated explanations are insufficient to understand the significance of this aspect. The paper should provide more comprehensive information regarding the comparison methods, experimental settings, and detailed analyses to establish a clear contribution.\n- It seems that the novelty of the proposed recursive method is relatively incremental, as the iterative computation of visual features conditioned on text input is a common technique in the multimodal domain, as seen in previous work (e.g., Paper [1]). The paper should address how the proposed method extends beyond existing techniques to justify its contribution.\n- The paper should include a thorough analysis of the significant performance gap observed between the proposed method and state-of-the-art methods, which are essential for a comprehensive evaluation of the proposed method's effectiveness.\nReferences:   [1] Improving one-stage visual grounding by recursive sub-query construction. ECCV, 2020.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "Z5Gq2d9i43",
        "length": 293,
        "human_text": "paper_topic_and_main_contributions: This paper proposes ReVise, a method that recursively generate explanations for vision-language tasks by recomputing visual features. The method is data efficient and achieves on-par performance with current methods for VL-NLE. They also showed the effectiveness of ReVise in self-training by generating pseudo ground truth annotations for explanations and self-train on these generated explanations.\n\nreasons_to_accept: - Paper is written with clarity and easy to follow.\n- Extensive experiments to show the effectiveness of ReVise against previous methods and also its use in self-training.\n- Method is data efficient, compared to existing approaches.\n- This method also allows for few-shot self-training, further addresses the issue of scarce annotations.\n\nreasons_to_reject: - I had a hard time understanding the results because of the various metrics. While the different metrics show ReVise performs the best, it would be nice if the authors can give a better idea of what metrics to look at and how to interpret them.  - Unclear evaluation: not sure whether the evaluation is conducted on answers or explanations.\n\nquestions_for_the_authors: A. Are the reported metrics evaluated on the answer or the explanation?\nB. Have you looked into how explanation quality varies in the iteration process?\n\ntypos_grammar_style_and_presentation_improvements: - Too many tables / figures (especially in page 6, 7, and 8.) Some can be moved to appendix.\n- Some citations should use \\citet\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "dhBphAA2OD",
        "length": 309,
        "human_text": "paper_topic_and_main_contributions: In this manuscript, the authors address the visual question answering problem, aiming to provide insightful explanations together with the answers. The proposed approach, namely the Recursive Visual Explanation algorithm (ReVisE), recursively generates an answer, together with an explanation, based on the encoded image. The generated answer and explanation will be used for the next step in the recursion. The motivation is to allow the model to improve the answer and explanation step by step in the recursion until it converges. The authors also proposed a few-shot self-training mechanism to train the model with low annotated explanation resources. Experimental results are shown to support the proposed method.\n\nreasons_to_accept: Recursive revising the answer and explanation is a promising direction towards explanation AI. \nThe self-training mechanism is interesting. \nThe manuscript is well organized, and the writing is easy to follow.\n\nreasons_to_reject: The convergence of the algorithm is questionable. \nThe improvement of most metrics is somewhat marginal, given times of recursion as the price.\n\nquestions_for_the_authors: My main concern is the convergence of such recursion. First, the answer and explanation generation involves a  sampling, with which $A_n = A_{n-1}$ doesn't necessarily lead to convergence. Second, why would the model tend to converge? There is no source of \"improvement\" at each step. Providing clear proof might be challenging given the complexity of LLM, but it would be better to give a persuasive motivation for the possible step-wise improvement that leads to convergence.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "30_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_30_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7102,
      "max_similarity": 0.7165,
      "avg_coverage": 0.7826333333333334,
      "max_coverage": 0.8333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 841,
      "avg_human_length": 305.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 8,
      "weaknesses_count": 12,
      "suggestions_count": 12
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "N890jVPGLf",
        "similarity": 0.7165,
        "coverage": 0.8333,
        "human_length": 315,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a recursive visual explanation method for step-by-step vision-language explanation generation. The proposed method appears to build upon the powerful BLIP-2 model. Experiments on three datasets demonstrate the effectiveness of the proposed method.\n\nreasons_to_accept: - The paper proposes a multi-step explanation generation method, and the experimental results indicate that the model's performance improves with more steps.\n- The paper proposes a few-shot self-training method, enabling training with generated pseudo-explanations.\n\nreasons_to_reject: - Lack of detailed description of the first contribution. 5% of the human-annotated explanations are insufficient to understand the significance of this aspect. The paper should provide more comprehensive information regarding the comparison methods, experimental settings, and detailed analyses to establish a clear contribution.\n- It seems that the novelty of the proposed recursive method is relatively incremental, as the iterative computation of visual features conditioned on text input is a common technique in the multimodal domain, as seen in previous work (e.g., Paper [1]). The paper should address how the proposed method extends beyond existing techniques to justify its contribution.\n- The paper should include a thorough analysis of the significant performance gap observed between the proposed method and state-of-the-art methods, which are essential for a comprehensive evaluation of the proposed method's effectiveness.\nReferences:   [1] Improving one-stage visual grounding by recursive sub-query construction. ECCV, 2020.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Z5Gq2d9i43",
        "similarity": 0.7162,
        "coverage": 0.7778,
        "human_length": 293,
        "human_text": "paper_topic_and_main_contributions: This paper proposes ReVise, a method that recursively generate explanations for vision-language tasks by recomputing visual features. The method is data efficient and achieves on-par performance with current methods for VL-NLE. They also showed the effectiveness of ReVise in self-training by generating pseudo ground truth annotations for explanations and self-train on these generated explanations.\n\nreasons_to_accept: - Paper is written with clarity and easy to follow.\n- Extensive experiments to show the effectiveness of ReVise against previous methods and also its use in self-training.\n- Method is data efficient, compared to existing approaches.\n- This method also allows for few-shot self-training, further addresses the issue of scarce annotations.\n\nreasons_to_reject: - I had a hard time understanding the results because of the various metrics. While the different metrics show ReVise performs the best, it would be nice if the authors can give a better idea of what metrics to look at and how to interpret them.  - Unclear evaluation: not sure whether the evaluation is conducted on answers or explanations.\n\nquestions_for_the_authors: A. Are the reported metrics evaluated on the answer or the explanation?\nB. Have you looked into how explanation quality varies in the iteration process?\n\ntypos_grammar_style_and_presentation_improvements: - Too many tables / figures (especially in page 6, 7, and 8.) Some can be moved to appendix.\n- Some citations should use \\citet\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "dhBphAA2OD",
        "similarity": 0.6979,
        "coverage": 0.7368,
        "human_length": 309,
        "human_text": "paper_topic_and_main_contributions: In this manuscript, the authors address the visual question answering problem, aiming to provide insightful explanations together with the answers. The proposed approach, namely the Recursive Visual Explanation algorithm (ReVisE), recursively generates an answer, together with an explanation, based on the encoded image. The generated answer and explanation will be used for the next step in the recursion. The motivation is to allow the model to improve the answer and explanation step by step in the recursion until it converges. The authors also proposed a few-shot self-training mechanism to train the model with low annotated explanation resources. Experimental results are shown to support the proposed method.\n\nreasons_to_accept: Recursive revising the answer and explanation is a promising direction towards explanation AI. \nThe self-training mechanism is interesting. \nThe manuscript is well organized, and the writing is easy to follow.\n\nreasons_to_reject: The convergence of the algorithm is questionable. \nThe improvement of most metrics is somewhat marginal, given times of recursion as the price.\n\nquestions_for_the_authors: My main concern is the convergence of such recursion. First, the answer and explanation generation involves a  sampling, with which $A_n = A_{n-1}$ doesn't necessarily lead to convergence. Second, why would the model tend to converge? There is no source of \"improvement\" at each step. Providing clear proof might be challenging given the complexity of LLM, but it would be better to give a persuasive motivation for the possible step-wise improvement that leads to convergence.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "N890jVPGLf",
        "length": 315,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a recursive visual explanation method for step-by-step vision-language explanation generation. The proposed method appears to build upon the powerful BLIP-2 model. Experiments on three datasets demonstrate the effectiveness of the proposed method.\n\nreasons_to_accept: - The paper proposes a multi-step explanation generation method, and the experimental results indicate that the model's performance improves with more steps.\n- The paper proposes a few-shot self-training method, enabling training with generated pseudo-explanations.\n\nreasons_to_reject: - Lack of detailed description of the first contribution. 5% of the human-annotated explanations are insufficient to understand the significance of this aspect. The paper should provide more comprehensive information regarding the comparison methods, experimental settings, and detailed analyses to establish a clear contribution.\n- It seems that the novelty of the proposed recursive method is relatively incremental, as the iterative computation of visual features conditioned on text input is a common technique in the multimodal domain, as seen in previous work (e.g., Paper [1]). The paper should address how the proposed method extends beyond existing techniques to justify its contribution.\n- The paper should include a thorough analysis of the significant performance gap observed between the proposed method and state-of-the-art methods, which are essential for a comprehensive evaluation of the proposed method's effectiveness.\nReferences:   [1] Improving one-stage visual grounding by recursive sub-query construction. ECCV, 2020.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "Z5Gq2d9i43",
        "length": 293,
        "human_text": "paper_topic_and_main_contributions: This paper proposes ReVise, a method that recursively generate explanations for vision-language tasks by recomputing visual features. The method is data efficient and achieves on-par performance with current methods for VL-NLE. They also showed the effectiveness of ReVise in self-training by generating pseudo ground truth annotations for explanations and self-train on these generated explanations.\n\nreasons_to_accept: - Paper is written with clarity and easy to follow.\n- Extensive experiments to show the effectiveness of ReVise against previous methods and also its use in self-training.\n- Method is data efficient, compared to existing approaches.\n- This method also allows for few-shot self-training, further addresses the issue of scarce annotations.\n\nreasons_to_reject: - I had a hard time understanding the results because of the various metrics. While the different metrics show ReVise performs the best, it would be nice if the authors can give a better idea of what metrics to look at and how to interpret them.  - Unclear evaluation: not sure whether the evaluation is conducted on answers or explanations.\n\nquestions_for_the_authors: A. Are the reported metrics evaluated on the answer or the explanation?\nB. Have you looked into how explanation quality varies in the iteration process?\n\ntypos_grammar_style_and_presentation_improvements: - Too many tables / figures (especially in page 6, 7, and 8.) Some can be moved to appendix.\n- Some citations should use \\citet\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "dhBphAA2OD",
        "length": 309,
        "human_text": "paper_topic_and_main_contributions: In this manuscript, the authors address the visual question answering problem, aiming to provide insightful explanations together with the answers. The proposed approach, namely the Recursive Visual Explanation algorithm (ReVisE), recursively generates an answer, together with an explanation, based on the encoded image. The generated answer and explanation will be used for the next step in the recursion. The motivation is to allow the model to improve the answer and explanation step by step in the recursion until it converges. The authors also proposed a few-shot self-training mechanism to train the model with low annotated explanation resources. Experimental results are shown to support the proposed method.\n\nreasons_to_accept: Recursive revising the answer and explanation is a promising direction towards explanation AI. \nThe self-training mechanism is interesting. \nThe manuscript is well organized, and the writing is easy to follow.\n\nreasons_to_reject: The convergence of the algorithm is questionable. \nThe improvement of most metrics is somewhat marginal, given times of recursion as the price.\n\nquestions_for_the_authors: My main concern is the convergence of such recursion. First, the answer and explanation generation involves a  sampling, with which $A_n = A_{n-1}$ doesn't necessarily lead to convergence. Second, why would the model tend to converge? There is no source of \"improvement\" at each step. Providing clear proof might be challenging given the complexity of LLM, but it would be better to give a persuasive motivation for the possible step-wise improvement that leads to convergence.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "81_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_81_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.7178,
      "max_similarity": 0.7312,
      "avg_coverage": 0.727775,
      "max_coverage": 0.7778
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 612,
      "avg_human_length": 245.25
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 12,
      "suggestions_count": 15
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "LChcaDLGVm",
        "similarity": 0.726,
        "coverage": 0.7333,
        "human_length": 191,
        "human_text": "paper_topic_and_main_contributions: This paper proposes FLATS, a principled method for OOD detection based on likelihood ratio. FLATS is shown to enhance other OOD detection methods by incorporating out-distribution density estimation.\n\nreasons_to_accept: 1. Backed by theoretical analysis, this paper introduce a novel principled OOD detection method FLATS that measures OOD-ness of an input through the likelihood ratio between OOD distribution and IND distribution.\n2. Moreover, FLATS can serve as a general framework which is able to enhance other OOD detection methods by considering OOD density estimation.\n3. A new SOTA on popular benchmarks is established. And the experiments and analysis are comprehensive and convincing.\n\nreasons_to_reject: 1. The relation between this work and \"Likelihood Ratios for Out-of-Distribution Detection\" should be discussed. Likelihood ratio is used in both of these works.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "A7GhRxaWJY",
        "similarity": 0.6964,
        "coverage": 0.7778,
        "human_length": 206,
        "human_text": "paper_topic_and_main_contributions: topic: detecting out-of-distribution (OOD) instances in NLP models main contribution: they think the Sota feather-based OOD detection methods such as MAha, and KNN is suboptimal since they only estimate in-distribution density, to address this issue, they propose FLATS, which can incorporate out-distribution density.\n\nreasons_to_accept: they propose FLATS, which can incorporate out-distribution density.\n\nreasons_to_reject: 1) The rationale behind opting for RoBERTa, as opposed to other language models with varying parameters like T5-3b, 11b, llama-3b, and 7b. It is crucial to present a comprehensive comparison with these different language models. \n2) In Section 5, it is recommended to provide a short explanation for the superiority of this method.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "YLAuVQWLr8",
        "similarity": 0.7176,
        "coverage": 0.75,
        "human_length": 202,
        "human_text": "paper_topic_and_main_contributions: OOD detection is an important topic but most of the previous works are empirical. This paper demonstrates a theoretical understanding that uses the likelihood ratio between out-distribution and in-distribution to measure the extent of OOD. Based on this, the authors claim that SOTA methods are suboptimal because they only estimate the in-distribution density. The authors propose a method named FLATS, which outperform previous methods on several OOD detection benchmark.\n\nreasons_to_accept: 1. This paper is well-written and organized. \n2. This paper has contributions from both theoretical and empirical perspectives: Not only provides a theoretical understanding and analyses of previous methods but also proposes a novel method that outperforms previous methods.\n\nreasons_to_reject: 1. Experimental validation is weak, only considering Roberta as the backbone.\n\nquestions_for_the_authors: 1. Have you conducted experiments on other backbones or other than NLP classification tasks?\n\nmissing_references: no\n\ntypos_grammar_style_and_presentation_improvements: no\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "UYRIg2cONc",
        "similarity": 0.7312,
        "coverage": 0.65,
        "human_length": 382,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the issue of detecting out-of-distribution (OOD) instances within the realm of Natural Language Processing (NLP) models. The paper's primary contributions are centered around the introduction of a novel methodology designed to assess the \"OOD-ness\" of a given test case. This is achieved through the application of a likelihood ratio, which juxtaposes the probabilities associated with out-of-distribution and in-distribution contexts. The paper posits that current state-of-the-art (SOTA) methods exhibit suboptimal performance due to their exclusive focus on estimating in-distribution density. In response, the paper presents a principled solution for OOD detection rooted in the concept of a likelihood ratio, denoted as FLATS. This framework holds the potential to serve as a versatile foundational structure, effectively bolstering various other OOD detection methodologies through the incorporation of out-distribution density.\n\nreasons_to_accept: 1. The paper's strength lies in its rigorous approach to addressing out-of-distribution (OOD) detection, introducing the well-grounded feature-based likelihood ratio score substantiated by Theorem 1. The extension of FLATS to diverse feature-based OOD scores highlights its versatility and departure from existing methods, showing potential for real-world applications.\n2. The experimental results underscore its contributions, as FLATS establishes itself as the new OOD detection state-of-the-art (SOTA), surpassing benchmarks like KNN and Maha. Experiments with various Pin and Pout estimation approaches further emphasize FLATS' adaptability and robustness, significantly boosting the paper's impact on the NLP community.\n\nreasons_to_reject: 1. The method's reliance on likelihood ratio-based OOD scores, as justified by Theorem 1, warrants closer scrutiny. The application of energy-based models (EBMs) for parameterization introduces an inherent complexity, raising questions about the scalability and efficiency of the proposed framework, particularly in real-world applications.\n2. The presented comparison with the KNN and Maha approaches is indicative, but a more detailed analysis of the strengths and weaknesses of FLATS in comparison to a broader spectrum of methods, including both traditional and contemporary alternatives, would contribute to a more nuanced understanding of its efficacy.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "LChcaDLGVm",
        "length": 191,
        "human_text": "paper_topic_and_main_contributions: This paper proposes FLATS, a principled method for OOD detection based on likelihood ratio. FLATS is shown to enhance other OOD detection methods by incorporating out-distribution density estimation.\n\nreasons_to_accept: 1. Backed by theoretical analysis, this paper introduce a novel principled OOD detection method FLATS that measures OOD-ness of an input through the likelihood ratio between OOD distribution and IND distribution.\n2. Moreover, FLATS can serve as a general framework which is able to enhance other OOD detection methods by considering OOD density estimation.\n3. A new SOTA on popular benchmarks is established. And the experiments and analysis are comprehensive and convincing.\n\nreasons_to_reject: 1. The relation between this work and \"Likelihood Ratios for Out-of-Distribution Detection\" should be discussed. Likelihood ratio is used in both of these works.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "A7GhRxaWJY",
        "length": 206,
        "human_text": "paper_topic_and_main_contributions: topic: detecting out-of-distribution (OOD) instances in NLP models main contribution: they think the Sota feather-based OOD detection methods such as MAha, and KNN is suboptimal since they only estimate in-distribution density, to address this issue, they propose FLATS, which can incorporate out-distribution density.\n\nreasons_to_accept: they propose FLATS, which can incorporate out-distribution density.\n\nreasons_to_reject: 1) The rationale behind opting for RoBERTa, as opposed to other language models with varying parameters like T5-3b, 11b, llama-3b, and 7b. It is crucial to present a comprehensive comparison with these different language models. \n2) In Section 5, it is recommended to provide a short explanation for the superiority of this method.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "YLAuVQWLr8",
        "length": 202,
        "human_text": "paper_topic_and_main_contributions: OOD detection is an important topic but most of the previous works are empirical. This paper demonstrates a theoretical understanding that uses the likelihood ratio between out-distribution and in-distribution to measure the extent of OOD. Based on this, the authors claim that SOTA methods are suboptimal because they only estimate the in-distribution density. The authors propose a method named FLATS, which outperform previous methods on several OOD detection benchmark.\n\nreasons_to_accept: 1. This paper is well-written and organized. \n2. This paper has contributions from both theoretical and empirical perspectives: Not only provides a theoretical understanding and analyses of previous methods but also proposes a novel method that outperforms previous methods.\n\nreasons_to_reject: 1. Experimental validation is weak, only considering Roberta as the backbone.\n\nquestions_for_the_authors: 1. Have you conducted experiments on other backbones or other than NLP classification tasks?\n\nmissing_references: no\n\ntypos_grammar_style_and_presentation_improvements: no\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "UYRIg2cONc",
        "length": 382,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the issue of detecting out-of-distribution (OOD) instances within the realm of Natural Language Processing (NLP) models. The paper's primary contributions are centered around the introduction of a novel methodology designed to assess the \"OOD-ness\" of a given test case. This is achieved through the application of a likelihood ratio, which juxtaposes the probabilities associated with out-of-distribution and in-distribution contexts. The paper posits that current state-of-the-art (SOTA) methods exhibit suboptimal performance due to their exclusive focus on estimating in-distribution density. In response, the paper presents a principled solution for OOD detection rooted in the concept of a likelihood ratio, denoted as FLATS. This framework holds the potential to serve as a versatile foundational structure, effectively bolstering various other OOD detection methodologies through the incorporation of out-distribution density.\n\nreasons_to_accept: 1. The paper's strength lies in its rigorous approach to addressing out-of-distribution (OOD) detection, introducing the well-grounded feature-based likelihood ratio score substantiated by Theorem 1. The extension of FLATS to diverse feature-based OOD scores highlights its versatility and departure from existing methods, showing potential for real-world applications.\n2. The experimental results underscore its contributions, as FLATS establishes itself as the new OOD detection state-of-the-art (SOTA), surpassing benchmarks like KNN and Maha. Experiments with various Pin and Pout estimation approaches further emphasize FLATS' adaptability and robustness, significantly boosting the paper's impact on the NLP community.\n\nreasons_to_reject: 1. The method's reliance on likelihood ratio-based OOD scores, as justified by Theorem 1, warrants closer scrutiny. The application of energy-based models (EBMs) for parameterization introduces an inherent complexity, raising questions about the scalability and efficiency of the proposed framework, particularly in real-world applications.\n2. The presented comparison with the KNN and Maha approaches is indicative, but a more detailed analysis of the strengths and weaknesses of FLATS in comparison to a broader spectrum of methods, including both traditional and contemporary alternatives, would contribute to a more nuanced understanding of its efficacy.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "81_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_81_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.7234,
      "max_similarity": 0.7364,
      "avg_coverage": 0.727775,
      "max_coverage": 0.7778
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 581,
      "avg_human_length": 245.25
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 10,
      "suggestions_count": 13
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "LChcaDLGVm",
        "similarity": 0.7319,
        "coverage": 0.7333,
        "human_length": 191,
        "human_text": "paper_topic_and_main_contributions: This paper proposes FLATS, a principled method for OOD detection based on likelihood ratio. FLATS is shown to enhance other OOD detection methods by incorporating out-distribution density estimation.\n\nreasons_to_accept: 1. Backed by theoretical analysis, this paper introduce a novel principled OOD detection method FLATS that measures OOD-ness of an input through the likelihood ratio between OOD distribution and IND distribution.\n2. Moreover, FLATS can serve as a general framework which is able to enhance other OOD detection methods by considering OOD density estimation.\n3. A new SOTA on popular benchmarks is established. And the experiments and analysis are comprehensive and convincing.\n\nreasons_to_reject: 1. The relation between this work and \"Likelihood Ratios for Out-of-Distribution Detection\" should be discussed. Likelihood ratio is used in both of these works.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "A7GhRxaWJY",
        "similarity": 0.7059,
        "coverage": 0.7778,
        "human_length": 206,
        "human_text": "paper_topic_and_main_contributions: topic: detecting out-of-distribution (OOD) instances in NLP models main contribution: they think the Sota feather-based OOD detection methods such as MAha, and KNN is suboptimal since they only estimate in-distribution density, to address this issue, they propose FLATS, which can incorporate out-distribution density.\n\nreasons_to_accept: they propose FLATS, which can incorporate out-distribution density.\n\nreasons_to_reject: 1) The rationale behind opting for RoBERTa, as opposed to other language models with varying parameters like T5-3b, 11b, llama-3b, and 7b. It is crucial to present a comprehensive comparison with these different language models. \n2) In Section 5, it is recommended to provide a short explanation for the superiority of this method.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "YLAuVQWLr8",
        "similarity": 0.7194,
        "coverage": 0.75,
        "human_length": 202,
        "human_text": "paper_topic_and_main_contributions: OOD detection is an important topic but most of the previous works are empirical. This paper demonstrates a theoretical understanding that uses the likelihood ratio between out-distribution and in-distribution to measure the extent of OOD. Based on this, the authors claim that SOTA methods are suboptimal because they only estimate the in-distribution density. The authors propose a method named FLATS, which outperform previous methods on several OOD detection benchmark.\n\nreasons_to_accept: 1. This paper is well-written and organized. \n2. This paper has contributions from both theoretical and empirical perspectives: Not only provides a theoretical understanding and analyses of previous methods but also proposes a novel method that outperforms previous methods.\n\nreasons_to_reject: 1. Experimental validation is weak, only considering Roberta as the backbone.\n\nquestions_for_the_authors: 1. Have you conducted experiments on other backbones or other than NLP classification tasks?\n\nmissing_references: no\n\ntypos_grammar_style_and_presentation_improvements: no\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "UYRIg2cONc",
        "similarity": 0.7364,
        "coverage": 0.65,
        "human_length": 382,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the issue of detecting out-of-distribution (OOD) instances within the realm of Natural Language Processing (NLP) models. The paper's primary contributions are centered around the introduction of a novel methodology designed to assess the \"OOD-ness\" of a given test case. This is achieved through the application of a likelihood ratio, which juxtaposes the probabilities associated with out-of-distribution and in-distribution contexts. The paper posits that current state-of-the-art (SOTA) methods exhibit suboptimal performance due to their exclusive focus on estimating in-distribution density. In response, the paper presents a principled solution for OOD detection rooted in the concept of a likelihood ratio, denoted as FLATS. This framework holds the potential to serve as a versatile foundational structure, effectively bolstering various other OOD detection methodologies through the incorporation of out-distribution density.\n\nreasons_to_accept: 1. The paper's strength lies in its rigorous approach to addressing out-of-distribution (OOD) detection, introducing the well-grounded feature-based likelihood ratio score substantiated by Theorem 1. The extension of FLATS to diverse feature-based OOD scores highlights its versatility and departure from existing methods, showing potential for real-world applications.\n2. The experimental results underscore its contributions, as FLATS establishes itself as the new OOD detection state-of-the-art (SOTA), surpassing benchmarks like KNN and Maha. Experiments with various Pin and Pout estimation approaches further emphasize FLATS' adaptability and robustness, significantly boosting the paper's impact on the NLP community.\n\nreasons_to_reject: 1. The method's reliance on likelihood ratio-based OOD scores, as justified by Theorem 1, warrants closer scrutiny. The application of energy-based models (EBMs) for parameterization introduces an inherent complexity, raising questions about the scalability and efficiency of the proposed framework, particularly in real-world applications.\n2. The presented comparison with the KNN and Maha approaches is indicative, but a more detailed analysis of the strengths and weaknesses of FLATS in comparison to a broader spectrum of methods, including both traditional and contemporary alternatives, would contribute to a more nuanced understanding of its efficacy.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "LChcaDLGVm",
        "length": 191,
        "human_text": "paper_topic_and_main_contributions: This paper proposes FLATS, a principled method for OOD detection based on likelihood ratio. FLATS is shown to enhance other OOD detection methods by incorporating out-distribution density estimation.\n\nreasons_to_accept: 1. Backed by theoretical analysis, this paper introduce a novel principled OOD detection method FLATS that measures OOD-ness of an input through the likelihood ratio between OOD distribution and IND distribution.\n2. Moreover, FLATS can serve as a general framework which is able to enhance other OOD detection methods by considering OOD density estimation.\n3. A new SOTA on popular benchmarks is established. And the experiments and analysis are comprehensive and convincing.\n\nreasons_to_reject: 1. The relation between this work and \"Likelihood Ratios for Out-of-Distribution Detection\" should be discussed. Likelihood ratio is used in both of these works.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "A7GhRxaWJY",
        "length": 206,
        "human_text": "paper_topic_and_main_contributions: topic: detecting out-of-distribution (OOD) instances in NLP models main contribution: they think the Sota feather-based OOD detection methods such as MAha, and KNN is suboptimal since they only estimate in-distribution density, to address this issue, they propose FLATS, which can incorporate out-distribution density.\n\nreasons_to_accept: they propose FLATS, which can incorporate out-distribution density.\n\nreasons_to_reject: 1) The rationale behind opting for RoBERTa, as opposed to other language models with varying parameters like T5-3b, 11b, llama-3b, and 7b. It is crucial to present a comprehensive comparison with these different language models. \n2) In Section 5, it is recommended to provide a short explanation for the superiority of this method.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "YLAuVQWLr8",
        "length": 202,
        "human_text": "paper_topic_and_main_contributions: OOD detection is an important topic but most of the previous works are empirical. This paper demonstrates a theoretical understanding that uses the likelihood ratio between out-distribution and in-distribution to measure the extent of OOD. Based on this, the authors claim that SOTA methods are suboptimal because they only estimate the in-distribution density. The authors propose a method named FLATS, which outperform previous methods on several OOD detection benchmark.\n\nreasons_to_accept: 1. This paper is well-written and organized. \n2. This paper has contributions from both theoretical and empirical perspectives: Not only provides a theoretical understanding and analyses of previous methods but also proposes a novel method that outperforms previous methods.\n\nreasons_to_reject: 1. Experimental validation is weak, only considering Roberta as the backbone.\n\nquestions_for_the_authors: 1. Have you conducted experiments on other backbones or other than NLP classification tasks?\n\nmissing_references: no\n\ntypos_grammar_style_and_presentation_improvements: no\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "UYRIg2cONc",
        "length": 382,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the issue of detecting out-of-distribution (OOD) instances within the realm of Natural Language Processing (NLP) models. The paper's primary contributions are centered around the introduction of a novel methodology designed to assess the \"OOD-ness\" of a given test case. This is achieved through the application of a likelihood ratio, which juxtaposes the probabilities associated with out-of-distribution and in-distribution contexts. The paper posits that current state-of-the-art (SOTA) methods exhibit suboptimal performance due to their exclusive focus on estimating in-distribution density. In response, the paper presents a principled solution for OOD detection rooted in the concept of a likelihood ratio, denoted as FLATS. This framework holds the potential to serve as a versatile foundational structure, effectively bolstering various other OOD detection methodologies through the incorporation of out-distribution density.\n\nreasons_to_accept: 1. The paper's strength lies in its rigorous approach to addressing out-of-distribution (OOD) detection, introducing the well-grounded feature-based likelihood ratio score substantiated by Theorem 1. The extension of FLATS to diverse feature-based OOD scores highlights its versatility and departure from existing methods, showing potential for real-world applications.\n2. The experimental results underscore its contributions, as FLATS establishes itself as the new OOD detection state-of-the-art (SOTA), surpassing benchmarks like KNN and Maha. Experiments with various Pin and Pout estimation approaches further emphasize FLATS' adaptability and robustness, significantly boosting the paper's impact on the NLP community.\n\nreasons_to_reject: 1. The method's reliance on likelihood ratio-based OOD scores, as justified by Theorem 1, warrants closer scrutiny. The application of energy-based models (EBMs) for parameterization introduces an inherent complexity, raising questions about the scalability and efficiency of the proposed framework, particularly in real-world applications.\n2. The presented comparison with the KNN and Maha approaches is indicative, but a more detailed analysis of the strengths and weaknesses of FLATS in comparison to a broader spectrum of methods, including both traditional and contemporary alternatives, would contribute to a more nuanced understanding of its efficacy.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "212_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_212_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.7250500000000001,
      "max_similarity": 0.7393,
      "avg_coverage": 0.519875,
      "max_coverage": 0.68
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 816,
      "avg_human_length": 524.75
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "Io5usUG5cO",
        "similarity": 0.7294,
        "coverage": 0.3273,
        "human_length": 684,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a unified summarization benchmark (USB) that supports 9 summarization-related tasks. \nTo construct USB, the authors first conduct a sophisticated preprocessing pipeline to contain source text of their interest. Then, given a wikipedia article, the authors perform human annotation by asking human annotators 1) identify relevant evidence for each sentence in the summary (wikipedia leading paragraph); 2) edit (delete) contents in the summary, which cannot be supported by the wikipedia article (input text). In this way, they obtain parallel data, including article, leading paragraph, modified leading paragraph, supporting evidence, etc, and design 8 tasks using those data. \nAfter building the USB, the authors conduct extensive experiments on it, and specifically analyze the OOD performance and influence of human annotated data compared with silver data. \nGenerally, this paper is well written and can provide some insights to the filed.\n\nreasons_to_accept: 1. This paper is well-written and easy to follow. \n2. The USB benchmark can serve as a valuable data source for future research on evidence mining and factuality. \n3. Extensive experiments and detailed analysis.\n\nreasons_to_reject: Generally, this paper is well motivated and can be valuable to the filed. \nThe only concern is the lack of an ethics consideration section to discuss the human annotation and evaluation detailedly, which correspondingly leads to the data quality concern.\n\nquestions_for_the_authors: Q1 Instead of stating they are Mechanical Turk workers, could the authors provide more detailed ethical consideration that discusses the detailed human annotation process. For example, what are the background of human annotators? Are they linguists or just native speakers? What are their education level? Are they compensated and how?\nQ2 How reliable are the human evaluation results? Have the authors measure their agreements (e.g. kappa)?\n\nmissing_references: Some statements are not alway true. \nL548-L554: \"There exist plenty of datasets... many of them were created heuristically\". However, *they are also many datasets that are human annotated*, for example, SAMSUM, QMSum, DialogSum. The authors could discuss more the difference compared with those abstractive datasets, instead of extractive ones.\nAlso, more recently, there is SummZoo benchmark, and more relevantly, there is MACSum, which contains multiple controllable attributes, and *is a unified benchmark as well*. However, they are not cited.\nZhang, Yusen, Yang Liu, Ziyi Yang, Yuwei Fang, Yulong Chen, Dragomir Radev, Chenguang Zhu, Michael Zeng, and Rui Zhang. \" Macsum: Controllable summarization with mixed attributes.\" Transactions of the Association for Computational Linguistics 11 (2023): 787-803.\nZhong, Ming, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan et al. \"QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization.\" In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5905-5921. 2021.\nGliwa, Bogdan, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. \" SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization.\" In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 70-79. 2019.\nChen, Yulong, Yang Liu, Liang Chen, and Yue Zhang. \" DialogSum: A Real-Life Scenario Dialogue Summarization Dataset.\" In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 5062-5074. 2021.\nChen, Yulong, Yang Liu, Ruochen Xu, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Yue Zhang. \" Unisumm: Unified few-shot summarization with multi-task pre-training and prefix-tuning.\" arXiv preprint arXiv:2211.09783 (2022).\n\nethical_concerns: No\n\njustification_for_ethical_concerns: The only concern is the lack of an ethics consideration section to discuss the human annotation and evaluation detailedly, which correspondingly leads to the data quality concern.\nIn particular, the authors did not provide a detailed ethical consideration section that discusses the detailed human annotation and evaluation process. For example, what are the background of human annotators? Are they linguists or just native speakers? What are their education level? Are they compensated and how?\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "MnOesQCWw9",
        "similarity": 0.7393,
        "coverage": 0.42,
        "human_length": 603,
        "human_text": "paper_topic_and_main_contributions: The authors propose a unified summarization benchmark (USB). They use the overall section of Wikipedia pages as the candidate summary of the rest of the source. Then, they ask annotators to find the evidence for each sentence in the candidate summary and correct the sentences with no evidence. They propose 8 tasks over the annotated dataset and evaluate it on fine-tuned models and LLMs. Results show that fine-tuned models outperform few-shot LLMs. They also split the dataset into 6 domains and evaluated the cross-domain performances.\n\nreasons_to_accept: 1. It is important to create a manually labeled summarization benchmark that includes various tasks. Especially for detecting the factual issues of the LLMs. \n2. Overall, the paper is well-written and easy to follow. I can understand the motivation for each step of dataset construction and experiment. \n3. The proposed benchmark is relatively comprehensive. It covers 8 tasks and 6 domains with an evaluation of two types of SOTA models. A number of insightful findings are proposed by analyzing experimental results.\n\nreasons_to_reject: 1. Quality of the summaries is unknown. Although there is verification for annotation of the evidence, it is not clear about the quality of the summaries themselves. How could we know that the overview section of Wikipedia can serve as a good summary of the rest of the source text? Probably, some key information in the content sections is missing as a summary. The overall section is not designed to serve as a summary initially. That's why we can find factual errors in it. So I think the authors also need to prove they can serve as summaries by some human annotation.\n2. Some proposed tasks are not natural. Since the dataset is not written but edited by humans, some tasks seem pseudo data as well. For instance, for EXT task, evidence is just for the overall section which may not be a good extractive summary. And for the TOPIC task, the topic-based summary is directly extracted from the overall section. I believe the readability will decrease. It is also a pseudo-summary rather than a human-written one. Similarly, in the COMP task, some coreference issues may happen. And it may not be a good summary of evidence without rewriting.\n3. Domain bias. Biographies contain 1514 samples which is much larger than the sum of the rest domains. The second largest domain only contains 150 samples. This may weaken your conclusions on the domain experiments.\nOverall, although some manual labels are involved, the construction of the dataset still contains some bias and noises which weaken the conclusions. Among the 8 tasks, the most useful or clean ones are fact-check-related tasks.\n\nquestions_for_the_authors: - QA line 80, if evidence was lacking, do you directly remove all sentences? Or do you remove only some related spans?\n- QB line 205, do you only consider hyperlinks as entities? Have you ever done an experiment to show its coverage of all entities?\n- QC line 248, how do you deal with the sentences that are verified to be wrong?\n\ntypos_grammar_style_and_presentation_improvements: For figure 2, I suggest to use 30 as pure white and 50 as pure blue to make the figure more clear.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "uKganOG6f8",
        "similarity": 0.7111,
        "coverage": 0.68,
        "human_length": 423,
        "human_text": "paper_topic_and_main_contributions: In this paper, a benchmark derived from human-annotated Wikipedia content is presented, encompassing eight closely connected tasks: extracted summarization, abstractive summarization, topic-based summarization, sentence compressing, evidence selection, factual accuracy predicting, unsupported span prediction, factual errors correction. The authors also conduct a comprehensive comparison between traditional fine-tuned models and the most recent LLMs across these tasks, using both automatic and human evaluations.\n\nreasons_to_accept: The paper is well written. High quality summarization datasets are always important for the research community. This paper integrated eight very interesting summarization-related tasks and annotated a relatively large amount of examples for training and evaluation. This will be helpful for everyone in the research community.\n\nreasons_to_reject: My main worry is the absence of human assessment for the ultimate version of the proposed dataset. Maynez et al. 2020 noted that ground truth summaries are prone to hallucinations. I believe that incorporating thorough human evaluation of the dataset would provide researchers with confidence to explore this dataset extensively. \nDue to the lack of human confirmation, I currently have some doubts about the quality of the annotated summaries. This is because, in Table 2, the difference in performance between ChatGPT and the fine-tuned models on generation tasks, as assessed w.r.t. the reference summaries, is quite substantial. \nThis indicates that summaries generated by fine-tuned models are more aligned with the annotated ground truth summaries, whereas summaries produced by ChatGPT do not closely resemble the ground truth. Interestingly, human evaluation highlighted a preference for summaries generated by ChatGPT. This suggests that the proximity to human-annotated summaries may not necessarily correlate with higher human satisfaction. Perhaps it's necessary to conduct a more thorough analysis of the distinctions between human-annotated summaries and the summaries generated by ChatGPT.\nAdditionally, the paper lacks an in-depth examination of the experimental outcomes. Of particular note is the notably poor performance of LLMs on tasks other than generation (such as EVEXT, EXT, FAC, UNSUP).\n\nquestions_for_the_authors: The Landmarks and Newspapers are not included in any experiments?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "qL89Gu8xqd",
        "similarity": 0.7204,
        "coverage": 0.6522,
        "human_length": 389,
        "human_text": "paper_topic_and_main_contributions: They present a benchmark based on Wikipedia, enhanced with a comprehensive set of crowd-sourced annotations, supporting 8 interconnected tasks: (i) extractive summarization, (ii) abstractive summarization, (iii) topic-centered summarization, (iv) compressing selected sentences to a single line summary, (v) identifying evidence backing a summary, (vi) determining a summary's factual accuracy, (vii) spotting unsupported segments in a summary, and (viii) rectifying factual errors in summaries. When comparing different approaches on this benchmark, they find that medium-sized fine-tuned models consistently surpass larger few-shot prompted models across several tasks. Regarding tasks tied to factuality, heuristics-based training data perform worse than training on much fewer human-labeled datasets. Their sourced articles span 6 fields, enabling cross-domain studies. Depending on the task, the quantity of training data can be more influential than its domain origin, but for some tasks, domain-specific training, even if scarce, proves more effective.\n\nreasons_to_accept: 1. The benchmark offers a diverse platform for training and assessment on 8 unique tasks focusing on essential yet overlooked facets of text summarization. \n2. Various models and training approaches, such as fine-tuning, few-shot prompting, and multi-task training, were evaluated. \n3. They provided insights on how different tasks generalize outside their original domain, pinpointing which tasks prioritize the volume of training data over its specific domain origin. \n4. The paper is well-written.\n\nreasons_to_reject: 1.The dataset is not written but edited by humans, and the quality of the dataset is unknown. Therefore, the evaluations done on this dataset might not be so trustworthy. \n2. The experimental analysis lacks depth, so the findings based on using this benchmark is not exciting enough.\n\nethical_concerns: No\n\nquestions_for_the_authors: could you please give more details about bout the annotators and how human build the dataset? More evidences for the quality of the datasets could be provided.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "Io5usUG5cO",
        "length": 684,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a unified summarization benchmark (USB) that supports 9 summarization-related tasks. \nTo construct USB, the authors first conduct a sophisticated preprocessing pipeline to contain source text of their interest. Then, given a wikipedia article, the authors perform human annotation by asking human annotators 1) identify relevant evidence for each sentence in the summary (wikipedia leading paragraph); 2) edit (delete) contents in the summary, which cannot be supported by the wikipedia article (input text). In this way, they obtain parallel data, including article, leading paragraph, modified leading paragraph, supporting evidence, etc, and design 8 tasks using those data. \nAfter building the USB, the authors conduct extensive experiments on it, and specifically analyze the OOD performance and influence of human annotated data compared with silver data. \nGenerally, this paper is well written and can provide some insights to the filed.\n\nreasons_to_accept: 1. This paper is well-written and easy to follow. \n2. The USB benchmark can serve as a valuable data source for future research on evidence mining and factuality. \n3. Extensive experiments and detailed analysis.\n\nreasons_to_reject: Generally, this paper is well motivated and can be valuable to the filed. \nThe only concern is the lack of an ethics consideration section to discuss the human annotation and evaluation detailedly, which correspondingly leads to the data quality concern.\n\nquestions_for_the_authors: Q1 Instead of stating they are Mechanical Turk workers, could the authors provide more detailed ethical consideration that discusses the detailed human annotation process. For example, what are the background of human annotators? Are they linguists or just native speakers? What are their education level? Are they compensated and how?\nQ2 How reliable are the human evaluation results? Have the authors measure their agreements (e.g. kappa)?\n\nmissing_references: Some statements are not alway true. \nL548-L554: \"There exist plenty of datasets... many of them were created heuristically\". However, *they are also many datasets that are human annotated*, for example, SAMSUM, QMSum, DialogSum. The authors could discuss more the difference compared with those abstractive datasets, instead of extractive ones.\nAlso, more recently, there is SummZoo benchmark, and more relevantly, there is MACSum, which contains multiple controllable attributes, and *is a unified benchmark as well*. However, they are not cited.\nZhang, Yusen, Yang Liu, Ziyi Yang, Yuwei Fang, Yulong Chen, Dragomir Radev, Chenguang Zhu, Michael Zeng, and Rui Zhang. \" Macsum: Controllable summarization with mixed attributes.\" Transactions of the Association for Computational Linguistics 11 (2023): 787-803.\nZhong, Ming, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan et al. \"QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization.\" In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5905-5921. 2021.\nGliwa, Bogdan, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. \" SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization.\" In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 70-79. 2019.\nChen, Yulong, Yang Liu, Liang Chen, and Yue Zhang. \" DialogSum: A Real-Life Scenario Dialogue Summarization Dataset.\" In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 5062-5074. 2021.\nChen, Yulong, Yang Liu, Ruochen Xu, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Yue Zhang. \" Unisumm: Unified few-shot summarization with multi-task pre-training and prefix-tuning.\" arXiv preprint arXiv:2211.09783 (2022).\n\nethical_concerns: No\n\njustification_for_ethical_concerns: The only concern is the lack of an ethics consideration section to discuss the human annotation and evaluation detailedly, which correspondingly leads to the data quality concern.\nIn particular, the authors did not provide a detailed ethical consideration section that discusses the detailed human annotation and evaluation process. For example, what are the background of human annotators? Are they linguists or just native speakers? What are their education level? Are they compensated and how?\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "MnOesQCWw9",
        "length": 603,
        "human_text": "paper_topic_and_main_contributions: The authors propose a unified summarization benchmark (USB). They use the overall section of Wikipedia pages as the candidate summary of the rest of the source. Then, they ask annotators to find the evidence for each sentence in the candidate summary and correct the sentences with no evidence. They propose 8 tasks over the annotated dataset and evaluate it on fine-tuned models and LLMs. Results show that fine-tuned models outperform few-shot LLMs. They also split the dataset into 6 domains and evaluated the cross-domain performances.\n\nreasons_to_accept: 1. It is important to create a manually labeled summarization benchmark that includes various tasks. Especially for detecting the factual issues of the LLMs. \n2. Overall, the paper is well-written and easy to follow. I can understand the motivation for each step of dataset construction and experiment. \n3. The proposed benchmark is relatively comprehensive. It covers 8 tasks and 6 domains with an evaluation of two types of SOTA models. A number of insightful findings are proposed by analyzing experimental results.\n\nreasons_to_reject: 1. Quality of the summaries is unknown. Although there is verification for annotation of the evidence, it is not clear about the quality of the summaries themselves. How could we know that the overview section of Wikipedia can serve as a good summary of the rest of the source text? Probably, some key information in the content sections is missing as a summary. The overall section is not designed to serve as a summary initially. That's why we can find factual errors in it. So I think the authors also need to prove they can serve as summaries by some human annotation.\n2. Some proposed tasks are not natural. Since the dataset is not written but edited by humans, some tasks seem pseudo data as well. For instance, for EXT task, evidence is just for the overall section which may not be a good extractive summary. And for the TOPIC task, the topic-based summary is directly extracted from the overall section. I believe the readability will decrease. It is also a pseudo-summary rather than a human-written one. Similarly, in the COMP task, some coreference issues may happen. And it may not be a good summary of evidence without rewriting.\n3. Domain bias. Biographies contain 1514 samples which is much larger than the sum of the rest domains. The second largest domain only contains 150 samples. This may weaken your conclusions on the domain experiments.\nOverall, although some manual labels are involved, the construction of the dataset still contains some bias and noises which weaken the conclusions. Among the 8 tasks, the most useful or clean ones are fact-check-related tasks.\n\nquestions_for_the_authors: - QA line 80, if evidence was lacking, do you directly remove all sentences? Or do you remove only some related spans?\n- QB line 205, do you only consider hyperlinks as entities? Have you ever done an experiment to show its coverage of all entities?\n- QC line 248, how do you deal with the sentences that are verified to be wrong?\n\ntypos_grammar_style_and_presentation_improvements: For figure 2, I suggest to use 30 as pure white and 50 as pure blue to make the figure more clear.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "uKganOG6f8",
        "length": 423,
        "human_text": "paper_topic_and_main_contributions: In this paper, a benchmark derived from human-annotated Wikipedia content is presented, encompassing eight closely connected tasks: extracted summarization, abstractive summarization, topic-based summarization, sentence compressing, evidence selection, factual accuracy predicting, unsupported span prediction, factual errors correction. The authors also conduct a comprehensive comparison between traditional fine-tuned models and the most recent LLMs across these tasks, using both automatic and human evaluations.\n\nreasons_to_accept: The paper is well written. High quality summarization datasets are always important for the research community. This paper integrated eight very interesting summarization-related tasks and annotated a relatively large amount of examples for training and evaluation. This will be helpful for everyone in the research community.\n\nreasons_to_reject: My main worry is the absence of human assessment for the ultimate version of the proposed dataset. Maynez et al. 2020 noted that ground truth summaries are prone to hallucinations. I believe that incorporating thorough human evaluation of the dataset would provide researchers with confidence to explore this dataset extensively. \nDue to the lack of human confirmation, I currently have some doubts about the quality of the annotated summaries. This is because, in Table 2, the difference in performance between ChatGPT and the fine-tuned models on generation tasks, as assessed w.r.t. the reference summaries, is quite substantial. \nThis indicates that summaries generated by fine-tuned models are more aligned with the annotated ground truth summaries, whereas summaries produced by ChatGPT do not closely resemble the ground truth. Interestingly, human evaluation highlighted a preference for summaries generated by ChatGPT. This suggests that the proximity to human-annotated summaries may not necessarily correlate with higher human satisfaction. Perhaps it's necessary to conduct a more thorough analysis of the distinctions between human-annotated summaries and the summaries generated by ChatGPT.\nAdditionally, the paper lacks an in-depth examination of the experimental outcomes. Of particular note is the notably poor performance of LLMs on tasks other than generation (such as EVEXT, EXT, FAC, UNSUP).\n\nquestions_for_the_authors: The Landmarks and Newspapers are not included in any experiments?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "qL89Gu8xqd",
        "length": 389,
        "human_text": "paper_topic_and_main_contributions: They present a benchmark based on Wikipedia, enhanced with a comprehensive set of crowd-sourced annotations, supporting 8 interconnected tasks: (i) extractive summarization, (ii) abstractive summarization, (iii) topic-centered summarization, (iv) compressing selected sentences to a single line summary, (v) identifying evidence backing a summary, (vi) determining a summary's factual accuracy, (vii) spotting unsupported segments in a summary, and (viii) rectifying factual errors in summaries. When comparing different approaches on this benchmark, they find that medium-sized fine-tuned models consistently surpass larger few-shot prompted models across several tasks. Regarding tasks tied to factuality, heuristics-based training data perform worse than training on much fewer human-labeled datasets. Their sourced articles span 6 fields, enabling cross-domain studies. Depending on the task, the quantity of training data can be more influential than its domain origin, but for some tasks, domain-specific training, even if scarce, proves more effective.\n\nreasons_to_accept: 1. The benchmark offers a diverse platform for training and assessment on 8 unique tasks focusing on essential yet overlooked facets of text summarization. \n2. Various models and training approaches, such as fine-tuning, few-shot prompting, and multi-task training, were evaluated. \n3. They provided insights on how different tasks generalize outside their original domain, pinpointing which tasks prioritize the volume of training data over its specific domain origin. \n4. The paper is well-written.\n\nreasons_to_reject: 1.The dataset is not written but edited by humans, and the quality of the dataset is unknown. Therefore, the evaluations done on this dataset might not be so trustworthy. \n2. The experimental analysis lacks depth, so the findings based on using this benchmark is not exciting enough.\n\nethical_concerns: No\n\nquestions_for_the_authors: could you please give more details about bout the annotators and how human build the dataset? More evidences for the quality of the datasets could be provided.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "212_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_212_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.7248,
      "max_similarity": 0.7395,
      "avg_coverage": 0.519875,
      "max_coverage": 0.68
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 803,
      "avg_human_length": 524.75
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "Io5usUG5cO",
        "similarity": 0.728,
        "coverage": 0.3273,
        "human_length": 684,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a unified summarization benchmark (USB) that supports 9 summarization-related tasks. \nTo construct USB, the authors first conduct a sophisticated preprocessing pipeline to contain source text of their interest. Then, given a wikipedia article, the authors perform human annotation by asking human annotators 1) identify relevant evidence for each sentence in the summary (wikipedia leading paragraph); 2) edit (delete) contents in the summary, which cannot be supported by the wikipedia article (input text). In this way, they obtain parallel data, including article, leading paragraph, modified leading paragraph, supporting evidence, etc, and design 8 tasks using those data. \nAfter building the USB, the authors conduct extensive experiments on it, and specifically analyze the OOD performance and influence of human annotated data compared with silver data. \nGenerally, this paper is well written and can provide some insights to the filed.\n\nreasons_to_accept: 1. This paper is well-written and easy to follow. \n2. The USB benchmark can serve as a valuable data source for future research on evidence mining and factuality. \n3. Extensive experiments and detailed analysis.\n\nreasons_to_reject: Generally, this paper is well motivated and can be valuable to the filed. \nThe only concern is the lack of an ethics consideration section to discuss the human annotation and evaluation detailedly, which correspondingly leads to the data quality concern.\n\nquestions_for_the_authors: Q1 Instead of stating they are Mechanical Turk workers, could the authors provide more detailed ethical consideration that discusses the detailed human annotation process. For example, what are the background of human annotators? Are they linguists or just native speakers? What are their education level? Are they compensated and how?\nQ2 How reliable are the human evaluation results? Have the authors measure their agreements (e.g. kappa)?\n\nmissing_references: Some statements are not alway true. \nL548-L554: \"There exist plenty of datasets... many of them were created heuristically\". However, *they are also many datasets that are human annotated*, for example, SAMSUM, QMSum, DialogSum. The authors could discuss more the difference compared with those abstractive datasets, instead of extractive ones.\nAlso, more recently, there is SummZoo benchmark, and more relevantly, there is MACSum, which contains multiple controllable attributes, and *is a unified benchmark as well*. However, they are not cited.\nZhang, Yusen, Yang Liu, Ziyi Yang, Yuwei Fang, Yulong Chen, Dragomir Radev, Chenguang Zhu, Michael Zeng, and Rui Zhang. \" Macsum: Controllable summarization with mixed attributes.\" Transactions of the Association for Computational Linguistics 11 (2023): 787-803.\nZhong, Ming, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan et al. \"QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization.\" In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5905-5921. 2021.\nGliwa, Bogdan, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. \" SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization.\" In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 70-79. 2019.\nChen, Yulong, Yang Liu, Liang Chen, and Yue Zhang. \" DialogSum: A Real-Life Scenario Dialogue Summarization Dataset.\" In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 5062-5074. 2021.\nChen, Yulong, Yang Liu, Ruochen Xu, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Yue Zhang. \" Unisumm: Unified few-shot summarization with multi-task pre-training and prefix-tuning.\" arXiv preprint arXiv:2211.09783 (2022).\n\nethical_concerns: No\n\njustification_for_ethical_concerns: The only concern is the lack of an ethics consideration section to discuss the human annotation and evaluation detailedly, which correspondingly leads to the data quality concern.\nIn particular, the authors did not provide a detailed ethical consideration section that discusses the detailed human annotation and evaluation process. For example, what are the background of human annotators? Are they linguists or just native speakers? What are their education level? Are they compensated and how?\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "MnOesQCWw9",
        "similarity": 0.7395,
        "coverage": 0.42,
        "human_length": 603,
        "human_text": "paper_topic_and_main_contributions: The authors propose a unified summarization benchmark (USB). They use the overall section of Wikipedia pages as the candidate summary of the rest of the source. Then, they ask annotators to find the evidence for each sentence in the candidate summary and correct the sentences with no evidence. They propose 8 tasks over the annotated dataset and evaluate it on fine-tuned models and LLMs. Results show that fine-tuned models outperform few-shot LLMs. They also split the dataset into 6 domains and evaluated the cross-domain performances.\n\nreasons_to_accept: 1. It is important to create a manually labeled summarization benchmark that includes various tasks. Especially for detecting the factual issues of the LLMs. \n2. Overall, the paper is well-written and easy to follow. I can understand the motivation for each step of dataset construction and experiment. \n3. The proposed benchmark is relatively comprehensive. It covers 8 tasks and 6 domains with an evaluation of two types of SOTA models. A number of insightful findings are proposed by analyzing experimental results.\n\nreasons_to_reject: 1. Quality of the summaries is unknown. Although there is verification for annotation of the evidence, it is not clear about the quality of the summaries themselves. How could we know that the overview section of Wikipedia can serve as a good summary of the rest of the source text? Probably, some key information in the content sections is missing as a summary. The overall section is not designed to serve as a summary initially. That's why we can find factual errors in it. So I think the authors also need to prove they can serve as summaries by some human annotation.\n2. Some proposed tasks are not natural. Since the dataset is not written but edited by humans, some tasks seem pseudo data as well. For instance, for EXT task, evidence is just for the overall section which may not be a good extractive summary. And for the TOPIC task, the topic-based summary is directly extracted from the overall section. I believe the readability will decrease. It is also a pseudo-summary rather than a human-written one. Similarly, in the COMP task, some coreference issues may happen. And it may not be a good summary of evidence without rewriting.\n3. Domain bias. Biographies contain 1514 samples which is much larger than the sum of the rest domains. The second largest domain only contains 150 samples. This may weaken your conclusions on the domain experiments.\nOverall, although some manual labels are involved, the construction of the dataset still contains some bias and noises which weaken the conclusions. Among the 8 tasks, the most useful or clean ones are fact-check-related tasks.\n\nquestions_for_the_authors: - QA line 80, if evidence was lacking, do you directly remove all sentences? Or do you remove only some related spans?\n- QB line 205, do you only consider hyperlinks as entities? Have you ever done an experiment to show its coverage of all entities?\n- QC line 248, how do you deal with the sentences that are verified to be wrong?\n\ntypos_grammar_style_and_presentation_improvements: For figure 2, I suggest to use 30 as pure white and 50 as pure blue to make the figure more clear.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "uKganOG6f8",
        "similarity": 0.7118,
        "coverage": 0.68,
        "human_length": 423,
        "human_text": "paper_topic_and_main_contributions: In this paper, a benchmark derived from human-annotated Wikipedia content is presented, encompassing eight closely connected tasks: extracted summarization, abstractive summarization, topic-based summarization, sentence compressing, evidence selection, factual accuracy predicting, unsupported span prediction, factual errors correction. The authors also conduct a comprehensive comparison between traditional fine-tuned models and the most recent LLMs across these tasks, using both automatic and human evaluations.\n\nreasons_to_accept: The paper is well written. High quality summarization datasets are always important for the research community. This paper integrated eight very interesting summarization-related tasks and annotated a relatively large amount of examples for training and evaluation. This will be helpful for everyone in the research community.\n\nreasons_to_reject: My main worry is the absence of human assessment for the ultimate version of the proposed dataset. Maynez et al. 2020 noted that ground truth summaries are prone to hallucinations. I believe that incorporating thorough human evaluation of the dataset would provide researchers with confidence to explore this dataset extensively. \nDue to the lack of human confirmation, I currently have some doubts about the quality of the annotated summaries. This is because, in Table 2, the difference in performance between ChatGPT and the fine-tuned models on generation tasks, as assessed w.r.t. the reference summaries, is quite substantial. \nThis indicates that summaries generated by fine-tuned models are more aligned with the annotated ground truth summaries, whereas summaries produced by ChatGPT do not closely resemble the ground truth. Interestingly, human evaluation highlighted a preference for summaries generated by ChatGPT. This suggests that the proximity to human-annotated summaries may not necessarily correlate with higher human satisfaction. Perhaps it's necessary to conduct a more thorough analysis of the distinctions between human-annotated summaries and the summaries generated by ChatGPT.\nAdditionally, the paper lacks an in-depth examination of the experimental outcomes. Of particular note is the notably poor performance of LLMs on tasks other than generation (such as EVEXT, EXT, FAC, UNSUP).\n\nquestions_for_the_authors: The Landmarks and Newspapers are not included in any experiments?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "qL89Gu8xqd",
        "similarity": 0.7199,
        "coverage": 0.6522,
        "human_length": 389,
        "human_text": "paper_topic_and_main_contributions: They present a benchmark based on Wikipedia, enhanced with a comprehensive set of crowd-sourced annotations, supporting 8 interconnected tasks: (i) extractive summarization, (ii) abstractive summarization, (iii) topic-centered summarization, (iv) compressing selected sentences to a single line summary, (v) identifying evidence backing a summary, (vi) determining a summary's factual accuracy, (vii) spotting unsupported segments in a summary, and (viii) rectifying factual errors in summaries. When comparing different approaches on this benchmark, they find that medium-sized fine-tuned models consistently surpass larger few-shot prompted models across several tasks. Regarding tasks tied to factuality, heuristics-based training data perform worse than training on much fewer human-labeled datasets. Their sourced articles span 6 fields, enabling cross-domain studies. Depending on the task, the quantity of training data can be more influential than its domain origin, but for some tasks, domain-specific training, even if scarce, proves more effective.\n\nreasons_to_accept: 1. The benchmark offers a diverse platform for training and assessment on 8 unique tasks focusing on essential yet overlooked facets of text summarization. \n2. Various models and training approaches, such as fine-tuning, few-shot prompting, and multi-task training, were evaluated. \n3. They provided insights on how different tasks generalize outside their original domain, pinpointing which tasks prioritize the volume of training data over its specific domain origin. \n4. The paper is well-written.\n\nreasons_to_reject: 1.The dataset is not written but edited by humans, and the quality of the dataset is unknown. Therefore, the evaluations done on this dataset might not be so trustworthy. \n2. The experimental analysis lacks depth, so the findings based on using this benchmark is not exciting enough.\n\nethical_concerns: No\n\nquestions_for_the_authors: could you please give more details about bout the annotators and how human build the dataset? More evidences for the quality of the datasets could be provided.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "Io5usUG5cO",
        "length": 684,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a unified summarization benchmark (USB) that supports 9 summarization-related tasks. \nTo construct USB, the authors first conduct a sophisticated preprocessing pipeline to contain source text of their interest. Then, given a wikipedia article, the authors perform human annotation by asking human annotators 1) identify relevant evidence for each sentence in the summary (wikipedia leading paragraph); 2) edit (delete) contents in the summary, which cannot be supported by the wikipedia article (input text). In this way, they obtain parallel data, including article, leading paragraph, modified leading paragraph, supporting evidence, etc, and design 8 tasks using those data. \nAfter building the USB, the authors conduct extensive experiments on it, and specifically analyze the OOD performance and influence of human annotated data compared with silver data. \nGenerally, this paper is well written and can provide some insights to the filed.\n\nreasons_to_accept: 1. This paper is well-written and easy to follow. \n2. The USB benchmark can serve as a valuable data source for future research on evidence mining and factuality. \n3. Extensive experiments and detailed analysis.\n\nreasons_to_reject: Generally, this paper is well motivated and can be valuable to the filed. \nThe only concern is the lack of an ethics consideration section to discuss the human annotation and evaluation detailedly, which correspondingly leads to the data quality concern.\n\nquestions_for_the_authors: Q1 Instead of stating they are Mechanical Turk workers, could the authors provide more detailed ethical consideration that discusses the detailed human annotation process. For example, what are the background of human annotators? Are they linguists or just native speakers? What are their education level? Are they compensated and how?\nQ2 How reliable are the human evaluation results? Have the authors measure their agreements (e.g. kappa)?\n\nmissing_references: Some statements are not alway true. \nL548-L554: \"There exist plenty of datasets... many of them were created heuristically\". However, *they are also many datasets that are human annotated*, for example, SAMSUM, QMSum, DialogSum. The authors could discuss more the difference compared with those abstractive datasets, instead of extractive ones.\nAlso, more recently, there is SummZoo benchmark, and more relevantly, there is MACSum, which contains multiple controllable attributes, and *is a unified benchmark as well*. However, they are not cited.\nZhang, Yusen, Yang Liu, Ziyi Yang, Yuwei Fang, Yulong Chen, Dragomir Radev, Chenguang Zhu, Michael Zeng, and Rui Zhang. \" Macsum: Controllable summarization with mixed attributes.\" Transactions of the Association for Computational Linguistics 11 (2023): 787-803.\nZhong, Ming, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan et al. \"QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization.\" In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5905-5921. 2021.\nGliwa, Bogdan, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. \" SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization.\" In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 70-79. 2019.\nChen, Yulong, Yang Liu, Liang Chen, and Yue Zhang. \" DialogSum: A Real-Life Scenario Dialogue Summarization Dataset.\" In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 5062-5074. 2021.\nChen, Yulong, Yang Liu, Ruochen Xu, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Yue Zhang. \" Unisumm: Unified few-shot summarization with multi-task pre-training and prefix-tuning.\" arXiv preprint arXiv:2211.09783 (2022).\n\nethical_concerns: No\n\njustification_for_ethical_concerns: The only concern is the lack of an ethics consideration section to discuss the human annotation and evaluation detailedly, which correspondingly leads to the data quality concern.\nIn particular, the authors did not provide a detailed ethical consideration section that discusses the detailed human annotation and evaluation process. For example, what are the background of human annotators? Are they linguists or just native speakers? What are their education level? Are they compensated and how?\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "MnOesQCWw9",
        "length": 603,
        "human_text": "paper_topic_and_main_contributions: The authors propose a unified summarization benchmark (USB). They use the overall section of Wikipedia pages as the candidate summary of the rest of the source. Then, they ask annotators to find the evidence for each sentence in the candidate summary and correct the sentences with no evidence. They propose 8 tasks over the annotated dataset and evaluate it on fine-tuned models and LLMs. Results show that fine-tuned models outperform few-shot LLMs. They also split the dataset into 6 domains and evaluated the cross-domain performances.\n\nreasons_to_accept: 1. It is important to create a manually labeled summarization benchmark that includes various tasks. Especially for detecting the factual issues of the LLMs. \n2. Overall, the paper is well-written and easy to follow. I can understand the motivation for each step of dataset construction and experiment. \n3. The proposed benchmark is relatively comprehensive. It covers 8 tasks and 6 domains with an evaluation of two types of SOTA models. A number of insightful findings are proposed by analyzing experimental results.\n\nreasons_to_reject: 1. Quality of the summaries is unknown. Although there is verification for annotation of the evidence, it is not clear about the quality of the summaries themselves. How could we know that the overview section of Wikipedia can serve as a good summary of the rest of the source text? Probably, some key information in the content sections is missing as a summary. The overall section is not designed to serve as a summary initially. That's why we can find factual errors in it. So I think the authors also need to prove they can serve as summaries by some human annotation.\n2. Some proposed tasks are not natural. Since the dataset is not written but edited by humans, some tasks seem pseudo data as well. For instance, for EXT task, evidence is just for the overall section which may not be a good extractive summary. And for the TOPIC task, the topic-based summary is directly extracted from the overall section. I believe the readability will decrease. It is also a pseudo-summary rather than a human-written one. Similarly, in the COMP task, some coreference issues may happen. And it may not be a good summary of evidence without rewriting.\n3. Domain bias. Biographies contain 1514 samples which is much larger than the sum of the rest domains. The second largest domain only contains 150 samples. This may weaken your conclusions on the domain experiments.\nOverall, although some manual labels are involved, the construction of the dataset still contains some bias and noises which weaken the conclusions. Among the 8 tasks, the most useful or clean ones are fact-check-related tasks.\n\nquestions_for_the_authors: - QA line 80, if evidence was lacking, do you directly remove all sentences? Or do you remove only some related spans?\n- QB line 205, do you only consider hyperlinks as entities? Have you ever done an experiment to show its coverage of all entities?\n- QC line 248, how do you deal with the sentences that are verified to be wrong?\n\ntypos_grammar_style_and_presentation_improvements: For figure 2, I suggest to use 30 as pure white and 50 as pure blue to make the figure more clear.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "uKganOG6f8",
        "length": 423,
        "human_text": "paper_topic_and_main_contributions: In this paper, a benchmark derived from human-annotated Wikipedia content is presented, encompassing eight closely connected tasks: extracted summarization, abstractive summarization, topic-based summarization, sentence compressing, evidence selection, factual accuracy predicting, unsupported span prediction, factual errors correction. The authors also conduct a comprehensive comparison between traditional fine-tuned models and the most recent LLMs across these tasks, using both automatic and human evaluations.\n\nreasons_to_accept: The paper is well written. High quality summarization datasets are always important for the research community. This paper integrated eight very interesting summarization-related tasks and annotated a relatively large amount of examples for training and evaluation. This will be helpful for everyone in the research community.\n\nreasons_to_reject: My main worry is the absence of human assessment for the ultimate version of the proposed dataset. Maynez et al. 2020 noted that ground truth summaries are prone to hallucinations. I believe that incorporating thorough human evaluation of the dataset would provide researchers with confidence to explore this dataset extensively. \nDue to the lack of human confirmation, I currently have some doubts about the quality of the annotated summaries. This is because, in Table 2, the difference in performance between ChatGPT and the fine-tuned models on generation tasks, as assessed w.r.t. the reference summaries, is quite substantial. \nThis indicates that summaries generated by fine-tuned models are more aligned with the annotated ground truth summaries, whereas summaries produced by ChatGPT do not closely resemble the ground truth. Interestingly, human evaluation highlighted a preference for summaries generated by ChatGPT. This suggests that the proximity to human-annotated summaries may not necessarily correlate with higher human satisfaction. Perhaps it's necessary to conduct a more thorough analysis of the distinctions between human-annotated summaries and the summaries generated by ChatGPT.\nAdditionally, the paper lacks an in-depth examination of the experimental outcomes. Of particular note is the notably poor performance of LLMs on tasks other than generation (such as EVEXT, EXT, FAC, UNSUP).\n\nquestions_for_the_authors: The Landmarks and Newspapers are not included in any experiments?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "qL89Gu8xqd",
        "length": 389,
        "human_text": "paper_topic_and_main_contributions: They present a benchmark based on Wikipedia, enhanced with a comprehensive set of crowd-sourced annotations, supporting 8 interconnected tasks: (i) extractive summarization, (ii) abstractive summarization, (iii) topic-centered summarization, (iv) compressing selected sentences to a single line summary, (v) identifying evidence backing a summary, (vi) determining a summary's factual accuracy, (vii) spotting unsupported segments in a summary, and (viii) rectifying factual errors in summaries. When comparing different approaches on this benchmark, they find that medium-sized fine-tuned models consistently surpass larger few-shot prompted models across several tasks. Regarding tasks tied to factuality, heuristics-based training data perform worse than training on much fewer human-labeled datasets. Their sourced articles span 6 fields, enabling cross-domain studies. Depending on the task, the quantity of training data can be more influential than its domain origin, but for some tasks, domain-specific training, even if scarce, proves more effective.\n\nreasons_to_accept: 1. The benchmark offers a diverse platform for training and assessment on 8 unique tasks focusing on essential yet overlooked facets of text summarization. \n2. Various models and training approaches, such as fine-tuning, few-shot prompting, and multi-task training, were evaluated. \n3. They provided insights on how different tasks generalize outside their original domain, pinpointing which tasks prioritize the volume of training data over its specific domain origin. \n4. The paper is well-written.\n\nreasons_to_reject: 1.The dataset is not written but edited by humans, and the quality of the dataset is unknown. Therefore, the evaluations done on this dataset might not be so trustworthy. \n2. The experimental analysis lacks depth, so the findings based on using this benchmark is not exciting enough.\n\nethical_concerns: No\n\nquestions_for_the_authors: could you please give more details about bout the annotators and how human build the dataset? More evidences for the quality of the datasets could be provided.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "21_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_21_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7086666666666667,
      "max_similarity": 0.7135,
      "avg_coverage": 0.7241666666666667,
      "max_coverage": 0.8182
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 748,
      "avg_human_length": 391.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 8,
      "weaknesses_count": 8,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "Mcn5t83XtZ",
        "similarity": 0.7135,
        "coverage": 0.64,
        "human_length": 448,
        "human_text": "paper_topic_and_main_contributions: The paper describes and evaluates an *NLP engineering experiment*. The authors try to predict the personality  of an author based on text written by the author. Their approach is to use a Large Language Model in an instructive way to rate an authors personality in a multi-turn dialog (following the chain-of-thoughts approach). The LLM, here GPT3.5, is gets first the text in question, some instructions on the answering format and finally multiple request about the authors personality. The rating request are based on psychological questionnaires (i.e., MBTI and Big 5). Finally, the LLM is asked to rate the text authors's personality on different dimensions.\nFrom my point of view, the paper provides two main contributions. First, it evaluates the usage LLMs in order to predict the personality of a person on the basis of text written by that person. Second, it compares two approaches of prompt engineering, one using single-turn instructions and another one using multi-turn instructions.\nFinally, both approaches as well as several other are compared in a formative evaluation.\n\nreasons_to_accept: - The paper describes in a reproducible way an interesting approach to predict aspect of an author's personality from text they have written.\n- The approach is validated in a feasible evaluation.\n- An ablation study shows the effectiveness go the multi-turn approach.\n- The work not only shows the technical possibility to predict personality (i.e., traits) using LLMs, but also the (relative) simplicity, which is in turn a warning for potential misuse.\n\nreasons_to_reject: - In the results section the best two (alternative!) approaches which works best for Kaggle are not show for Essays. Thus the results are not complete.\n- A specific discussion is missing and the limitations are very short. Obvious limitations like application to only one language (which is fine, but should be named) or the rather artificial datasets are not mentioned.\n- The description of the two used data sets is very short. I would aspect at least additional information on the mean length of the texts to get a feeling for the needed amount of data per text author.\n\nquestions_for_the_authors: Question A: Why do you not give the results for TrigNet and DDGCN in Table 1 for the Essays dataset, as they work best on the Kaggle dataset?\n\ntypos_grammar_style_and_presentation_improvements: - Line 118-121: The sentence sounds somehow incomplete.\n- Line 167: the ChatGPT -> remove the - Line 356: Implementation\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Nopuj64i7i",
        "similarity": 0.7115,
        "coverage": 0.7143,
        "human_length": 507,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a method to use item-based psychology questionnaires as a way to improve personality detection capabilities of LLMs in a CoT framework. They prompt the model to rate individual items from the questionnaire at each turn and leverage the historical rating results to determine the underlying personality trait.\n\nreasons_to_accept: This paper proposes the first use of item-based personality related questionnaires to establish personality traits exhibited in text. Their method works better than simple standard prompting.\n\nreasons_to_reject: CoT does not just guide LLMs through arbitrary reasoning steps, but through a definitive order of reasoning steps. Furthermore, the abstract nature of the personality detection task render the step-by-step nature of the chain-of-thought problem solving approach challenging even for humans.\nHow can high and low be defined for the Big 5 traits? In its original form, this is a regression task, not a classification one.\nThere are no statistical significance tests b/w PsyCot and Standard baseline, and PsyCoT and RoBERTa.\nThe best model for these tasks is a simple regression model - Park et al (mentioned in missing references). However, the paper does not compare to that model and neither does it compare to Lynn et al 2020, which is another strong performing model. It is hard to contextualize the results in this paper and pit them against what the community knows.\n\nquestions_for_the_authors: A: The Big 5 personality traits elicit outcomes in a dimensional form. When this method elicits the outcome in the most dimensional form, why do you reduce it back to 2 classes?\nB: Post order matters (missing ref: Nelson Liu et al., 2023) matters for these LLMs, given that some messages have more information than others for Personality estimation (missing ref: Lynn et al 2020). How do you account for bias introduced due to order of posts in your prompting setup?\nC: Missing row in results table: Item informed high/low classification score (What is the performance when you aggregate the scores for each item) D: 10 vs 44 item scale is expected to have a big drop. Performance of 20 items scale would be more interesting to see since the 20 item scale has shown convergent validity (ref: Table 1 in Park et al., 2015)\n\nmissing_references: The following papers are critically missing: Automatic Personality Assessment Through Social Media Language - Park et al VERY similar baseline - Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation - Ganesan et al\n\ntypos_grammar_style_and_presentation_improvements: Line 256: By-Produce -> Byproduct\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "s90JGpvPSG",
        "similarity": 0.701,
        "coverage": 0.8182,
        "human_length": 219,
        "human_text": "paper_topic_and_main_contributions: The paper proposes PsyCoT, a chain-of-thought method specialized in personality detection tasks. The methodology uses chain-of-thought reasoning in the form of answering psychological questionnaires. The authors investigate the correlation between the trait score and the chosen personality and explore the patterns of score distributions in terms of different traits.\n\nreasons_to_accept: The methodology is interesting enough to understand the ability of LLMs for personality detection, and the explanations about details are clear. The authors analyze the correlation between scores and traits to predict some patterns between the personality traits and the LLM's behaviors. Overall, it is easy to follow and good enough to inspire researchers in a similar field to analyze or predict authors' personalities.\n\nreasons_to_reject: PsyCoT is basically coming from a popular method, Chain-of-Thought. It is weak to say the proposed method is innovative and original though the application to personality detection has been yet underexplored.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "Mcn5t83XtZ",
        "length": 448,
        "human_text": "paper_topic_and_main_contributions: The paper describes and evaluates an *NLP engineering experiment*. The authors try to predict the personality  of an author based on text written by the author. Their approach is to use a Large Language Model in an instructive way to rate an authors personality in a multi-turn dialog (following the chain-of-thoughts approach). The LLM, here GPT3.5, is gets first the text in question, some instructions on the answering format and finally multiple request about the authors personality. The rating request are based on psychological questionnaires (i.e., MBTI and Big 5). Finally, the LLM is asked to rate the text authors's personality on different dimensions.\nFrom my point of view, the paper provides two main contributions. First, it evaluates the usage LLMs in order to predict the personality of a person on the basis of text written by that person. Second, it compares two approaches of prompt engineering, one using single-turn instructions and another one using multi-turn instructions.\nFinally, both approaches as well as several other are compared in a formative evaluation.\n\nreasons_to_accept: - The paper describes in a reproducible way an interesting approach to predict aspect of an author's personality from text they have written.\n- The approach is validated in a feasible evaluation.\n- An ablation study shows the effectiveness go the multi-turn approach.\n- The work not only shows the technical possibility to predict personality (i.e., traits) using LLMs, but also the (relative) simplicity, which is in turn a warning for potential misuse.\n\nreasons_to_reject: - In the results section the best two (alternative!) approaches which works best for Kaggle are not show for Essays. Thus the results are not complete.\n- A specific discussion is missing and the limitations are very short. Obvious limitations like application to only one language (which is fine, but should be named) or the rather artificial datasets are not mentioned.\n- The description of the two used data sets is very short. I would aspect at least additional information on the mean length of the texts to get a feeling for the needed amount of data per text author.\n\nquestions_for_the_authors: Question A: Why do you not give the results for TrigNet and DDGCN in Table 1 for the Essays dataset, as they work best on the Kaggle dataset?\n\ntypos_grammar_style_and_presentation_improvements: - Line 118-121: The sentence sounds somehow incomplete.\n- Line 167: the ChatGPT -> remove the - Line 356: Implementation\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "Nopuj64i7i",
        "length": 507,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a method to use item-based psychology questionnaires as a way to improve personality detection capabilities of LLMs in a CoT framework. They prompt the model to rate individual items from the questionnaire at each turn and leverage the historical rating results to determine the underlying personality trait.\n\nreasons_to_accept: This paper proposes the first use of item-based personality related questionnaires to establish personality traits exhibited in text. Their method works better than simple standard prompting.\n\nreasons_to_reject: CoT does not just guide LLMs through arbitrary reasoning steps, but through a definitive order of reasoning steps. Furthermore, the abstract nature of the personality detection task render the step-by-step nature of the chain-of-thought problem solving approach challenging even for humans.\nHow can high and low be defined for the Big 5 traits? In its original form, this is a regression task, not a classification one.\nThere are no statistical significance tests b/w PsyCot and Standard baseline, and PsyCoT and RoBERTa.\nThe best model for these tasks is a simple regression model - Park et al (mentioned in missing references). However, the paper does not compare to that model and neither does it compare to Lynn et al 2020, which is another strong performing model. It is hard to contextualize the results in this paper and pit them against what the community knows.\n\nquestions_for_the_authors: A: The Big 5 personality traits elicit outcomes in a dimensional form. When this method elicits the outcome in the most dimensional form, why do you reduce it back to 2 classes?\nB: Post order matters (missing ref: Nelson Liu et al., 2023) matters for these LLMs, given that some messages have more information than others for Personality estimation (missing ref: Lynn et al 2020). How do you account for bias introduced due to order of posts in your prompting setup?\nC: Missing row in results table: Item informed high/low classification score (What is the performance when you aggregate the scores for each item) D: 10 vs 44 item scale is expected to have a big drop. Performance of 20 items scale would be more interesting to see since the 20 item scale has shown convergent validity (ref: Table 1 in Park et al., 2015)\n\nmissing_references: The following papers are critically missing: Automatic Personality Assessment Through Social Media Language - Park et al VERY similar baseline - Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation - Ganesan et al\n\ntypos_grammar_style_and_presentation_improvements: Line 256: By-Produce -> Byproduct\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "s90JGpvPSG",
        "length": 219,
        "human_text": "paper_topic_and_main_contributions: The paper proposes PsyCoT, a chain-of-thought method specialized in personality detection tasks. The methodology uses chain-of-thought reasoning in the form of answering psychological questionnaires. The authors investigate the correlation between the trait score and the chosen personality and explore the patterns of score distributions in terms of different traits.\n\nreasons_to_accept: The methodology is interesting enough to understand the ability of LLMs for personality detection, and the explanations about details are clear. The authors analyze the correlation between scores and traits to predict some patterns between the personality traits and the LLM's behaviors. Overall, it is easy to follow and good enough to inspire researchers in a similar field to analyze or predict authors' personalities.\n\nreasons_to_reject: PsyCoT is basically coming from a popular method, Chain-of-Thought. It is weak to say the proposed method is innovative and original though the application to personality detection has been yet underexplored.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "21_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_21_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7066333333333334,
      "max_similarity": 0.7104,
      "avg_coverage": 0.6816333333333334,
      "max_coverage": 0.8182
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 821,
      "avg_human_length": 391.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 8,
      "weaknesses_count": 8,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "Mcn5t83XtZ",
        "similarity": 0.7104,
        "coverage": 0.56,
        "human_length": 448,
        "human_text": "paper_topic_and_main_contributions: The paper describes and evaluates an *NLP engineering experiment*. The authors try to predict the personality  of an author based on text written by the author. Their approach is to use a Large Language Model in an instructive way to rate an authors personality in a multi-turn dialog (following the chain-of-thoughts approach). The LLM, here GPT3.5, is gets first the text in question, some instructions on the answering format and finally multiple request about the authors personality. The rating request are based on psychological questionnaires (i.e., MBTI and Big 5). Finally, the LLM is asked to rate the text authors's personality on different dimensions.\nFrom my point of view, the paper provides two main contributions. First, it evaluates the usage LLMs in order to predict the personality of a person on the basis of text written by that person. Second, it compares two approaches of prompt engineering, one using single-turn instructions and another one using multi-turn instructions.\nFinally, both approaches as well as several other are compared in a formative evaluation.\n\nreasons_to_accept: - The paper describes in a reproducible way an interesting approach to predict aspect of an author's personality from text they have written.\n- The approach is validated in a feasible evaluation.\n- An ablation study shows the effectiveness go the multi-turn approach.\n- The work not only shows the technical possibility to predict personality (i.e., traits) using LLMs, but also the (relative) simplicity, which is in turn a warning for potential misuse.\n\nreasons_to_reject: - In the results section the best two (alternative!) approaches which works best for Kaggle are not show for Essays. Thus the results are not complete.\n- A specific discussion is missing and the limitations are very short. Obvious limitations like application to only one language (which is fine, but should be named) or the rather artificial datasets are not mentioned.\n- The description of the two used data sets is very short. I would aspect at least additional information on the mean length of the texts to get a feeling for the needed amount of data per text author.\n\nquestions_for_the_authors: Question A: Why do you not give the results for TrigNet and DDGCN in Table 1 for the Essays dataset, as they work best on the Kaggle dataset?\n\ntypos_grammar_style_and_presentation_improvements: - Line 118-121: The sentence sounds somehow incomplete.\n- Line 167: the ChatGPT -> remove the - Line 356: Implementation\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Nopuj64i7i",
        "similarity": 0.7099,
        "coverage": 0.6667,
        "human_length": 507,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a method to use item-based psychology questionnaires as a way to improve personality detection capabilities of LLMs in a CoT framework. They prompt the model to rate individual items from the questionnaire at each turn and leverage the historical rating results to determine the underlying personality trait.\n\nreasons_to_accept: This paper proposes the first use of item-based personality related questionnaires to establish personality traits exhibited in text. Their method works better than simple standard prompting.\n\nreasons_to_reject: CoT does not just guide LLMs through arbitrary reasoning steps, but through a definitive order of reasoning steps. Furthermore, the abstract nature of the personality detection task render the step-by-step nature of the chain-of-thought problem solving approach challenging even for humans.\nHow can high and low be defined for the Big 5 traits? In its original form, this is a regression task, not a classification one.\nThere are no statistical significance tests b/w PsyCot and Standard baseline, and PsyCoT and RoBERTa.\nThe best model for these tasks is a simple regression model - Park et al (mentioned in missing references). However, the paper does not compare to that model and neither does it compare to Lynn et al 2020, which is another strong performing model. It is hard to contextualize the results in this paper and pit them against what the community knows.\n\nquestions_for_the_authors: A: The Big 5 personality traits elicit outcomes in a dimensional form. When this method elicits the outcome in the most dimensional form, why do you reduce it back to 2 classes?\nB: Post order matters (missing ref: Nelson Liu et al., 2023) matters for these LLMs, given that some messages have more information than others for Personality estimation (missing ref: Lynn et al 2020). How do you account for bias introduced due to order of posts in your prompting setup?\nC: Missing row in results table: Item informed high/low classification score (What is the performance when you aggregate the scores for each item) D: 10 vs 44 item scale is expected to have a big drop. Performance of 20 items scale would be more interesting to see since the 20 item scale has shown convergent validity (ref: Table 1 in Park et al., 2015)\n\nmissing_references: The following papers are critically missing: Automatic Personality Assessment Through Social Media Language - Park et al VERY similar baseline - Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation - Ganesan et al\n\ntypos_grammar_style_and_presentation_improvements: Line 256: By-Produce -> Byproduct\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "s90JGpvPSG",
        "similarity": 0.6996,
        "coverage": 0.8182,
        "human_length": 219,
        "human_text": "paper_topic_and_main_contributions: The paper proposes PsyCoT, a chain-of-thought method specialized in personality detection tasks. The methodology uses chain-of-thought reasoning in the form of answering psychological questionnaires. The authors investigate the correlation between the trait score and the chosen personality and explore the patterns of score distributions in terms of different traits.\n\nreasons_to_accept: The methodology is interesting enough to understand the ability of LLMs for personality detection, and the explanations about details are clear. The authors analyze the correlation between scores and traits to predict some patterns between the personality traits and the LLM's behaviors. Overall, it is easy to follow and good enough to inspire researchers in a similar field to analyze or predict authors' personalities.\n\nreasons_to_reject: PsyCoT is basically coming from a popular method, Chain-of-Thought. It is weak to say the proposed method is innovative and original though the application to personality detection has been yet underexplored.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "Mcn5t83XtZ",
        "length": 448,
        "human_text": "paper_topic_and_main_contributions: The paper describes and evaluates an *NLP engineering experiment*. The authors try to predict the personality  of an author based on text written by the author. Their approach is to use a Large Language Model in an instructive way to rate an authors personality in a multi-turn dialog (following the chain-of-thoughts approach). The LLM, here GPT3.5, is gets first the text in question, some instructions on the answering format and finally multiple request about the authors personality. The rating request are based on psychological questionnaires (i.e., MBTI and Big 5). Finally, the LLM is asked to rate the text authors's personality on different dimensions.\nFrom my point of view, the paper provides two main contributions. First, it evaluates the usage LLMs in order to predict the personality of a person on the basis of text written by that person. Second, it compares two approaches of prompt engineering, one using single-turn instructions and another one using multi-turn instructions.\nFinally, both approaches as well as several other are compared in a formative evaluation.\n\nreasons_to_accept: - The paper describes in a reproducible way an interesting approach to predict aspect of an author's personality from text they have written.\n- The approach is validated in a feasible evaluation.\n- An ablation study shows the effectiveness go the multi-turn approach.\n- The work not only shows the technical possibility to predict personality (i.e., traits) using LLMs, but also the (relative) simplicity, which is in turn a warning for potential misuse.\n\nreasons_to_reject: - In the results section the best two (alternative!) approaches which works best for Kaggle are not show for Essays. Thus the results are not complete.\n- A specific discussion is missing and the limitations are very short. Obvious limitations like application to only one language (which is fine, but should be named) or the rather artificial datasets are not mentioned.\n- The description of the two used data sets is very short. I would aspect at least additional information on the mean length of the texts to get a feeling for the needed amount of data per text author.\n\nquestions_for_the_authors: Question A: Why do you not give the results for TrigNet and DDGCN in Table 1 for the Essays dataset, as they work best on the Kaggle dataset?\n\ntypos_grammar_style_and_presentation_improvements: - Line 118-121: The sentence sounds somehow incomplete.\n- Line 167: the ChatGPT -> remove the - Line 356: Implementation\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "Nopuj64i7i",
        "length": 507,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a method to use item-based psychology questionnaires as a way to improve personality detection capabilities of LLMs in a CoT framework. They prompt the model to rate individual items from the questionnaire at each turn and leverage the historical rating results to determine the underlying personality trait.\n\nreasons_to_accept: This paper proposes the first use of item-based personality related questionnaires to establish personality traits exhibited in text. Their method works better than simple standard prompting.\n\nreasons_to_reject: CoT does not just guide LLMs through arbitrary reasoning steps, but through a definitive order of reasoning steps. Furthermore, the abstract nature of the personality detection task render the step-by-step nature of the chain-of-thought problem solving approach challenging even for humans.\nHow can high and low be defined for the Big 5 traits? In its original form, this is a regression task, not a classification one.\nThere are no statistical significance tests b/w PsyCot and Standard baseline, and PsyCoT and RoBERTa.\nThe best model for these tasks is a simple regression model - Park et al (mentioned in missing references). However, the paper does not compare to that model and neither does it compare to Lynn et al 2020, which is another strong performing model. It is hard to contextualize the results in this paper and pit them against what the community knows.\n\nquestions_for_the_authors: A: The Big 5 personality traits elicit outcomes in a dimensional form. When this method elicits the outcome in the most dimensional form, why do you reduce it back to 2 classes?\nB: Post order matters (missing ref: Nelson Liu et al., 2023) matters for these LLMs, given that some messages have more information than others for Personality estimation (missing ref: Lynn et al 2020). How do you account for bias introduced due to order of posts in your prompting setup?\nC: Missing row in results table: Item informed high/low classification score (What is the performance when you aggregate the scores for each item) D: 10 vs 44 item scale is expected to have a big drop. Performance of 20 items scale would be more interesting to see since the 20 item scale has shown convergent validity (ref: Table 1 in Park et al., 2015)\n\nmissing_references: The following papers are critically missing: Automatic Personality Assessment Through Social Media Language - Park et al VERY similar baseline - Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation - Ganesan et al\n\ntypos_grammar_style_and_presentation_improvements: Line 256: By-Produce -> Byproduct\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "s90JGpvPSG",
        "length": 219,
        "human_text": "paper_topic_and_main_contributions: The paper proposes PsyCoT, a chain-of-thought method specialized in personality detection tasks. The methodology uses chain-of-thought reasoning in the form of answering psychological questionnaires. The authors investigate the correlation between the trait score and the chosen personality and explore the patterns of score distributions in terms of different traits.\n\nreasons_to_accept: The methodology is interesting enough to understand the ability of LLMs for personality detection, and the explanations about details are clear. The authors analyze the correlation between scores and traits to predict some patterns between the personality traits and the LLM's behaviors. Overall, it is easy to follow and good enough to inspire researchers in a similar field to analyze or predict authors' personalities.\n\nreasons_to_reject: PsyCoT is basically coming from a popular method, Chain-of-Thought. It is weak to say the proposed method is innovative and original though the application to personality detection has been yet underexplored.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "64_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_64_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7025,
      "max_similarity": 0.7443,
      "avg_coverage": 0.6077666666666667,
      "max_coverage": 0.8182
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 846,
      "avg_human_length": 449.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 9,
      "weaknesses_count": 13,
      "suggestions_count": 18
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "7W3TjPW5w5",
        "similarity": 0.7443,
        "coverage": 0.2778,
        "human_length": 928,
        "human_text": "paper_topic_and_main_contributions: The paper presents a new method called CQE for the extraction of mentions of quantity, unit, and concept in textual documents. CQE consists of a set of 61 syntax-based rules and a BERT-based classifier for unit disambiguation. The classifier is trained with examples automatically generated using ChatGPT. \nThe paper also presents NewsQuant, which is a new corpus manually annotated with quantity, unit, and concept mentions where quantity and units are normalized. \nCQE is compared to three other quantity extractors and GPT3. The experiments show that CQE outperforms them on NewsQuant.\n\nreasons_to_accept: The extraction of quantities, units, and the entity measured is a difficult problem that has many applications in science, technics, and the general domain. There are relatively few methods and corpora available. This paper describes a new work for the English language that will be made available on GitHub.  The method is compared to four state-of-the-art methods that it outperforms. The paper is globally clear and convincing. The experiments are detailed. The result analysis section and the A6 appendix provide relevant hypotheses and examples.\n\nreasons_to_reject: The notion of \u2018concept\u2019 is insufficiently described and formalized. Concepts are defined as \u201ceither properties being measured or entities that the value is referring to or is acting upon\u201d, which is vague. The CQE method is designed \u201cto capture as many concepts as possible.\u201d This has consequences for the manual annotation of the NewsQuant corpus where the inter-annotator agreement is low, and for the comparison of CQE and GPT3 (Table 5), which is based on a questionable reference.\nMore generally, a more formal definition would make the evaluation of the method and, furthermore, its comparison with other methods more robust. It would also broaden the scope of application. As a source of inspiration, the authors could look at SSN and SOSA that distinguish between the measured property and the entity (or part of entity) observed and spatiotemporal conditions.\nThe same remark applies to \u2018change\u2019. IllQ achieves lower performances for Value+change extraction. How much could it be due to a different understanding of what \u2018change\u2019 is? The definition of IllQ for the \u2018change\u2019 entity would be needed here.\nIt is standard to provide the annotators with written guidelines, along with the corpus distribution. It would improve the annotation quality and reusability.\nThe reader understands that the outputs of the methods are not directly comparable, and some post-processing is applied to unify them that add information to the direct output. This is not a correct procedure, whatever is the post-processing is. The comparison should be done on the minimal common subset instead of \u201cimproving\u201d the existing methods to mimic the CQE results.\nMore precisely, the comparison of units extracted by IllQ and Q3 and by CQE as described in section 4.4 Implementation seems unfair and must be clarified. The reader understands that the outputs of the three methods are not directly comparable, and some post-processing is applied to normalize and unify them. The paper must provide the performance score of the extraction of units of the three methods without alteration for a fair comparison. The post-processing needed for comparing the normalized extracted units must be detailed and show that no bias is introduced. The mention \u201cUnit normalization does not work for the majority of the times\u201d for IllQ and GPT3 should be replaced by \u201cUnit normalization not available\u201d to be consistent with Table 1 (line 10).\nAnother postprocessing is added to R-Txt model (Line 478-479) that adds extra annotations for all types of quantities. This makes R-Txt and CQE methods not comparable. The evaluation should have been done for the only quantities that R-Txt can predict.\n\nquestions_for_the_authors: A.\tPlease define \u201chelper tokens\u201d.\nB.\tCould you explain why the method computes the similarity between subclauses by comparing sequences instead of parsed trees? ( section 3.2.3) C.\tLine 366-367: \u201cthe conversion between metric units is not supported\u201d. Is the conversion of other units, such as volumes (ml, l) supported?\nD.\tWhy has Grobid-quantities method cited in the Related Work section (line 87) not been compared?\nE.\tAre units expressed by Greek letters handled by CQE?\n\ntypos_grammar_style_and_presentation_improvements: About the formalization of units and linked concepts, these three papers could be useful to look at.\nRijgersberg, H., Van Assem, M., & Top, J. (2013). Ontology of units of measure and related concepts. Semantic Web, 4(1), 3-13. \nJanowicz, K., Haller, A., Cox, S. J., Le Phuoc, D., & Lefran\u00e7ois, M. (2019). SOSA: A lightweight ontology for sensors, observations, samples, and actuators. Journal of Web Semantics, 56, 1-10.\nCompton, M., Barnaghi, P., Bermudez, L., Garcia-Castro, R., Corcho, O., Cox, S., ... & Taylor, K. (2012). The SSN ontology of the W3C semantic sensor network incubator group. Journal of Web Semantics, 17, 25-32.\nAbout the extraction of units, measures, and entities, the reference [Berrahou et al., 2017] may interesting to look at.\nBerrahou, S. L., Buche, P., Dibie, J., & Roche, M. (2017). Xart: Discovery of correlated arguments of n-ary relations in text. Expert Systems with Applications, 73, 115-124.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: NA\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "YT60pDkdPR",
        "similarity": 0.6673,
        "coverage": 0.7273,
        "human_length": 200,
        "human_text": "paper_topic_and_main_contributions: This paper describes an interesting solution for a seemly simple problem with a dedicated approach to extract quantities, units, modifiers, and concepts from textual sources.\n\nreasons_to_accept: This work is of a refreshing return to ingenious solutions to problems seemly simple, but that can elude state of the art solutions. The paper is well written and easy to follow. The developed work is sound and the originality resides in narrowing the application to a single problem and solving it with a high precision and recall compared to baselines.\n\nreasons_to_reject: The options and decisions taken could have been more explored and analyzed in greater detail.\n\nquestions_for_the_authors: No specific question to ask.\n\nmissing_references: As far as I could tell, the presented references are enough.\n\ntypos_grammar_style_and_presentation_improvements: As far as I could tell, there were no typos or grammar issues.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "H3WlCwjnpv",
        "similarity": 0.6959,
        "coverage": 0.8182,
        "human_length": 219,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel framework for quantity extraction from unstructured text, utilising dependency parsing and a dictionary of units. The key contributions include a novel methodology for detecting concepts associated with identified quantities and introducing a new benchmark dataset for evaluating quantity extraction methods.\n\nreasons_to_accept: The paper lays a strong mathematical foundation for the proposed methodology, ensuring its reproducibility. The experimental design and results are well-documented, enabling a clear understanding to the reader. Moreover, the introduction of a new benchmark dataset is a significant contribution to the field.\n\nreasons_to_reject: While the paper presents a sound framework, including a visual representation of the proposed methodology would have been beneficial.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "7W3TjPW5w5",
        "length": 928,
        "human_text": "paper_topic_and_main_contributions: The paper presents a new method called CQE for the extraction of mentions of quantity, unit, and concept in textual documents. CQE consists of a set of 61 syntax-based rules and a BERT-based classifier for unit disambiguation. The classifier is trained with examples automatically generated using ChatGPT. \nThe paper also presents NewsQuant, which is a new corpus manually annotated with quantity, unit, and concept mentions where quantity and units are normalized. \nCQE is compared to three other quantity extractors and GPT3. The experiments show that CQE outperforms them on NewsQuant.\n\nreasons_to_accept: The extraction of quantities, units, and the entity measured is a difficult problem that has many applications in science, technics, and the general domain. There are relatively few methods and corpora available. This paper describes a new work for the English language that will be made available on GitHub.  The method is compared to four state-of-the-art methods that it outperforms. The paper is globally clear and convincing. The experiments are detailed. The result analysis section and the A6 appendix provide relevant hypotheses and examples.\n\nreasons_to_reject: The notion of \u2018concept\u2019 is insufficiently described and formalized. Concepts are defined as \u201ceither properties being measured or entities that the value is referring to or is acting upon\u201d, which is vague. The CQE method is designed \u201cto capture as many concepts as possible.\u201d This has consequences for the manual annotation of the NewsQuant corpus where the inter-annotator agreement is low, and for the comparison of CQE and GPT3 (Table 5), which is based on a questionable reference.\nMore generally, a more formal definition would make the evaluation of the method and, furthermore, its comparison with other methods more robust. It would also broaden the scope of application. As a source of inspiration, the authors could look at SSN and SOSA that distinguish between the measured property and the entity (or part of entity) observed and spatiotemporal conditions.\nThe same remark applies to \u2018change\u2019. IllQ achieves lower performances for Value+change extraction. How much could it be due to a different understanding of what \u2018change\u2019 is? The definition of IllQ for the \u2018change\u2019 entity would be needed here.\nIt is standard to provide the annotators with written guidelines, along with the corpus distribution. It would improve the annotation quality and reusability.\nThe reader understands that the outputs of the methods are not directly comparable, and some post-processing is applied to unify them that add information to the direct output. This is not a correct procedure, whatever is the post-processing is. The comparison should be done on the minimal common subset instead of \u201cimproving\u201d the existing methods to mimic the CQE results.\nMore precisely, the comparison of units extracted by IllQ and Q3 and by CQE as described in section 4.4 Implementation seems unfair and must be clarified. The reader understands that the outputs of the three methods are not directly comparable, and some post-processing is applied to normalize and unify them. The paper must provide the performance score of the extraction of units of the three methods without alteration for a fair comparison. The post-processing needed for comparing the normalized extracted units must be detailed and show that no bias is introduced. The mention \u201cUnit normalization does not work for the majority of the times\u201d for IllQ and GPT3 should be replaced by \u201cUnit normalization not available\u201d to be consistent with Table 1 (line 10).\nAnother postprocessing is added to R-Txt model (Line 478-479) that adds extra annotations for all types of quantities. This makes R-Txt and CQE methods not comparable. The evaluation should have been done for the only quantities that R-Txt can predict.\n\nquestions_for_the_authors: A.\tPlease define \u201chelper tokens\u201d.\nB.\tCould you explain why the method computes the similarity between subclauses by comparing sequences instead of parsed trees? ( section 3.2.3) C.\tLine 366-367: \u201cthe conversion between metric units is not supported\u201d. Is the conversion of other units, such as volumes (ml, l) supported?\nD.\tWhy has Grobid-quantities method cited in the Related Work section (line 87) not been compared?\nE.\tAre units expressed by Greek letters handled by CQE?\n\ntypos_grammar_style_and_presentation_improvements: About the formalization of units and linked concepts, these three papers could be useful to look at.\nRijgersberg, H., Van Assem, M., & Top, J. (2013). Ontology of units of measure and related concepts. Semantic Web, 4(1), 3-13. \nJanowicz, K., Haller, A., Cox, S. J., Le Phuoc, D., & Lefran\u00e7ois, M. (2019). SOSA: A lightweight ontology for sensors, observations, samples, and actuators. Journal of Web Semantics, 56, 1-10.\nCompton, M., Barnaghi, P., Bermudez, L., Garcia-Castro, R., Corcho, O., Cox, S., ... & Taylor, K. (2012). The SSN ontology of the W3C semantic sensor network incubator group. Journal of Web Semantics, 17, 25-32.\nAbout the extraction of units, measures, and entities, the reference [Berrahou et al., 2017] may interesting to look at.\nBerrahou, S. L., Buche, P., Dibie, J., & Roche, M. (2017). Xart: Discovery of correlated arguments of n-ary relations in text. Expert Systems with Applications, 73, 115-124.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: NA\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "YT60pDkdPR",
        "length": 200,
        "human_text": "paper_topic_and_main_contributions: This paper describes an interesting solution for a seemly simple problem with a dedicated approach to extract quantities, units, modifiers, and concepts from textual sources.\n\nreasons_to_accept: This work is of a refreshing return to ingenious solutions to problems seemly simple, but that can elude state of the art solutions. The paper is well written and easy to follow. The developed work is sound and the originality resides in narrowing the application to a single problem and solving it with a high precision and recall compared to baselines.\n\nreasons_to_reject: The options and decisions taken could have been more explored and analyzed in greater detail.\n\nquestions_for_the_authors: No specific question to ask.\n\nmissing_references: As far as I could tell, the presented references are enough.\n\ntypos_grammar_style_and_presentation_improvements: As far as I could tell, there were no typos or grammar issues.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "H3WlCwjnpv",
        "length": 219,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel framework for quantity extraction from unstructured text, utilising dependency parsing and a dictionary of units. The key contributions include a novel methodology for detecting concepts associated with identified quantities and introducing a new benchmark dataset for evaluating quantity extraction methods.\n\nreasons_to_accept: The paper lays a strong mathematical foundation for the proposed methodology, ensuring its reproducibility. The experimental design and results are well-documented, enabling a clear understanding to the reader. Moreover, the introduction of a new benchmark dataset is a significant contribution to the field.\n\nreasons_to_reject: While the paper presents a sound framework, including a visual representation of the proposed methodology would have been beneficial.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "64_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_64_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7040333333333333,
      "max_similarity": 0.7462,
      "avg_coverage": 0.6077666666666667,
      "max_coverage": 0.8182
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 871,
      "avg_human_length": 449.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 9,
      "weaknesses_count": 13,
      "suggestions_count": 17
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "7W3TjPW5w5",
        "similarity": 0.7462,
        "coverage": 0.2778,
        "human_length": 928,
        "human_text": "paper_topic_and_main_contributions: The paper presents a new method called CQE for the extraction of mentions of quantity, unit, and concept in textual documents. CQE consists of a set of 61 syntax-based rules and a BERT-based classifier for unit disambiguation. The classifier is trained with examples automatically generated using ChatGPT. \nThe paper also presents NewsQuant, which is a new corpus manually annotated with quantity, unit, and concept mentions where quantity and units are normalized. \nCQE is compared to three other quantity extractors and GPT3. The experiments show that CQE outperforms them on NewsQuant.\n\nreasons_to_accept: The extraction of quantities, units, and the entity measured is a difficult problem that has many applications in science, technics, and the general domain. There are relatively few methods and corpora available. This paper describes a new work for the English language that will be made available on GitHub.  The method is compared to four state-of-the-art methods that it outperforms. The paper is globally clear and convincing. The experiments are detailed. The result analysis section and the A6 appendix provide relevant hypotheses and examples.\n\nreasons_to_reject: The notion of \u2018concept\u2019 is insufficiently described and formalized. Concepts are defined as \u201ceither properties being measured or entities that the value is referring to or is acting upon\u201d, which is vague. The CQE method is designed \u201cto capture as many concepts as possible.\u201d This has consequences for the manual annotation of the NewsQuant corpus where the inter-annotator agreement is low, and for the comparison of CQE and GPT3 (Table 5), which is based on a questionable reference.\nMore generally, a more formal definition would make the evaluation of the method and, furthermore, its comparison with other methods more robust. It would also broaden the scope of application. As a source of inspiration, the authors could look at SSN and SOSA that distinguish between the measured property and the entity (or part of entity) observed and spatiotemporal conditions.\nThe same remark applies to \u2018change\u2019. IllQ achieves lower performances for Value+change extraction. How much could it be due to a different understanding of what \u2018change\u2019 is? The definition of IllQ for the \u2018change\u2019 entity would be needed here.\nIt is standard to provide the annotators with written guidelines, along with the corpus distribution. It would improve the annotation quality and reusability.\nThe reader understands that the outputs of the methods are not directly comparable, and some post-processing is applied to unify them that add information to the direct output. This is not a correct procedure, whatever is the post-processing is. The comparison should be done on the minimal common subset instead of \u201cimproving\u201d the existing methods to mimic the CQE results.\nMore precisely, the comparison of units extracted by IllQ and Q3 and by CQE as described in section 4.4 Implementation seems unfair and must be clarified. The reader understands that the outputs of the three methods are not directly comparable, and some post-processing is applied to normalize and unify them. The paper must provide the performance score of the extraction of units of the three methods without alteration for a fair comparison. The post-processing needed for comparing the normalized extracted units must be detailed and show that no bias is introduced. The mention \u201cUnit normalization does not work for the majority of the times\u201d for IllQ and GPT3 should be replaced by \u201cUnit normalization not available\u201d to be consistent with Table 1 (line 10).\nAnother postprocessing is added to R-Txt model (Line 478-479) that adds extra annotations for all types of quantities. This makes R-Txt and CQE methods not comparable. The evaluation should have been done for the only quantities that R-Txt can predict.\n\nquestions_for_the_authors: A.\tPlease define \u201chelper tokens\u201d.\nB.\tCould you explain why the method computes the similarity between subclauses by comparing sequences instead of parsed trees? ( section 3.2.3) C.\tLine 366-367: \u201cthe conversion between metric units is not supported\u201d. Is the conversion of other units, such as volumes (ml, l) supported?\nD.\tWhy has Grobid-quantities method cited in the Related Work section (line 87) not been compared?\nE.\tAre units expressed by Greek letters handled by CQE?\n\ntypos_grammar_style_and_presentation_improvements: About the formalization of units and linked concepts, these three papers could be useful to look at.\nRijgersberg, H., Van Assem, M., & Top, J. (2013). Ontology of units of measure and related concepts. Semantic Web, 4(1), 3-13. \nJanowicz, K., Haller, A., Cox, S. J., Le Phuoc, D., & Lefran\u00e7ois, M. (2019). SOSA: A lightweight ontology for sensors, observations, samples, and actuators. Journal of Web Semantics, 56, 1-10.\nCompton, M., Barnaghi, P., Bermudez, L., Garcia-Castro, R., Corcho, O., Cox, S., ... & Taylor, K. (2012). The SSN ontology of the W3C semantic sensor network incubator group. Journal of Web Semantics, 17, 25-32.\nAbout the extraction of units, measures, and entities, the reference [Berrahou et al., 2017] may interesting to look at.\nBerrahou, S. L., Buche, P., Dibie, J., & Roche, M. (2017). Xart: Discovery of correlated arguments of n-ary relations in text. Expert Systems with Applications, 73, 115-124.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: NA\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "YT60pDkdPR",
        "similarity": 0.6688,
        "coverage": 0.7273,
        "human_length": 200,
        "human_text": "paper_topic_and_main_contributions: This paper describes an interesting solution for a seemly simple problem with a dedicated approach to extract quantities, units, modifiers, and concepts from textual sources.\n\nreasons_to_accept: This work is of a refreshing return to ingenious solutions to problems seemly simple, but that can elude state of the art solutions. The paper is well written and easy to follow. The developed work is sound and the originality resides in narrowing the application to a single problem and solving it with a high precision and recall compared to baselines.\n\nreasons_to_reject: The options and decisions taken could have been more explored and analyzed in greater detail.\n\nquestions_for_the_authors: No specific question to ask.\n\nmissing_references: As far as I could tell, the presented references are enough.\n\ntypos_grammar_style_and_presentation_improvements: As far as I could tell, there were no typos or grammar issues.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "H3WlCwjnpv",
        "similarity": 0.6971,
        "coverage": 0.8182,
        "human_length": 219,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel framework for quantity extraction from unstructured text, utilising dependency parsing and a dictionary of units. The key contributions include a novel methodology for detecting concepts associated with identified quantities and introducing a new benchmark dataset for evaluating quantity extraction methods.\n\nreasons_to_accept: The paper lays a strong mathematical foundation for the proposed methodology, ensuring its reproducibility. The experimental design and results are well-documented, enabling a clear understanding to the reader. Moreover, the introduction of a new benchmark dataset is a significant contribution to the field.\n\nreasons_to_reject: While the paper presents a sound framework, including a visual representation of the proposed methodology would have been beneficial.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "7W3TjPW5w5",
        "length": 928,
        "human_text": "paper_topic_and_main_contributions: The paper presents a new method called CQE for the extraction of mentions of quantity, unit, and concept in textual documents. CQE consists of a set of 61 syntax-based rules and a BERT-based classifier for unit disambiguation. The classifier is trained with examples automatically generated using ChatGPT. \nThe paper also presents NewsQuant, which is a new corpus manually annotated with quantity, unit, and concept mentions where quantity and units are normalized. \nCQE is compared to three other quantity extractors and GPT3. The experiments show that CQE outperforms them on NewsQuant.\n\nreasons_to_accept: The extraction of quantities, units, and the entity measured is a difficult problem that has many applications in science, technics, and the general domain. There are relatively few methods and corpora available. This paper describes a new work for the English language that will be made available on GitHub.  The method is compared to four state-of-the-art methods that it outperforms. The paper is globally clear and convincing. The experiments are detailed. The result analysis section and the A6 appendix provide relevant hypotheses and examples.\n\nreasons_to_reject: The notion of \u2018concept\u2019 is insufficiently described and formalized. Concepts are defined as \u201ceither properties being measured or entities that the value is referring to or is acting upon\u201d, which is vague. The CQE method is designed \u201cto capture as many concepts as possible.\u201d This has consequences for the manual annotation of the NewsQuant corpus where the inter-annotator agreement is low, and for the comparison of CQE and GPT3 (Table 5), which is based on a questionable reference.\nMore generally, a more formal definition would make the evaluation of the method and, furthermore, its comparison with other methods more robust. It would also broaden the scope of application. As a source of inspiration, the authors could look at SSN and SOSA that distinguish between the measured property and the entity (or part of entity) observed and spatiotemporal conditions.\nThe same remark applies to \u2018change\u2019. IllQ achieves lower performances for Value+change extraction. How much could it be due to a different understanding of what \u2018change\u2019 is? The definition of IllQ for the \u2018change\u2019 entity would be needed here.\nIt is standard to provide the annotators with written guidelines, along with the corpus distribution. It would improve the annotation quality and reusability.\nThe reader understands that the outputs of the methods are not directly comparable, and some post-processing is applied to unify them that add information to the direct output. This is not a correct procedure, whatever is the post-processing is. The comparison should be done on the minimal common subset instead of \u201cimproving\u201d the existing methods to mimic the CQE results.\nMore precisely, the comparison of units extracted by IllQ and Q3 and by CQE as described in section 4.4 Implementation seems unfair and must be clarified. The reader understands that the outputs of the three methods are not directly comparable, and some post-processing is applied to normalize and unify them. The paper must provide the performance score of the extraction of units of the three methods without alteration for a fair comparison. The post-processing needed for comparing the normalized extracted units must be detailed and show that no bias is introduced. The mention \u201cUnit normalization does not work for the majority of the times\u201d for IllQ and GPT3 should be replaced by \u201cUnit normalization not available\u201d to be consistent with Table 1 (line 10).\nAnother postprocessing is added to R-Txt model (Line 478-479) that adds extra annotations for all types of quantities. This makes R-Txt and CQE methods not comparable. The evaluation should have been done for the only quantities that R-Txt can predict.\n\nquestions_for_the_authors: A.\tPlease define \u201chelper tokens\u201d.\nB.\tCould you explain why the method computes the similarity between subclauses by comparing sequences instead of parsed trees? ( section 3.2.3) C.\tLine 366-367: \u201cthe conversion between metric units is not supported\u201d. Is the conversion of other units, such as volumes (ml, l) supported?\nD.\tWhy has Grobid-quantities method cited in the Related Work section (line 87) not been compared?\nE.\tAre units expressed by Greek letters handled by CQE?\n\ntypos_grammar_style_and_presentation_improvements: About the formalization of units and linked concepts, these three papers could be useful to look at.\nRijgersberg, H., Van Assem, M., & Top, J. (2013). Ontology of units of measure and related concepts. Semantic Web, 4(1), 3-13. \nJanowicz, K., Haller, A., Cox, S. J., Le Phuoc, D., & Lefran\u00e7ois, M. (2019). SOSA: A lightweight ontology for sensors, observations, samples, and actuators. Journal of Web Semantics, 56, 1-10.\nCompton, M., Barnaghi, P., Bermudez, L., Garcia-Castro, R., Corcho, O., Cox, S., ... & Taylor, K. (2012). The SSN ontology of the W3C semantic sensor network incubator group. Journal of Web Semantics, 17, 25-32.\nAbout the extraction of units, measures, and entities, the reference [Berrahou et al., 2017] may interesting to look at.\nBerrahou, S. L., Buche, P., Dibie, J., & Roche, M. (2017). Xart: Discovery of correlated arguments of n-ary relations in text. Expert Systems with Applications, 73, 115-124.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: NA\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "YT60pDkdPR",
        "length": 200,
        "human_text": "paper_topic_and_main_contributions: This paper describes an interesting solution for a seemly simple problem with a dedicated approach to extract quantities, units, modifiers, and concepts from textual sources.\n\nreasons_to_accept: This work is of a refreshing return to ingenious solutions to problems seemly simple, but that can elude state of the art solutions. The paper is well written and easy to follow. The developed work is sound and the originality resides in narrowing the application to a single problem and solving it with a high precision and recall compared to baselines.\n\nreasons_to_reject: The options and decisions taken could have been more explored and analyzed in greater detail.\n\nquestions_for_the_authors: No specific question to ask.\n\nmissing_references: As far as I could tell, the presented references are enough.\n\ntypos_grammar_style_and_presentation_improvements: As far as I could tell, there were no typos or grammar issues.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "H3WlCwjnpv",
        "length": 219,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel framework for quantity extraction from unstructured text, utilising dependency parsing and a dictionary of units. The key contributions include a novel methodology for detecting concepts associated with identified quantities and introducing a new benchmark dataset for evaluating quantity extraction methods.\n\nreasons_to_accept: The paper lays a strong mathematical foundation for the proposed methodology, ensuring its reproducibility. The experimental design and results are well-documented, enabling a clear understanding to the reader. Moreover, the introduction of a new benchmark dataset is a significant contribution to the field.\n\nreasons_to_reject: While the paper presents a sound framework, including a visual representation of the proposed methodology would have been beneficial.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "41_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_41_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7307666666666668,
      "max_similarity": 0.7449,
      "avg_coverage": 0.528,
      "max_coverage": 0.5455
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 557,
      "avg_human_length": 354.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 7,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "vEZrAdu2Ds",
        "similarity": 0.7449,
        "coverage": 0.5385,
        "human_length": 529,
        "human_text": "paper_topic_and_main_contributions: This submission introduces an audio alignment for an argumentation corpus. The authors use the aligned audio for experimenting with non-textual features for argument labeling.  They find that using audio features improves segmentation for text-based classification, yielding (slightly) better annotations than when using text-based segmentation.\nThe two contributions (resources & experiments) are both interesting, but each of them lacking certain aspects.\nFor the resources, I would have liked to see a more detailed description, for example:  - how many speakers are there? how long are the continuous segments per speaker (if you have speaker change)? \n - is the speaker identity annotated in the data? \n - how did you deal with incorrect alignments from the automatic alignment process? For resource papers, it is always important to me to give information about the data gathering process, because this information cannot be recovered later on by other researchers (in contrast to e.g. new experiments). \nSome of this information can be found in the actual data, but it is in my opinion still good to directly report it.\nFor the experiments, there are two aspects:  - the authors report accuracy and F1 score.  Their baseline model has the highest accuracy, but only the F1 score is discussed in the paper (where their own model has higher scores). It is not clear to me why a metric is reported but then completely ignored. As the accuracy paints a different picture than F1, it would be good to at least have a succinct explanation for this decision  - it is not clear to me why the authors chose to report the accuracy of a pure text-based classifier and a pure audio-based classifier but not a classifier using both as input. I am not claiming that it needs to be in the paper but using all available features is probably what most people think about first and I would add a short explanation (could very well be \"we are not interested in it\", but better formulated) I wrote these topics here instead of in \"reasons to reject\" because these are aspects where I would like to see more but at the same recognize that this is a focused short paper. If anything, it seems to me that the page restrictions are a bit too small for this content; I am e.g. also missing a more in-depth description of the classifiers you used in your experiments. Other than that, I enjoyed reading this submission.\n\nreasons_to_accept: - Provides a valuable resource  - has an interesting experiment set, given the space restrictions\n\nreasons_to_reject: - the classification systems used in the experiments are not described, as far as I can see. There is only Fig 1 showing the data flow.\n\nquestions_for_the_authors: A) Why did you not discuss the accuracy / why do think that F1 is the relevant metric?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "JbAUrnqFYZ",
        "similarity": 0.7074,
        "coverage": 0.5455,
        "human_length": 152,
        "human_text": "paper_topic_and_main_contributions: The papers presents a novel corpus of *spoken* argumentation from argument mining. It also presents results from an experiment using this corpus and showing that including speech features improves the predictive performance.\n\nreasons_to_accept: The paper presents an interesting and useful corpus. It is well-written and overall it looks sound in technical terms.\n\nreasons_to_reject: No reason to reject.\n\nquestions_for_the_authors: I enjoyed reading the paper very much. I have no question for the authors. I apologize to the authors if this is not valuable feedback.\n\nmissing_references: None\n\ntypos_grammar_style_and_presentation_improvements: None\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "MHxgT9MA4d",
        "similarity": 0.74,
        "coverage": 0.5,
        "human_length": 381,
        "human_text": "paper_topic_and_main_contributions: This paper proposes VivesDebate-Speech, which augments the VivesDebate corpus with spoken argumentation in an attempt to leverage audio features for argument mining tasks. VivesDebate-Speech represents a significant leap forward for the research community in terms of size and versatility. Compared to the few previously available audio-based argumentative corpora, VivesDebate-Speech provides more than 12 hours of spoken argumentation, which doubles the size of the existing largest audio-based corpus. The paper also highlights the advantages of integrating audio information into the argumentation pipeline, and provides a baseline for future research in this area The authors point out that human argumentation presents a linguistically heterogeneous nature that requires careful investigation, which makes it reasonable to incorporate audio features into argumentative corpora. The creation of this corpus represents an important contribution to the intersection of speech processing and argument mining communities.\n\nreasons_to_accept: 1) The creation of a comprehensive and versatile corpus of spoken argumentation, which provides a valuable resource for researchers in the field of audio-based argument mining, as well as the potential for new insights and advancements in this field.\n2) This paper highlights the advantages of integrating audio information into the argumentation pipeline, and provides a baseline for future research in this area.\n\nreasons_to_reject: 1) The lack of comprehensive results from experiments performed using VivesDebate-Speech, such as using models of different structures and of various sizes.\n\nquestions_for_the_authors: 1) How do you deal with possible noise in audio, for example, mispronunciation or repeated words?\n2) How does VivesDebate-Speech compare to other publicly available resources for audio-based argument mining, and what are the unique contributions of this corpus besides its size?\n3) Why does the incorporation of audio features hurt the accuracy compared with E2E BIO (Table 2)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "vEZrAdu2Ds",
        "length": 529,
        "human_text": "paper_topic_and_main_contributions: This submission introduces an audio alignment for an argumentation corpus. The authors use the aligned audio for experimenting with non-textual features for argument labeling.  They find that using audio features improves segmentation for text-based classification, yielding (slightly) better annotations than when using text-based segmentation.\nThe two contributions (resources & experiments) are both interesting, but each of them lacking certain aspects.\nFor the resources, I would have liked to see a more detailed description, for example:  - how many speakers are there? how long are the continuous segments per speaker (if you have speaker change)? \n - is the speaker identity annotated in the data? \n - how did you deal with incorrect alignments from the automatic alignment process? For resource papers, it is always important to me to give information about the data gathering process, because this information cannot be recovered later on by other researchers (in contrast to e.g. new experiments). \nSome of this information can be found in the actual data, but it is in my opinion still good to directly report it.\nFor the experiments, there are two aspects:  - the authors report accuracy and F1 score.  Their baseline model has the highest accuracy, but only the F1 score is discussed in the paper (where their own model has higher scores). It is not clear to me why a metric is reported but then completely ignored. As the accuracy paints a different picture than F1, it would be good to at least have a succinct explanation for this decision  - it is not clear to me why the authors chose to report the accuracy of a pure text-based classifier and a pure audio-based classifier but not a classifier using both as input. I am not claiming that it needs to be in the paper but using all available features is probably what most people think about first and I would add a short explanation (could very well be \"we are not interested in it\", but better formulated) I wrote these topics here instead of in \"reasons to reject\" because these are aspects where I would like to see more but at the same recognize that this is a focused short paper. If anything, it seems to me that the page restrictions are a bit too small for this content; I am e.g. also missing a more in-depth description of the classifiers you used in your experiments. Other than that, I enjoyed reading this submission.\n\nreasons_to_accept: - Provides a valuable resource  - has an interesting experiment set, given the space restrictions\n\nreasons_to_reject: - the classification systems used in the experiments are not described, as far as I can see. There is only Fig 1 showing the data flow.\n\nquestions_for_the_authors: A) Why did you not discuss the accuracy / why do think that F1 is the relevant metric?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "JbAUrnqFYZ",
        "length": 152,
        "human_text": "paper_topic_and_main_contributions: The papers presents a novel corpus of *spoken* argumentation from argument mining. It also presents results from an experiment using this corpus and showing that including speech features improves the predictive performance.\n\nreasons_to_accept: The paper presents an interesting and useful corpus. It is well-written and overall it looks sound in technical terms.\n\nreasons_to_reject: No reason to reject.\n\nquestions_for_the_authors: I enjoyed reading the paper very much. I have no question for the authors. I apologize to the authors if this is not valuable feedback.\n\nmissing_references: None\n\ntypos_grammar_style_and_presentation_improvements: None\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "MHxgT9MA4d",
        "length": 381,
        "human_text": "paper_topic_and_main_contributions: This paper proposes VivesDebate-Speech, which augments the VivesDebate corpus with spoken argumentation in an attempt to leverage audio features for argument mining tasks. VivesDebate-Speech represents a significant leap forward for the research community in terms of size and versatility. Compared to the few previously available audio-based argumentative corpora, VivesDebate-Speech provides more than 12 hours of spoken argumentation, which doubles the size of the existing largest audio-based corpus. The paper also highlights the advantages of integrating audio information into the argumentation pipeline, and provides a baseline for future research in this area The authors point out that human argumentation presents a linguistically heterogeneous nature that requires careful investigation, which makes it reasonable to incorporate audio features into argumentative corpora. The creation of this corpus represents an important contribution to the intersection of speech processing and argument mining communities.\n\nreasons_to_accept: 1) The creation of a comprehensive and versatile corpus of spoken argumentation, which provides a valuable resource for researchers in the field of audio-based argument mining, as well as the potential for new insights and advancements in this field.\n2) This paper highlights the advantages of integrating audio information into the argumentation pipeline, and provides a baseline for future research in this area.\n\nreasons_to_reject: 1) The lack of comprehensive results from experiments performed using VivesDebate-Speech, such as using models of different structures and of various sizes.\n\nquestions_for_the_authors: 1) How do you deal with possible noise in audio, for example, mispronunciation or repeated words?\n2) How does VivesDebate-Speech compare to other publicly available resources for audio-based argument mining, and what are the unique contributions of this corpus besides its size?\n3) Why does the incorporation of audio features hurt the accuracy compared with E2E BIO (Table 2)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "41_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_41_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7256666666666667,
      "max_similarity": 0.7392,
      "avg_coverage": 0.5919333333333333,
      "max_coverage": 0.6364
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 457,
      "avg_human_length": 354.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 7,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "vEZrAdu2Ds",
        "similarity": 0.7392,
        "coverage": 0.5769,
        "human_length": 529,
        "human_text": "paper_topic_and_main_contributions: This submission introduces an audio alignment for an argumentation corpus. The authors use the aligned audio for experimenting with non-textual features for argument labeling.  They find that using audio features improves segmentation for text-based classification, yielding (slightly) better annotations than when using text-based segmentation.\nThe two contributions (resources & experiments) are both interesting, but each of them lacking certain aspects.\nFor the resources, I would have liked to see a more detailed description, for example:  - how many speakers are there? how long are the continuous segments per speaker (if you have speaker change)? \n - is the speaker identity annotated in the data? \n - how did you deal with incorrect alignments from the automatic alignment process? For resource papers, it is always important to me to give information about the data gathering process, because this information cannot be recovered later on by other researchers (in contrast to e.g. new experiments). \nSome of this information can be found in the actual data, but it is in my opinion still good to directly report it.\nFor the experiments, there are two aspects:  - the authors report accuracy and F1 score.  Their baseline model has the highest accuracy, but only the F1 score is discussed in the paper (where their own model has higher scores). It is not clear to me why a metric is reported but then completely ignored. As the accuracy paints a different picture than F1, it would be good to at least have a succinct explanation for this decision  - it is not clear to me why the authors chose to report the accuracy of a pure text-based classifier and a pure audio-based classifier but not a classifier using both as input. I am not claiming that it needs to be in the paper but using all available features is probably what most people think about first and I would add a short explanation (could very well be \"we are not interested in it\", but better formulated) I wrote these topics here instead of in \"reasons to reject\" because these are aspects where I would like to see more but at the same recognize that this is a focused short paper. If anything, it seems to me that the page restrictions are a bit too small for this content; I am e.g. also missing a more in-depth description of the classifiers you used in your experiments. Other than that, I enjoyed reading this submission.\n\nreasons_to_accept: - Provides a valuable resource  - has an interesting experiment set, given the space restrictions\n\nreasons_to_reject: - the classification systems used in the experiments are not described, as far as I can see. There is only Fig 1 showing the data flow.\n\nquestions_for_the_authors: A) Why did you not discuss the accuracy / why do think that F1 is the relevant metric?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "JbAUrnqFYZ",
        "similarity": 0.7027,
        "coverage": 0.6364,
        "human_length": 152,
        "human_text": "paper_topic_and_main_contributions: The papers presents a novel corpus of *spoken* argumentation from argument mining. It also presents results from an experiment using this corpus and showing that including speech features improves the predictive performance.\n\nreasons_to_accept: The paper presents an interesting and useful corpus. It is well-written and overall it looks sound in technical terms.\n\nreasons_to_reject: No reason to reject.\n\nquestions_for_the_authors: I enjoyed reading the paper very much. I have no question for the authors. I apologize to the authors if this is not valuable feedback.\n\nmissing_references: None\n\ntypos_grammar_style_and_presentation_improvements: None\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "MHxgT9MA4d",
        "similarity": 0.7351,
        "coverage": 0.5625,
        "human_length": 381,
        "human_text": "paper_topic_and_main_contributions: This paper proposes VivesDebate-Speech, which augments the VivesDebate corpus with spoken argumentation in an attempt to leverage audio features for argument mining tasks. VivesDebate-Speech represents a significant leap forward for the research community in terms of size and versatility. Compared to the few previously available audio-based argumentative corpora, VivesDebate-Speech provides more than 12 hours of spoken argumentation, which doubles the size of the existing largest audio-based corpus. The paper also highlights the advantages of integrating audio information into the argumentation pipeline, and provides a baseline for future research in this area The authors point out that human argumentation presents a linguistically heterogeneous nature that requires careful investigation, which makes it reasonable to incorporate audio features into argumentative corpora. The creation of this corpus represents an important contribution to the intersection of speech processing and argument mining communities.\n\nreasons_to_accept: 1) The creation of a comprehensive and versatile corpus of spoken argumentation, which provides a valuable resource for researchers in the field of audio-based argument mining, as well as the potential for new insights and advancements in this field.\n2) This paper highlights the advantages of integrating audio information into the argumentation pipeline, and provides a baseline for future research in this area.\n\nreasons_to_reject: 1) The lack of comprehensive results from experiments performed using VivesDebate-Speech, such as using models of different structures and of various sizes.\n\nquestions_for_the_authors: 1) How do you deal with possible noise in audio, for example, mispronunciation or repeated words?\n2) How does VivesDebate-Speech compare to other publicly available resources for audio-based argument mining, and what are the unique contributions of this corpus besides its size?\n3) Why does the incorporation of audio features hurt the accuracy compared with E2E BIO (Table 2)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "vEZrAdu2Ds",
        "length": 529,
        "human_text": "paper_topic_and_main_contributions: This submission introduces an audio alignment for an argumentation corpus. The authors use the aligned audio for experimenting with non-textual features for argument labeling.  They find that using audio features improves segmentation for text-based classification, yielding (slightly) better annotations than when using text-based segmentation.\nThe two contributions (resources & experiments) are both interesting, but each of them lacking certain aspects.\nFor the resources, I would have liked to see a more detailed description, for example:  - how many speakers are there? how long are the continuous segments per speaker (if you have speaker change)? \n - is the speaker identity annotated in the data? \n - how did you deal with incorrect alignments from the automatic alignment process? For resource papers, it is always important to me to give information about the data gathering process, because this information cannot be recovered later on by other researchers (in contrast to e.g. new experiments). \nSome of this information can be found in the actual data, but it is in my opinion still good to directly report it.\nFor the experiments, there are two aspects:  - the authors report accuracy and F1 score.  Their baseline model has the highest accuracy, but only the F1 score is discussed in the paper (where their own model has higher scores). It is not clear to me why a metric is reported but then completely ignored. As the accuracy paints a different picture than F1, it would be good to at least have a succinct explanation for this decision  - it is not clear to me why the authors chose to report the accuracy of a pure text-based classifier and a pure audio-based classifier but not a classifier using both as input. I am not claiming that it needs to be in the paper but using all available features is probably what most people think about first and I would add a short explanation (could very well be \"we are not interested in it\", but better formulated) I wrote these topics here instead of in \"reasons to reject\" because these are aspects where I would like to see more but at the same recognize that this is a focused short paper. If anything, it seems to me that the page restrictions are a bit too small for this content; I am e.g. also missing a more in-depth description of the classifiers you used in your experiments. Other than that, I enjoyed reading this submission.\n\nreasons_to_accept: - Provides a valuable resource  - has an interesting experiment set, given the space restrictions\n\nreasons_to_reject: - the classification systems used in the experiments are not described, as far as I can see. There is only Fig 1 showing the data flow.\n\nquestions_for_the_authors: A) Why did you not discuss the accuracy / why do think that F1 is the relevant metric?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "JbAUrnqFYZ",
        "length": 152,
        "human_text": "paper_topic_and_main_contributions: The papers presents a novel corpus of *spoken* argumentation from argument mining. It also presents results from an experiment using this corpus and showing that including speech features improves the predictive performance.\n\nreasons_to_accept: The paper presents an interesting and useful corpus. It is well-written and overall it looks sound in technical terms.\n\nreasons_to_reject: No reason to reject.\n\nquestions_for_the_authors: I enjoyed reading the paper very much. I have no question for the authors. I apologize to the authors if this is not valuable feedback.\n\nmissing_references: None\n\ntypos_grammar_style_and_presentation_improvements: None\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "MHxgT9MA4d",
        "length": 381,
        "human_text": "paper_topic_and_main_contributions: This paper proposes VivesDebate-Speech, which augments the VivesDebate corpus with spoken argumentation in an attempt to leverage audio features for argument mining tasks. VivesDebate-Speech represents a significant leap forward for the research community in terms of size and versatility. Compared to the few previously available audio-based argumentative corpora, VivesDebate-Speech provides more than 12 hours of spoken argumentation, which doubles the size of the existing largest audio-based corpus. The paper also highlights the advantages of integrating audio information into the argumentation pipeline, and provides a baseline for future research in this area The authors point out that human argumentation presents a linguistically heterogeneous nature that requires careful investigation, which makes it reasonable to incorporate audio features into argumentative corpora. The creation of this corpus represents an important contribution to the intersection of speech processing and argument mining communities.\n\nreasons_to_accept: 1) The creation of a comprehensive and versatile corpus of spoken argumentation, which provides a valuable resource for researchers in the field of audio-based argument mining, as well as the potential for new insights and advancements in this field.\n2) This paper highlights the advantages of integrating audio information into the argumentation pipeline, and provides a baseline for future research in this area.\n\nreasons_to_reject: 1) The lack of comprehensive results from experiments performed using VivesDebate-Speech, such as using models of different structures and of various sizes.\n\nquestions_for_the_authors: 1) How do you deal with possible noise in audio, for example, mispronunciation or repeated words?\n2) How does VivesDebate-Speech compare to other publicly available resources for audio-based argument mining, and what are the unique contributions of this corpus besides its size?\n3) Why does the incorporation of audio features hurt the accuracy compared with E2E BIO (Table 2)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "2_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_2_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7357999999999999,
      "max_similarity": 0.7439,
      "avg_coverage": 0.2874333333333333,
      "max_coverage": 0.3571
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 275,
      "avg_human_length": 667.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 5,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "rfX6ne8ne4",
        "similarity": 0.7432,
        "coverage": 0.1852,
        "human_length": 920,
        "human_text": "paper_topic_and_main_contributions: The paper investigates the effect of co-occurrence statistics on the ability of large language models to correctly answer simple factual questions (of the subject-relation-object form). The paper specifically checks whether simple co-occurrences between the subject and the object in the pretraining data, can lead the models to incorrectly answer factual questions where the co-occurrence diverges from the correct answer. As it is difficult to check this causal relation by direct manipulation (given the enormous costs of pretraining large language models), a correlation study was conducted. Results show correlation between the co-occurrence statistics of a triplet and the ability of the model to answer correctly questions on it. \nThe paper further explores mitigation strategies to combat this bias. Two approaches are explored: debiased finetuning and knowledge editing. The former approach presents limited gains. Knowledge editing does show promise, but may be restricted as it requires weeding out the incorrect facts one by one (if I understand correctly). It may also cause unintended changes to unrelated facts.\n\nreasons_to_accept: -- The paper addresses an important and timely topic, namely the ability of LLMs to act as knowledge bases.\n-- The related work section is very elaborate and provides insight into the field at hand.\n-- The arguments of the paper are well-presented, and the writing is generally clear.\n\nreasons_to_reject: -- I am somewhat confused as to the exact claim the paper is making. While the results clearly show a correlation between the co-occurrence statistics of a triplet and the performance of the model on it, it is not clear to me whether this in fact proves that there is a bias where such simple surface statistics push the language model astray from making the right prediction. What it does show is that questions where the degree of co-occurrence is smaller are more difficult for the model. The introduction reads \u201cin which frequently co-occurred words are preferred over the correct answer.\u201d I could not see how the experiments directly make this point. Simple correlation seems to me insufficient in this case, since making mistakes with little co-occurrence doesn\u2019t mean that there is a different option with higher co-occurrence.\nIn order to show that the behavior is biased, I would expect the paper to shows that the surface statistics interfere in some sense with the prediction of the model, in a way that would make it predict such answer even when it is not true. For example, I would have expected the paper to examine questions which we would expect (based on their prevalence in the training data) the model to answer correctly, and show that in these cases it tends to make more errors where there is a strong collocation and that the mistakes is towards the collocating words. If the paper indeed makes this kind of more subtle claim and I have missed it, I would welcome a response from the authors on this matter. Thank you. \nFollowing rebuttal: the results you have posted are helpful and address this comment. Please include them in the next version.\n-- The results of the attempts to mitigate the bias are not very strong. I should say that I do not see it in itself as grounds for rejection.\n-- Some important presentational details are not sufficiently clear (see below).\n\nquestions_for_the_authors: -- I would appreciate your response to my first point above.\n-- Was there any attempt to look at stronger, proprietary models? I'm asking because they are much stronger and may exhibit diff behavior.\n-- The point about memorization is unclear to me. Why does co-occurrence imply memorization? on the contrary, if the model \"saw\" the correct answer and was overridden by co-occurrence this is the opposite of memorization.\n-- Why is knowledge editing a mitigation strategy. Obviously wrong knowledge should be corrected, but this is independent of the cause of the mistake, which is the subject of this paper, isn't it?\n\nmissing_references: --\n\ntypos_grammar_style_and_presentation_improvements: Local comments: -- What was claimed exactly in previous work with respect to the discussed bias and how does that differ from the work here?\n-- l. 136: you write \"Our work is the 136 first to investigate the effects of finetuning on the 137 correlation between term frequency statistics and 138 factual knowledge of LLMs.\" This was not sufficiently clear to me:  I thought your main claim is about the relation between these stats in pretraining and in model behavior. This should be made clearer. Otherwise, a well written previous work section.\n-- Section 4.2: Why not use the more standard names then like marginal probability, joint probability and PMI?  -- Figures 2b, 3: The graphs do not show much in my opinion. Could they be explained in a sentence in the text? what does the figure here contribute?\n-- Figure 5 (and in other places in the paper): can you also compute correlation w/o binning? what does that turn out to be?\n-- Section 6.1: a more formal definition of the filtering method should be given.\nGrammar: -- l. 442: performances s.b. performance -- l. 501: changes s.b. change\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "EIKFZmuV6r",
        "similarity": 0.7439,
        "coverage": 0.32,
        "human_length": 577,
        "human_text": "paper_topic_and_main_contributions: The paper argues that large language models suffer from the so-called 'co-occurrence bias', i.e. when asked a factual question they tend to assign higher probabilities to words that have higher co-occurrence statistics with the query instead of the correct answer. The authors show that this bias persists in models of different sizes and cannot be effectively removed using fine-tuning, but can be partially remedied with knowledge editing. In order to prove their point, the authors introduce a series of term-frequency baselines, two of which are co-occurrence statistics. They show that these statistics can be used as a shortcut feature (in some settings, selecting the most frequently co-occurrent word results in 60% accuracy on the test set) and explain the actual behaviour of the models to a large extent. The paper explores two mitigation strategies: fine-tuning on a debiased version of the training dataset and rank-one model editing (ROME). While fine-tuning is not particularly effective, ROME shows promising results on frequent relation-like facts but does not really help with rarer ones.\n\nreasons_to_accept: In my opinion, the most interesting aspect of the paper are the frequency baselines introduced to analyse the structure of the training dataset and to explain the behaviour of the models. They bring the co-occurrence/frequency bias influencing the behaviour of the models to fore, and the analysis is convincing.\n\nreasons_to_reject: The main point of the paper -- that LLMs suffer from co-occurrency bias -- is not particularly new. Other papers investigating this issue are mentioned in the Related Work section. The authors claim that their work is the fist \"to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs\" (ll. 136--139), but there is no discussion of why fine-tuning should help at all, and in the end it does not, which amounts to a weak negative result. The section on mitigating occupies less than 1.5 pages and does not contain any methodological insights.\n\nquestions_for_the_authors: In the discussion of fine-tuning in lines 347--348, it is said that \"the models may learn appropriate cadidate sets during finetuning.\" Does this mean that there is some amount of knowledge leakage in fine-tuning and that the testing set-up is slightly different from the zero-shot setting?\nIn Figure 6, we see that the accuracy of the models' outputs _improves_ when the conditional probability of the object given the subject goes down from around 1/64 to 0. This goes against the general trend of the positive association between conditional probability and accuracy. Is there any interpretation for this?\n\ntypos_grammar_style_and_presentation_improvements: Some of the statements in the paper look confusing. In lines 87--89, the authors point out that \"relying heavily on co-occurrence is not appropriate for understanding the accurate meaning behind words.\" This is probably true, but this is all we have when training LLMs, and the paper does not propose a way out. Similarly, in lines 173--174, the authors state that \"relying on co-occurrence implies simple memorization.\" Again, the relationship between co-occurrence and memorization is more complicated, and equating one with the other seems erroneous.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "6J9oy63MjV",
        "similarity": 0.7203,
        "coverage": 0.3571,
        "human_length": 504,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a framework to probe the shortcuts in LLMs. The framework firsts test the accuracy of LLM on factual knowledge dataset like LAMA. Then the framework computes the correlation of the subject and the object in the pretraining corpus. Results on GPT models (from 125M to 6B) show that, as the co-occurrence of subject and object decreases, the accuracy also decreases. Larger models (GPT-3.5 of 175B) also suffer from the problem. The authors also find that simple baseline based on co-occurrence is sufficient to surpass the performance of GPT-J 6B. Finally, the authors also give solution to mitigate the shortcut problem.\n\nreasons_to_accept: -\tThe paper contains detailed analysis of shortcut problem regarding token co-occurrence. \n-\tThe paper investigates an important area of verifying the factual knowledge of LLMs. \n-\tThe paper is well-written, and is free of significant presentation issues. \n-\tAlong with the identification of the problem, the paper also proposes to mitigate the problem.\n\nreasons_to_reject: -\tOne more experiment should be done to verify the claim that \"answers with higher co-occurrence are more likely to be generated\": The authors should count in each question, whether the model's generated answer has a high count in the pretraining corpus. Currently there is only a table (Table 1) showing similar results, i.e., the wrong answers have a relatively lower count in the pretraining corpus. However, quantitative results over the whole dataset should be given. \n-\tThere are existing work probing the shortcut learning problem of language models. The authors should elaborate more on the work to claim that they are the first to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs. For example: [2].\n\nquestions_for_the_authors: -\tThe paper focuses only on token-level shortcuts. Can the findings be generalized to other kinds of shortcuts, such as word-overlap (Right for the wrong reasons: Diagnosing syntactic heuris- tics in natural language inference) or length (Annotation artifacts in natural language inference data) shortcuts? \n-\tThe proposed mitigation methods include balancing the data and knowledge editing. However, I am curious about whether demonstrations or Chain-of-Thoughts [1] can also solve the issue. \n-\tFig. 3 shows that smaller model cannot memorize the data. Then why it also has the shortcut problem like other larger models who memorize the biases in the pretraining data?\n\nmissing_references: [1] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [2] Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models (NAACL Findings 2022)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "rfX6ne8ne4",
        "length": 920,
        "human_text": "paper_topic_and_main_contributions: The paper investigates the effect of co-occurrence statistics on the ability of large language models to correctly answer simple factual questions (of the subject-relation-object form). The paper specifically checks whether simple co-occurrences between the subject and the object in the pretraining data, can lead the models to incorrectly answer factual questions where the co-occurrence diverges from the correct answer. As it is difficult to check this causal relation by direct manipulation (given the enormous costs of pretraining large language models), a correlation study was conducted. Results show correlation between the co-occurrence statistics of a triplet and the ability of the model to answer correctly questions on it. \nThe paper further explores mitigation strategies to combat this bias. Two approaches are explored: debiased finetuning and knowledge editing. The former approach presents limited gains. Knowledge editing does show promise, but may be restricted as it requires weeding out the incorrect facts one by one (if I understand correctly). It may also cause unintended changes to unrelated facts.\n\nreasons_to_accept: -- The paper addresses an important and timely topic, namely the ability of LLMs to act as knowledge bases.\n-- The related work section is very elaborate and provides insight into the field at hand.\n-- The arguments of the paper are well-presented, and the writing is generally clear.\n\nreasons_to_reject: -- I am somewhat confused as to the exact claim the paper is making. While the results clearly show a correlation between the co-occurrence statistics of a triplet and the performance of the model on it, it is not clear to me whether this in fact proves that there is a bias where such simple surface statistics push the language model astray from making the right prediction. What it does show is that questions where the degree of co-occurrence is smaller are more difficult for the model. The introduction reads \u201cin which frequently co-occurred words are preferred over the correct answer.\u201d I could not see how the experiments directly make this point. Simple correlation seems to me insufficient in this case, since making mistakes with little co-occurrence doesn\u2019t mean that there is a different option with higher co-occurrence.\nIn order to show that the behavior is biased, I would expect the paper to shows that the surface statistics interfere in some sense with the prediction of the model, in a way that would make it predict such answer even when it is not true. For example, I would have expected the paper to examine questions which we would expect (based on their prevalence in the training data) the model to answer correctly, and show that in these cases it tends to make more errors where there is a strong collocation and that the mistakes is towards the collocating words. If the paper indeed makes this kind of more subtle claim and I have missed it, I would welcome a response from the authors on this matter. Thank you. \nFollowing rebuttal: the results you have posted are helpful and address this comment. Please include them in the next version.\n-- The results of the attempts to mitigate the bias are not very strong. I should say that I do not see it in itself as grounds for rejection.\n-- Some important presentational details are not sufficiently clear (see below).\n\nquestions_for_the_authors: -- I would appreciate your response to my first point above.\n-- Was there any attempt to look at stronger, proprietary models? I'm asking because they are much stronger and may exhibit diff behavior.\n-- The point about memorization is unclear to me. Why does co-occurrence imply memorization? on the contrary, if the model \"saw\" the correct answer and was overridden by co-occurrence this is the opposite of memorization.\n-- Why is knowledge editing a mitigation strategy. Obviously wrong knowledge should be corrected, but this is independent of the cause of the mistake, which is the subject of this paper, isn't it?\n\nmissing_references: --\n\ntypos_grammar_style_and_presentation_improvements: Local comments: -- What was claimed exactly in previous work with respect to the discussed bias and how does that differ from the work here?\n-- l. 136: you write \"Our work is the 136 first to investigate the effects of finetuning on the 137 correlation between term frequency statistics and 138 factual knowledge of LLMs.\" This was not sufficiently clear to me:  I thought your main claim is about the relation between these stats in pretraining and in model behavior. This should be made clearer. Otherwise, a well written previous work section.\n-- Section 4.2: Why not use the more standard names then like marginal probability, joint probability and PMI?  -- Figures 2b, 3: The graphs do not show much in my opinion. Could they be explained in a sentence in the text? what does the figure here contribute?\n-- Figure 5 (and in other places in the paper): can you also compute correlation w/o binning? what does that turn out to be?\n-- Section 6.1: a more formal definition of the filtering method should be given.\nGrammar: -- l. 442: performances s.b. performance -- l. 501: changes s.b. change\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "EIKFZmuV6r",
        "length": 577,
        "human_text": "paper_topic_and_main_contributions: The paper argues that large language models suffer from the so-called 'co-occurrence bias', i.e. when asked a factual question they tend to assign higher probabilities to words that have higher co-occurrence statistics with the query instead of the correct answer. The authors show that this bias persists in models of different sizes and cannot be effectively removed using fine-tuning, but can be partially remedied with knowledge editing. In order to prove their point, the authors introduce a series of term-frequency baselines, two of which are co-occurrence statistics. They show that these statistics can be used as a shortcut feature (in some settings, selecting the most frequently co-occurrent word results in 60% accuracy on the test set) and explain the actual behaviour of the models to a large extent. The paper explores two mitigation strategies: fine-tuning on a debiased version of the training dataset and rank-one model editing (ROME). While fine-tuning is not particularly effective, ROME shows promising results on frequent relation-like facts but does not really help with rarer ones.\n\nreasons_to_accept: In my opinion, the most interesting aspect of the paper are the frequency baselines introduced to analyse the structure of the training dataset and to explain the behaviour of the models. They bring the co-occurrence/frequency bias influencing the behaviour of the models to fore, and the analysis is convincing.\n\nreasons_to_reject: The main point of the paper -- that LLMs suffer from co-occurrency bias -- is not particularly new. Other papers investigating this issue are mentioned in the Related Work section. The authors claim that their work is the fist \"to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs\" (ll. 136--139), but there is no discussion of why fine-tuning should help at all, and in the end it does not, which amounts to a weak negative result. The section on mitigating occupies less than 1.5 pages and does not contain any methodological insights.\n\nquestions_for_the_authors: In the discussion of fine-tuning in lines 347--348, it is said that \"the models may learn appropriate cadidate sets during finetuning.\" Does this mean that there is some amount of knowledge leakage in fine-tuning and that the testing set-up is slightly different from the zero-shot setting?\nIn Figure 6, we see that the accuracy of the models' outputs _improves_ when the conditional probability of the object given the subject goes down from around 1/64 to 0. This goes against the general trend of the positive association between conditional probability and accuracy. Is there any interpretation for this?\n\ntypos_grammar_style_and_presentation_improvements: Some of the statements in the paper look confusing. In lines 87--89, the authors point out that \"relying heavily on co-occurrence is not appropriate for understanding the accurate meaning behind words.\" This is probably true, but this is all we have when training LLMs, and the paper does not propose a way out. Similarly, in lines 173--174, the authors state that \"relying on co-occurrence implies simple memorization.\" Again, the relationship between co-occurrence and memorization is more complicated, and equating one with the other seems erroneous.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "6J9oy63MjV",
        "length": 504,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a framework to probe the shortcuts in LLMs. The framework firsts test the accuracy of LLM on factual knowledge dataset like LAMA. Then the framework computes the correlation of the subject and the object in the pretraining corpus. Results on GPT models (from 125M to 6B) show that, as the co-occurrence of subject and object decreases, the accuracy also decreases. Larger models (GPT-3.5 of 175B) also suffer from the problem. The authors also find that simple baseline based on co-occurrence is sufficient to surpass the performance of GPT-J 6B. Finally, the authors also give solution to mitigate the shortcut problem.\n\nreasons_to_accept: -\tThe paper contains detailed analysis of shortcut problem regarding token co-occurrence. \n-\tThe paper investigates an important area of verifying the factual knowledge of LLMs. \n-\tThe paper is well-written, and is free of significant presentation issues. \n-\tAlong with the identification of the problem, the paper also proposes to mitigate the problem.\n\nreasons_to_reject: -\tOne more experiment should be done to verify the claim that \"answers with higher co-occurrence are more likely to be generated\": The authors should count in each question, whether the model's generated answer has a high count in the pretraining corpus. Currently there is only a table (Table 1) showing similar results, i.e., the wrong answers have a relatively lower count in the pretraining corpus. However, quantitative results over the whole dataset should be given. \n-\tThere are existing work probing the shortcut learning problem of language models. The authors should elaborate more on the work to claim that they are the first to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs. For example: [2].\n\nquestions_for_the_authors: -\tThe paper focuses only on token-level shortcuts. Can the findings be generalized to other kinds of shortcuts, such as word-overlap (Right for the wrong reasons: Diagnosing syntactic heuris- tics in natural language inference) or length (Annotation artifacts in natural language inference data) shortcuts? \n-\tThe proposed mitigation methods include balancing the data and knowledge editing. However, I am curious about whether demonstrations or Chain-of-Thoughts [1] can also solve the issue. \n-\tFig. 3 shows that smaller model cannot memorize the data. Then why it also has the shortcut problem like other larger models who memorize the biases in the pretraining data?\n\nmissing_references: [1] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [2] Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models (NAACL Findings 2022)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "2_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_2_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7369333333333333,
      "max_similarity": 0.7458,
      "avg_coverage": 0.2936,
      "max_coverage": 0.3571
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 271,
      "avg_human_length": 667.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 5,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "rfX6ne8ne4",
        "similarity": 0.7443,
        "coverage": 0.2037,
        "human_length": 920,
        "human_text": "paper_topic_and_main_contributions: The paper investigates the effect of co-occurrence statistics on the ability of large language models to correctly answer simple factual questions (of the subject-relation-object form). The paper specifically checks whether simple co-occurrences between the subject and the object in the pretraining data, can lead the models to incorrectly answer factual questions where the co-occurrence diverges from the correct answer. As it is difficult to check this causal relation by direct manipulation (given the enormous costs of pretraining large language models), a correlation study was conducted. Results show correlation between the co-occurrence statistics of a triplet and the ability of the model to answer correctly questions on it. \nThe paper further explores mitigation strategies to combat this bias. Two approaches are explored: debiased finetuning and knowledge editing. The former approach presents limited gains. Knowledge editing does show promise, but may be restricted as it requires weeding out the incorrect facts one by one (if I understand correctly). It may also cause unintended changes to unrelated facts.\n\nreasons_to_accept: -- The paper addresses an important and timely topic, namely the ability of LLMs to act as knowledge bases.\n-- The related work section is very elaborate and provides insight into the field at hand.\n-- The arguments of the paper are well-presented, and the writing is generally clear.\n\nreasons_to_reject: -- I am somewhat confused as to the exact claim the paper is making. While the results clearly show a correlation between the co-occurrence statistics of a triplet and the performance of the model on it, it is not clear to me whether this in fact proves that there is a bias where such simple surface statistics push the language model astray from making the right prediction. What it does show is that questions where the degree of co-occurrence is smaller are more difficult for the model. The introduction reads \u201cin which frequently co-occurred words are preferred over the correct answer.\u201d I could not see how the experiments directly make this point. Simple correlation seems to me insufficient in this case, since making mistakes with little co-occurrence doesn\u2019t mean that there is a different option with higher co-occurrence.\nIn order to show that the behavior is biased, I would expect the paper to shows that the surface statistics interfere in some sense with the prediction of the model, in a way that would make it predict such answer even when it is not true. For example, I would have expected the paper to examine questions which we would expect (based on their prevalence in the training data) the model to answer correctly, and show that in these cases it tends to make more errors where there is a strong collocation and that the mistakes is towards the collocating words. If the paper indeed makes this kind of more subtle claim and I have missed it, I would welcome a response from the authors on this matter. Thank you. \nFollowing rebuttal: the results you have posted are helpful and address this comment. Please include them in the next version.\n-- The results of the attempts to mitigate the bias are not very strong. I should say that I do not see it in itself as grounds for rejection.\n-- Some important presentational details are not sufficiently clear (see below).\n\nquestions_for_the_authors: -- I would appreciate your response to my first point above.\n-- Was there any attempt to look at stronger, proprietary models? I'm asking because they are much stronger and may exhibit diff behavior.\n-- The point about memorization is unclear to me. Why does co-occurrence imply memorization? on the contrary, if the model \"saw\" the correct answer and was overridden by co-occurrence this is the opposite of memorization.\n-- Why is knowledge editing a mitigation strategy. Obviously wrong knowledge should be corrected, but this is independent of the cause of the mistake, which is the subject of this paper, isn't it?\n\nmissing_references: --\n\ntypos_grammar_style_and_presentation_improvements: Local comments: -- What was claimed exactly in previous work with respect to the discussed bias and how does that differ from the work here?\n-- l. 136: you write \"Our work is the 136 first to investigate the effects of finetuning on the 137 correlation between term frequency statistics and 138 factual knowledge of LLMs.\" This was not sufficiently clear to me:  I thought your main claim is about the relation between these stats in pretraining and in model behavior. This should be made clearer. Otherwise, a well written previous work section.\n-- Section 4.2: Why not use the more standard names then like marginal probability, joint probability and PMI?  -- Figures 2b, 3: The graphs do not show much in my opinion. Could they be explained in a sentence in the text? what does the figure here contribute?\n-- Figure 5 (and in other places in the paper): can you also compute correlation w/o binning? what does that turn out to be?\n-- Section 6.1: a more formal definition of the filtering method should be given.\nGrammar: -- l. 442: performances s.b. performance -- l. 501: changes s.b. change\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "EIKFZmuV6r",
        "similarity": 0.7458,
        "coverage": 0.32,
        "human_length": 577,
        "human_text": "paper_topic_and_main_contributions: The paper argues that large language models suffer from the so-called 'co-occurrence bias', i.e. when asked a factual question they tend to assign higher probabilities to words that have higher co-occurrence statistics with the query instead of the correct answer. The authors show that this bias persists in models of different sizes and cannot be effectively removed using fine-tuning, but can be partially remedied with knowledge editing. In order to prove their point, the authors introduce a series of term-frequency baselines, two of which are co-occurrence statistics. They show that these statistics can be used as a shortcut feature (in some settings, selecting the most frequently co-occurrent word results in 60% accuracy on the test set) and explain the actual behaviour of the models to a large extent. The paper explores two mitigation strategies: fine-tuning on a debiased version of the training dataset and rank-one model editing (ROME). While fine-tuning is not particularly effective, ROME shows promising results on frequent relation-like facts but does not really help with rarer ones.\n\nreasons_to_accept: In my opinion, the most interesting aspect of the paper are the frequency baselines introduced to analyse the structure of the training dataset and to explain the behaviour of the models. They bring the co-occurrence/frequency bias influencing the behaviour of the models to fore, and the analysis is convincing.\n\nreasons_to_reject: The main point of the paper -- that LLMs suffer from co-occurrency bias -- is not particularly new. Other papers investigating this issue are mentioned in the Related Work section. The authors claim that their work is the fist \"to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs\" (ll. 136--139), but there is no discussion of why fine-tuning should help at all, and in the end it does not, which amounts to a weak negative result. The section on mitigating occupies less than 1.5 pages and does not contain any methodological insights.\n\nquestions_for_the_authors: In the discussion of fine-tuning in lines 347--348, it is said that \"the models may learn appropriate cadidate sets during finetuning.\" Does this mean that there is some amount of knowledge leakage in fine-tuning and that the testing set-up is slightly different from the zero-shot setting?\nIn Figure 6, we see that the accuracy of the models' outputs _improves_ when the conditional probability of the object given the subject goes down from around 1/64 to 0. This goes against the general trend of the positive association between conditional probability and accuracy. Is there any interpretation for this?\n\ntypos_grammar_style_and_presentation_improvements: Some of the statements in the paper look confusing. In lines 87--89, the authors point out that \"relying heavily on co-occurrence is not appropriate for understanding the accurate meaning behind words.\" This is probably true, but this is all we have when training LLMs, and the paper does not propose a way out. Similarly, in lines 173--174, the authors state that \"relying on co-occurrence implies simple memorization.\" Again, the relationship between co-occurrence and memorization is more complicated, and equating one with the other seems erroneous.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "6J9oy63MjV",
        "similarity": 0.7207,
        "coverage": 0.3571,
        "human_length": 504,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a framework to probe the shortcuts in LLMs. The framework firsts test the accuracy of LLM on factual knowledge dataset like LAMA. Then the framework computes the correlation of the subject and the object in the pretraining corpus. Results on GPT models (from 125M to 6B) show that, as the co-occurrence of subject and object decreases, the accuracy also decreases. Larger models (GPT-3.5 of 175B) also suffer from the problem. The authors also find that simple baseline based on co-occurrence is sufficient to surpass the performance of GPT-J 6B. Finally, the authors also give solution to mitigate the shortcut problem.\n\nreasons_to_accept: -\tThe paper contains detailed analysis of shortcut problem regarding token co-occurrence. \n-\tThe paper investigates an important area of verifying the factual knowledge of LLMs. \n-\tThe paper is well-written, and is free of significant presentation issues. \n-\tAlong with the identification of the problem, the paper also proposes to mitigate the problem.\n\nreasons_to_reject: -\tOne more experiment should be done to verify the claim that \"answers with higher co-occurrence are more likely to be generated\": The authors should count in each question, whether the model's generated answer has a high count in the pretraining corpus. Currently there is only a table (Table 1) showing similar results, i.e., the wrong answers have a relatively lower count in the pretraining corpus. However, quantitative results over the whole dataset should be given. \n-\tThere are existing work probing the shortcut learning problem of language models. The authors should elaborate more on the work to claim that they are the first to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs. For example: [2].\n\nquestions_for_the_authors: -\tThe paper focuses only on token-level shortcuts. Can the findings be generalized to other kinds of shortcuts, such as word-overlap (Right for the wrong reasons: Diagnosing syntactic heuris- tics in natural language inference) or length (Annotation artifacts in natural language inference data) shortcuts? \n-\tThe proposed mitigation methods include balancing the data and knowledge editing. However, I am curious about whether demonstrations or Chain-of-Thoughts [1] can also solve the issue. \n-\tFig. 3 shows that smaller model cannot memorize the data. Then why it also has the shortcut problem like other larger models who memorize the biases in the pretraining data?\n\nmissing_references: [1] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [2] Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models (NAACL Findings 2022)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "rfX6ne8ne4",
        "length": 920,
        "human_text": "paper_topic_and_main_contributions: The paper investigates the effect of co-occurrence statistics on the ability of large language models to correctly answer simple factual questions (of the subject-relation-object form). The paper specifically checks whether simple co-occurrences between the subject and the object in the pretraining data, can lead the models to incorrectly answer factual questions where the co-occurrence diverges from the correct answer. As it is difficult to check this causal relation by direct manipulation (given the enormous costs of pretraining large language models), a correlation study was conducted. Results show correlation between the co-occurrence statistics of a triplet and the ability of the model to answer correctly questions on it. \nThe paper further explores mitigation strategies to combat this bias. Two approaches are explored: debiased finetuning and knowledge editing. The former approach presents limited gains. Knowledge editing does show promise, but may be restricted as it requires weeding out the incorrect facts one by one (if I understand correctly). It may also cause unintended changes to unrelated facts.\n\nreasons_to_accept: -- The paper addresses an important and timely topic, namely the ability of LLMs to act as knowledge bases.\n-- The related work section is very elaborate and provides insight into the field at hand.\n-- The arguments of the paper are well-presented, and the writing is generally clear.\n\nreasons_to_reject: -- I am somewhat confused as to the exact claim the paper is making. While the results clearly show a correlation between the co-occurrence statistics of a triplet and the performance of the model on it, it is not clear to me whether this in fact proves that there is a bias where such simple surface statistics push the language model astray from making the right prediction. What it does show is that questions where the degree of co-occurrence is smaller are more difficult for the model. The introduction reads \u201cin which frequently co-occurred words are preferred over the correct answer.\u201d I could not see how the experiments directly make this point. Simple correlation seems to me insufficient in this case, since making mistakes with little co-occurrence doesn\u2019t mean that there is a different option with higher co-occurrence.\nIn order to show that the behavior is biased, I would expect the paper to shows that the surface statistics interfere in some sense with the prediction of the model, in a way that would make it predict such answer even when it is not true. For example, I would have expected the paper to examine questions which we would expect (based on their prevalence in the training data) the model to answer correctly, and show that in these cases it tends to make more errors where there is a strong collocation and that the mistakes is towards the collocating words. If the paper indeed makes this kind of more subtle claim and I have missed it, I would welcome a response from the authors on this matter. Thank you. \nFollowing rebuttal: the results you have posted are helpful and address this comment. Please include them in the next version.\n-- The results of the attempts to mitigate the bias are not very strong. I should say that I do not see it in itself as grounds for rejection.\n-- Some important presentational details are not sufficiently clear (see below).\n\nquestions_for_the_authors: -- I would appreciate your response to my first point above.\n-- Was there any attempt to look at stronger, proprietary models? I'm asking because they are much stronger and may exhibit diff behavior.\n-- The point about memorization is unclear to me. Why does co-occurrence imply memorization? on the contrary, if the model \"saw\" the correct answer and was overridden by co-occurrence this is the opposite of memorization.\n-- Why is knowledge editing a mitigation strategy. Obviously wrong knowledge should be corrected, but this is independent of the cause of the mistake, which is the subject of this paper, isn't it?\n\nmissing_references: --\n\ntypos_grammar_style_and_presentation_improvements: Local comments: -- What was claimed exactly in previous work with respect to the discussed bias and how does that differ from the work here?\n-- l. 136: you write \"Our work is the 136 first to investigate the effects of finetuning on the 137 correlation between term frequency statistics and 138 factual knowledge of LLMs.\" This was not sufficiently clear to me:  I thought your main claim is about the relation between these stats in pretraining and in model behavior. This should be made clearer. Otherwise, a well written previous work section.\n-- Section 4.2: Why not use the more standard names then like marginal probability, joint probability and PMI?  -- Figures 2b, 3: The graphs do not show much in my opinion. Could they be explained in a sentence in the text? what does the figure here contribute?\n-- Figure 5 (and in other places in the paper): can you also compute correlation w/o binning? what does that turn out to be?\n-- Section 6.1: a more formal definition of the filtering method should be given.\nGrammar: -- l. 442: performances s.b. performance -- l. 501: changes s.b. change\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "EIKFZmuV6r",
        "length": 577,
        "human_text": "paper_topic_and_main_contributions: The paper argues that large language models suffer from the so-called 'co-occurrence bias', i.e. when asked a factual question they tend to assign higher probabilities to words that have higher co-occurrence statistics with the query instead of the correct answer. The authors show that this bias persists in models of different sizes and cannot be effectively removed using fine-tuning, but can be partially remedied with knowledge editing. In order to prove their point, the authors introduce a series of term-frequency baselines, two of which are co-occurrence statistics. They show that these statistics can be used as a shortcut feature (in some settings, selecting the most frequently co-occurrent word results in 60% accuracy on the test set) and explain the actual behaviour of the models to a large extent. The paper explores two mitigation strategies: fine-tuning on a debiased version of the training dataset and rank-one model editing (ROME). While fine-tuning is not particularly effective, ROME shows promising results on frequent relation-like facts but does not really help with rarer ones.\n\nreasons_to_accept: In my opinion, the most interesting aspect of the paper are the frequency baselines introduced to analyse the structure of the training dataset and to explain the behaviour of the models. They bring the co-occurrence/frequency bias influencing the behaviour of the models to fore, and the analysis is convincing.\n\nreasons_to_reject: The main point of the paper -- that LLMs suffer from co-occurrency bias -- is not particularly new. Other papers investigating this issue are mentioned in the Related Work section. The authors claim that their work is the fist \"to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs\" (ll. 136--139), but there is no discussion of why fine-tuning should help at all, and in the end it does not, which amounts to a weak negative result. The section on mitigating occupies less than 1.5 pages and does not contain any methodological insights.\n\nquestions_for_the_authors: In the discussion of fine-tuning in lines 347--348, it is said that \"the models may learn appropriate cadidate sets during finetuning.\" Does this mean that there is some amount of knowledge leakage in fine-tuning and that the testing set-up is slightly different from the zero-shot setting?\nIn Figure 6, we see that the accuracy of the models' outputs _improves_ when the conditional probability of the object given the subject goes down from around 1/64 to 0. This goes against the general trend of the positive association between conditional probability and accuracy. Is there any interpretation for this?\n\ntypos_grammar_style_and_presentation_improvements: Some of the statements in the paper look confusing. In lines 87--89, the authors point out that \"relying heavily on co-occurrence is not appropriate for understanding the accurate meaning behind words.\" This is probably true, but this is all we have when training LLMs, and the paper does not propose a way out. Similarly, in lines 173--174, the authors state that \"relying on co-occurrence implies simple memorization.\" Again, the relationship between co-occurrence and memorization is more complicated, and equating one with the other seems erroneous.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "6J9oy63MjV",
        "length": 504,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a framework to probe the shortcuts in LLMs. The framework firsts test the accuracy of LLM on factual knowledge dataset like LAMA. Then the framework computes the correlation of the subject and the object in the pretraining corpus. Results on GPT models (from 125M to 6B) show that, as the co-occurrence of subject and object decreases, the accuracy also decreases. Larger models (GPT-3.5 of 175B) also suffer from the problem. The authors also find that simple baseline based on co-occurrence is sufficient to surpass the performance of GPT-J 6B. Finally, the authors also give solution to mitigate the shortcut problem.\n\nreasons_to_accept: -\tThe paper contains detailed analysis of shortcut problem regarding token co-occurrence. \n-\tThe paper investigates an important area of verifying the factual knowledge of LLMs. \n-\tThe paper is well-written, and is free of significant presentation issues. \n-\tAlong with the identification of the problem, the paper also proposes to mitigate the problem.\n\nreasons_to_reject: -\tOne more experiment should be done to verify the claim that \"answers with higher co-occurrence are more likely to be generated\": The authors should count in each question, whether the model's generated answer has a high count in the pretraining corpus. Currently there is only a table (Table 1) showing similar results, i.e., the wrong answers have a relatively lower count in the pretraining corpus. However, quantitative results over the whole dataset should be given. \n-\tThere are existing work probing the shortcut learning problem of language models. The authors should elaborate more on the work to claim that they are the first to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs. For example: [2].\n\nquestions_for_the_authors: -\tThe paper focuses only on token-level shortcuts. Can the findings be generalized to other kinds of shortcuts, such as word-overlap (Right for the wrong reasons: Diagnosing syntactic heuris- tics in natural language inference) or length (Annotation artifacts in natural language inference data) shortcuts? \n-\tThe proposed mitigation methods include balancing the data and knowledge editing. However, I am curious about whether demonstrations or Chain-of-Thoughts [1] can also solve the issue. \n-\tFig. 3 shows that smaller model cannot memorize the data. Then why it also has the shortcut problem like other larger models who memorize the biases in the pretraining data?\n\nmissing_references: [1] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [2] Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models (NAACL Findings 2022)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "174_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_174_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7063,
      "max_similarity": 0.7305,
      "avg_coverage": 0.6108333333333333,
      "max_coverage": 0.875
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 517,
      "avg_human_length": 398.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 6,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "olmWcRRdVR",
        "similarity": 0.685,
        "coverage": 0.875,
        "human_length": 209,
        "human_text": "paper_topic_and_main_contributions: This paper studies generative language models for designing clinical trials, focusing on inclusive and exclusive criteria generation. The authors propose a novel prompting-based framework that they train from scratch using existing clinical trial announcements.  The model outputs both clinical trial criteria and the reasoning steps used to generate the criteria.\n\nreasons_to_accept: - important medical application - good performance - several analyses are performed to give insight into the results\n\nreasons_to_reject: - The paper is quite dense and hard to follow (for instance, the neural prompt section is quite difficult for readers who aren't prompting experts) - It's not clear if the data and code will be made available - There are no details about the implementation and experimental setup: nb of runs, hyperparameters,...\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 1: Could not reproduce the results here no matter how hard they tried."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "0heuaY41cg",
        "similarity": 0.7034,
        "coverage": 0.5333,
        "human_length": 267,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a method named AutoTrial to aid the design of clinical eligibility criteria using language models. The method contains three main technical features: instruction prompt tuning for controllable generation, scalable and efficient knowledge incorporation via in-context learning and multi-step reasoning. Extensive experiments show that this method outperforms many strong baselines by a large margin, and the generated criteria texts are fluent, coherent, clinically accurate and of high-quality.\n\nreasons_to_accept: 1. This work shows the retrieved criteria from relevant trails as the exemplars could be used to improve the quality of generation. Ane the in-context exemplar serves as a good template for the model's multi-step reasoning outputs. \n2. A discrete and neural prompting approach is proposed to better solve the clinical trial design problem and the method outperforms many strong baselines. \n3. Ablation has been shown to justify the importance of RAG and Prompt module, and the MSR is shown to contribute in the model interpretability.\n\nreasons_to_reject: In the clinical trial setting, the coverage of the external knowledge base may not be great enough for all the possible queries. If the retrieval confidence is not good enough, how to facilitate the in-context learning abilities of language models?\n\ntypos_grammar_style_and_presentation_improvements: sec 4.3 baseline methods\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "xWz8sukBka",
        "similarity": 0.7305,
        "coverage": 0.4242,
        "human_length": 720,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a novel task: the automation of clinical trial design, with a specific focus on eligibility criteria. The objective is to potentially aid the design of clinical trial protocols. The authors systematically address this task by defining the problem, creating a dedicated dataset, introducing a model termed \"AutoTrial,\" and rigorously assessing its performance.\n\nreasons_to_accept: The paper demonstrates novelty through the introduction of a new task: the automation of clinical trial design through the creation of a language model. This has the potential to aid clinical trial design, thus contributing to the advancement of drug development by increasing the likelihood of successful trials. The motivation for clinical trial protocol design, which is a challenging task, is effectively communicated. Overall, the paper is readable and understandable.  In terms of assessment, the authors evaluated their proposed model, considering a comprehensive array of baseline models and employing various metrics to portray its effectiveness. Given the novelty of this task, it is worth considering whether the chosen metrics provide the most optimal evaluation for this specific context, although seemed reasonable.\n\nreasons_to_reject: While the authors have motivated the need for a clinical trial automation process, the authors should be more careful in their word choice. A significant example to this end: it was mentioned in the first paragraph of the introduction, \"one area where generative LLMs have shown significant potential is in clinical trial protocol design\" without any reference which contradicts the paper's main claim that they are \"first to develop LLMs focusing on trial design\". The authors also pointed out that the models often lack the capability of \"ability to adapt expert instructions\" in the introduction without any reference or evidence to it. It is crucial to demonstrate how the generic models fail or at least provide a reference to this claim.\nThe authors are missing a number of key details in Section 3 , making it difficult to reproduce the paper's results. An example of this is in the pre-training of AutoTrial,  the authors used title, disease, treatment, etc. as the input (i.e. trial setup), however, did not mention the maximum length considered for the input. Generally, clinical trial documents are lengthy documents, and it is of utmost importance in this scenario.  While the importance of the \"maximum acceptable length\" parameter is acknowledged in Section 4.4, attributing potential performance discrepancies between exclusion and inclusion criteria to truncation, no discussion is presented on the length selected. Additionally, the paper lacks in-depth details about the model used, describing it merely as a \"decoder-based causal language modeling architecture.\"\nThe problem setup in Section 3.1 is not as clearly demonstrated as it could be. It would be highly beneficial if the different input attributes, such as 'targeting instruction,' 'reasoning steps,' and 'textual description of the given instruction,' were illustrated with a sample input. Given that the paper's primary focus revolves around 'the criteria of trials,' a list of potential criteria (e.g., age, gender, BMI, etc) for the eligibility section (inclusion/exclusion) would greatly aid the reader in comprehending the core concepts.\nIn summary, while the authors have effectively emphasized the need for clinical trial automation, they should revise the article providing missing details, particularly regarding model parameters, model specifications, and sample input demonstrations, which would greatly enhance the clarity and reproducibility of the paper.\n\nquestions_for_the_authors: It would be interesting to see further clarification about the choice of evaluation metrics from the authors.\n\ntypos_grammar_style_and_presentation_improvements: There are a few grammatical mistakes and reference issues. For example, in section 3.3 the authors referred to the dataset as section 3.1 which will 4.1. There is a spelling mistake in the title of section 4.3 ( 'mehtods'). There are styling issues throughout the paper while referring to different sections.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "olmWcRRdVR",
        "length": 209,
        "human_text": "paper_topic_and_main_contributions: This paper studies generative language models for designing clinical trials, focusing on inclusive and exclusive criteria generation. The authors propose a novel prompting-based framework that they train from scratch using existing clinical trial announcements.  The model outputs both clinical trial criteria and the reasoning steps used to generate the criteria.\n\nreasons_to_accept: - important medical application - good performance - several analyses are performed to give insight into the results\n\nreasons_to_reject: - The paper is quite dense and hard to follow (for instance, the neural prompt section is quite difficult for readers who aren't prompting experts) - It's not clear if the data and code will be made available - There are no details about the implementation and experimental setup: nb of runs, hyperparameters,...\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 1: Could not reproduce the results here no matter how hard they tried.",
        "has_content": true
      },
      {
        "reviewer_id": "0heuaY41cg",
        "length": 267,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a method named AutoTrial to aid the design of clinical eligibility criteria using language models. The method contains three main technical features: instruction prompt tuning for controllable generation, scalable and efficient knowledge incorporation via in-context learning and multi-step reasoning. Extensive experiments show that this method outperforms many strong baselines by a large margin, and the generated criteria texts are fluent, coherent, clinically accurate and of high-quality.\n\nreasons_to_accept: 1. This work shows the retrieved criteria from relevant trails as the exemplars could be used to improve the quality of generation. Ane the in-context exemplar serves as a good template for the model's multi-step reasoning outputs. \n2. A discrete and neural prompting approach is proposed to better solve the clinical trial design problem and the method outperforms many strong baselines. \n3. Ablation has been shown to justify the importance of RAG and Prompt module, and the MSR is shown to contribute in the model interpretability.\n\nreasons_to_reject: In the clinical trial setting, the coverage of the external knowledge base may not be great enough for all the possible queries. If the retrieval confidence is not good enough, how to facilitate the in-context learning abilities of language models?\n\ntypos_grammar_style_and_presentation_improvements: sec 4.3 baseline methods\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "xWz8sukBka",
        "length": 720,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a novel task: the automation of clinical trial design, with a specific focus on eligibility criteria. The objective is to potentially aid the design of clinical trial protocols. The authors systematically address this task by defining the problem, creating a dedicated dataset, introducing a model termed \"AutoTrial,\" and rigorously assessing its performance.\n\nreasons_to_accept: The paper demonstrates novelty through the introduction of a new task: the automation of clinical trial design through the creation of a language model. This has the potential to aid clinical trial design, thus contributing to the advancement of drug development by increasing the likelihood of successful trials. The motivation for clinical trial protocol design, which is a challenging task, is effectively communicated. Overall, the paper is readable and understandable.  In terms of assessment, the authors evaluated their proposed model, considering a comprehensive array of baseline models and employing various metrics to portray its effectiveness. Given the novelty of this task, it is worth considering whether the chosen metrics provide the most optimal evaluation for this specific context, although seemed reasonable.\n\nreasons_to_reject: While the authors have motivated the need for a clinical trial automation process, the authors should be more careful in their word choice. A significant example to this end: it was mentioned in the first paragraph of the introduction, \"one area where generative LLMs have shown significant potential is in clinical trial protocol design\" without any reference which contradicts the paper's main claim that they are \"first to develop LLMs focusing on trial design\". The authors also pointed out that the models often lack the capability of \"ability to adapt expert instructions\" in the introduction without any reference or evidence to it. It is crucial to demonstrate how the generic models fail or at least provide a reference to this claim.\nThe authors are missing a number of key details in Section 3 , making it difficult to reproduce the paper's results. An example of this is in the pre-training of AutoTrial,  the authors used title, disease, treatment, etc. as the input (i.e. trial setup), however, did not mention the maximum length considered for the input. Generally, clinical trial documents are lengthy documents, and it is of utmost importance in this scenario.  While the importance of the \"maximum acceptable length\" parameter is acknowledged in Section 4.4, attributing potential performance discrepancies between exclusion and inclusion criteria to truncation, no discussion is presented on the length selected. Additionally, the paper lacks in-depth details about the model used, describing it merely as a \"decoder-based causal language modeling architecture.\"\nThe problem setup in Section 3.1 is not as clearly demonstrated as it could be. It would be highly beneficial if the different input attributes, such as 'targeting instruction,' 'reasoning steps,' and 'textual description of the given instruction,' were illustrated with a sample input. Given that the paper's primary focus revolves around 'the criteria of trials,' a list of potential criteria (e.g., age, gender, BMI, etc) for the eligibility section (inclusion/exclusion) would greatly aid the reader in comprehending the core concepts.\nIn summary, while the authors have effectively emphasized the need for clinical trial automation, they should revise the article providing missing details, particularly regarding model parameters, model specifications, and sample input demonstrations, which would greatly enhance the clarity and reproducibility of the paper.\n\nquestions_for_the_authors: It would be interesting to see further clarification about the choice of evaluation metrics from the authors.\n\ntypos_grammar_style_and_presentation_improvements: There are a few grammatical mistakes and reference issues. For example, in section 3.3 the authors referred to the dataset as section 3.1 which will 4.1. There is a spelling mistake in the title of section 4.3 ( 'mehtods'). There are styling issues throughout the paper while referring to different sections.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "174_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_174_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7060666666666666,
      "max_similarity": 0.729,
      "avg_coverage": 0.6108333333333333,
      "max_coverage": 0.875
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 458,
      "avg_human_length": 398.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 6,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "olmWcRRdVR",
        "similarity": 0.6887,
        "coverage": 0.875,
        "human_length": 209,
        "human_text": "paper_topic_and_main_contributions: This paper studies generative language models for designing clinical trials, focusing on inclusive and exclusive criteria generation. The authors propose a novel prompting-based framework that they train from scratch using existing clinical trial announcements.  The model outputs both clinical trial criteria and the reasoning steps used to generate the criteria.\n\nreasons_to_accept: - important medical application - good performance - several analyses are performed to give insight into the results\n\nreasons_to_reject: - The paper is quite dense and hard to follow (for instance, the neural prompt section is quite difficult for readers who aren't prompting experts) - It's not clear if the data and code will be made available - There are no details about the implementation and experimental setup: nb of runs, hyperparameters,...\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 1: Could not reproduce the results here no matter how hard they tried."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "0heuaY41cg",
        "similarity": 0.7005,
        "coverage": 0.5333,
        "human_length": 267,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a method named AutoTrial to aid the design of clinical eligibility criteria using language models. The method contains three main technical features: instruction prompt tuning for controllable generation, scalable and efficient knowledge incorporation via in-context learning and multi-step reasoning. Extensive experiments show that this method outperforms many strong baselines by a large margin, and the generated criteria texts are fluent, coherent, clinically accurate and of high-quality.\n\nreasons_to_accept: 1. This work shows the retrieved criteria from relevant trails as the exemplars could be used to improve the quality of generation. Ane the in-context exemplar serves as a good template for the model's multi-step reasoning outputs. \n2. A discrete and neural prompting approach is proposed to better solve the clinical trial design problem and the method outperforms many strong baselines. \n3. Ablation has been shown to justify the importance of RAG and Prompt module, and the MSR is shown to contribute in the model interpretability.\n\nreasons_to_reject: In the clinical trial setting, the coverage of the external knowledge base may not be great enough for all the possible queries. If the retrieval confidence is not good enough, how to facilitate the in-context learning abilities of language models?\n\ntypos_grammar_style_and_presentation_improvements: sec 4.3 baseline methods\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "xWz8sukBka",
        "similarity": 0.729,
        "coverage": 0.4242,
        "human_length": 720,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a novel task: the automation of clinical trial design, with a specific focus on eligibility criteria. The objective is to potentially aid the design of clinical trial protocols. The authors systematically address this task by defining the problem, creating a dedicated dataset, introducing a model termed \"AutoTrial,\" and rigorously assessing its performance.\n\nreasons_to_accept: The paper demonstrates novelty through the introduction of a new task: the automation of clinical trial design through the creation of a language model. This has the potential to aid clinical trial design, thus contributing to the advancement of drug development by increasing the likelihood of successful trials. The motivation for clinical trial protocol design, which is a challenging task, is effectively communicated. Overall, the paper is readable and understandable.  In terms of assessment, the authors evaluated their proposed model, considering a comprehensive array of baseline models and employing various metrics to portray its effectiveness. Given the novelty of this task, it is worth considering whether the chosen metrics provide the most optimal evaluation for this specific context, although seemed reasonable.\n\nreasons_to_reject: While the authors have motivated the need for a clinical trial automation process, the authors should be more careful in their word choice. A significant example to this end: it was mentioned in the first paragraph of the introduction, \"one area where generative LLMs have shown significant potential is in clinical trial protocol design\" without any reference which contradicts the paper's main claim that they are \"first to develop LLMs focusing on trial design\". The authors also pointed out that the models often lack the capability of \"ability to adapt expert instructions\" in the introduction without any reference or evidence to it. It is crucial to demonstrate how the generic models fail or at least provide a reference to this claim.\nThe authors are missing a number of key details in Section 3 , making it difficult to reproduce the paper's results. An example of this is in the pre-training of AutoTrial,  the authors used title, disease, treatment, etc. as the input (i.e. trial setup), however, did not mention the maximum length considered for the input. Generally, clinical trial documents are lengthy documents, and it is of utmost importance in this scenario.  While the importance of the \"maximum acceptable length\" parameter is acknowledged in Section 4.4, attributing potential performance discrepancies between exclusion and inclusion criteria to truncation, no discussion is presented on the length selected. Additionally, the paper lacks in-depth details about the model used, describing it merely as a \"decoder-based causal language modeling architecture.\"\nThe problem setup in Section 3.1 is not as clearly demonstrated as it could be. It would be highly beneficial if the different input attributes, such as 'targeting instruction,' 'reasoning steps,' and 'textual description of the given instruction,' were illustrated with a sample input. Given that the paper's primary focus revolves around 'the criteria of trials,' a list of potential criteria (e.g., age, gender, BMI, etc) for the eligibility section (inclusion/exclusion) would greatly aid the reader in comprehending the core concepts.\nIn summary, while the authors have effectively emphasized the need for clinical trial automation, they should revise the article providing missing details, particularly regarding model parameters, model specifications, and sample input demonstrations, which would greatly enhance the clarity and reproducibility of the paper.\n\nquestions_for_the_authors: It would be interesting to see further clarification about the choice of evaluation metrics from the authors.\n\ntypos_grammar_style_and_presentation_improvements: There are a few grammatical mistakes and reference issues. For example, in section 3.3 the authors referred to the dataset as section 3.1 which will 4.1. There is a spelling mistake in the title of section 4.3 ( 'mehtods'). There are styling issues throughout the paper while referring to different sections.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "olmWcRRdVR",
        "length": 209,
        "human_text": "paper_topic_and_main_contributions: This paper studies generative language models for designing clinical trials, focusing on inclusive and exclusive criteria generation. The authors propose a novel prompting-based framework that they train from scratch using existing clinical trial announcements.  The model outputs both clinical trial criteria and the reasoning steps used to generate the criteria.\n\nreasons_to_accept: - important medical application - good performance - several analyses are performed to give insight into the results\n\nreasons_to_reject: - The paper is quite dense and hard to follow (for instance, the neural prompt section is quite difficult for readers who aren't prompting experts) - It's not clear if the data and code will be made available - There are no details about the implementation and experimental setup: nb of runs, hyperparameters,...\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 1: Could not reproduce the results here no matter how hard they tried.",
        "has_content": true
      },
      {
        "reviewer_id": "0heuaY41cg",
        "length": 267,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a method named AutoTrial to aid the design of clinical eligibility criteria using language models. The method contains three main technical features: instruction prompt tuning for controllable generation, scalable and efficient knowledge incorporation via in-context learning and multi-step reasoning. Extensive experiments show that this method outperforms many strong baselines by a large margin, and the generated criteria texts are fluent, coherent, clinically accurate and of high-quality.\n\nreasons_to_accept: 1. This work shows the retrieved criteria from relevant trails as the exemplars could be used to improve the quality of generation. Ane the in-context exemplar serves as a good template for the model's multi-step reasoning outputs. \n2. A discrete and neural prompting approach is proposed to better solve the clinical trial design problem and the method outperforms many strong baselines. \n3. Ablation has been shown to justify the importance of RAG and Prompt module, and the MSR is shown to contribute in the model interpretability.\n\nreasons_to_reject: In the clinical trial setting, the coverage of the external knowledge base may not be great enough for all the possible queries. If the retrieval confidence is not good enough, how to facilitate the in-context learning abilities of language models?\n\ntypos_grammar_style_and_presentation_improvements: sec 4.3 baseline methods\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "xWz8sukBka",
        "length": 720,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a novel task: the automation of clinical trial design, with a specific focus on eligibility criteria. The objective is to potentially aid the design of clinical trial protocols. The authors systematically address this task by defining the problem, creating a dedicated dataset, introducing a model termed \"AutoTrial,\" and rigorously assessing its performance.\n\nreasons_to_accept: The paper demonstrates novelty through the introduction of a new task: the automation of clinical trial design through the creation of a language model. This has the potential to aid clinical trial design, thus contributing to the advancement of drug development by increasing the likelihood of successful trials. The motivation for clinical trial protocol design, which is a challenging task, is effectively communicated. Overall, the paper is readable and understandable.  In terms of assessment, the authors evaluated their proposed model, considering a comprehensive array of baseline models and employing various metrics to portray its effectiveness. Given the novelty of this task, it is worth considering whether the chosen metrics provide the most optimal evaluation for this specific context, although seemed reasonable.\n\nreasons_to_reject: While the authors have motivated the need for a clinical trial automation process, the authors should be more careful in their word choice. A significant example to this end: it was mentioned in the first paragraph of the introduction, \"one area where generative LLMs have shown significant potential is in clinical trial protocol design\" without any reference which contradicts the paper's main claim that they are \"first to develop LLMs focusing on trial design\". The authors also pointed out that the models often lack the capability of \"ability to adapt expert instructions\" in the introduction without any reference or evidence to it. It is crucial to demonstrate how the generic models fail or at least provide a reference to this claim.\nThe authors are missing a number of key details in Section 3 , making it difficult to reproduce the paper's results. An example of this is in the pre-training of AutoTrial,  the authors used title, disease, treatment, etc. as the input (i.e. trial setup), however, did not mention the maximum length considered for the input. Generally, clinical trial documents are lengthy documents, and it is of utmost importance in this scenario.  While the importance of the \"maximum acceptable length\" parameter is acknowledged in Section 4.4, attributing potential performance discrepancies between exclusion and inclusion criteria to truncation, no discussion is presented on the length selected. Additionally, the paper lacks in-depth details about the model used, describing it merely as a \"decoder-based causal language modeling architecture.\"\nThe problem setup in Section 3.1 is not as clearly demonstrated as it could be. It would be highly beneficial if the different input attributes, such as 'targeting instruction,' 'reasoning steps,' and 'textual description of the given instruction,' were illustrated with a sample input. Given that the paper's primary focus revolves around 'the criteria of trials,' a list of potential criteria (e.g., age, gender, BMI, etc) for the eligibility section (inclusion/exclusion) would greatly aid the reader in comprehending the core concepts.\nIn summary, while the authors have effectively emphasized the need for clinical trial automation, they should revise the article providing missing details, particularly regarding model parameters, model specifications, and sample input demonstrations, which would greatly enhance the clarity and reproducibility of the paper.\n\nquestions_for_the_authors: It would be interesting to see further clarification about the choice of evaluation metrics from the authors.\n\ntypos_grammar_style_and_presentation_improvements: There are a few grammatical mistakes and reference issues. For example, in section 3.3 the authors referred to the dataset as section 3.1 which will 4.1. There is a spelling mistake in the title of section 4.3 ( 'mehtods'). There are styling issues throughout the paper while referring to different sections.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "131_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_131_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7334999999999999,
      "max_similarity": 0.7497,
      "avg_coverage": 0.49756666666666666,
      "max_coverage": 0.5789
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 419,
      "avg_human_length": 492.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 6,
      "suggestions_count": 12
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "0720gzafb8",
        "similarity": 0.7233,
        "coverage": 0.5789,
        "human_length": 392,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a structured causal model (SCM) to mitigate entity bias in PLMs/LLMs.  To mitigate the conflicts between parametric knowledge and contextual knowledge, the paper first replaces entities with placeholders for the input sentence and then finds similar entities via LLM, thirdly generates the definition of placeholders with similar entities, and finally combines the definition, context, and prompt question to a whole input sentence. The experimental results show that the framework proposed by this paper could get better results compared with baseline models.\n\nreasons_to_accept: The paper proposes a causal view of entity bias in PLM/LLM models, which is interesting and has practical significance.\n\nreasons_to_reject: Various prompt-based learning methods have should that the classification results made by PLM/LLM can be affected by prompt templates. \ns it unclear whether the improvement in results from this method is specific because it mitigates the entity bias in PLM/LLM, or generates a more specific prompting template for PLM/LLM to make decisions.\nI think that LLM does not have entity bias because it has been pre-trained on a large amount of unsupervised/semi-supervised data. The entity bias in prompts given to LLM may be the reason why there is entity bias in  LLM inference results. The paper should analyze this point of view.\n Using the top k nearest neighbors to construct the convex hull seems like the prototype-based method. As the value of k increases, the center point of the convex hull should theoretically be closer to the true meaning of this entity category. In other words, the center point of the box will have less bias towards this entity category. This means the center of the hull moves further away from the original entity. But it also means the original entity has a bigger bias, why the results become worse?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "zWZGnjsEHv",
        "similarity": 0.7497,
        "coverage": 0.5,
        "human_length": 707,
        "human_text": "paper_topic_and_main_contributions: This paper introduces techniques for mitigating entity bias as both a training-time intervention and prompting intervention for white-box and black-box LLMs respectively. They conduct an analysis of past entity bias mitigating techniques, and introduce a structured casual model to frame their technique.  The causal method is separated into a training time intervention, which consists of swapping a specific entities embedding in the lowest layer with an embedding at the center of a convex hull built off of closely related entities.  They also propose a method for use with black-box LLMs, which focuses on prompting the LLM to consider related entities in addition to a specific one.\nThey experiment with their interventions on the two earlier mentioned styles of LLMs and show that the training-time intervention improves over past methods in this study, specifically in the OOD performance. They also show that their prompting intervention aids black-box LLMs in mitigating entity bias as compared to their baseline techniques.\n\nreasons_to_accept: A) The paper is generally well-written and does a thorough analysis of past work in the entity bias space. \nB) Figures 3 and 4 in particular are well-illustrative of the techniques used. \nC) The ablation study on the in-context prompting section also shows clearly which components of their prompting strategy contribute to performance. \nD) Methods that use causal approaches with LLMs are important in achieving high performance in applications and reducing bias/errors. \nE) The authors report a robust set of experiments with multiple runs and error bars.\n\nreasons_to_reject: A) In-domain performance drops when applying the method as a training-time intervention. Although this is understandably an OOD mitigation technique, it would be nice to have consistent performance in domain B) There could be additional experiments in the in-context prompting section, as some important baselines for in-context prompting have not been included.\n\nquestions_for_the_authors: A) Where do you get the set of entity neighbors to compute the convex hull from?  Were they extracted from the relevant datasets, or from an associated KB?\nB) In figure 1 there is an option for the model to choose from two given options in order to answer the entity relation question, however, in figure 4 that option is not present. Are options used in your experiments, and if so, how does the method work without those given options?\n(If no options given), One issue I see is that the method may not debias across multiple axes. How do you ensure that the training time intervention can debias across multiple axes? In the given example, Bill Gates is decoupled from his role as a founder and is shown to be a visitor. But how can the intervention debias across his nationality, his gender, etc.?\nC) In in-context example learning, how can the prompting strategy be applied to multiple entities? Has any analysis been done on if entity bias is sought for only a fraction of the entities present in the sample and what effect that has on performance?\nD) Can you elaborate more on why in-domain performance decreases when using your method? In what situations would a practitioner utilize this method given this drop?\nE) For in-context learning, can\u2019t entity bias be mitigated through other prompting techniques, such as demonstrations via in-context examples, CoT techniques, self-checking, etc.? How would these methods compare as a baseline against your method?  F) I would like further discussion of this point from the limitations section: \u201cIn this paper, we only consider zero-shot prompting for black-box LLMs, because this will help us to control variables during causal analysis.\u201d\n\ntypos_grammar_style_and_presentation_improvements: The \u201cPLM\u201d usage in the abstract is not defined.\nFigure 3 caption could use further explanation so that it is more easily understood on quick passes.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "1Y0yMFs2jw",
        "similarity": 0.7275,
        "coverage": 0.4138,
        "human_length": 377,
        "human_text": "paper_topic_and_main_contributions: The paper aims to eliminate entity bias in pre-trained language models through causal intervention methods. In particular, the paper proposes an entity perturbation method that perturbs the input text with similar entities from the convex hull of the target entity\u2019s representation. Such causal intervention at the input layer is shown to be effective on both training-time and in-context interventions. The proposed debiasing approach is evaluated on relation extraction data sets (TACRED and EntRED) and reading comprehension data set (TriviaQA).  Experimental results suggest strong performance gain over other baselines.\n\nreasons_to_accept: 1. Strong empirical results demonstrating the effectiveness of the approach.\n2. The paper shows ingenuity in constructing a prompt that is effective for debiasing entity bias. It will be interesting to the community to follow up on this work and see whether similar approaches can help to mitigate other types of biases.\n3. The paper is well written and easy to follow\n\nreasons_to_reject: 1. It seems the effectiveness of the proposed method is over-reliant on either the LLMs ability to provide similar entities (in-context intervention) or a pre-trained model\u2019s ability to retrieve similar entities (training-time intervention).\n2. There are some missing details in the paper. See the \u201cQuestions\u201d section for details.\n\nquestions_for_the_authors: 1. How do you obtain the entity embeddings for constructing the convex hull during training time intervention? Are you using a pre-trained model that provides such embeddings?\n2. How does debiasing impact long-tail entities? In particular, given the sparsity of the embedding space near the long-tail entities, how can we construct a convex hull?\n3. What is the interpretation of OOD in the experiments? Are these the entities not seen in training?\n\ntypos_grammar_style_and_presentation_improvements: Too many footnotes in the paper, some of these should be incorporated into the main text and some others should be placed in the Appendix section when they contain specific details, e.g., choice of an experiment set up.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "0720gzafb8",
        "length": 392,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a structured causal model (SCM) to mitigate entity bias in PLMs/LLMs.  To mitigate the conflicts between parametric knowledge and contextual knowledge, the paper first replaces entities with placeholders for the input sentence and then finds similar entities via LLM, thirdly generates the definition of placeholders with similar entities, and finally combines the definition, context, and prompt question to a whole input sentence. The experimental results show that the framework proposed by this paper could get better results compared with baseline models.\n\nreasons_to_accept: The paper proposes a causal view of entity bias in PLM/LLM models, which is interesting and has practical significance.\n\nreasons_to_reject: Various prompt-based learning methods have should that the classification results made by PLM/LLM can be affected by prompt templates. \ns it unclear whether the improvement in results from this method is specific because it mitigates the entity bias in PLM/LLM, or generates a more specific prompting template for PLM/LLM to make decisions.\nI think that LLM does not have entity bias because it has been pre-trained on a large amount of unsupervised/semi-supervised data. The entity bias in prompts given to LLM may be the reason why there is entity bias in  LLM inference results. The paper should analyze this point of view.\n Using the top k nearest neighbors to construct the convex hull seems like the prototype-based method. As the value of k increases, the center point of the convex hull should theoretically be closer to the true meaning of this entity category. In other words, the center point of the box will have less bias towards this entity category. This means the center of the hull moves further away from the original entity. But it also means the original entity has a bigger bias, why the results become worse?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "zWZGnjsEHv",
        "length": 707,
        "human_text": "paper_topic_and_main_contributions: This paper introduces techniques for mitigating entity bias as both a training-time intervention and prompting intervention for white-box and black-box LLMs respectively. They conduct an analysis of past entity bias mitigating techniques, and introduce a structured casual model to frame their technique.  The causal method is separated into a training time intervention, which consists of swapping a specific entities embedding in the lowest layer with an embedding at the center of a convex hull built off of closely related entities.  They also propose a method for use with black-box LLMs, which focuses on prompting the LLM to consider related entities in addition to a specific one.\nThey experiment with their interventions on the two earlier mentioned styles of LLMs and show that the training-time intervention improves over past methods in this study, specifically in the OOD performance. They also show that their prompting intervention aids black-box LLMs in mitigating entity bias as compared to their baseline techniques.\n\nreasons_to_accept: A) The paper is generally well-written and does a thorough analysis of past work in the entity bias space. \nB) Figures 3 and 4 in particular are well-illustrative of the techniques used. \nC) The ablation study on the in-context prompting section also shows clearly which components of their prompting strategy contribute to performance. \nD) Methods that use causal approaches with LLMs are important in achieving high performance in applications and reducing bias/errors. \nE) The authors report a robust set of experiments with multiple runs and error bars.\n\nreasons_to_reject: A) In-domain performance drops when applying the method as a training-time intervention. Although this is understandably an OOD mitigation technique, it would be nice to have consistent performance in domain B) There could be additional experiments in the in-context prompting section, as some important baselines for in-context prompting have not been included.\n\nquestions_for_the_authors: A) Where do you get the set of entity neighbors to compute the convex hull from?  Were they extracted from the relevant datasets, or from an associated KB?\nB) In figure 1 there is an option for the model to choose from two given options in order to answer the entity relation question, however, in figure 4 that option is not present. Are options used in your experiments, and if so, how does the method work without those given options?\n(If no options given), One issue I see is that the method may not debias across multiple axes. How do you ensure that the training time intervention can debias across multiple axes? In the given example, Bill Gates is decoupled from his role as a founder and is shown to be a visitor. But how can the intervention debias across his nationality, his gender, etc.?\nC) In in-context example learning, how can the prompting strategy be applied to multiple entities? Has any analysis been done on if entity bias is sought for only a fraction of the entities present in the sample and what effect that has on performance?\nD) Can you elaborate more on why in-domain performance decreases when using your method? In what situations would a practitioner utilize this method given this drop?\nE) For in-context learning, can\u2019t entity bias be mitigated through other prompting techniques, such as demonstrations via in-context examples, CoT techniques, self-checking, etc.? How would these methods compare as a baseline against your method?  F) I would like further discussion of this point from the limitations section: \u201cIn this paper, we only consider zero-shot prompting for black-box LLMs, because this will help us to control variables during causal analysis.\u201d\n\ntypos_grammar_style_and_presentation_improvements: The \u201cPLM\u201d usage in the abstract is not defined.\nFigure 3 caption could use further explanation so that it is more easily understood on quick passes.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "1Y0yMFs2jw",
        "length": 377,
        "human_text": "paper_topic_and_main_contributions: The paper aims to eliminate entity bias in pre-trained language models through causal intervention methods. In particular, the paper proposes an entity perturbation method that perturbs the input text with similar entities from the convex hull of the target entity\u2019s representation. Such causal intervention at the input layer is shown to be effective on both training-time and in-context interventions. The proposed debiasing approach is evaluated on relation extraction data sets (TACRED and EntRED) and reading comprehension data set (TriviaQA).  Experimental results suggest strong performance gain over other baselines.\n\nreasons_to_accept: 1. Strong empirical results demonstrating the effectiveness of the approach.\n2. The paper shows ingenuity in constructing a prompt that is effective for debiasing entity bias. It will be interesting to the community to follow up on this work and see whether similar approaches can help to mitigate other types of biases.\n3. The paper is well written and easy to follow\n\nreasons_to_reject: 1. It seems the effectiveness of the proposed method is over-reliant on either the LLMs ability to provide similar entities (in-context intervention) or a pre-trained model\u2019s ability to retrieve similar entities (training-time intervention).\n2. There are some missing details in the paper. See the \u201cQuestions\u201d section for details.\n\nquestions_for_the_authors: 1. How do you obtain the entity embeddings for constructing the convex hull during training time intervention? Are you using a pre-trained model that provides such embeddings?\n2. How does debiasing impact long-tail entities? In particular, given the sparsity of the embedding space near the long-tail entities, how can we construct a convex hull?\n3. What is the interpretation of OOD in the experiments? Are these the entities not seen in training?\n\ntypos_grammar_style_and_presentation_improvements: Too many footnotes in the paper, some of these should be incorporated into the main text and some others should be placed in the Appendix section when they contain specific details, e.g., choice of an experiment set up.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "131_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_131_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7334999999999999,
      "max_similarity": 0.7497,
      "avg_coverage": 0.49756666666666666,
      "max_coverage": 0.5789
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 419,
      "avg_human_length": 492.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 6,
      "suggestions_count": 12
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "0720gzafb8",
        "similarity": 0.7233,
        "coverage": 0.5789,
        "human_length": 392,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a structured causal model (SCM) to mitigate entity bias in PLMs/LLMs.  To mitigate the conflicts between parametric knowledge and contextual knowledge, the paper first replaces entities with placeholders for the input sentence and then finds similar entities via LLM, thirdly generates the definition of placeholders with similar entities, and finally combines the definition, context, and prompt question to a whole input sentence. The experimental results show that the framework proposed by this paper could get better results compared with baseline models.\n\nreasons_to_accept: The paper proposes a causal view of entity bias in PLM/LLM models, which is interesting and has practical significance.\n\nreasons_to_reject: Various prompt-based learning methods have should that the classification results made by PLM/LLM can be affected by prompt templates. \ns it unclear whether the improvement in results from this method is specific because it mitigates the entity bias in PLM/LLM, or generates a more specific prompting template for PLM/LLM to make decisions.\nI think that LLM does not have entity bias because it has been pre-trained on a large amount of unsupervised/semi-supervised data. The entity bias in prompts given to LLM may be the reason why there is entity bias in  LLM inference results. The paper should analyze this point of view.\n Using the top k nearest neighbors to construct the convex hull seems like the prototype-based method. As the value of k increases, the center point of the convex hull should theoretically be closer to the true meaning of this entity category. In other words, the center point of the box will have less bias towards this entity category. This means the center of the hull moves further away from the original entity. But it also means the original entity has a bigger bias, why the results become worse?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "zWZGnjsEHv",
        "similarity": 0.7497,
        "coverage": 0.5,
        "human_length": 707,
        "human_text": "paper_topic_and_main_contributions: This paper introduces techniques for mitigating entity bias as both a training-time intervention and prompting intervention for white-box and black-box LLMs respectively. They conduct an analysis of past entity bias mitigating techniques, and introduce a structured casual model to frame their technique.  The causal method is separated into a training time intervention, which consists of swapping a specific entities embedding in the lowest layer with an embedding at the center of a convex hull built off of closely related entities.  They also propose a method for use with black-box LLMs, which focuses on prompting the LLM to consider related entities in addition to a specific one.\nThey experiment with their interventions on the two earlier mentioned styles of LLMs and show that the training-time intervention improves over past methods in this study, specifically in the OOD performance. They also show that their prompting intervention aids black-box LLMs in mitigating entity bias as compared to their baseline techniques.\n\nreasons_to_accept: A) The paper is generally well-written and does a thorough analysis of past work in the entity bias space. \nB) Figures 3 and 4 in particular are well-illustrative of the techniques used. \nC) The ablation study on the in-context prompting section also shows clearly which components of their prompting strategy contribute to performance. \nD) Methods that use causal approaches with LLMs are important in achieving high performance in applications and reducing bias/errors. \nE) The authors report a robust set of experiments with multiple runs and error bars.\n\nreasons_to_reject: A) In-domain performance drops when applying the method as a training-time intervention. Although this is understandably an OOD mitigation technique, it would be nice to have consistent performance in domain B) There could be additional experiments in the in-context prompting section, as some important baselines for in-context prompting have not been included.\n\nquestions_for_the_authors: A) Where do you get the set of entity neighbors to compute the convex hull from?  Were they extracted from the relevant datasets, or from an associated KB?\nB) In figure 1 there is an option for the model to choose from two given options in order to answer the entity relation question, however, in figure 4 that option is not present. Are options used in your experiments, and if so, how does the method work without those given options?\n(If no options given), One issue I see is that the method may not debias across multiple axes. How do you ensure that the training time intervention can debias across multiple axes? In the given example, Bill Gates is decoupled from his role as a founder and is shown to be a visitor. But how can the intervention debias across his nationality, his gender, etc.?\nC) In in-context example learning, how can the prompting strategy be applied to multiple entities? Has any analysis been done on if entity bias is sought for only a fraction of the entities present in the sample and what effect that has on performance?\nD) Can you elaborate more on why in-domain performance decreases when using your method? In what situations would a practitioner utilize this method given this drop?\nE) For in-context learning, can\u2019t entity bias be mitigated through other prompting techniques, such as demonstrations via in-context examples, CoT techniques, self-checking, etc.? How would these methods compare as a baseline against your method?  F) I would like further discussion of this point from the limitations section: \u201cIn this paper, we only consider zero-shot prompting for black-box LLMs, because this will help us to control variables during causal analysis.\u201d\n\ntypos_grammar_style_and_presentation_improvements: The \u201cPLM\u201d usage in the abstract is not defined.\nFigure 3 caption could use further explanation so that it is more easily understood on quick passes.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "1Y0yMFs2jw",
        "similarity": 0.7275,
        "coverage": 0.4138,
        "human_length": 377,
        "human_text": "paper_topic_and_main_contributions: The paper aims to eliminate entity bias in pre-trained language models through causal intervention methods. In particular, the paper proposes an entity perturbation method that perturbs the input text with similar entities from the convex hull of the target entity\u2019s representation. Such causal intervention at the input layer is shown to be effective on both training-time and in-context interventions. The proposed debiasing approach is evaluated on relation extraction data sets (TACRED and EntRED) and reading comprehension data set (TriviaQA).  Experimental results suggest strong performance gain over other baselines.\n\nreasons_to_accept: 1. Strong empirical results demonstrating the effectiveness of the approach.\n2. The paper shows ingenuity in constructing a prompt that is effective for debiasing entity bias. It will be interesting to the community to follow up on this work and see whether similar approaches can help to mitigate other types of biases.\n3. The paper is well written and easy to follow\n\nreasons_to_reject: 1. It seems the effectiveness of the proposed method is over-reliant on either the LLMs ability to provide similar entities (in-context intervention) or a pre-trained model\u2019s ability to retrieve similar entities (training-time intervention).\n2. There are some missing details in the paper. See the \u201cQuestions\u201d section for details.\n\nquestions_for_the_authors: 1. How do you obtain the entity embeddings for constructing the convex hull during training time intervention? Are you using a pre-trained model that provides such embeddings?\n2. How does debiasing impact long-tail entities? In particular, given the sparsity of the embedding space near the long-tail entities, how can we construct a convex hull?\n3. What is the interpretation of OOD in the experiments? Are these the entities not seen in training?\n\ntypos_grammar_style_and_presentation_improvements: Too many footnotes in the paper, some of these should be incorporated into the main text and some others should be placed in the Appendix section when they contain specific details, e.g., choice of an experiment set up.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "0720gzafb8",
        "length": 392,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a structured causal model (SCM) to mitigate entity bias in PLMs/LLMs.  To mitigate the conflicts between parametric knowledge and contextual knowledge, the paper first replaces entities with placeholders for the input sentence and then finds similar entities via LLM, thirdly generates the definition of placeholders with similar entities, and finally combines the definition, context, and prompt question to a whole input sentence. The experimental results show that the framework proposed by this paper could get better results compared with baseline models.\n\nreasons_to_accept: The paper proposes a causal view of entity bias in PLM/LLM models, which is interesting and has practical significance.\n\nreasons_to_reject: Various prompt-based learning methods have should that the classification results made by PLM/LLM can be affected by prompt templates. \ns it unclear whether the improvement in results from this method is specific because it mitigates the entity bias in PLM/LLM, or generates a more specific prompting template for PLM/LLM to make decisions.\nI think that LLM does not have entity bias because it has been pre-trained on a large amount of unsupervised/semi-supervised data. The entity bias in prompts given to LLM may be the reason why there is entity bias in  LLM inference results. The paper should analyze this point of view.\n Using the top k nearest neighbors to construct the convex hull seems like the prototype-based method. As the value of k increases, the center point of the convex hull should theoretically be closer to the true meaning of this entity category. In other words, the center point of the box will have less bias towards this entity category. This means the center of the hull moves further away from the original entity. But it also means the original entity has a bigger bias, why the results become worse?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "zWZGnjsEHv",
        "length": 707,
        "human_text": "paper_topic_and_main_contributions: This paper introduces techniques for mitigating entity bias as both a training-time intervention and prompting intervention for white-box and black-box LLMs respectively. They conduct an analysis of past entity bias mitigating techniques, and introduce a structured casual model to frame their technique.  The causal method is separated into a training time intervention, which consists of swapping a specific entities embedding in the lowest layer with an embedding at the center of a convex hull built off of closely related entities.  They also propose a method for use with black-box LLMs, which focuses on prompting the LLM to consider related entities in addition to a specific one.\nThey experiment with their interventions on the two earlier mentioned styles of LLMs and show that the training-time intervention improves over past methods in this study, specifically in the OOD performance. They also show that their prompting intervention aids black-box LLMs in mitigating entity bias as compared to their baseline techniques.\n\nreasons_to_accept: A) The paper is generally well-written and does a thorough analysis of past work in the entity bias space. \nB) Figures 3 and 4 in particular are well-illustrative of the techniques used. \nC) The ablation study on the in-context prompting section also shows clearly which components of their prompting strategy contribute to performance. \nD) Methods that use causal approaches with LLMs are important in achieving high performance in applications and reducing bias/errors. \nE) The authors report a robust set of experiments with multiple runs and error bars.\n\nreasons_to_reject: A) In-domain performance drops when applying the method as a training-time intervention. Although this is understandably an OOD mitigation technique, it would be nice to have consistent performance in domain B) There could be additional experiments in the in-context prompting section, as some important baselines for in-context prompting have not been included.\n\nquestions_for_the_authors: A) Where do you get the set of entity neighbors to compute the convex hull from?  Were they extracted from the relevant datasets, or from an associated KB?\nB) In figure 1 there is an option for the model to choose from two given options in order to answer the entity relation question, however, in figure 4 that option is not present. Are options used in your experiments, and if so, how does the method work without those given options?\n(If no options given), One issue I see is that the method may not debias across multiple axes. How do you ensure that the training time intervention can debias across multiple axes? In the given example, Bill Gates is decoupled from his role as a founder and is shown to be a visitor. But how can the intervention debias across his nationality, his gender, etc.?\nC) In in-context example learning, how can the prompting strategy be applied to multiple entities? Has any analysis been done on if entity bias is sought for only a fraction of the entities present in the sample and what effect that has on performance?\nD) Can you elaborate more on why in-domain performance decreases when using your method? In what situations would a practitioner utilize this method given this drop?\nE) For in-context learning, can\u2019t entity bias be mitigated through other prompting techniques, such as demonstrations via in-context examples, CoT techniques, self-checking, etc.? How would these methods compare as a baseline against your method?  F) I would like further discussion of this point from the limitations section: \u201cIn this paper, we only consider zero-shot prompting for black-box LLMs, because this will help us to control variables during causal analysis.\u201d\n\ntypos_grammar_style_and_presentation_improvements: The \u201cPLM\u201d usage in the abstract is not defined.\nFigure 3 caption could use further explanation so that it is more easily understood on quick passes.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "1Y0yMFs2jw",
        "length": 377,
        "human_text": "paper_topic_and_main_contributions: The paper aims to eliminate entity bias in pre-trained language models through causal intervention methods. In particular, the paper proposes an entity perturbation method that perturbs the input text with similar entities from the convex hull of the target entity\u2019s representation. Such causal intervention at the input layer is shown to be effective on both training-time and in-context interventions. The proposed debiasing approach is evaluated on relation extraction data sets (TACRED and EntRED) and reading comprehension data set (TriviaQA).  Experimental results suggest strong performance gain over other baselines.\n\nreasons_to_accept: 1. Strong empirical results demonstrating the effectiveness of the approach.\n2. The paper shows ingenuity in constructing a prompt that is effective for debiasing entity bias. It will be interesting to the community to follow up on this work and see whether similar approaches can help to mitigate other types of biases.\n3. The paper is well written and easy to follow\n\nreasons_to_reject: 1. It seems the effectiveness of the proposed method is over-reliant on either the LLMs ability to provide similar entities (in-context intervention) or a pre-trained model\u2019s ability to retrieve similar entities (training-time intervention).\n2. There are some missing details in the paper. See the \u201cQuestions\u201d section for details.\n\nquestions_for_the_authors: 1. How do you obtain the entity embeddings for constructing the convex hull during training time intervention? Are you using a pre-trained model that provides such embeddings?\n2. How does debiasing impact long-tail entities? In particular, given the sparsity of the embedding space near the long-tail entities, how can we construct a convex hull?\n3. What is the interpretation of OOD in the experiments? Are these the entities not seen in training?\n\ntypos_grammar_style_and_presentation_improvements: Too many footnotes in the paper, some of these should be incorporated into the main text and some others should be placed in the Appendix section when they contain specific details, e.g., choice of an experiment set up.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "29_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_29_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7372333333333333,
      "max_similarity": 0.761,
      "avg_coverage": 0.7056666666666667,
      "max_coverage": 0.8462
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 571,
      "avg_human_length": 258.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 13
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "PQsr3FbfSf",
        "similarity": 0.7363,
        "coverage": 0.4583,
        "human_length": 344,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the cross-domain few-shot RE task from causal perspective. The authors propose a novel framework CausalGF based on structural causal model. The structural graph provides a theoretical explanation of feature selection and fusion for classification. Experimental results show that the proposed model outperforms baseline models on two benchmark datasets of this task.\n\nreasons_to_accept: This paper proposes a novel theoretical framework for classification feature selection and fusion, and experimental results demonstrate the effectiveness of the proposed method.\n\nreasons_to_reject: The motivation of graph topology selection is not well presented. See details in the first question in \u201cQuestion For The Authors\u201d.\n\nquestions_for_the_authors: 1. In Figure 1, the edge from C to X is blocked and the counterfactual S* and E* are connected to X. However, it\u2019s easy to observe another alternative, which blocks the edge from C to Y and connects S* and E* to Y. The resulting graph will be like S*, E*, L, X connected to Y and C connected to X. In the prompt encoding step, h_S and h_E will be separately weighted and contribute to the total causal effect, which is different to the proposed method. Here\u2019s my question: why are you choosing the graph described in the paper instead of the above alternative? Is concatenating h_S and h_E better than separating them? If yes, would you please explain the reason? \n2. Is this approach capable of migrating to regular cross-domain RE task? \n3. In Figure 2, training on source domain is using S* and E*, but they are not used in inference. I don\u2019t think S* and E* include label information, so would you please explain the reason or point out if I am missing some information?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "LnxyqRQbk3",
        "similarity": 0.7144,
        "coverage": 0.8462,
        "human_length": 192,
        "human_text": "paper_topic_and_main_contributions: Previous work has mainly focused on transferring knowledge between domains through shared feature representations without analyzing the impact of each factor that may produce databases based on the characteristics of each domain. In this paper, the authors proposed a new framework CausalGF which is the first work analyzing data bias and the influence of various factors from a causal perspective in cross-domain few-shot RE tasks. The CausalGF outperforms previous SOTA methods in all scenarios.\n\nreasons_to_accept: 1. The symbols are clear and the content is correct and comparatively novel.   2. It is the first work analyzing data bias and the influence of various factors from a causal perspective in cross-domain few-shot RE tasks.\n\nreasons_to_reject: 1. Some content in the appendix needs to be adjusted to the main text to improve clarity and conciseness. For example, consider moving a brief description of the baseline from Appendix D to Section 3.1.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "Tmzcec5ieK",
        "similarity": 0.761,
        "coverage": 0.8125,
        "human_length": 240,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the cross-domain few-shot Relation Extraction task. It proposes an unified structral causal model called CausalGF from the causal perspective. This model adaptively fills the data bias gap across diffrent domains, using intervention and counterfactual generation causal operation to achieve this purpose. Extensive experiments on different datasets and settings demonstrate the effectiveness of this approach.\n\nreasons_to_accept: (1) The causal perspective to mitigate the domain gap for cross-domain RE task is sound.\n(2) The method is elaborated through theoretical proof, which is interesting.\n(3) The method obtains significant improvements over multiple baseline models.\n\nreasons_to_reject: (1) Lack of necessary case study to demonstrate why this method works.\n(2) The reason why syntactic structure and entities are distinct cross-domain gaps is not clarified.\n(3) Detailed experiment settings should be provided such as hyperparameter and data preprocessing.\n\nquestions_for_the_authors: Please refer to \"Reasons To Reject\".\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "PQsr3FbfSf",
        "length": 344,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the cross-domain few-shot RE task from causal perspective. The authors propose a novel framework CausalGF based on structural causal model. The structural graph provides a theoretical explanation of feature selection and fusion for classification. Experimental results show that the proposed model outperforms baseline models on two benchmark datasets of this task.\n\nreasons_to_accept: This paper proposes a novel theoretical framework for classification feature selection and fusion, and experimental results demonstrate the effectiveness of the proposed method.\n\nreasons_to_reject: The motivation of graph topology selection is not well presented. See details in the first question in \u201cQuestion For The Authors\u201d.\n\nquestions_for_the_authors: 1. In Figure 1, the edge from C to X is blocked and the counterfactual S* and E* are connected to X. However, it\u2019s easy to observe another alternative, which blocks the edge from C to Y and connects S* and E* to Y. The resulting graph will be like S*, E*, L, X connected to Y and C connected to X. In the prompt encoding step, h_S and h_E will be separately weighted and contribute to the total causal effect, which is different to the proposed method. Here\u2019s my question: why are you choosing the graph described in the paper instead of the above alternative? Is concatenating h_S and h_E better than separating them? If yes, would you please explain the reason? \n2. Is this approach capable of migrating to regular cross-domain RE task? \n3. In Figure 2, training on source domain is using S* and E*, but they are not used in inference. I don\u2019t think S* and E* include label information, so would you please explain the reason or point out if I am missing some information?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "LnxyqRQbk3",
        "length": 192,
        "human_text": "paper_topic_and_main_contributions: Previous work has mainly focused on transferring knowledge between domains through shared feature representations without analyzing the impact of each factor that may produce databases based on the characteristics of each domain. In this paper, the authors proposed a new framework CausalGF which is the first work analyzing data bias and the influence of various factors from a causal perspective in cross-domain few-shot RE tasks. The CausalGF outperforms previous SOTA methods in all scenarios.\n\nreasons_to_accept: 1. The symbols are clear and the content is correct and comparatively novel.   2. It is the first work analyzing data bias and the influence of various factors from a causal perspective in cross-domain few-shot RE tasks.\n\nreasons_to_reject: 1. Some content in the appendix needs to be adjusted to the main text to improve clarity and conciseness. For example, consider moving a brief description of the baseline from Appendix D to Section 3.1.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "Tmzcec5ieK",
        "length": 240,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the cross-domain few-shot Relation Extraction task. It proposes an unified structral causal model called CausalGF from the causal perspective. This model adaptively fills the data bias gap across diffrent domains, using intervention and counterfactual generation causal operation to achieve this purpose. Extensive experiments on different datasets and settings demonstrate the effectiveness of this approach.\n\nreasons_to_accept: (1) The causal perspective to mitigate the domain gap for cross-domain RE task is sound.\n(2) The method is elaborated through theoretical proof, which is interesting.\n(3) The method obtains significant improvements over multiple baseline models.\n\nreasons_to_reject: (1) Lack of necessary case study to demonstrate why this method works.\n(2) The reason why syntactic structure and entities are distinct cross-domain gaps is not clarified.\n(3) Detailed experiment settings should be provided such as hyperparameter and data preprocessing.\n\nquestions_for_the_authors: Please refer to \"Reasons To Reject\".\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "29_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_29_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7337333333333333,
      "max_similarity": 0.7531,
      "avg_coverage": 0.7521333333333334,
      "max_coverage": 0.9231
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 582,
      "avg_human_length": 258.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "PQsr3FbfSf",
        "similarity": 0.7295,
        "coverage": 0.4583,
        "human_length": 344,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the cross-domain few-shot RE task from causal perspective. The authors propose a novel framework CausalGF based on structural causal model. The structural graph provides a theoretical explanation of feature selection and fusion for classification. Experimental results show that the proposed model outperforms baseline models on two benchmark datasets of this task.\n\nreasons_to_accept: This paper proposes a novel theoretical framework for classification feature selection and fusion, and experimental results demonstrate the effectiveness of the proposed method.\n\nreasons_to_reject: The motivation of graph topology selection is not well presented. See details in the first question in \u201cQuestion For The Authors\u201d.\n\nquestions_for_the_authors: 1. In Figure 1, the edge from C to X is blocked and the counterfactual S* and E* are connected to X. However, it\u2019s easy to observe another alternative, which blocks the edge from C to Y and connects S* and E* to Y. The resulting graph will be like S*, E*, L, X connected to Y and C connected to X. In the prompt encoding step, h_S and h_E will be separately weighted and contribute to the total causal effect, which is different to the proposed method. Here\u2019s my question: why are you choosing the graph described in the paper instead of the above alternative? Is concatenating h_S and h_E better than separating them? If yes, would you please explain the reason? \n2. Is this approach capable of migrating to regular cross-domain RE task? \n3. In Figure 2, training on source domain is using S* and E*, but they are not used in inference. I don\u2019t think S* and E* include label information, so would you please explain the reason or point out if I am missing some information?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "LnxyqRQbk3",
        "similarity": 0.7186,
        "coverage": 0.9231,
        "human_length": 192,
        "human_text": "paper_topic_and_main_contributions: Previous work has mainly focused on transferring knowledge between domains through shared feature representations without analyzing the impact of each factor that may produce databases based on the characteristics of each domain. In this paper, the authors proposed a new framework CausalGF which is the first work analyzing data bias and the influence of various factors from a causal perspective in cross-domain few-shot RE tasks. The CausalGF outperforms previous SOTA methods in all scenarios.\n\nreasons_to_accept: 1. The symbols are clear and the content is correct and comparatively novel.   2. It is the first work analyzing data bias and the influence of various factors from a causal perspective in cross-domain few-shot RE tasks.\n\nreasons_to_reject: 1. Some content in the appendix needs to be adjusted to the main text to improve clarity and conciseness. For example, consider moving a brief description of the baseline from Appendix D to Section 3.1.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "Tmzcec5ieK",
        "similarity": 0.7531,
        "coverage": 0.875,
        "human_length": 240,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the cross-domain few-shot Relation Extraction task. It proposes an unified structral causal model called CausalGF from the causal perspective. This model adaptively fills the data bias gap across diffrent domains, using intervention and counterfactual generation causal operation to achieve this purpose. Extensive experiments on different datasets and settings demonstrate the effectiveness of this approach.\n\nreasons_to_accept: (1) The causal perspective to mitigate the domain gap for cross-domain RE task is sound.\n(2) The method is elaborated through theoretical proof, which is interesting.\n(3) The method obtains significant improvements over multiple baseline models.\n\nreasons_to_reject: (1) Lack of necessary case study to demonstrate why this method works.\n(2) The reason why syntactic structure and entities are distinct cross-domain gaps is not clarified.\n(3) Detailed experiment settings should be provided such as hyperparameter and data preprocessing.\n\nquestions_for_the_authors: Please refer to \"Reasons To Reject\".\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "PQsr3FbfSf",
        "length": 344,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the cross-domain few-shot RE task from causal perspective. The authors propose a novel framework CausalGF based on structural causal model. The structural graph provides a theoretical explanation of feature selection and fusion for classification. Experimental results show that the proposed model outperforms baseline models on two benchmark datasets of this task.\n\nreasons_to_accept: This paper proposes a novel theoretical framework for classification feature selection and fusion, and experimental results demonstrate the effectiveness of the proposed method.\n\nreasons_to_reject: The motivation of graph topology selection is not well presented. See details in the first question in \u201cQuestion For The Authors\u201d.\n\nquestions_for_the_authors: 1. In Figure 1, the edge from C to X is blocked and the counterfactual S* and E* are connected to X. However, it\u2019s easy to observe another alternative, which blocks the edge from C to Y and connects S* and E* to Y. The resulting graph will be like S*, E*, L, X connected to Y and C connected to X. In the prompt encoding step, h_S and h_E will be separately weighted and contribute to the total causal effect, which is different to the proposed method. Here\u2019s my question: why are you choosing the graph described in the paper instead of the above alternative? Is concatenating h_S and h_E better than separating them? If yes, would you please explain the reason? \n2. Is this approach capable of migrating to regular cross-domain RE task? \n3. In Figure 2, training on source domain is using S* and E*, but they are not used in inference. I don\u2019t think S* and E* include label information, so would you please explain the reason or point out if I am missing some information?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "LnxyqRQbk3",
        "length": 192,
        "human_text": "paper_topic_and_main_contributions: Previous work has mainly focused on transferring knowledge between domains through shared feature representations without analyzing the impact of each factor that may produce databases based on the characteristics of each domain. In this paper, the authors proposed a new framework CausalGF which is the first work analyzing data bias and the influence of various factors from a causal perspective in cross-domain few-shot RE tasks. The CausalGF outperforms previous SOTA methods in all scenarios.\n\nreasons_to_accept: 1. The symbols are clear and the content is correct and comparatively novel.   2. It is the first work analyzing data bias and the influence of various factors from a causal perspective in cross-domain few-shot RE tasks.\n\nreasons_to_reject: 1. Some content in the appendix needs to be adjusted to the main text to improve clarity and conciseness. For example, consider moving a brief description of the baseline from Appendix D to Section 3.1.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "Tmzcec5ieK",
        "length": 240,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the cross-domain few-shot Relation Extraction task. It proposes an unified structral causal model called CausalGF from the causal perspective. This model adaptively fills the data bias gap across diffrent domains, using intervention and counterfactual generation causal operation to achieve this purpose. Extensive experiments on different datasets and settings demonstrate the effectiveness of this approach.\n\nreasons_to_accept: (1) The causal perspective to mitigate the domain gap for cross-domain RE task is sound.\n(2) The method is elaborated through theoretical proof, which is interesting.\n(3) The method obtains significant improvements over multiple baseline models.\n\nreasons_to_reject: (1) Lack of necessary case study to demonstrate why this method works.\n(2) The reason why syntactic structure and entities are distinct cross-domain gaps is not clarified.\n(3) Detailed experiment settings should be provided such as hyperparameter and data preprocessing.\n\nquestions_for_the_authors: Please refer to \"Reasons To Reject\".\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "37_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_37_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.707125,
      "max_similarity": 0.7212,
      "avg_coverage": 0.34845,
      "max_coverage": 0.4333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 445,
      "avg_human_length": 519.5
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 9,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "1YqlJFHjZM",
        "similarity": 0.703,
        "coverage": 0.3793,
        "human_length": 389,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a novel method called INSTRUCTOR for unsupervised conversational dense retrieval. The key idea is to leverage large language models (LLMs) to provide supervised signals to guide the training of retrievers, without requiring any labeled data. The main contributions are: 1. An unsupervised training framework where LLMs generate soft session-passage relevance scores to instruct the retrievers.\n2. Three strategies to accurately estimate relevance from different perspectives: conversational retrieval as conversation generation, question rewrite as latent variable, and question response as posterior guide.\nThe paper addresses the lack of labeled data for conversational retrieval, and demonstrates how large pretrained models can be leveraged for unsupervised learning in this setting.\n\nreasons_to_accept: 1. The paper tackles an important problem of unsupervised conversational retrieval training. The way using LLMs to provide training signals is a novel in the conversational context (although it has been validated in ad-hoc search), and the three strategies provide nice ways to leverage LLMs from different perspectives of the conversation. This can enable building conversational retrievers without labeled data.\n2. Thorough ablation studies analyzing the effect of different proposed strategies.\n3. Strong empirical results, surpassing supervised approaches on two datasets.\n4. Well-written paper with sufficient details to reproduce the approach.\n\nreasons_to_reject: 1. Limited analysis on how different types of conversations/questions affect the quality of LLM supervision.\n2. Unclear how the approach scales as the dataset size increases. Memory and compute costs?\n\nquestions_for_the_authors: Can you provide more analysis into how the quality of LLM-generated supervision signals varies across different types of conversations or questions? Any patterns in where it works better/worse?\n\ntypos_grammar_style_and_presentation_improvements: Line 143: \"discover\" -> \"discovering\" Line 289: Extra space before period Section 3.2: Consider using sub-section headings for each strategy Figure 1: Increase font size of text in the boxes Figure 3: Improve spacing around \"+\" symbols\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "tbXSUJZoMg",
        "similarity": 0.7025,
        "coverage": 0.3125,
        "human_length": 434,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new unsupervised method \u201cInstructor\u201d for training conversational dense retrieval models. Instructor leverages the knowledge from large language models (LLMs) to generate supervision signals to instruct the retriever. To distill the knowledge from the LLMs to the retriever, the authors design three methods to calculate the relevance score more precisely. Sufficient experimental results on four datasets under various settings demonstrate the effectiveness of the proposed method. Overall, the paper is well-written and easy to understand. The experiments look very solid. Though the technical depth is not so deep, and the distillation idea is not very novel, I suggest a weak accept on this paper.\n\nreasons_to_accept: 1. The proposed method leverages LLMs for knowledge distillation, which is a good application of LLMs on real research. \n2. The experiments are sufficient and solid. The results show significant improvement over existing studies. \n3. The paper is well-written and easy to understand.\n\nreasons_to_reject: 1. Distilling knowledge from LLM to retriever, or more generally, distilling knowledge from reranker to retriever is not a new idea [1, 2]. \n2. Missing some recent literature in the related work part.\nReference: 1. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent, Sun et al. 2023 2. Large language models are effective text rankers with pairwise ranking prompting, Qin et al., 2023\n\nquestions_for_the_authors: Q1: How do you compute the confidence $\\log p(r|I^{r}_{c,q},c,q)$ by ChatGPT? It seems that it cannot return the generation probability through API. Even if it is text-davinci, it can only provide the probability of the first five tokens. How do you deal with this problem?\nQ2: The proposed method is applied to several ad-hoc dense retriever. Is it also possible to apply it for conversational dense retrieval model?\n\nmissing_references: [1] Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search, Mao et al., 2023 [2] ConvGQR: Generative Query Reformulation for Conversational Search, Mo et al., 2023 [3] Learning to Relate to Previous Turns in Conversational Search, Mo et al., 2023\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "6VbSd6EedP",
        "similarity": 0.7212,
        "coverage": 0.2687,
        "human_length": 806,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method called INSTRUCTOR for training conversational dense retrievers in an unsupervised manner, without needing labeled session-passage pairs. The key problem it addresses is the lack of training data for conversational retrieval, where annotating session-passage relevance is difficult. Most existing methods rely on supervised fine-tuning.\nThe main contributions are: 1. An unsupervised training framework where large language models estimate session-passage relevance scores to guide retriever training.\n2. Three strategies to more accurately calculate relevance using the language model: modeling retrieval as conversation generation, question rewriting as a latent variable, and using question responses as a posterior guide.\n3. Experiments showing INSTRUCTOR significantly improves various ad-hoc retrievers like DPR and ANCE, even surpassing supervised methods.\nIn summary, the paper proposes a novel unsupervised approach for training conversational retrievers by instructing them with large language models. The method and thorough experiments are the key contributions.\n\nreasons_to_accept: 1. Addresses an important practical problem that lacking of conversational retrieval training data and proposes a creative method for leveraging large language models to provide training signal without any data annotation.\n2. Thorough and rigorous experiments across diverse settings like low-resource and zero-shot. Surpassing supervised methods is a notable result.\nOverall, the novel unsupervised learning approach, thorough experiments, and analyses make this a valuable contribution.\n\nreasons_to_reject: 1. The approach relies heavily on large proprietary language models, which have limitations like bias and lack of transparency and there are limited error analysis to understand cases where the method fails. This could be important since the LLM is used as a black-box model and the quality of generated results are without guarantee. Meanwhile, the workflow details (e.g. the concrete prompt and the reformulated query) for the proposed three strategies should be much clearer.\n2. There is no detailed analysis on how the unsupervised training objectives affect the retriever representations and no comparison with query reformulation methods. Besides, the cost and the latency of using LLM should also be indicated.\n3. Lack of control of injecting noise (bring by the generated text from LLM) and the analysis of how these expansion terms will influence the retrieval results. This is important in terms of the query analysis and avoid the harmful terms generation.\nThe main risks relate to overselling the generality of the approach and glossing over potential issues with language models. But the paper seems reasonably cautious about limitations. Overall the methodology appears solid despite some aspects needing deeper analysis.\n\nquestions_for_the_authors: 1. Can this framework designed without LLM? ( i.e. How should we think is this method flexible/feasible or not) and how can we ensure the reproduction as the current LLM normally with fast version iteration. If not, is it still a flexible framework?\n2. What is the cost and the latency of using LLM, especially on the two big QReCC and TopiOCQA datasets? Please show statistic information.\n3. How can we control the generated text by LLM will not bring noise to the query encoder training and how can we analysis via experiments? In other words, how to evaluate the quality of the generated text by LLM for IR evaluation.  4. How to consider the comparison with the manually rewritten query and the query reformulation approaches, especially on the cast datasets.\n5. For Eq. 7, why we can assume log p(z), log p(c) and log p(q | c) as instants and what is the relation between eq. 7 and the quality of the generated text?\n\nmissing_references: 1. Open-retrieval conversational question answering. Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W Bruce Croft, Mohit Iyyer. ( SIGIR 2020) 2. Explicit Query Rewriting for Conversational Dense Retrieval. Hongjin Qian, Zhicheng Dou. ( EMNLP 2022) 3. ConvGQR: Generative Query Reformulation for Conversational Search. Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huang, Jian-Yun Nie (ACL 2023) 4. Learning to Relate to Previous Turns in Conversational Search. Fengran Mo, Jian-Yun Nie, Kaiyu Huang, Kelong Mao, Yutao Zhu, Peng Li, Yang Liu (SIGKDD 2023) 5. CoSPLADE: Contextualizing SPLADE for Conversational Information Retrieval. Nam Hai Le, Thomas Gerald, Thibault Formal, Jian-Yun Nie, Benjamin Piwowarski, Laure Soulier. ( ECIR 2023) The query reformulation methods could be included to be discussed and some recent publications could be added to the related work later to make the literature review more complete.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "ce66DuSrBw",
        "similarity": 0.7018,
        "coverage": 0.4333,
        "human_length": 449,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of conversational passage retrieval. The authors propose a method, named Instructor, to use frozen LLMs to provide retriever training signals. Specifically, the relevance score between a passage and a conversational query (i.e., conversation history + current user question) is calculated as the generation probability of the passage by a LLM, conditioning on the conversational query. The authors propose three ways to calculate such generation probabilities. Then they train the retriever with the KL loss between LM generation probabilities and passage-query similarities from the retriever.\nThey conduct experiments on three conversational retrieval benchmarks and achiever significant improvements with such training guidance from LLMs.\n\nreasons_to_accept: 1. The paper is well-written and easy to follow. \n2. They propose a novel framework to fine-tune the retriever model to handle conversational queries. \n3. They did extensive experiments to show improvements of their approach on three different benchmarks.\n\nreasons_to_reject: 1. My major concern is that you might be able to achieve impressive results with weak retrieval supervision from the conversation data, without the need to query LLMs as instructors. The training data used, namely QReCC and TopiOCQA, comes with both question and answer turns. You can apply the same method used in CONQRR (Wu et al., 2022) to derive \"gold\" passages and fine-tune retrievers with those weak labels. My guess is that the performance can already be good enough, which may also explain why QRPG gives the highest score as it uses the next answer turns. Also, you'll need to use hard negatives in retriever training as that's widely acknowledged to boost performance.\n2. I don't agree with claim that \"annotating session-passage pairs is much more difficult than collecting conversation data\". The datasets you use are collected by having annotators search for passages first before writing agent utterances. How could annotating passages more difficult than labeling conversation data?\n3. You are using LLM generation probability to calculate the document relevance, which is then used as training signals for retrievers. Why don't you simply use LLM generation prob to rank passages? You may get even better results than fine-tuning the retriever.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "1YqlJFHjZM",
        "length": 389,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a novel method called INSTRUCTOR for unsupervised conversational dense retrieval. The key idea is to leverage large language models (LLMs) to provide supervised signals to guide the training of retrievers, without requiring any labeled data. The main contributions are: 1. An unsupervised training framework where LLMs generate soft session-passage relevance scores to instruct the retrievers.\n2. Three strategies to accurately estimate relevance from different perspectives: conversational retrieval as conversation generation, question rewrite as latent variable, and question response as posterior guide.\nThe paper addresses the lack of labeled data for conversational retrieval, and demonstrates how large pretrained models can be leveraged for unsupervised learning in this setting.\n\nreasons_to_accept: 1. The paper tackles an important problem of unsupervised conversational retrieval training. The way using LLMs to provide training signals is a novel in the conversational context (although it has been validated in ad-hoc search), and the three strategies provide nice ways to leverage LLMs from different perspectives of the conversation. This can enable building conversational retrievers without labeled data.\n2. Thorough ablation studies analyzing the effect of different proposed strategies.\n3. Strong empirical results, surpassing supervised approaches on two datasets.\n4. Well-written paper with sufficient details to reproduce the approach.\n\nreasons_to_reject: 1. Limited analysis on how different types of conversations/questions affect the quality of LLM supervision.\n2. Unclear how the approach scales as the dataset size increases. Memory and compute costs?\n\nquestions_for_the_authors: Can you provide more analysis into how the quality of LLM-generated supervision signals varies across different types of conversations or questions? Any patterns in where it works better/worse?\n\ntypos_grammar_style_and_presentation_improvements: Line 143: \"discover\" -> \"discovering\" Line 289: Extra space before period Section 3.2: Consider using sub-section headings for each strategy Figure 1: Increase font size of text in the boxes Figure 3: Improve spacing around \"+\" symbols\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "tbXSUJZoMg",
        "length": 434,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new unsupervised method \u201cInstructor\u201d for training conversational dense retrieval models. Instructor leverages the knowledge from large language models (LLMs) to generate supervision signals to instruct the retriever. To distill the knowledge from the LLMs to the retriever, the authors design three methods to calculate the relevance score more precisely. Sufficient experimental results on four datasets under various settings demonstrate the effectiveness of the proposed method. Overall, the paper is well-written and easy to understand. The experiments look very solid. Though the technical depth is not so deep, and the distillation idea is not very novel, I suggest a weak accept on this paper.\n\nreasons_to_accept: 1. The proposed method leverages LLMs for knowledge distillation, which is a good application of LLMs on real research. \n2. The experiments are sufficient and solid. The results show significant improvement over existing studies. \n3. The paper is well-written and easy to understand.\n\nreasons_to_reject: 1. Distilling knowledge from LLM to retriever, or more generally, distilling knowledge from reranker to retriever is not a new idea [1, 2]. \n2. Missing some recent literature in the related work part.\nReference: 1. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent, Sun et al. 2023 2. Large language models are effective text rankers with pairwise ranking prompting, Qin et al., 2023\n\nquestions_for_the_authors: Q1: How do you compute the confidence $\\log p(r|I^{r}_{c,q},c,q)$ by ChatGPT? It seems that it cannot return the generation probability through API. Even if it is text-davinci, it can only provide the probability of the first five tokens. How do you deal with this problem?\nQ2: The proposed method is applied to several ad-hoc dense retriever. Is it also possible to apply it for conversational dense retrieval model?\n\nmissing_references: [1] Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search, Mao et al., 2023 [2] ConvGQR: Generative Query Reformulation for Conversational Search, Mo et al., 2023 [3] Learning to Relate to Previous Turns in Conversational Search, Mo et al., 2023\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "6VbSd6EedP",
        "length": 806,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method called INSTRUCTOR for training conversational dense retrievers in an unsupervised manner, without needing labeled session-passage pairs. The key problem it addresses is the lack of training data for conversational retrieval, where annotating session-passage relevance is difficult. Most existing methods rely on supervised fine-tuning.\nThe main contributions are: 1. An unsupervised training framework where large language models estimate session-passage relevance scores to guide retriever training.\n2. Three strategies to more accurately calculate relevance using the language model: modeling retrieval as conversation generation, question rewriting as a latent variable, and using question responses as a posterior guide.\n3. Experiments showing INSTRUCTOR significantly improves various ad-hoc retrievers like DPR and ANCE, even surpassing supervised methods.\nIn summary, the paper proposes a novel unsupervised approach for training conversational retrievers by instructing them with large language models. The method and thorough experiments are the key contributions.\n\nreasons_to_accept: 1. Addresses an important practical problem that lacking of conversational retrieval training data and proposes a creative method for leveraging large language models to provide training signal without any data annotation.\n2. Thorough and rigorous experiments across diverse settings like low-resource and zero-shot. Surpassing supervised methods is a notable result.\nOverall, the novel unsupervised learning approach, thorough experiments, and analyses make this a valuable contribution.\n\nreasons_to_reject: 1. The approach relies heavily on large proprietary language models, which have limitations like bias and lack of transparency and there are limited error analysis to understand cases where the method fails. This could be important since the LLM is used as a black-box model and the quality of generated results are without guarantee. Meanwhile, the workflow details (e.g. the concrete prompt and the reformulated query) for the proposed three strategies should be much clearer.\n2. There is no detailed analysis on how the unsupervised training objectives affect the retriever representations and no comparison with query reformulation methods. Besides, the cost and the latency of using LLM should also be indicated.\n3. Lack of control of injecting noise (bring by the generated text from LLM) and the analysis of how these expansion terms will influence the retrieval results. This is important in terms of the query analysis and avoid the harmful terms generation.\nThe main risks relate to overselling the generality of the approach and glossing over potential issues with language models. But the paper seems reasonably cautious about limitations. Overall the methodology appears solid despite some aspects needing deeper analysis.\n\nquestions_for_the_authors: 1. Can this framework designed without LLM? ( i.e. How should we think is this method flexible/feasible or not) and how can we ensure the reproduction as the current LLM normally with fast version iteration. If not, is it still a flexible framework?\n2. What is the cost and the latency of using LLM, especially on the two big QReCC and TopiOCQA datasets? Please show statistic information.\n3. How can we control the generated text by LLM will not bring noise to the query encoder training and how can we analysis via experiments? In other words, how to evaluate the quality of the generated text by LLM for IR evaluation.  4. How to consider the comparison with the manually rewritten query and the query reformulation approaches, especially on the cast datasets.\n5. For Eq. 7, why we can assume log p(z), log p(c) and log p(q | c) as instants and what is the relation between eq. 7 and the quality of the generated text?\n\nmissing_references: 1. Open-retrieval conversational question answering. Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W Bruce Croft, Mohit Iyyer. ( SIGIR 2020) 2. Explicit Query Rewriting for Conversational Dense Retrieval. Hongjin Qian, Zhicheng Dou. ( EMNLP 2022) 3. ConvGQR: Generative Query Reformulation for Conversational Search. Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huang, Jian-Yun Nie (ACL 2023) 4. Learning to Relate to Previous Turns in Conversational Search. Fengran Mo, Jian-Yun Nie, Kaiyu Huang, Kelong Mao, Yutao Zhu, Peng Li, Yang Liu (SIGKDD 2023) 5. CoSPLADE: Contextualizing SPLADE for Conversational Information Retrieval. Nam Hai Le, Thomas Gerald, Thibault Formal, Jian-Yun Nie, Benjamin Piwowarski, Laure Soulier. ( ECIR 2023) The query reformulation methods could be included to be discussed and some recent publications could be added to the related work later to make the literature review more complete.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "ce66DuSrBw",
        "length": 449,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of conversational passage retrieval. The authors propose a method, named Instructor, to use frozen LLMs to provide retriever training signals. Specifically, the relevance score between a passage and a conversational query (i.e., conversation history + current user question) is calculated as the generation probability of the passage by a LLM, conditioning on the conversational query. The authors propose three ways to calculate such generation probabilities. Then they train the retriever with the KL loss between LM generation probabilities and passage-query similarities from the retriever.\nThey conduct experiments on three conversational retrieval benchmarks and achiever significant improvements with such training guidance from LLMs.\n\nreasons_to_accept: 1. The paper is well-written and easy to follow. \n2. They propose a novel framework to fine-tune the retriever model to handle conversational queries. \n3. They did extensive experiments to show improvements of their approach on three different benchmarks.\n\nreasons_to_reject: 1. My major concern is that you might be able to achieve impressive results with weak retrieval supervision from the conversation data, without the need to query LLMs as instructors. The training data used, namely QReCC and TopiOCQA, comes with both question and answer turns. You can apply the same method used in CONQRR (Wu et al., 2022) to derive \"gold\" passages and fine-tune retrievers with those weak labels. My guess is that the performance can already be good enough, which may also explain why QRPG gives the highest score as it uses the next answer turns. Also, you'll need to use hard negatives in retriever training as that's widely acknowledged to boost performance.\n2. I don't agree with claim that \"annotating session-passage pairs is much more difficult than collecting conversation data\". The datasets you use are collected by having annotators search for passages first before writing agent utterances. How could annotating passages more difficult than labeling conversation data?\n3. You are using LLM generation probability to calculate the document relevance, which is then used as training signals for retrievers. Why don't you simply use LLM generation prob to rank passages? You may get even better results than fine-tuning the retriever.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "37_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_37_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.70375,
      "max_similarity": 0.7207,
      "avg_coverage": 0.337175,
      "max_coverage": 0.4138
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 443,
      "avg_human_length": 519.5
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 9,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "1YqlJFHjZM",
        "similarity": 0.6984,
        "coverage": 0.4138,
        "human_length": 389,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a novel method called INSTRUCTOR for unsupervised conversational dense retrieval. The key idea is to leverage large language models (LLMs) to provide supervised signals to guide the training of retrievers, without requiring any labeled data. The main contributions are: 1. An unsupervised training framework where LLMs generate soft session-passage relevance scores to instruct the retrievers.\n2. Three strategies to accurately estimate relevance from different perspectives: conversational retrieval as conversation generation, question rewrite as latent variable, and question response as posterior guide.\nThe paper addresses the lack of labeled data for conversational retrieval, and demonstrates how large pretrained models can be leveraged for unsupervised learning in this setting.\n\nreasons_to_accept: 1. The paper tackles an important problem of unsupervised conversational retrieval training. The way using LLMs to provide training signals is a novel in the conversational context (although it has been validated in ad-hoc search), and the three strategies provide nice ways to leverage LLMs from different perspectives of the conversation. This can enable building conversational retrievers without labeled data.\n2. Thorough ablation studies analyzing the effect of different proposed strategies.\n3. Strong empirical results, surpassing supervised approaches on two datasets.\n4. Well-written paper with sufficient details to reproduce the approach.\n\nreasons_to_reject: 1. Limited analysis on how different types of conversations/questions affect the quality of LLM supervision.\n2. Unclear how the approach scales as the dataset size increases. Memory and compute costs?\n\nquestions_for_the_authors: Can you provide more analysis into how the quality of LLM-generated supervision signals varies across different types of conversations or questions? Any patterns in where it works better/worse?\n\ntypos_grammar_style_and_presentation_improvements: Line 143: \"discover\" -> \"discovering\" Line 289: Extra space before period Section 3.2: Consider using sub-section headings for each strategy Figure 1: Increase font size of text in the boxes Figure 3: Improve spacing around \"+\" symbols\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "tbXSUJZoMg",
        "similarity": 0.6966,
        "coverage": 0.2812,
        "human_length": 434,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new unsupervised method \u201cInstructor\u201d for training conversational dense retrieval models. Instructor leverages the knowledge from large language models (LLMs) to generate supervision signals to instruct the retriever. To distill the knowledge from the LLMs to the retriever, the authors design three methods to calculate the relevance score more precisely. Sufficient experimental results on four datasets under various settings demonstrate the effectiveness of the proposed method. Overall, the paper is well-written and easy to understand. The experiments look very solid. Though the technical depth is not so deep, and the distillation idea is not very novel, I suggest a weak accept on this paper.\n\nreasons_to_accept: 1. The proposed method leverages LLMs for knowledge distillation, which is a good application of LLMs on real research. \n2. The experiments are sufficient and solid. The results show significant improvement over existing studies. \n3. The paper is well-written and easy to understand.\n\nreasons_to_reject: 1. Distilling knowledge from LLM to retriever, or more generally, distilling knowledge from reranker to retriever is not a new idea [1, 2]. \n2. Missing some recent literature in the related work part.\nReference: 1. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent, Sun et al. 2023 2. Large language models are effective text rankers with pairwise ranking prompting, Qin et al., 2023\n\nquestions_for_the_authors: Q1: How do you compute the confidence $\\log p(r|I^{r}_{c,q},c,q)$ by ChatGPT? It seems that it cannot return the generation probability through API. Even if it is text-davinci, it can only provide the probability of the first five tokens. How do you deal with this problem?\nQ2: The proposed method is applied to several ad-hoc dense retriever. Is it also possible to apply it for conversational dense retrieval model?\n\nmissing_references: [1] Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search, Mao et al., 2023 [2] ConvGQR: Generative Query Reformulation for Conversational Search, Mo et al., 2023 [3] Learning to Relate to Previous Turns in Conversational Search, Mo et al., 2023\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "6VbSd6EedP",
        "similarity": 0.7207,
        "coverage": 0.2537,
        "human_length": 806,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method called INSTRUCTOR for training conversational dense retrievers in an unsupervised manner, without needing labeled session-passage pairs. The key problem it addresses is the lack of training data for conversational retrieval, where annotating session-passage relevance is difficult. Most existing methods rely on supervised fine-tuning.\nThe main contributions are: 1. An unsupervised training framework where large language models estimate session-passage relevance scores to guide retriever training.\n2. Three strategies to more accurately calculate relevance using the language model: modeling retrieval as conversation generation, question rewriting as a latent variable, and using question responses as a posterior guide.\n3. Experiments showing INSTRUCTOR significantly improves various ad-hoc retrievers like DPR and ANCE, even surpassing supervised methods.\nIn summary, the paper proposes a novel unsupervised approach for training conversational retrievers by instructing them with large language models. The method and thorough experiments are the key contributions.\n\nreasons_to_accept: 1. Addresses an important practical problem that lacking of conversational retrieval training data and proposes a creative method for leveraging large language models to provide training signal without any data annotation.\n2. Thorough and rigorous experiments across diverse settings like low-resource and zero-shot. Surpassing supervised methods is a notable result.\nOverall, the novel unsupervised learning approach, thorough experiments, and analyses make this a valuable contribution.\n\nreasons_to_reject: 1. The approach relies heavily on large proprietary language models, which have limitations like bias and lack of transparency and there are limited error analysis to understand cases where the method fails. This could be important since the LLM is used as a black-box model and the quality of generated results are without guarantee. Meanwhile, the workflow details (e.g. the concrete prompt and the reformulated query) for the proposed three strategies should be much clearer.\n2. There is no detailed analysis on how the unsupervised training objectives affect the retriever representations and no comparison with query reformulation methods. Besides, the cost and the latency of using LLM should also be indicated.\n3. Lack of control of injecting noise (bring by the generated text from LLM) and the analysis of how these expansion terms will influence the retrieval results. This is important in terms of the query analysis and avoid the harmful terms generation.\nThe main risks relate to overselling the generality of the approach and glossing over potential issues with language models. But the paper seems reasonably cautious about limitations. Overall the methodology appears solid despite some aspects needing deeper analysis.\n\nquestions_for_the_authors: 1. Can this framework designed without LLM? ( i.e. How should we think is this method flexible/feasible or not) and how can we ensure the reproduction as the current LLM normally with fast version iteration. If not, is it still a flexible framework?\n2. What is the cost and the latency of using LLM, especially on the two big QReCC and TopiOCQA datasets? Please show statistic information.\n3. How can we control the generated text by LLM will not bring noise to the query encoder training and how can we analysis via experiments? In other words, how to evaluate the quality of the generated text by LLM for IR evaluation.  4. How to consider the comparison with the manually rewritten query and the query reformulation approaches, especially on the cast datasets.\n5. For Eq. 7, why we can assume log p(z), log p(c) and log p(q | c) as instants and what is the relation between eq. 7 and the quality of the generated text?\n\nmissing_references: 1. Open-retrieval conversational question answering. Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W Bruce Croft, Mohit Iyyer. ( SIGIR 2020) 2. Explicit Query Rewriting for Conversational Dense Retrieval. Hongjin Qian, Zhicheng Dou. ( EMNLP 2022) 3. ConvGQR: Generative Query Reformulation for Conversational Search. Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huang, Jian-Yun Nie (ACL 2023) 4. Learning to Relate to Previous Turns in Conversational Search. Fengran Mo, Jian-Yun Nie, Kaiyu Huang, Kelong Mao, Yutao Zhu, Peng Li, Yang Liu (SIGKDD 2023) 5. CoSPLADE: Contextualizing SPLADE for Conversational Information Retrieval. Nam Hai Le, Thomas Gerald, Thibault Formal, Jian-Yun Nie, Benjamin Piwowarski, Laure Soulier. ( ECIR 2023) The query reformulation methods could be included to be discussed and some recent publications could be added to the related work later to make the literature review more complete.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "ce66DuSrBw",
        "similarity": 0.6993,
        "coverage": 0.4,
        "human_length": 449,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of conversational passage retrieval. The authors propose a method, named Instructor, to use frozen LLMs to provide retriever training signals. Specifically, the relevance score between a passage and a conversational query (i.e., conversation history + current user question) is calculated as the generation probability of the passage by a LLM, conditioning on the conversational query. The authors propose three ways to calculate such generation probabilities. Then they train the retriever with the KL loss between LM generation probabilities and passage-query similarities from the retriever.\nThey conduct experiments on three conversational retrieval benchmarks and achiever significant improvements with such training guidance from LLMs.\n\nreasons_to_accept: 1. The paper is well-written and easy to follow. \n2. They propose a novel framework to fine-tune the retriever model to handle conversational queries. \n3. They did extensive experiments to show improvements of their approach on three different benchmarks.\n\nreasons_to_reject: 1. My major concern is that you might be able to achieve impressive results with weak retrieval supervision from the conversation data, without the need to query LLMs as instructors. The training data used, namely QReCC and TopiOCQA, comes with both question and answer turns. You can apply the same method used in CONQRR (Wu et al., 2022) to derive \"gold\" passages and fine-tune retrievers with those weak labels. My guess is that the performance can already be good enough, which may also explain why QRPG gives the highest score as it uses the next answer turns. Also, you'll need to use hard negatives in retriever training as that's widely acknowledged to boost performance.\n2. I don't agree with claim that \"annotating session-passage pairs is much more difficult than collecting conversation data\". The datasets you use are collected by having annotators search for passages first before writing agent utterances. How could annotating passages more difficult than labeling conversation data?\n3. You are using LLM generation probability to calculate the document relevance, which is then used as training signals for retrievers. Why don't you simply use LLM generation prob to rank passages? You may get even better results than fine-tuning the retriever.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "1YqlJFHjZM",
        "length": 389,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a novel method called INSTRUCTOR for unsupervised conversational dense retrieval. The key idea is to leverage large language models (LLMs) to provide supervised signals to guide the training of retrievers, without requiring any labeled data. The main contributions are: 1. An unsupervised training framework where LLMs generate soft session-passage relevance scores to instruct the retrievers.\n2. Three strategies to accurately estimate relevance from different perspectives: conversational retrieval as conversation generation, question rewrite as latent variable, and question response as posterior guide.\nThe paper addresses the lack of labeled data for conversational retrieval, and demonstrates how large pretrained models can be leveraged for unsupervised learning in this setting.\n\nreasons_to_accept: 1. The paper tackles an important problem of unsupervised conversational retrieval training. The way using LLMs to provide training signals is a novel in the conversational context (although it has been validated in ad-hoc search), and the three strategies provide nice ways to leverage LLMs from different perspectives of the conversation. This can enable building conversational retrievers without labeled data.\n2. Thorough ablation studies analyzing the effect of different proposed strategies.\n3. Strong empirical results, surpassing supervised approaches on two datasets.\n4. Well-written paper with sufficient details to reproduce the approach.\n\nreasons_to_reject: 1. Limited analysis on how different types of conversations/questions affect the quality of LLM supervision.\n2. Unclear how the approach scales as the dataset size increases. Memory and compute costs?\n\nquestions_for_the_authors: Can you provide more analysis into how the quality of LLM-generated supervision signals varies across different types of conversations or questions? Any patterns in where it works better/worse?\n\ntypos_grammar_style_and_presentation_improvements: Line 143: \"discover\" -> \"discovering\" Line 289: Extra space before period Section 3.2: Consider using sub-section headings for each strategy Figure 1: Increase font size of text in the boxes Figure 3: Improve spacing around \"+\" symbols\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "tbXSUJZoMg",
        "length": 434,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new unsupervised method \u201cInstructor\u201d for training conversational dense retrieval models. Instructor leverages the knowledge from large language models (LLMs) to generate supervision signals to instruct the retriever. To distill the knowledge from the LLMs to the retriever, the authors design three methods to calculate the relevance score more precisely. Sufficient experimental results on four datasets under various settings demonstrate the effectiveness of the proposed method. Overall, the paper is well-written and easy to understand. The experiments look very solid. Though the technical depth is not so deep, and the distillation idea is not very novel, I suggest a weak accept on this paper.\n\nreasons_to_accept: 1. The proposed method leverages LLMs for knowledge distillation, which is a good application of LLMs on real research. \n2. The experiments are sufficient and solid. The results show significant improvement over existing studies. \n3. The paper is well-written and easy to understand.\n\nreasons_to_reject: 1. Distilling knowledge from LLM to retriever, or more generally, distilling knowledge from reranker to retriever is not a new idea [1, 2]. \n2. Missing some recent literature in the related work part.\nReference: 1. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent, Sun et al. 2023 2. Large language models are effective text rankers with pairwise ranking prompting, Qin et al., 2023\n\nquestions_for_the_authors: Q1: How do you compute the confidence $\\log p(r|I^{r}_{c,q},c,q)$ by ChatGPT? It seems that it cannot return the generation probability through API. Even if it is text-davinci, it can only provide the probability of the first five tokens. How do you deal with this problem?\nQ2: The proposed method is applied to several ad-hoc dense retriever. Is it also possible to apply it for conversational dense retrieval model?\n\nmissing_references: [1] Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search, Mao et al., 2023 [2] ConvGQR: Generative Query Reformulation for Conversational Search, Mo et al., 2023 [3] Learning to Relate to Previous Turns in Conversational Search, Mo et al., 2023\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "6VbSd6EedP",
        "length": 806,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method called INSTRUCTOR for training conversational dense retrievers in an unsupervised manner, without needing labeled session-passage pairs. The key problem it addresses is the lack of training data for conversational retrieval, where annotating session-passage relevance is difficult. Most existing methods rely on supervised fine-tuning.\nThe main contributions are: 1. An unsupervised training framework where large language models estimate session-passage relevance scores to guide retriever training.\n2. Three strategies to more accurately calculate relevance using the language model: modeling retrieval as conversation generation, question rewriting as a latent variable, and using question responses as a posterior guide.\n3. Experiments showing INSTRUCTOR significantly improves various ad-hoc retrievers like DPR and ANCE, even surpassing supervised methods.\nIn summary, the paper proposes a novel unsupervised approach for training conversational retrievers by instructing them with large language models. The method and thorough experiments are the key contributions.\n\nreasons_to_accept: 1. Addresses an important practical problem that lacking of conversational retrieval training data and proposes a creative method for leveraging large language models to provide training signal without any data annotation.\n2. Thorough and rigorous experiments across diverse settings like low-resource and zero-shot. Surpassing supervised methods is a notable result.\nOverall, the novel unsupervised learning approach, thorough experiments, and analyses make this a valuable contribution.\n\nreasons_to_reject: 1. The approach relies heavily on large proprietary language models, which have limitations like bias and lack of transparency and there are limited error analysis to understand cases where the method fails. This could be important since the LLM is used as a black-box model and the quality of generated results are without guarantee. Meanwhile, the workflow details (e.g. the concrete prompt and the reformulated query) for the proposed three strategies should be much clearer.\n2. There is no detailed analysis on how the unsupervised training objectives affect the retriever representations and no comparison with query reformulation methods. Besides, the cost and the latency of using LLM should also be indicated.\n3. Lack of control of injecting noise (bring by the generated text from LLM) and the analysis of how these expansion terms will influence the retrieval results. This is important in terms of the query analysis and avoid the harmful terms generation.\nThe main risks relate to overselling the generality of the approach and glossing over potential issues with language models. But the paper seems reasonably cautious about limitations. Overall the methodology appears solid despite some aspects needing deeper analysis.\n\nquestions_for_the_authors: 1. Can this framework designed without LLM? ( i.e. How should we think is this method flexible/feasible or not) and how can we ensure the reproduction as the current LLM normally with fast version iteration. If not, is it still a flexible framework?\n2. What is the cost and the latency of using LLM, especially on the two big QReCC and TopiOCQA datasets? Please show statistic information.\n3. How can we control the generated text by LLM will not bring noise to the query encoder training and how can we analysis via experiments? In other words, how to evaluate the quality of the generated text by LLM for IR evaluation.  4. How to consider the comparison with the manually rewritten query and the query reformulation approaches, especially on the cast datasets.\n5. For Eq. 7, why we can assume log p(z), log p(c) and log p(q | c) as instants and what is the relation between eq. 7 and the quality of the generated text?\n\nmissing_references: 1. Open-retrieval conversational question answering. Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W Bruce Croft, Mohit Iyyer. ( SIGIR 2020) 2. Explicit Query Rewriting for Conversational Dense Retrieval. Hongjin Qian, Zhicheng Dou. ( EMNLP 2022) 3. ConvGQR: Generative Query Reformulation for Conversational Search. Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huang, Jian-Yun Nie (ACL 2023) 4. Learning to Relate to Previous Turns in Conversational Search. Fengran Mo, Jian-Yun Nie, Kaiyu Huang, Kelong Mao, Yutao Zhu, Peng Li, Yang Liu (SIGKDD 2023) 5. CoSPLADE: Contextualizing SPLADE for Conversational Information Retrieval. Nam Hai Le, Thomas Gerald, Thibault Formal, Jian-Yun Nie, Benjamin Piwowarski, Laure Soulier. ( ECIR 2023) The query reformulation methods could be included to be discussed and some recent publications could be added to the related work later to make the literature review more complete.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "ce66DuSrBw",
        "length": 449,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of conversational passage retrieval. The authors propose a method, named Instructor, to use frozen LLMs to provide retriever training signals. Specifically, the relevance score between a passage and a conversational query (i.e., conversation history + current user question) is calculated as the generation probability of the passage by a LLM, conditioning on the conversational query. The authors propose three ways to calculate such generation probabilities. Then they train the retriever with the KL loss between LM generation probabilities and passage-query similarities from the retriever.\nThey conduct experiments on three conversational retrieval benchmarks and achiever significant improvements with such training guidance from LLMs.\n\nreasons_to_accept: 1. The paper is well-written and easy to follow. \n2. They propose a novel framework to fine-tune the retriever model to handle conversational queries. \n3. They did extensive experiments to show improvements of their approach on three different benchmarks.\n\nreasons_to_reject: 1. My major concern is that you might be able to achieve impressive results with weak retrieval supervision from the conversation data, without the need to query LLMs as instructors. The training data used, namely QReCC and TopiOCQA, comes with both question and answer turns. You can apply the same method used in CONQRR (Wu et al., 2022) to derive \"gold\" passages and fine-tune retrievers with those weak labels. My guess is that the performance can already be good enough, which may also explain why QRPG gives the highest score as it uses the next answer turns. Also, you'll need to use hard negatives in retriever training as that's widely acknowledged to boost performance.\n2. I don't agree with claim that \"annotating session-passage pairs is much more difficult than collecting conversation data\". The datasets you use are collected by having annotators search for passages first before writing agent utterances. How could annotating passages more difficult than labeling conversation data?\n3. You are using LLM generation probability to calculate the document relevance, which is then used as training signals for retrievers. Why don't you simply use LLM generation prob to rank passages? You may get even better results than fine-tuning the retriever.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "196_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_196_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.73,
      "max_similarity": 0.759,
      "avg_coverage": 0.6260666666666667,
      "max_coverage": 0.875
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 716,
      "avg_human_length": 385.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 11,
      "suggestions_count": 12
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "SdD9HBjyTc",
        "similarity": 0.759,
        "coverage": 0.4815,
        "human_length": 568,
        "human_text": "paper_topic_and_main_contributions: This paper investigates whether or not self-generated natural language explanations can improve code-to-code translation performance. The paper proposes an explain-then-translate method and analyzes its effectiveness across 19 different programming languages and 3 types of explanations. It also proposes 5 different heuristics to select better explanations to improve the performance of its explain-then-translate method. Finally, it conducts multiple ablation studies comparing the robustness and effectiveness of natural language explanations to programming language examples, commonly seen in few shot learning.\n\nreasons_to_accept: 1. This paper is to my knowledge the first highly detailed study into how self-generated natural language explanations can improve code-to-code translation performance. \n2. This paper provides detailed examples of the various prompts used in the study, which will be helpful for the future reproduction of the results in the study as well as future research in this area.   3. This paper provides novel heuristic selection strategies that improve the performance of its explain-then-translate method, as well as showcase the potential for growth and, thus, can be used as a blueprint for future research in the area of code-to-code translation. \n4. This paper provides results of novel ablation experiments that demonstrate how natural language explanations can be more robust and offer better performance when compared to automatically generated programming examples that are used in one-shot learning.\n\nreasons_to_reject: 1. The authors claim to release their code and dataset that they used for this paper, however I cannot find any links to these resources in the paper. \n2. Section 3, where the authors explain some of their methodology on how they are prompting the LLM, is quite vague, as it does not mention what LLM they are using nor does it clearly outline how they are using their explain-then-translate method to prompt the LLM, both of which may make reproducibility more difficult.\n\nquestions_for_the_authors: Question A: In Section 4.3, why were 0-shot explanations used for exp, while 4-shot explanations were used for exp-lbl and exp-lbl-d? I don\u2019t see a good reason for being inconsistent here. \nQuestion B: Is there any mention in the paper of the compute resources used to run the inference performed in the experiments? Since all of the inference was performed on ChatGPT, perhaps information on the number of API keys used, as well as the time it took to run all the inference would be helpful as well. \nQuestion C: Why were only a subset of all the possible translation directions used when experimenting with alternative translation directions in Section 4.2? \nQuestion D: Why was ChatGPT mentioned as the LLM model used for inference only until the conclusion? I believe that is a relevant detail that could be included much earlier.\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: Lines 050-051, \u20260-shot setting\u2026 \u2192 \u20260-shot performance\u2026 Line 094, \u2026code generation prompt\u2026 \u2192 \u2026code generation prompts\u2026 Line 111-115, \u2026over CodeXGLUE\u2026 \u2192 \u2026over CodeXGLUE and TransCoder\u2026 Line 491, \u2026Python-Java++-PHP\u2026 \u2192 \u2026Python-Java-PHP\u2026\n\nethical_concerns: No\n\njustification_for_ethical_concerns: N/A\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Ap6PHpYGbi",
        "similarity": 0.7077,
        "coverage": 0.875,
        "human_length": 168,
        "human_text": "paper_topic_and_main_contributions: This paper experimented with chain-of-thought prompting in the case of programming language translation. With the aid of zero-shot and few-shot prompting, the authors came across important insights such as, adding simpler explanations turning out more useful in case of low-resourced ones and using simpler heuristics to perform better explanation selection.\n\nreasons_to_accept: - Detailed study and adequate explanations regarding the reasonings - Detailed ablation study\n\nreasons_to_reject: - this study is based on only chatgpt without any room of comparative evaluation. There needs to be some analysis comprising other llm so that the behaviors and finds could be confidently described as general.\n\nquestions_for_the_authors: - could you add some comparative analysis comprising multiple LLMs focusing on the key findings?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "sJCXwTa4uK",
        "similarity": 0.7233,
        "coverage": 0.5217,
        "human_length": 420,
        "human_text": "paper_topic_and_main_contributions: This paper is concerned with chain-of-thought prompting as applied to program source code translation. More in particular, it applies this method (originally from Chen et al., Teaching large language models to self-debug, 2023 ) extending the number of languages to 19, across low- and high-resource ones.\n\nreasons_to_accept: This paper presents a battery of translation pairs and prompt variations. Most importantly, it summarizes the vast experimental evidence into nuggets that hopefully will be valuable to theorists. As such, it's a sound empirical work and well suited for EMNLP.\n\nreasons_to_reject: To me, using a black-box, proprietary model like ChatGPT which periodically changes under the hood is a major scientific liability. This paper as a whole won't likely be reproducible in a few weeks' to months' time.  It is really unfortunate that the authors chose this and not a \"weaker\" but reproducible model, especially considering the wealth of OSS code LMs we have nowadays (CodeT5, StarCoder, etc.). I get it, ChatGPT is apparently \"strong\" in a number of areas, very convenient to use etc., but this is not what good science is about.    If the paper used an open-source model instead, or at the very least made a comparison to an OSS LM, I would be much more inclined to recommend it for acceptance.\n---  ## Rebuttal acknowlegement I appreciate the authors' response and I gladly increase my score.\n\nquestions_for_the_authors: A The 3.3. Metrics section is very brief; could you elaborate on that comment \"n, k\" often dependent of sampling temperature k ? What does this dependency look like?\n\ntypos_grammar_style_and_presentation_improvements: Only in the Conclusions section we learn that the model under test is ChatGPT. This fact should be made clear in the beginning of the work, or at least in the Experiments section.\nIt is really hard to see a linear fit in Figure 2, seeing the data. I wonder if that correlation could be faceted by controlling for e.g. language dataset size.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "SdD9HBjyTc",
        "length": 568,
        "human_text": "paper_topic_and_main_contributions: This paper investigates whether or not self-generated natural language explanations can improve code-to-code translation performance. The paper proposes an explain-then-translate method and analyzes its effectiveness across 19 different programming languages and 3 types of explanations. It also proposes 5 different heuristics to select better explanations to improve the performance of its explain-then-translate method. Finally, it conducts multiple ablation studies comparing the robustness and effectiveness of natural language explanations to programming language examples, commonly seen in few shot learning.\n\nreasons_to_accept: 1. This paper is to my knowledge the first highly detailed study into how self-generated natural language explanations can improve code-to-code translation performance. \n2. This paper provides detailed examples of the various prompts used in the study, which will be helpful for the future reproduction of the results in the study as well as future research in this area.   3. This paper provides novel heuristic selection strategies that improve the performance of its explain-then-translate method, as well as showcase the potential for growth and, thus, can be used as a blueprint for future research in the area of code-to-code translation. \n4. This paper provides results of novel ablation experiments that demonstrate how natural language explanations can be more robust and offer better performance when compared to automatically generated programming examples that are used in one-shot learning.\n\nreasons_to_reject: 1. The authors claim to release their code and dataset that they used for this paper, however I cannot find any links to these resources in the paper. \n2. Section 3, where the authors explain some of their methodology on how they are prompting the LLM, is quite vague, as it does not mention what LLM they are using nor does it clearly outline how they are using their explain-then-translate method to prompt the LLM, both of which may make reproducibility more difficult.\n\nquestions_for_the_authors: Question A: In Section 4.3, why were 0-shot explanations used for exp, while 4-shot explanations were used for exp-lbl and exp-lbl-d? I don\u2019t see a good reason for being inconsistent here. \nQuestion B: Is there any mention in the paper of the compute resources used to run the inference performed in the experiments? Since all of the inference was performed on ChatGPT, perhaps information on the number of API keys used, as well as the time it took to run all the inference would be helpful as well. \nQuestion C: Why were only a subset of all the possible translation directions used when experimenting with alternative translation directions in Section 4.2? \nQuestion D: Why was ChatGPT mentioned as the LLM model used for inference only until the conclusion? I believe that is a relevant detail that could be included much earlier.\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: Lines 050-051, \u20260-shot setting\u2026 \u2192 \u20260-shot performance\u2026 Line 094, \u2026code generation prompt\u2026 \u2192 \u2026code generation prompts\u2026 Line 111-115, \u2026over CodeXGLUE\u2026 \u2192 \u2026over CodeXGLUE and TransCoder\u2026 Line 491, \u2026Python-Java++-PHP\u2026 \u2192 \u2026Python-Java-PHP\u2026\n\nethical_concerns: No\n\njustification_for_ethical_concerns: N/A\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "Ap6PHpYGbi",
        "length": 168,
        "human_text": "paper_topic_and_main_contributions: This paper experimented with chain-of-thought prompting in the case of programming language translation. With the aid of zero-shot and few-shot prompting, the authors came across important insights such as, adding simpler explanations turning out more useful in case of low-resourced ones and using simpler heuristics to perform better explanation selection.\n\nreasons_to_accept: - Detailed study and adequate explanations regarding the reasonings - Detailed ablation study\n\nreasons_to_reject: - this study is based on only chatgpt without any room of comparative evaluation. There needs to be some analysis comprising other llm so that the behaviors and finds could be confidently described as general.\n\nquestions_for_the_authors: - could you add some comparative analysis comprising multiple LLMs focusing on the key findings?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "sJCXwTa4uK",
        "length": 420,
        "human_text": "paper_topic_and_main_contributions: This paper is concerned with chain-of-thought prompting as applied to program source code translation. More in particular, it applies this method (originally from Chen et al., Teaching large language models to self-debug, 2023 ) extending the number of languages to 19, across low- and high-resource ones.\n\nreasons_to_accept: This paper presents a battery of translation pairs and prompt variations. Most importantly, it summarizes the vast experimental evidence into nuggets that hopefully will be valuable to theorists. As such, it's a sound empirical work and well suited for EMNLP.\n\nreasons_to_reject: To me, using a black-box, proprietary model like ChatGPT which periodically changes under the hood is a major scientific liability. This paper as a whole won't likely be reproducible in a few weeks' to months' time.  It is really unfortunate that the authors chose this and not a \"weaker\" but reproducible model, especially considering the wealth of OSS code LMs we have nowadays (CodeT5, StarCoder, etc.). I get it, ChatGPT is apparently \"strong\" in a number of areas, very convenient to use etc., but this is not what good science is about.    If the paper used an open-source model instead, or at the very least made a comparison to an OSS LM, I would be much more inclined to recommend it for acceptance.\n---  ## Rebuttal acknowlegement I appreciate the authors' response and I gladly increase my score.\n\nquestions_for_the_authors: A The 3.3. Metrics section is very brief; could you elaborate on that comment \"n, k\" often dependent of sampling temperature k ? What does this dependency look like?\n\ntypos_grammar_style_and_presentation_improvements: Only in the Conclusions section we learn that the model under test is ChatGPT. This fact should be made clear in the beginning of the work, or at least in the Experiments section.\nIt is really hard to see a linear fit in Figure 2, seeing the data. I wonder if that correlation could be faceted by controlling for e.g. language dataset size.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "196_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_196_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7277999999999999,
      "max_similarity": 0.7551,
      "avg_coverage": 0.6260666666666667,
      "max_coverage": 0.875
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 733,
      "avg_human_length": 385.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 11,
      "suggestions_count": 12
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "SdD9HBjyTc",
        "similarity": 0.7551,
        "coverage": 0.4815,
        "human_length": 568,
        "human_text": "paper_topic_and_main_contributions: This paper investigates whether or not self-generated natural language explanations can improve code-to-code translation performance. The paper proposes an explain-then-translate method and analyzes its effectiveness across 19 different programming languages and 3 types of explanations. It also proposes 5 different heuristics to select better explanations to improve the performance of its explain-then-translate method. Finally, it conducts multiple ablation studies comparing the robustness and effectiveness of natural language explanations to programming language examples, commonly seen in few shot learning.\n\nreasons_to_accept: 1. This paper is to my knowledge the first highly detailed study into how self-generated natural language explanations can improve code-to-code translation performance. \n2. This paper provides detailed examples of the various prompts used in the study, which will be helpful for the future reproduction of the results in the study as well as future research in this area.   3. This paper provides novel heuristic selection strategies that improve the performance of its explain-then-translate method, as well as showcase the potential for growth and, thus, can be used as a blueprint for future research in the area of code-to-code translation. \n4. This paper provides results of novel ablation experiments that demonstrate how natural language explanations can be more robust and offer better performance when compared to automatically generated programming examples that are used in one-shot learning.\n\nreasons_to_reject: 1. The authors claim to release their code and dataset that they used for this paper, however I cannot find any links to these resources in the paper. \n2. Section 3, where the authors explain some of their methodology on how they are prompting the LLM, is quite vague, as it does not mention what LLM they are using nor does it clearly outline how they are using their explain-then-translate method to prompt the LLM, both of which may make reproducibility more difficult.\n\nquestions_for_the_authors: Question A: In Section 4.3, why were 0-shot explanations used for exp, while 4-shot explanations were used for exp-lbl and exp-lbl-d? I don\u2019t see a good reason for being inconsistent here. \nQuestion B: Is there any mention in the paper of the compute resources used to run the inference performed in the experiments? Since all of the inference was performed on ChatGPT, perhaps information on the number of API keys used, as well as the time it took to run all the inference would be helpful as well. \nQuestion C: Why were only a subset of all the possible translation directions used when experimenting with alternative translation directions in Section 4.2? \nQuestion D: Why was ChatGPT mentioned as the LLM model used for inference only until the conclusion? I believe that is a relevant detail that could be included much earlier.\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: Lines 050-051, \u20260-shot setting\u2026 \u2192 \u20260-shot performance\u2026 Line 094, \u2026code generation prompt\u2026 \u2192 \u2026code generation prompts\u2026 Line 111-115, \u2026over CodeXGLUE\u2026 \u2192 \u2026over CodeXGLUE and TransCoder\u2026 Line 491, \u2026Python-Java++-PHP\u2026 \u2192 \u2026Python-Java-PHP\u2026\n\nethical_concerns: No\n\njustification_for_ethical_concerns: N/A\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Ap6PHpYGbi",
        "similarity": 0.7055,
        "coverage": 0.875,
        "human_length": 168,
        "human_text": "paper_topic_and_main_contributions: This paper experimented with chain-of-thought prompting in the case of programming language translation. With the aid of zero-shot and few-shot prompting, the authors came across important insights such as, adding simpler explanations turning out more useful in case of low-resourced ones and using simpler heuristics to perform better explanation selection.\n\nreasons_to_accept: - Detailed study and adequate explanations regarding the reasonings - Detailed ablation study\n\nreasons_to_reject: - this study is based on only chatgpt without any room of comparative evaluation. There needs to be some analysis comprising other llm so that the behaviors and finds could be confidently described as general.\n\nquestions_for_the_authors: - could you add some comparative analysis comprising multiple LLMs focusing on the key findings?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "sJCXwTa4uK",
        "similarity": 0.7228,
        "coverage": 0.5217,
        "human_length": 420,
        "human_text": "paper_topic_and_main_contributions: This paper is concerned with chain-of-thought prompting as applied to program source code translation. More in particular, it applies this method (originally from Chen et al., Teaching large language models to self-debug, 2023 ) extending the number of languages to 19, across low- and high-resource ones.\n\nreasons_to_accept: This paper presents a battery of translation pairs and prompt variations. Most importantly, it summarizes the vast experimental evidence into nuggets that hopefully will be valuable to theorists. As such, it's a sound empirical work and well suited for EMNLP.\n\nreasons_to_reject: To me, using a black-box, proprietary model like ChatGPT which periodically changes under the hood is a major scientific liability. This paper as a whole won't likely be reproducible in a few weeks' to months' time.  It is really unfortunate that the authors chose this and not a \"weaker\" but reproducible model, especially considering the wealth of OSS code LMs we have nowadays (CodeT5, StarCoder, etc.). I get it, ChatGPT is apparently \"strong\" in a number of areas, very convenient to use etc., but this is not what good science is about.    If the paper used an open-source model instead, or at the very least made a comparison to an OSS LM, I would be much more inclined to recommend it for acceptance.\n---  ## Rebuttal acknowlegement I appreciate the authors' response and I gladly increase my score.\n\nquestions_for_the_authors: A The 3.3. Metrics section is very brief; could you elaborate on that comment \"n, k\" often dependent of sampling temperature k ? What does this dependency look like?\n\ntypos_grammar_style_and_presentation_improvements: Only in the Conclusions section we learn that the model under test is ChatGPT. This fact should be made clear in the beginning of the work, or at least in the Experiments section.\nIt is really hard to see a linear fit in Figure 2, seeing the data. I wonder if that correlation could be faceted by controlling for e.g. language dataset size.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "SdD9HBjyTc",
        "length": 568,
        "human_text": "paper_topic_and_main_contributions: This paper investigates whether or not self-generated natural language explanations can improve code-to-code translation performance. The paper proposes an explain-then-translate method and analyzes its effectiveness across 19 different programming languages and 3 types of explanations. It also proposes 5 different heuristics to select better explanations to improve the performance of its explain-then-translate method. Finally, it conducts multiple ablation studies comparing the robustness and effectiveness of natural language explanations to programming language examples, commonly seen in few shot learning.\n\nreasons_to_accept: 1. This paper is to my knowledge the first highly detailed study into how self-generated natural language explanations can improve code-to-code translation performance. \n2. This paper provides detailed examples of the various prompts used in the study, which will be helpful for the future reproduction of the results in the study as well as future research in this area.   3. This paper provides novel heuristic selection strategies that improve the performance of its explain-then-translate method, as well as showcase the potential for growth and, thus, can be used as a blueprint for future research in the area of code-to-code translation. \n4. This paper provides results of novel ablation experiments that demonstrate how natural language explanations can be more robust and offer better performance when compared to automatically generated programming examples that are used in one-shot learning.\n\nreasons_to_reject: 1. The authors claim to release their code and dataset that they used for this paper, however I cannot find any links to these resources in the paper. \n2. Section 3, where the authors explain some of their methodology on how they are prompting the LLM, is quite vague, as it does not mention what LLM they are using nor does it clearly outline how they are using their explain-then-translate method to prompt the LLM, both of which may make reproducibility more difficult.\n\nquestions_for_the_authors: Question A: In Section 4.3, why were 0-shot explanations used for exp, while 4-shot explanations were used for exp-lbl and exp-lbl-d? I don\u2019t see a good reason for being inconsistent here. \nQuestion B: Is there any mention in the paper of the compute resources used to run the inference performed in the experiments? Since all of the inference was performed on ChatGPT, perhaps information on the number of API keys used, as well as the time it took to run all the inference would be helpful as well. \nQuestion C: Why were only a subset of all the possible translation directions used when experimenting with alternative translation directions in Section 4.2? \nQuestion D: Why was ChatGPT mentioned as the LLM model used for inference only until the conclusion? I believe that is a relevant detail that could be included much earlier.\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: Lines 050-051, \u20260-shot setting\u2026 \u2192 \u20260-shot performance\u2026 Line 094, \u2026code generation prompt\u2026 \u2192 \u2026code generation prompts\u2026 Line 111-115, \u2026over CodeXGLUE\u2026 \u2192 \u2026over CodeXGLUE and TransCoder\u2026 Line 491, \u2026Python-Java++-PHP\u2026 \u2192 \u2026Python-Java-PHP\u2026\n\nethical_concerns: No\n\njustification_for_ethical_concerns: N/A\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "Ap6PHpYGbi",
        "length": 168,
        "human_text": "paper_topic_and_main_contributions: This paper experimented with chain-of-thought prompting in the case of programming language translation. With the aid of zero-shot and few-shot prompting, the authors came across important insights such as, adding simpler explanations turning out more useful in case of low-resourced ones and using simpler heuristics to perform better explanation selection.\n\nreasons_to_accept: - Detailed study and adequate explanations regarding the reasonings - Detailed ablation study\n\nreasons_to_reject: - this study is based on only chatgpt without any room of comparative evaluation. There needs to be some analysis comprising other llm so that the behaviors and finds could be confidently described as general.\n\nquestions_for_the_authors: - could you add some comparative analysis comprising multiple LLMs focusing on the key findings?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "sJCXwTa4uK",
        "length": 420,
        "human_text": "paper_topic_and_main_contributions: This paper is concerned with chain-of-thought prompting as applied to program source code translation. More in particular, it applies this method (originally from Chen et al., Teaching large language models to self-debug, 2023 ) extending the number of languages to 19, across low- and high-resource ones.\n\nreasons_to_accept: This paper presents a battery of translation pairs and prompt variations. Most importantly, it summarizes the vast experimental evidence into nuggets that hopefully will be valuable to theorists. As such, it's a sound empirical work and well suited for EMNLP.\n\nreasons_to_reject: To me, using a black-box, proprietary model like ChatGPT which periodically changes under the hood is a major scientific liability. This paper as a whole won't likely be reproducible in a few weeks' to months' time.  It is really unfortunate that the authors chose this and not a \"weaker\" but reproducible model, especially considering the wealth of OSS code LMs we have nowadays (CodeT5, StarCoder, etc.). I get it, ChatGPT is apparently \"strong\" in a number of areas, very convenient to use etc., but this is not what good science is about.    If the paper used an open-source model instead, or at the very least made a comparison to an OSS LM, I would be much more inclined to recommend it for acceptance.\n---  ## Rebuttal acknowlegement I appreciate the authors' response and I gladly increase my score.\n\nquestions_for_the_authors: A The 3.3. Metrics section is very brief; could you elaborate on that comment \"n, k\" often dependent of sampling temperature k ? What does this dependency look like?\n\ntypos_grammar_style_and_presentation_improvements: Only in the Conclusions section we learn that the model under test is ChatGPT. This fact should be made clear in the beginning of the work, or at least in the Experiments section.\nIt is really hard to see a linear fit in Figure 2, seeing the data. I wonder if that correlation could be faceted by controlling for e.g. language dataset size.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "57_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_57_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.707875,
      "max_similarity": 0.7226,
      "avg_coverage": 0.411375,
      "max_coverage": 0.5455
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 480,
      "avg_human_length": 488.75
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "UbfUI9GJBf",
        "similarity": 0.7127,
        "coverage": 0.2857,
        "human_length": 888,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of (untyped) relation extraction (i.e. extracted pairs are not labeled) from web pages and in a zero-shot setting (i.e. testing is done on web pages from a vertical/domain not seen during training). The difficulty is obviously in the generalization to documents that can be different not only in their textual and semantic content, but also in their HTML/XML structure and layout.    The proposed solution follows existing multimodal methods that embed both text and document structure information. More specifically, and as stated in the paper, it is strongly inspired by the MarkupLM model (Li et al., 2021), which embeds absolute XML Paths along with textual elements within a transformer encoder.  The main contribution of the paper is the extension of the MarkupLM model (Li et al., 2021) with 2 features: popularity of text nodes (i.e. number of web pages where a text element occurs) and relative XML Path between text elements. The first feature is encoded in the input sequence embeddings (of text nodes), and the second is encoded as a bias term in the self-attention components. The experiments and ablation study show that the two features improve performance.\n\nreasons_to_accept: - The proposed extension of the MarkupLM model, with the encoding of text node \"popularity\" and relative XML paths between text nodes, is relatively simple and improves performance in zero-shot relation extraction setting, as shown by the experiments and the ablation study.\n\nreasons_to_reject: - Two important technical aspects of the proposed method are missing or are not clear:  \t- (1) Regarding the representation of model input: from section 4 and in particular lines 280-284, the model input is defined as the sequence of \"text nodes\" (w_i, xpath_i) from the XML/HTML document, which is confusing, as text nodes can be phrases, sentences or even paragraphs... In Figure 3 though, it seems that the model input is a sequence of tokens, not text nodes, which would make more sense, but the paper does not mention tokenization at all. \n\t- (2) Regarding the extractions: from the provided examples (Table 4), the model extracts pairs of text nodes, such as \"(Color type, Technicolor prints)\". If, as it is probably the case, model input is a sequence of tokens (from tokenized text nodes), then h_i and h_j in the Biaffine component (equation at line 383) are representations of tokens, not text nodes and hence, P(x_i, x_j) is about tokens x_i and x_j. Therefore, the paper lacks a description of how tokens are grouped in order to produce pairs of text nodes from the predicted pairs of tokens.\n\nquestions_for_the_authors: A. There seems to be a confusion in the use of the notions of text nodes, words and tokens. A text node is an element from the DOM tree and can be a token or word, a phrase, a sentence or even a paragraph. Could you please clarify what is the nature of the elements in the input sequence for the model, i.e. what w_i means in the (w_i, xpath_i) notation? Is it a text node or a token? If they are tokens, are the text nodes tokenized with a BPE tokenizer?  B. If the input of the model is a sequence of tokens (with their xpaths), then h_i and h_j in the Biafine equation (line 383) are representations of tokens, and x_i and x_j (line 384) are tokens, not text nodes. So how does the system group tokens to produce pairs of text nodes, such as \"(Color type, Technicolor prints)\" ?\nC. It is claimed that \"contrastive learning\" is used to train the model (section 4.4) but I am not sure if that's correct: the loss as defined in lines 377-393 seems to be a normal supervised cross-entropy loss (prediction vs true label on a single sample) and not a contrastive loss.  So it seems that the negative sampling is used simply to alleviate the problem of class imbalance and not for a contrastive loss. Could you please clarify?\n\nmissing_references: IMO, the following paper should be cited and discussed. Although their experiments with the SWDE dataset are not comparable due to different settings, the main contribution of the paper is multimodal (text + XML structure) representation learning, where various structural features are encoded along with text elements: Deng, Xiang, Prashant Shiralkar, Colin Lockard, Binxuan Huang, and Huan Sun. \u2018 DOM-LM: Learning Generalizable Representations for HTML Documents\u2019. arXiv, 25 January 2022. [https://doi.org/10.48550/arXiv.2201.10608](https://doi.org/10.48550/arXiv.2201.10608).\n\ntypos_grammar_style_and_presentation_improvements: Typos: line 143: gathering --> gather figure 2 caption: hundreds web pages --> hundreds of web pages Throughout the paper: the definite article \"the\" is sometimes used in plural noun phrases where it should not be, e.g. \"which rely on the syntactic constraints\" --> \"which rely on syntactic constraints\". Proofreading is probably needed to fix all the cases.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "lNA6p0cMoJ",
        "similarity": 0.6966,
        "coverage": 0.5,
        "human_length": 223,
        "human_text": "paper_topic_and_main_contributions: This paper studies the approach to understand semi-structured web pages utilizing the relation between text nodes within and across pages. \nIt proposes a framework ReXMiner to encode the shortest relative paths in document object model and the popularity info of text node. \nIt also uses the contrastive learning approach to facilitate the training. \nExperiments conducted on SWDE dataset shows the model achieves or outperforms the SOTA performance.\n\nreasons_to_accept: Paper is well structured and clear to follow. \nPaper tackles one important problem in web text mining field. \nPaper shows the proposed solution can outperform the state of art models. \nPaper show cases the gained benefits by combining more structural or popularity information.\n\nreasons_to_reject: Authors can consider to test more on other datasets. \nIt will be even better to compare the inference/training time.\n\nquestions_for_the_authors: I am wondering if the author has considered IDF as the popularity of text node has been considered.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "qW1hcix7Au",
        "similarity": 0.7226,
        "coverage": 0.5455,
        "human_length": 418,
        "human_text": "paper_topic_and_main_contributions: In this paper, a new method called ReXMiner is proposed for zero-shot relation extraction in web mining. It encodes the shortest relative paths in the DOM tree of the web page and counts the occurrence of the same text node across different web pages. Experiments on public benchmarks show that this method outperforms the state-of-the-art baselines in the task of zero-shot relation extraction in web mining.\n\nreasons_to_accept: This paper proposes a kind of novel and effective feature fomulation for web pages by encoding the shortest relative paths in the DOM tree and counting the occurrence of the text node.\nThe transformer-based model ReXMiner using the above features reaches the state-of-the-art in the task of zero-shot relation extraction for web mining, which is hopeful to facilitate the exploitation of the unlabelled web corpus.\n\nreasons_to_reject: The contribution of this paper seems somewhat insufficient as a long paper. As a considerable part of ReXMiner comes from the existing work of the model MarkupLM, the adding part including relative path bias and popularity embeddings are more like an incremental work. I believe this paper should be enriched in breadth or depth to fully demonstrate the advantages and importance of the new approach. For example, more experiments on other datasets and tasks about web pages, like what MarkupLM does, or more analytical experiments illustrating the principles of the new method.\n\nquestions_for_the_authors: A. The relative path sequences seem to be simply put together so information about direction and lowest common ancestor is not fed into the model. Does this information have any effect on the model?\nB. In this model, path prefixes are added to the first several layers of attention weight as bias and relative paths are added to the next layers of attention weight as bias. How do they interact with each other and ultimately affect the model?\nC. The number of these layers is controlled by hyperparameters, how are these hyperparameters determined and do they have a significant effect on the model?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "ZTu82cIymT",
        "similarity": 0.6996,
        "coverage": 0.3143,
        "human_length": 426,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the challenge of understanding semi-structured web pages in web mining, particularly when little is known about the subject or template of a new page. The authors note that current methods, which involve embedding XML source code into the transformer or encoding the rendered layout with graph neural networks, do not adequately account for the relationships between text nodes within and across pages.\nThe main contributions of this paper are the development and implementation of ReXMiner, which offers a more accurate and efficient method for key-value pair extraction within a web page. ReXMiner achieves this by encoding the shortest relative paths in the Document Object Model (DOM) tree of the web page. It also incorporates the popularity of each text node by counting the occurrence of the same text node across different web pages. \nThe authors use contrastive learning to address the issue of sparsity in relation extraction.\n\nreasons_to_accept: 1. easy to follow 2. The technical details are solid and reasonable.\n\nreasons_to_reject: 1. lack comparison to existing PLM-based methods, such as [1,2] and openai-gpt based methods. \n2. The components in the paper are popular approaches, I suggest that the authors give more insight and example on how each component (e.g. contrasive loss) works on each scenario. \n3. poor reproducibility, without any code available for evaluation.\nReferences (see missing references)\n\nquestions_for_the_authors: 1. What are the zero-shot results of recent LLMs, e.g., gpt4, on the task?    2. This point may worth discussion: in industrial practice, few shot may always be acceptable, so why do authors insist on zero-shot as the paper position?\n\nmissing_references: [1] Li, Zimeng, et al. \"WIERT: Web Information Extraction via Render Tree.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.\n[2] Xie, Chenhao, et al. \"Webke: Knowledge extraction from semi-structured web with pre-trained markup language model.\" Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 2021.\n\nethical_concerns: No\n\ntypos_grammar_style_and_presentation_improvements: Too many arxiv references, some of which may already have a conference version\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "UbfUI9GJBf",
        "length": 888,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of (untyped) relation extraction (i.e. extracted pairs are not labeled) from web pages and in a zero-shot setting (i.e. testing is done on web pages from a vertical/domain not seen during training). The difficulty is obviously in the generalization to documents that can be different not only in their textual and semantic content, but also in their HTML/XML structure and layout.    The proposed solution follows existing multimodal methods that embed both text and document structure information. More specifically, and as stated in the paper, it is strongly inspired by the MarkupLM model (Li et al., 2021), which embeds absolute XML Paths along with textual elements within a transformer encoder.  The main contribution of the paper is the extension of the MarkupLM model (Li et al., 2021) with 2 features: popularity of text nodes (i.e. number of web pages where a text element occurs) and relative XML Path between text elements. The first feature is encoded in the input sequence embeddings (of text nodes), and the second is encoded as a bias term in the self-attention components. The experiments and ablation study show that the two features improve performance.\n\nreasons_to_accept: - The proposed extension of the MarkupLM model, with the encoding of text node \"popularity\" and relative XML paths between text nodes, is relatively simple and improves performance in zero-shot relation extraction setting, as shown by the experiments and the ablation study.\n\nreasons_to_reject: - Two important technical aspects of the proposed method are missing or are not clear:  \t- (1) Regarding the representation of model input: from section 4 and in particular lines 280-284, the model input is defined as the sequence of \"text nodes\" (w_i, xpath_i) from the XML/HTML document, which is confusing, as text nodes can be phrases, sentences or even paragraphs... In Figure 3 though, it seems that the model input is a sequence of tokens, not text nodes, which would make more sense, but the paper does not mention tokenization at all. \n\t- (2) Regarding the extractions: from the provided examples (Table 4), the model extracts pairs of text nodes, such as \"(Color type, Technicolor prints)\". If, as it is probably the case, model input is a sequence of tokens (from tokenized text nodes), then h_i and h_j in the Biaffine component (equation at line 383) are representations of tokens, not text nodes and hence, P(x_i, x_j) is about tokens x_i and x_j. Therefore, the paper lacks a description of how tokens are grouped in order to produce pairs of text nodes from the predicted pairs of tokens.\n\nquestions_for_the_authors: A. There seems to be a confusion in the use of the notions of text nodes, words and tokens. A text node is an element from the DOM tree and can be a token or word, a phrase, a sentence or even a paragraph. Could you please clarify what is the nature of the elements in the input sequence for the model, i.e. what w_i means in the (w_i, xpath_i) notation? Is it a text node or a token? If they are tokens, are the text nodes tokenized with a BPE tokenizer?  B. If the input of the model is a sequence of tokens (with their xpaths), then h_i and h_j in the Biafine equation (line 383) are representations of tokens, and x_i and x_j (line 384) are tokens, not text nodes. So how does the system group tokens to produce pairs of text nodes, such as \"(Color type, Technicolor prints)\" ?\nC. It is claimed that \"contrastive learning\" is used to train the model (section 4.4) but I am not sure if that's correct: the loss as defined in lines 377-393 seems to be a normal supervised cross-entropy loss (prediction vs true label on a single sample) and not a contrastive loss.  So it seems that the negative sampling is used simply to alleviate the problem of class imbalance and not for a contrastive loss. Could you please clarify?\n\nmissing_references: IMO, the following paper should be cited and discussed. Although their experiments with the SWDE dataset are not comparable due to different settings, the main contribution of the paper is multimodal (text + XML structure) representation learning, where various structural features are encoded along with text elements: Deng, Xiang, Prashant Shiralkar, Colin Lockard, Binxuan Huang, and Huan Sun. \u2018 DOM-LM: Learning Generalizable Representations for HTML Documents\u2019. arXiv, 25 January 2022. [https://doi.org/10.48550/arXiv.2201.10608](https://doi.org/10.48550/arXiv.2201.10608).\n\ntypos_grammar_style_and_presentation_improvements: Typos: line 143: gathering --> gather figure 2 caption: hundreds web pages --> hundreds of web pages Throughout the paper: the definite article \"the\" is sometimes used in plural noun phrases where it should not be, e.g. \"which rely on the syntactic constraints\" --> \"which rely on syntactic constraints\". Proofreading is probably needed to fix all the cases.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "lNA6p0cMoJ",
        "length": 223,
        "human_text": "paper_topic_and_main_contributions: This paper studies the approach to understand semi-structured web pages utilizing the relation between text nodes within and across pages. \nIt proposes a framework ReXMiner to encode the shortest relative paths in document object model and the popularity info of text node. \nIt also uses the contrastive learning approach to facilitate the training. \nExperiments conducted on SWDE dataset shows the model achieves or outperforms the SOTA performance.\n\nreasons_to_accept: Paper is well structured and clear to follow. \nPaper tackles one important problem in web text mining field. \nPaper shows the proposed solution can outperform the state of art models. \nPaper show cases the gained benefits by combining more structural or popularity information.\n\nreasons_to_reject: Authors can consider to test more on other datasets. \nIt will be even better to compare the inference/training time.\n\nquestions_for_the_authors: I am wondering if the author has considered IDF as the popularity of text node has been considered.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "qW1hcix7Au",
        "length": 418,
        "human_text": "paper_topic_and_main_contributions: In this paper, a new method called ReXMiner is proposed for zero-shot relation extraction in web mining. It encodes the shortest relative paths in the DOM tree of the web page and counts the occurrence of the same text node across different web pages. Experiments on public benchmarks show that this method outperforms the state-of-the-art baselines in the task of zero-shot relation extraction in web mining.\n\nreasons_to_accept: This paper proposes a kind of novel and effective feature fomulation for web pages by encoding the shortest relative paths in the DOM tree and counting the occurrence of the text node.\nThe transformer-based model ReXMiner using the above features reaches the state-of-the-art in the task of zero-shot relation extraction for web mining, which is hopeful to facilitate the exploitation of the unlabelled web corpus.\n\nreasons_to_reject: The contribution of this paper seems somewhat insufficient as a long paper. As a considerable part of ReXMiner comes from the existing work of the model MarkupLM, the adding part including relative path bias and popularity embeddings are more like an incremental work. I believe this paper should be enriched in breadth or depth to fully demonstrate the advantages and importance of the new approach. For example, more experiments on other datasets and tasks about web pages, like what MarkupLM does, or more analytical experiments illustrating the principles of the new method.\n\nquestions_for_the_authors: A. The relative path sequences seem to be simply put together so information about direction and lowest common ancestor is not fed into the model. Does this information have any effect on the model?\nB. In this model, path prefixes are added to the first several layers of attention weight as bias and relative paths are added to the next layers of attention weight as bias. How do they interact with each other and ultimately affect the model?\nC. The number of these layers is controlled by hyperparameters, how are these hyperparameters determined and do they have a significant effect on the model?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "ZTu82cIymT",
        "length": 426,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the challenge of understanding semi-structured web pages in web mining, particularly when little is known about the subject or template of a new page. The authors note that current methods, which involve embedding XML source code into the transformer or encoding the rendered layout with graph neural networks, do not adequately account for the relationships between text nodes within and across pages.\nThe main contributions of this paper are the development and implementation of ReXMiner, which offers a more accurate and efficient method for key-value pair extraction within a web page. ReXMiner achieves this by encoding the shortest relative paths in the Document Object Model (DOM) tree of the web page. It also incorporates the popularity of each text node by counting the occurrence of the same text node across different web pages. \nThe authors use contrastive learning to address the issue of sparsity in relation extraction.\n\nreasons_to_accept: 1. easy to follow 2. The technical details are solid and reasonable.\n\nreasons_to_reject: 1. lack comparison to existing PLM-based methods, such as [1,2] and openai-gpt based methods. \n2. The components in the paper are popular approaches, I suggest that the authors give more insight and example on how each component (e.g. contrasive loss) works on each scenario. \n3. poor reproducibility, without any code available for evaluation.\nReferences (see missing references)\n\nquestions_for_the_authors: 1. What are the zero-shot results of recent LLMs, e.g., gpt4, on the task?    2. This point may worth discussion: in industrial practice, few shot may always be acceptable, so why do authors insist on zero-shot as the paper position?\n\nmissing_references: [1] Li, Zimeng, et al. \"WIERT: Web Information Extraction via Render Tree.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.\n[2] Xie, Chenhao, et al. \"Webke: Knowledge extraction from semi-structured web with pre-trained markup language model.\" Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 2021.\n\nethical_concerns: No\n\ntypos_grammar_style_and_presentation_improvements: Too many arxiv references, some of which may already have a conference version\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "57_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_57_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.705325,
      "max_similarity": 0.7217,
      "avg_coverage": 0.404225,
      "max_coverage": 0.5455
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 451,
      "avg_human_length": 488.75
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "UbfUI9GJBf",
        "similarity": 0.7112,
        "coverage": 0.2857,
        "human_length": 888,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of (untyped) relation extraction (i.e. extracted pairs are not labeled) from web pages and in a zero-shot setting (i.e. testing is done on web pages from a vertical/domain not seen during training). The difficulty is obviously in the generalization to documents that can be different not only in their textual and semantic content, but also in their HTML/XML structure and layout.    The proposed solution follows existing multimodal methods that embed both text and document structure information. More specifically, and as stated in the paper, it is strongly inspired by the MarkupLM model (Li et al., 2021), which embeds absolute XML Paths along with textual elements within a transformer encoder.  The main contribution of the paper is the extension of the MarkupLM model (Li et al., 2021) with 2 features: popularity of text nodes (i.e. number of web pages where a text element occurs) and relative XML Path between text elements. The first feature is encoded in the input sequence embeddings (of text nodes), and the second is encoded as a bias term in the self-attention components. The experiments and ablation study show that the two features improve performance.\n\nreasons_to_accept: - The proposed extension of the MarkupLM model, with the encoding of text node \"popularity\" and relative XML paths between text nodes, is relatively simple and improves performance in zero-shot relation extraction setting, as shown by the experiments and the ablation study.\n\nreasons_to_reject: - Two important technical aspects of the proposed method are missing or are not clear:  \t- (1) Regarding the representation of model input: from section 4 and in particular lines 280-284, the model input is defined as the sequence of \"text nodes\" (w_i, xpath_i) from the XML/HTML document, which is confusing, as text nodes can be phrases, sentences or even paragraphs... In Figure 3 though, it seems that the model input is a sequence of tokens, not text nodes, which would make more sense, but the paper does not mention tokenization at all. \n\t- (2) Regarding the extractions: from the provided examples (Table 4), the model extracts pairs of text nodes, such as \"(Color type, Technicolor prints)\". If, as it is probably the case, model input is a sequence of tokens (from tokenized text nodes), then h_i and h_j in the Biaffine component (equation at line 383) are representations of tokens, not text nodes and hence, P(x_i, x_j) is about tokens x_i and x_j. Therefore, the paper lacks a description of how tokens are grouped in order to produce pairs of text nodes from the predicted pairs of tokens.\n\nquestions_for_the_authors: A. There seems to be a confusion in the use of the notions of text nodes, words and tokens. A text node is an element from the DOM tree and can be a token or word, a phrase, a sentence or even a paragraph. Could you please clarify what is the nature of the elements in the input sequence for the model, i.e. what w_i means in the (w_i, xpath_i) notation? Is it a text node or a token? If they are tokens, are the text nodes tokenized with a BPE tokenizer?  B. If the input of the model is a sequence of tokens (with their xpaths), then h_i and h_j in the Biafine equation (line 383) are representations of tokens, and x_i and x_j (line 384) are tokens, not text nodes. So how does the system group tokens to produce pairs of text nodes, such as \"(Color type, Technicolor prints)\" ?\nC. It is claimed that \"contrastive learning\" is used to train the model (section 4.4) but I am not sure if that's correct: the loss as defined in lines 377-393 seems to be a normal supervised cross-entropy loss (prediction vs true label on a single sample) and not a contrastive loss.  So it seems that the negative sampling is used simply to alleviate the problem of class imbalance and not for a contrastive loss. Could you please clarify?\n\nmissing_references: IMO, the following paper should be cited and discussed. Although their experiments with the SWDE dataset are not comparable due to different settings, the main contribution of the paper is multimodal (text + XML structure) representation learning, where various structural features are encoded along with text elements: Deng, Xiang, Prashant Shiralkar, Colin Lockard, Binxuan Huang, and Huan Sun. \u2018 DOM-LM: Learning Generalizable Representations for HTML Documents\u2019. arXiv, 25 January 2022. [https://doi.org/10.48550/arXiv.2201.10608](https://doi.org/10.48550/arXiv.2201.10608).\n\ntypos_grammar_style_and_presentation_improvements: Typos: line 143: gathering --> gather figure 2 caption: hundreds web pages --> hundreds of web pages Throughout the paper: the definite article \"the\" is sometimes used in plural noun phrases where it should not be, e.g. \"which rely on the syntactic constraints\" --> \"which rely on syntactic constraints\". Proofreading is probably needed to fix all the cases.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "lNA6p0cMoJ",
        "similarity": 0.6937,
        "coverage": 0.5,
        "human_length": 223,
        "human_text": "paper_topic_and_main_contributions: This paper studies the approach to understand semi-structured web pages utilizing the relation between text nodes within and across pages. \nIt proposes a framework ReXMiner to encode the shortest relative paths in document object model and the popularity info of text node. \nIt also uses the contrastive learning approach to facilitate the training. \nExperiments conducted on SWDE dataset shows the model achieves or outperforms the SOTA performance.\n\nreasons_to_accept: Paper is well structured and clear to follow. \nPaper tackles one important problem in web text mining field. \nPaper shows the proposed solution can outperform the state of art models. \nPaper show cases the gained benefits by combining more structural or popularity information.\n\nreasons_to_reject: Authors can consider to test more on other datasets. \nIt will be even better to compare the inference/training time.\n\nquestions_for_the_authors: I am wondering if the author has considered IDF as the popularity of text node has been considered.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "qW1hcix7Au",
        "similarity": 0.7217,
        "coverage": 0.5455,
        "human_length": 418,
        "human_text": "paper_topic_and_main_contributions: In this paper, a new method called ReXMiner is proposed for zero-shot relation extraction in web mining. It encodes the shortest relative paths in the DOM tree of the web page and counts the occurrence of the same text node across different web pages. Experiments on public benchmarks show that this method outperforms the state-of-the-art baselines in the task of zero-shot relation extraction in web mining.\n\nreasons_to_accept: This paper proposes a kind of novel and effective feature fomulation for web pages by encoding the shortest relative paths in the DOM tree and counting the occurrence of the text node.\nThe transformer-based model ReXMiner using the above features reaches the state-of-the-art in the task of zero-shot relation extraction for web mining, which is hopeful to facilitate the exploitation of the unlabelled web corpus.\n\nreasons_to_reject: The contribution of this paper seems somewhat insufficient as a long paper. As a considerable part of ReXMiner comes from the existing work of the model MarkupLM, the adding part including relative path bias and popularity embeddings are more like an incremental work. I believe this paper should be enriched in breadth or depth to fully demonstrate the advantages and importance of the new approach. For example, more experiments on other datasets and tasks about web pages, like what MarkupLM does, or more analytical experiments illustrating the principles of the new method.\n\nquestions_for_the_authors: A. The relative path sequences seem to be simply put together so information about direction and lowest common ancestor is not fed into the model. Does this information have any effect on the model?\nB. In this model, path prefixes are added to the first several layers of attention weight as bias and relative paths are added to the next layers of attention weight as bias. How do they interact with each other and ultimately affect the model?\nC. The number of these layers is controlled by hyperparameters, how are these hyperparameters determined and do they have a significant effect on the model?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "ZTu82cIymT",
        "similarity": 0.6947,
        "coverage": 0.2857,
        "human_length": 426,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the challenge of understanding semi-structured web pages in web mining, particularly when little is known about the subject or template of a new page. The authors note that current methods, which involve embedding XML source code into the transformer or encoding the rendered layout with graph neural networks, do not adequately account for the relationships between text nodes within and across pages.\nThe main contributions of this paper are the development and implementation of ReXMiner, which offers a more accurate and efficient method for key-value pair extraction within a web page. ReXMiner achieves this by encoding the shortest relative paths in the Document Object Model (DOM) tree of the web page. It also incorporates the popularity of each text node by counting the occurrence of the same text node across different web pages. \nThe authors use contrastive learning to address the issue of sparsity in relation extraction.\n\nreasons_to_accept: 1. easy to follow 2. The technical details are solid and reasonable.\n\nreasons_to_reject: 1. lack comparison to existing PLM-based methods, such as [1,2] and openai-gpt based methods. \n2. The components in the paper are popular approaches, I suggest that the authors give more insight and example on how each component (e.g. contrasive loss) works on each scenario. \n3. poor reproducibility, without any code available for evaluation.\nReferences (see missing references)\n\nquestions_for_the_authors: 1. What are the zero-shot results of recent LLMs, e.g., gpt4, on the task?    2. This point may worth discussion: in industrial practice, few shot may always be acceptable, so why do authors insist on zero-shot as the paper position?\n\nmissing_references: [1] Li, Zimeng, et al. \"WIERT: Web Information Extraction via Render Tree.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.\n[2] Xie, Chenhao, et al. \"Webke: Knowledge extraction from semi-structured web with pre-trained markup language model.\" Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 2021.\n\nethical_concerns: No\n\ntypos_grammar_style_and_presentation_improvements: Too many arxiv references, some of which may already have a conference version\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "UbfUI9GJBf",
        "length": 888,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of (untyped) relation extraction (i.e. extracted pairs are not labeled) from web pages and in a zero-shot setting (i.e. testing is done on web pages from a vertical/domain not seen during training). The difficulty is obviously in the generalization to documents that can be different not only in their textual and semantic content, but also in their HTML/XML structure and layout.    The proposed solution follows existing multimodal methods that embed both text and document structure information. More specifically, and as stated in the paper, it is strongly inspired by the MarkupLM model (Li et al., 2021), which embeds absolute XML Paths along with textual elements within a transformer encoder.  The main contribution of the paper is the extension of the MarkupLM model (Li et al., 2021) with 2 features: popularity of text nodes (i.e. number of web pages where a text element occurs) and relative XML Path between text elements. The first feature is encoded in the input sequence embeddings (of text nodes), and the second is encoded as a bias term in the self-attention components. The experiments and ablation study show that the two features improve performance.\n\nreasons_to_accept: - The proposed extension of the MarkupLM model, with the encoding of text node \"popularity\" and relative XML paths between text nodes, is relatively simple and improves performance in zero-shot relation extraction setting, as shown by the experiments and the ablation study.\n\nreasons_to_reject: - Two important technical aspects of the proposed method are missing or are not clear:  \t- (1) Regarding the representation of model input: from section 4 and in particular lines 280-284, the model input is defined as the sequence of \"text nodes\" (w_i, xpath_i) from the XML/HTML document, which is confusing, as text nodes can be phrases, sentences or even paragraphs... In Figure 3 though, it seems that the model input is a sequence of tokens, not text nodes, which would make more sense, but the paper does not mention tokenization at all. \n\t- (2) Regarding the extractions: from the provided examples (Table 4), the model extracts pairs of text nodes, such as \"(Color type, Technicolor prints)\". If, as it is probably the case, model input is a sequence of tokens (from tokenized text nodes), then h_i and h_j in the Biaffine component (equation at line 383) are representations of tokens, not text nodes and hence, P(x_i, x_j) is about tokens x_i and x_j. Therefore, the paper lacks a description of how tokens are grouped in order to produce pairs of text nodes from the predicted pairs of tokens.\n\nquestions_for_the_authors: A. There seems to be a confusion in the use of the notions of text nodes, words and tokens. A text node is an element from the DOM tree and can be a token or word, a phrase, a sentence or even a paragraph. Could you please clarify what is the nature of the elements in the input sequence for the model, i.e. what w_i means in the (w_i, xpath_i) notation? Is it a text node or a token? If they are tokens, are the text nodes tokenized with a BPE tokenizer?  B. If the input of the model is a sequence of tokens (with their xpaths), then h_i and h_j in the Biafine equation (line 383) are representations of tokens, and x_i and x_j (line 384) are tokens, not text nodes. So how does the system group tokens to produce pairs of text nodes, such as \"(Color type, Technicolor prints)\" ?\nC. It is claimed that \"contrastive learning\" is used to train the model (section 4.4) but I am not sure if that's correct: the loss as defined in lines 377-393 seems to be a normal supervised cross-entropy loss (prediction vs true label on a single sample) and not a contrastive loss.  So it seems that the negative sampling is used simply to alleviate the problem of class imbalance and not for a contrastive loss. Could you please clarify?\n\nmissing_references: IMO, the following paper should be cited and discussed. Although their experiments with the SWDE dataset are not comparable due to different settings, the main contribution of the paper is multimodal (text + XML structure) representation learning, where various structural features are encoded along with text elements: Deng, Xiang, Prashant Shiralkar, Colin Lockard, Binxuan Huang, and Huan Sun. \u2018 DOM-LM: Learning Generalizable Representations for HTML Documents\u2019. arXiv, 25 January 2022. [https://doi.org/10.48550/arXiv.2201.10608](https://doi.org/10.48550/arXiv.2201.10608).\n\ntypos_grammar_style_and_presentation_improvements: Typos: line 143: gathering --> gather figure 2 caption: hundreds web pages --> hundreds of web pages Throughout the paper: the definite article \"the\" is sometimes used in plural noun phrases where it should not be, e.g. \"which rely on the syntactic constraints\" --> \"which rely on syntactic constraints\". Proofreading is probably needed to fix all the cases.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "lNA6p0cMoJ",
        "length": 223,
        "human_text": "paper_topic_and_main_contributions: This paper studies the approach to understand semi-structured web pages utilizing the relation between text nodes within and across pages. \nIt proposes a framework ReXMiner to encode the shortest relative paths in document object model and the popularity info of text node. \nIt also uses the contrastive learning approach to facilitate the training. \nExperiments conducted on SWDE dataset shows the model achieves or outperforms the SOTA performance.\n\nreasons_to_accept: Paper is well structured and clear to follow. \nPaper tackles one important problem in web text mining field. \nPaper shows the proposed solution can outperform the state of art models. \nPaper show cases the gained benefits by combining more structural or popularity information.\n\nreasons_to_reject: Authors can consider to test more on other datasets. \nIt will be even better to compare the inference/training time.\n\nquestions_for_the_authors: I am wondering if the author has considered IDF as the popularity of text node has been considered.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "qW1hcix7Au",
        "length": 418,
        "human_text": "paper_topic_and_main_contributions: In this paper, a new method called ReXMiner is proposed for zero-shot relation extraction in web mining. It encodes the shortest relative paths in the DOM tree of the web page and counts the occurrence of the same text node across different web pages. Experiments on public benchmarks show that this method outperforms the state-of-the-art baselines in the task of zero-shot relation extraction in web mining.\n\nreasons_to_accept: This paper proposes a kind of novel and effective feature fomulation for web pages by encoding the shortest relative paths in the DOM tree and counting the occurrence of the text node.\nThe transformer-based model ReXMiner using the above features reaches the state-of-the-art in the task of zero-shot relation extraction for web mining, which is hopeful to facilitate the exploitation of the unlabelled web corpus.\n\nreasons_to_reject: The contribution of this paper seems somewhat insufficient as a long paper. As a considerable part of ReXMiner comes from the existing work of the model MarkupLM, the adding part including relative path bias and popularity embeddings are more like an incremental work. I believe this paper should be enriched in breadth or depth to fully demonstrate the advantages and importance of the new approach. For example, more experiments on other datasets and tasks about web pages, like what MarkupLM does, or more analytical experiments illustrating the principles of the new method.\n\nquestions_for_the_authors: A. The relative path sequences seem to be simply put together so information about direction and lowest common ancestor is not fed into the model. Does this information have any effect on the model?\nB. In this model, path prefixes are added to the first several layers of attention weight as bias and relative paths are added to the next layers of attention weight as bias. How do they interact with each other and ultimately affect the model?\nC. The number of these layers is controlled by hyperparameters, how are these hyperparameters determined and do they have a significant effect on the model?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "ZTu82cIymT",
        "length": 426,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the challenge of understanding semi-structured web pages in web mining, particularly when little is known about the subject or template of a new page. The authors note that current methods, which involve embedding XML source code into the transformer or encoding the rendered layout with graph neural networks, do not adequately account for the relationships between text nodes within and across pages.\nThe main contributions of this paper are the development and implementation of ReXMiner, which offers a more accurate and efficient method for key-value pair extraction within a web page. ReXMiner achieves this by encoding the shortest relative paths in the Document Object Model (DOM) tree of the web page. It also incorporates the popularity of each text node by counting the occurrence of the same text node across different web pages. \nThe authors use contrastive learning to address the issue of sparsity in relation extraction.\n\nreasons_to_accept: 1. easy to follow 2. The technical details are solid and reasonable.\n\nreasons_to_reject: 1. lack comparison to existing PLM-based methods, such as [1,2] and openai-gpt based methods. \n2. The components in the paper are popular approaches, I suggest that the authors give more insight and example on how each component (e.g. contrasive loss) works on each scenario. \n3. poor reproducibility, without any code available for evaluation.\nReferences (see missing references)\n\nquestions_for_the_authors: 1. What are the zero-shot results of recent LLMs, e.g., gpt4, on the task?    2. This point may worth discussion: in industrial practice, few shot may always be acceptable, so why do authors insist on zero-shot as the paper position?\n\nmissing_references: [1] Li, Zimeng, et al. \"WIERT: Web Information Extraction via Render Tree.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.\n[2] Xie, Chenhao, et al. \"Webke: Knowledge extraction from semi-structured web with pre-trained markup language model.\" Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 2021.\n\nethical_concerns: No\n\ntypos_grammar_style_and_presentation_improvements: Too many arxiv references, some of which may already have a conference version\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "12_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_12_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.7059249999999999,
      "max_similarity": 0.7392,
      "avg_coverage": 0.40645,
      "max_coverage": 0.5417
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 414,
      "avg_human_length": 508.25
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "8Bg7T1CXO6",
        "similarity": 0.7096,
        "coverage": 0.4348,
        "human_length": 453,
        "human_text": "paper_topic_and_main_contributions: This paper explores the hypothesis that there exists, somewhere in a large language model, a generalizable representation of the truth or falsity of a statement. The authors use a small special-purpose network to train a classifier that can convert the outputs of any hidden layer into an estimate of the truthfulness of the statement. The performance of classifiers of this type leads the authors to conclude that the generalized representation is indeed present.\nIn service of the hypothesis, the authors create a small corpus of examples, spanning several domains. This will be a resource for replications and future studies.\n\nreasons_to_accept: This is a very interesting and important question. The experiments are generally well-designed and informative. \nThe organization and narrative of the paper is excellent, making very clear what was done and why, with good examples.\n\nreasons_to_reject: There are significant numbers of small writing errors that do not impede comprehension or readability. A careful rewrite by an author who did not do much of the initial writing would fix this. These are obvious errors that would be caught by an on-line style and grammar checker.\nThe authors test five layers. This is a multiple compatsions setting, so statistics aiming to show that the result is real should use some kind of correction for multiple comparisons.  It is also worth, for safety, running a simple control in which the true/false labels are randomly permuted, so that each sentence is associated with the truth value of a randomly chosen counterpart. If all is well, we will find that the model is unable to predict the permuted labels with any accuracy.  This will help to establish beyond a reasonable doubt that the observed performance is due to the ability to detect truth, not to some strange effect of the behavior of large capacity models capable of learning almost anything. Possibly, doing this test is overly paranoid, but since it is cheap to do and the result is high-impact, why not?\nI don't like the use of the anthropomorphic term \"lying\" in the title and intro. It's just about OK, because of the very good discussion of why the model might produce false statements even though both another LLM and SAPLMA can tell that they are false. But I would still prefer a more moderate phrasing in the title.\n\ntypos_grammar_style_and_presentation_improvements: see commenta bove\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "teW6R1nwYi",
        "similarity": 0.7392,
        "coverage": 0.2857,
        "human_length": 570,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a method called SAPLMA to detect whether statements generated by large language models (LLMs) like GPT are true or false. The key idea is to use the hidden layer activations of an LLM and classify whether the LLM \"believes\" the statement is true/false. The authors create a dataset of true/false factual statements across 6 topics, train a classifier on 5 sets and test on the hold-out set. This ensures the classifier learns to extract the LLM's internal signal of truth/falsehood rather than topic-specific knowledge.\nExperiments with OPT-6.7B showed that SAPLMA achieves ~70% accuracy on held-out topics, significantly outperforming 1) few-shot prompting of the LLM itself 2) classifiers trained on BERT embeddings. The 20th hidden layer works best. SAPLMA also gets 63% accuracy on statements generated by the LLM, again outperforming baselines.\n\nreasons_to_accept: - Addresses an important issue - false information generated confidently by large LLMs. Proposes a practical method to detect such false statements.\n- Thorough experiments with different LLM layer activations, different topics, and human-labeled LLM-generated statements. Compares well to baselines.\n\nreasons_to_reject: - I don't know why the author claimed that \"we expect that the BERT embeddings will perform very poorly, as they focus on the semantics of the statement rather than on whether a statement is true or false. \" In my opinion, the classifier trained on OPT-175B and the classifier trained on BERT should be serving exactly the same function. The only difference is the model size, BERT is relatively small, while OPT-6.7B is more powerful. It's weird to say that \"BERT embedding focus on the semantics, not true/falsehood\". This should be your conclusion/explanation after you see the experiment results, not your claim before doing the experiment. Please explain if I misunderstand your claim here.\n- As you didn't consider inputting any grounded documents when examining the truthfulness of the model, a better way is that you should only construct the examples using the pretraining data of OPT-6.7B. You showed that the final accuracy is around 70%, but we cannot know whether the remaining 30% accuracy gap is caused by #1) OPT-6.7B has not been trained on some of the new knowledge existing in your new dataset, so it's unsure (lying) about its prediction, even if the input sentence is truthful #2) the classifier is just not good enough. If you can construct a dataset in this way, we can leave out reason #1 and only focus on reason #2.\n- No experiments on larger models like GPT-175B or LLaMA-65B. Unclear if findings transfer to more capable LLMs.\n- The classifier architecture used seems arbitrary. No tuning of hyperparameters. More analysis is needed on optimal architectures.\n\nquestions_for_the_authors: - A. Can you explain why you \"expect that the BERT embeddings will perform very poorly, as they focus on the semantics of the statement rather than on whether a statement is true or false.\" ? It's unclear for me.\n- B. Can you provide the experiment results for larger models like OPT-175B?\n- C. Can you provide some ablation study or analysis to show that your classifier architecture is optimal?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "lC8li2PUPs",
        "similarity": 0.69,
        "coverage": 0.3636,
        "human_length": 430,
        "human_text": "paper_topic_and_main_contributions: The paper shows evidence for the encoding of the factuality-related information in the latent representations of LMs, even when they generate nonfactual text, providing evidence for a disparity between the content of the representation space and the nature of the generated text.\n\nreasons_to_accept: The paper addresses an important question -- hallucinations in LM generation and ways to identify them. The key result is nontrivial, showing that models encode the truthfulness of an answer regardless of the text they actually generate as a continuation of the prompt. The leave-on-out design of the probe is nice and is showing good generalization to different domains (although it would be nice to *also* report numbers on the same domain).\n\nreasons_to_reject: - Testing on a single dataset and a single model. At the very least, the results should be replicated on different OPT models and on different factual datasets.\n- Several design decisions are not clearly motivated - see questions for the authors. Particularly, it is not clear whether the GPT-generated portion of the data was validated.\n- Assuming the data is balanced, the accuracy scores reported are low. This puts into questions the claims that models \"know\" when the statement is true.\n- It is not clear whether the information extracted by the probe is being used by the model (in the causal sense). It would be nice to include linear probes and perform linear erasure / amnesic probing experiments.\n\nquestions_for_the_authors: - The methods section reads \"We used a reliable source that included a table with several properties for each instance\". What is this source?\n- Did you verify the scientific-facts portion of the data, generated by ChatGPT?\n- The paper reports that trying to prompt GPT directly to decide whether the statement is true of false yielded random accuracy. This is a very surprising - is that true for all categories in the data? Did you try to look at the probabilities of the actual true and false answers, rather than at the probabilities of the tokens \"true\" and \"false\"?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "TANNXBr2Bw",
        "similarity": 0.6849,
        "coverage": 0.5417,
        "human_length": 580,
        "human_text": "paper_topic_and_main_contributions: The paper describes a method to get a model to identify the veracity of its own generations at a low additional cost. It shows that with a simple classifier on top of certain layers of a model's output hidden states, the veracity prediction of model generations can be learned with a modest amount of training data, and is generalizable to unseen domains.\n\nreasons_to_accept: The problem targeted by this paper, the hallucination issue and improving trust in LLMs, is one of great importance to the deployment of LLMs. The proposed method shows promising performance with lightweight architecture, experiments are thorough in the different conditions, generalization from artificial training data to actual model generations is also covered.\nIn the analysis of why a model would generate false claims even when they are able to discern them from true ones, the first reason discussed is also inspiring.\n\nreasons_to_reject: The method is only evaluated on a dataset that the authors created with the paper, where the exact sources of information for these datasets, as well as the construction details (e.g. how the tables are turned into sentences, etc.), are not mentioned (lines 324-325) While the authors compared with a BERT classifier and few-shot self-correction, the pool of baselines feels a bit thin, with a lack of external baselines tackling the same problem (e.g. the missing reference listed below).\nThe scope of the paper is limited to simple factoid sentences, where the veracity of factoid claims embedded in more complex utterances, or the veracity of the more complex utterances themselves are not covered.\nA number of presentation improvements could be made, for instance, the second reason-for-generation failure in lines 105-111 is not obvious, need further support.\n\nquestions_for_the_authors: I have a few questions to the authors: A. Regarding BERT classifiers and the strategy of using generation probabilities, what would the results be, if they are posed a simplified task of comparing pairs of true / false statements? ( Given the construction method of the dataset, the true and false statements should be in pairs) This would serve as a measure to the information these perplexity/pseudo-perplexity values have on the veracity of statements, if the length/frequency factors etc. could be marginalized.\nB. Given the difference in accuracy values with the change in thresholds, it feels it would ultimately be better to report AUC values instead of these accuracy values, for the AUC values are a more comprehensive measure of discriminative power along the spectrum of thresholds. Thoughts?\nC. Considering future work, is it possible for the LM to tell at which tokens did its own outputs begin to derail?\n\nmissing_references: The following paper also tackles the problem of having a LM critique on its own predictions: Language Models (Mostly) Know What They Know (https://arxiv.org/abs/2207.05221). While it mostly concerns the problem of calibration of logit likelihoods, the two papers are of similar spirits and the methods may serve as additional baselines.\n\ntypos_grammar_style_and_presentation_improvements: lines 018-020: repetitive to lines 008-010; line 118: capital M in Model; line 205: multilingual line 222: text line 267: LM/LLM, not LMM\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "8Bg7T1CXO6",
        "length": 453,
        "human_text": "paper_topic_and_main_contributions: This paper explores the hypothesis that there exists, somewhere in a large language model, a generalizable representation of the truth or falsity of a statement. The authors use a small special-purpose network to train a classifier that can convert the outputs of any hidden layer into an estimate of the truthfulness of the statement. The performance of classifiers of this type leads the authors to conclude that the generalized representation is indeed present.\nIn service of the hypothesis, the authors create a small corpus of examples, spanning several domains. This will be a resource for replications and future studies.\n\nreasons_to_accept: This is a very interesting and important question. The experiments are generally well-designed and informative. \nThe organization and narrative of the paper is excellent, making very clear what was done and why, with good examples.\n\nreasons_to_reject: There are significant numbers of small writing errors that do not impede comprehension or readability. A careful rewrite by an author who did not do much of the initial writing would fix this. These are obvious errors that would be caught by an on-line style and grammar checker.\nThe authors test five layers. This is a multiple compatsions setting, so statistics aiming to show that the result is real should use some kind of correction for multiple comparisons.  It is also worth, for safety, running a simple control in which the true/false labels are randomly permuted, so that each sentence is associated with the truth value of a randomly chosen counterpart. If all is well, we will find that the model is unable to predict the permuted labels with any accuracy.  This will help to establish beyond a reasonable doubt that the observed performance is due to the ability to detect truth, not to some strange effect of the behavior of large capacity models capable of learning almost anything. Possibly, doing this test is overly paranoid, but since it is cheap to do and the result is high-impact, why not?\nI don't like the use of the anthropomorphic term \"lying\" in the title and intro. It's just about OK, because of the very good discussion of why the model might produce false statements even though both another LLM and SAPLMA can tell that they are false. But I would still prefer a more moderate phrasing in the title.\n\ntypos_grammar_style_and_presentation_improvements: see commenta bove\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "teW6R1nwYi",
        "length": 570,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a method called SAPLMA to detect whether statements generated by large language models (LLMs) like GPT are true or false. The key idea is to use the hidden layer activations of an LLM and classify whether the LLM \"believes\" the statement is true/false. The authors create a dataset of true/false factual statements across 6 topics, train a classifier on 5 sets and test on the hold-out set. This ensures the classifier learns to extract the LLM's internal signal of truth/falsehood rather than topic-specific knowledge.\nExperiments with OPT-6.7B showed that SAPLMA achieves ~70% accuracy on held-out topics, significantly outperforming 1) few-shot prompting of the LLM itself 2) classifiers trained on BERT embeddings. The 20th hidden layer works best. SAPLMA also gets 63% accuracy on statements generated by the LLM, again outperforming baselines.\n\nreasons_to_accept: - Addresses an important issue - false information generated confidently by large LLMs. Proposes a practical method to detect such false statements.\n- Thorough experiments with different LLM layer activations, different topics, and human-labeled LLM-generated statements. Compares well to baselines.\n\nreasons_to_reject: - I don't know why the author claimed that \"we expect that the BERT embeddings will perform very poorly, as they focus on the semantics of the statement rather than on whether a statement is true or false. \" In my opinion, the classifier trained on OPT-175B and the classifier trained on BERT should be serving exactly the same function. The only difference is the model size, BERT is relatively small, while OPT-6.7B is more powerful. It's weird to say that \"BERT embedding focus on the semantics, not true/falsehood\". This should be your conclusion/explanation after you see the experiment results, not your claim before doing the experiment. Please explain if I misunderstand your claim here.\n- As you didn't consider inputting any grounded documents when examining the truthfulness of the model, a better way is that you should only construct the examples using the pretraining data of OPT-6.7B. You showed that the final accuracy is around 70%, but we cannot know whether the remaining 30% accuracy gap is caused by #1) OPT-6.7B has not been trained on some of the new knowledge existing in your new dataset, so it's unsure (lying) about its prediction, even if the input sentence is truthful #2) the classifier is just not good enough. If you can construct a dataset in this way, we can leave out reason #1 and only focus on reason #2.\n- No experiments on larger models like GPT-175B or LLaMA-65B. Unclear if findings transfer to more capable LLMs.\n- The classifier architecture used seems arbitrary. No tuning of hyperparameters. More analysis is needed on optimal architectures.\n\nquestions_for_the_authors: - A. Can you explain why you \"expect that the BERT embeddings will perform very poorly, as they focus on the semantics of the statement rather than on whether a statement is true or false.\" ? It's unclear for me.\n- B. Can you provide the experiment results for larger models like OPT-175B?\n- C. Can you provide some ablation study or analysis to show that your classifier architecture is optimal?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "lC8li2PUPs",
        "length": 430,
        "human_text": "paper_topic_and_main_contributions: The paper shows evidence for the encoding of the factuality-related information in the latent representations of LMs, even when they generate nonfactual text, providing evidence for a disparity between the content of the representation space and the nature of the generated text.\n\nreasons_to_accept: The paper addresses an important question -- hallucinations in LM generation and ways to identify them. The key result is nontrivial, showing that models encode the truthfulness of an answer regardless of the text they actually generate as a continuation of the prompt. The leave-on-out design of the probe is nice and is showing good generalization to different domains (although it would be nice to *also* report numbers on the same domain).\n\nreasons_to_reject: - Testing on a single dataset and a single model. At the very least, the results should be replicated on different OPT models and on different factual datasets.\n- Several design decisions are not clearly motivated - see questions for the authors. Particularly, it is not clear whether the GPT-generated portion of the data was validated.\n- Assuming the data is balanced, the accuracy scores reported are low. This puts into questions the claims that models \"know\" when the statement is true.\n- It is not clear whether the information extracted by the probe is being used by the model (in the causal sense). It would be nice to include linear probes and perform linear erasure / amnesic probing experiments.\n\nquestions_for_the_authors: - The methods section reads \"We used a reliable source that included a table with several properties for each instance\". What is this source?\n- Did you verify the scientific-facts portion of the data, generated by ChatGPT?\n- The paper reports that trying to prompt GPT directly to decide whether the statement is true of false yielded random accuracy. This is a very surprising - is that true for all categories in the data? Did you try to look at the probabilities of the actual true and false answers, rather than at the probabilities of the tokens \"true\" and \"false\"?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "TANNXBr2Bw",
        "length": 580,
        "human_text": "paper_topic_and_main_contributions: The paper describes a method to get a model to identify the veracity of its own generations at a low additional cost. It shows that with a simple classifier on top of certain layers of a model's output hidden states, the veracity prediction of model generations can be learned with a modest amount of training data, and is generalizable to unseen domains.\n\nreasons_to_accept: The problem targeted by this paper, the hallucination issue and improving trust in LLMs, is one of great importance to the deployment of LLMs. The proposed method shows promising performance with lightweight architecture, experiments are thorough in the different conditions, generalization from artificial training data to actual model generations is also covered.\nIn the analysis of why a model would generate false claims even when they are able to discern them from true ones, the first reason discussed is also inspiring.\n\nreasons_to_reject: The method is only evaluated on a dataset that the authors created with the paper, where the exact sources of information for these datasets, as well as the construction details (e.g. how the tables are turned into sentences, etc.), are not mentioned (lines 324-325) While the authors compared with a BERT classifier and few-shot self-correction, the pool of baselines feels a bit thin, with a lack of external baselines tackling the same problem (e.g. the missing reference listed below).\nThe scope of the paper is limited to simple factoid sentences, where the veracity of factoid claims embedded in more complex utterances, or the veracity of the more complex utterances themselves are not covered.\nA number of presentation improvements could be made, for instance, the second reason-for-generation failure in lines 105-111 is not obvious, need further support.\n\nquestions_for_the_authors: I have a few questions to the authors: A. Regarding BERT classifiers and the strategy of using generation probabilities, what would the results be, if they are posed a simplified task of comparing pairs of true / false statements? ( Given the construction method of the dataset, the true and false statements should be in pairs) This would serve as a measure to the information these perplexity/pseudo-perplexity values have on the veracity of statements, if the length/frequency factors etc. could be marginalized.\nB. Given the difference in accuracy values with the change in thresholds, it feels it would ultimately be better to report AUC values instead of these accuracy values, for the AUC values are a more comprehensive measure of discriminative power along the spectrum of thresholds. Thoughts?\nC. Considering future work, is it possible for the LM to tell at which tokens did its own outputs begin to derail?\n\nmissing_references: The following paper also tackles the problem of having a LM critique on its own predictions: Language Models (Mostly) Know What They Know (https://arxiv.org/abs/2207.05221). While it mostly concerns the problem of calibration of logit likelihoods, the two papers are of similar spirits and the methods may serve as additional baselines.\n\ntypos_grammar_style_and_presentation_improvements: lines 018-020: repetitive to lines 008-010; line 118: capital M in Model; line 205: multilingual line 222: text line 267: LM/LLM, not LMM\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "12_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_12_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.7063999999999999,
      "max_similarity": 0.7383,
      "avg_coverage": 0.40645,
      "max_coverage": 0.5417
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 427,
      "avg_human_length": 508.25
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "8Bg7T1CXO6",
        "similarity": 0.7105,
        "coverage": 0.4348,
        "human_length": 453,
        "human_text": "paper_topic_and_main_contributions: This paper explores the hypothesis that there exists, somewhere in a large language model, a generalizable representation of the truth or falsity of a statement. The authors use a small special-purpose network to train a classifier that can convert the outputs of any hidden layer into an estimate of the truthfulness of the statement. The performance of classifiers of this type leads the authors to conclude that the generalized representation is indeed present.\nIn service of the hypothesis, the authors create a small corpus of examples, spanning several domains. This will be a resource for replications and future studies.\n\nreasons_to_accept: This is a very interesting and important question. The experiments are generally well-designed and informative. \nThe organization and narrative of the paper is excellent, making very clear what was done and why, with good examples.\n\nreasons_to_reject: There are significant numbers of small writing errors that do not impede comprehension or readability. A careful rewrite by an author who did not do much of the initial writing would fix this. These are obvious errors that would be caught by an on-line style and grammar checker.\nThe authors test five layers. This is a multiple compatsions setting, so statistics aiming to show that the result is real should use some kind of correction for multiple comparisons.  It is also worth, for safety, running a simple control in which the true/false labels are randomly permuted, so that each sentence is associated with the truth value of a randomly chosen counterpart. If all is well, we will find that the model is unable to predict the permuted labels with any accuracy.  This will help to establish beyond a reasonable doubt that the observed performance is due to the ability to detect truth, not to some strange effect of the behavior of large capacity models capable of learning almost anything. Possibly, doing this test is overly paranoid, but since it is cheap to do and the result is high-impact, why not?\nI don't like the use of the anthropomorphic term \"lying\" in the title and intro. It's just about OK, because of the very good discussion of why the model might produce false statements even though both another LLM and SAPLMA can tell that they are false. But I would still prefer a more moderate phrasing in the title.\n\ntypos_grammar_style_and_presentation_improvements: see commenta bove\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "teW6R1nwYi",
        "similarity": 0.7383,
        "coverage": 0.2857,
        "human_length": 570,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a method called SAPLMA to detect whether statements generated by large language models (LLMs) like GPT are true or false. The key idea is to use the hidden layer activations of an LLM and classify whether the LLM \"believes\" the statement is true/false. The authors create a dataset of true/false factual statements across 6 topics, train a classifier on 5 sets and test on the hold-out set. This ensures the classifier learns to extract the LLM's internal signal of truth/falsehood rather than topic-specific knowledge.\nExperiments with OPT-6.7B showed that SAPLMA achieves ~70% accuracy on held-out topics, significantly outperforming 1) few-shot prompting of the LLM itself 2) classifiers trained on BERT embeddings. The 20th hidden layer works best. SAPLMA also gets 63% accuracy on statements generated by the LLM, again outperforming baselines.\n\nreasons_to_accept: - Addresses an important issue - false information generated confidently by large LLMs. Proposes a practical method to detect such false statements.\n- Thorough experiments with different LLM layer activations, different topics, and human-labeled LLM-generated statements. Compares well to baselines.\n\nreasons_to_reject: - I don't know why the author claimed that \"we expect that the BERT embeddings will perform very poorly, as they focus on the semantics of the statement rather than on whether a statement is true or false. \" In my opinion, the classifier trained on OPT-175B and the classifier trained on BERT should be serving exactly the same function. The only difference is the model size, BERT is relatively small, while OPT-6.7B is more powerful. It's weird to say that \"BERT embedding focus on the semantics, not true/falsehood\". This should be your conclusion/explanation after you see the experiment results, not your claim before doing the experiment. Please explain if I misunderstand your claim here.\n- As you didn't consider inputting any grounded documents when examining the truthfulness of the model, a better way is that you should only construct the examples using the pretraining data of OPT-6.7B. You showed that the final accuracy is around 70%, but we cannot know whether the remaining 30% accuracy gap is caused by #1) OPT-6.7B has not been trained on some of the new knowledge existing in your new dataset, so it's unsure (lying) about its prediction, even if the input sentence is truthful #2) the classifier is just not good enough. If you can construct a dataset in this way, we can leave out reason #1 and only focus on reason #2.\n- No experiments on larger models like GPT-175B or LLaMA-65B. Unclear if findings transfer to more capable LLMs.\n- The classifier architecture used seems arbitrary. No tuning of hyperparameters. More analysis is needed on optimal architectures.\n\nquestions_for_the_authors: - A. Can you explain why you \"expect that the BERT embeddings will perform very poorly, as they focus on the semantics of the statement rather than on whether a statement is true or false.\" ? It's unclear for me.\n- B. Can you provide the experiment results for larger models like OPT-175B?\n- C. Can you provide some ablation study or analysis to show that your classifier architecture is optimal?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "lC8li2PUPs",
        "similarity": 0.6913,
        "coverage": 0.3636,
        "human_length": 430,
        "human_text": "paper_topic_and_main_contributions: The paper shows evidence for the encoding of the factuality-related information in the latent representations of LMs, even when they generate nonfactual text, providing evidence for a disparity between the content of the representation space and the nature of the generated text.\n\nreasons_to_accept: The paper addresses an important question -- hallucinations in LM generation and ways to identify them. The key result is nontrivial, showing that models encode the truthfulness of an answer regardless of the text they actually generate as a continuation of the prompt. The leave-on-out design of the probe is nice and is showing good generalization to different domains (although it would be nice to *also* report numbers on the same domain).\n\nreasons_to_reject: - Testing on a single dataset and a single model. At the very least, the results should be replicated on different OPT models and on different factual datasets.\n- Several design decisions are not clearly motivated - see questions for the authors. Particularly, it is not clear whether the GPT-generated portion of the data was validated.\n- Assuming the data is balanced, the accuracy scores reported are low. This puts into questions the claims that models \"know\" when the statement is true.\n- It is not clear whether the information extracted by the probe is being used by the model (in the causal sense). It would be nice to include linear probes and perform linear erasure / amnesic probing experiments.\n\nquestions_for_the_authors: - The methods section reads \"We used a reliable source that included a table with several properties for each instance\". What is this source?\n- Did you verify the scientific-facts portion of the data, generated by ChatGPT?\n- The paper reports that trying to prompt GPT directly to decide whether the statement is true of false yielded random accuracy. This is a very surprising - is that true for all categories in the data? Did you try to look at the probabilities of the actual true and false answers, rather than at the probabilities of the tokens \"true\" and \"false\"?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "TANNXBr2Bw",
        "similarity": 0.6855,
        "coverage": 0.5417,
        "human_length": 580,
        "human_text": "paper_topic_and_main_contributions: The paper describes a method to get a model to identify the veracity of its own generations at a low additional cost. It shows that with a simple classifier on top of certain layers of a model's output hidden states, the veracity prediction of model generations can be learned with a modest amount of training data, and is generalizable to unseen domains.\n\nreasons_to_accept: The problem targeted by this paper, the hallucination issue and improving trust in LLMs, is one of great importance to the deployment of LLMs. The proposed method shows promising performance with lightweight architecture, experiments are thorough in the different conditions, generalization from artificial training data to actual model generations is also covered.\nIn the analysis of why a model would generate false claims even when they are able to discern them from true ones, the first reason discussed is also inspiring.\n\nreasons_to_reject: The method is only evaluated on a dataset that the authors created with the paper, where the exact sources of information for these datasets, as well as the construction details (e.g. how the tables are turned into sentences, etc.), are not mentioned (lines 324-325) While the authors compared with a BERT classifier and few-shot self-correction, the pool of baselines feels a bit thin, with a lack of external baselines tackling the same problem (e.g. the missing reference listed below).\nThe scope of the paper is limited to simple factoid sentences, where the veracity of factoid claims embedded in more complex utterances, or the veracity of the more complex utterances themselves are not covered.\nA number of presentation improvements could be made, for instance, the second reason-for-generation failure in lines 105-111 is not obvious, need further support.\n\nquestions_for_the_authors: I have a few questions to the authors: A. Regarding BERT classifiers and the strategy of using generation probabilities, what would the results be, if they are posed a simplified task of comparing pairs of true / false statements? ( Given the construction method of the dataset, the true and false statements should be in pairs) This would serve as a measure to the information these perplexity/pseudo-perplexity values have on the veracity of statements, if the length/frequency factors etc. could be marginalized.\nB. Given the difference in accuracy values with the change in thresholds, it feels it would ultimately be better to report AUC values instead of these accuracy values, for the AUC values are a more comprehensive measure of discriminative power along the spectrum of thresholds. Thoughts?\nC. Considering future work, is it possible for the LM to tell at which tokens did its own outputs begin to derail?\n\nmissing_references: The following paper also tackles the problem of having a LM critique on its own predictions: Language Models (Mostly) Know What They Know (https://arxiv.org/abs/2207.05221). While it mostly concerns the problem of calibration of logit likelihoods, the two papers are of similar spirits and the methods may serve as additional baselines.\n\ntypos_grammar_style_and_presentation_improvements: lines 018-020: repetitive to lines 008-010; line 118: capital M in Model; line 205: multilingual line 222: text line 267: LM/LLM, not LMM\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "8Bg7T1CXO6",
        "length": 453,
        "human_text": "paper_topic_and_main_contributions: This paper explores the hypothesis that there exists, somewhere in a large language model, a generalizable representation of the truth or falsity of a statement. The authors use a small special-purpose network to train a classifier that can convert the outputs of any hidden layer into an estimate of the truthfulness of the statement. The performance of classifiers of this type leads the authors to conclude that the generalized representation is indeed present.\nIn service of the hypothesis, the authors create a small corpus of examples, spanning several domains. This will be a resource for replications and future studies.\n\nreasons_to_accept: This is a very interesting and important question. The experiments are generally well-designed and informative. \nThe organization and narrative of the paper is excellent, making very clear what was done and why, with good examples.\n\nreasons_to_reject: There are significant numbers of small writing errors that do not impede comprehension or readability. A careful rewrite by an author who did not do much of the initial writing would fix this. These are obvious errors that would be caught by an on-line style and grammar checker.\nThe authors test five layers. This is a multiple compatsions setting, so statistics aiming to show that the result is real should use some kind of correction for multiple comparisons.  It is also worth, for safety, running a simple control in which the true/false labels are randomly permuted, so that each sentence is associated with the truth value of a randomly chosen counterpart. If all is well, we will find that the model is unable to predict the permuted labels with any accuracy.  This will help to establish beyond a reasonable doubt that the observed performance is due to the ability to detect truth, not to some strange effect of the behavior of large capacity models capable of learning almost anything. Possibly, doing this test is overly paranoid, but since it is cheap to do and the result is high-impact, why not?\nI don't like the use of the anthropomorphic term \"lying\" in the title and intro. It's just about OK, because of the very good discussion of why the model might produce false statements even though both another LLM and SAPLMA can tell that they are false. But I would still prefer a more moderate phrasing in the title.\n\ntypos_grammar_style_and_presentation_improvements: see commenta bove\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "teW6R1nwYi",
        "length": 570,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a method called SAPLMA to detect whether statements generated by large language models (LLMs) like GPT are true or false. The key idea is to use the hidden layer activations of an LLM and classify whether the LLM \"believes\" the statement is true/false. The authors create a dataset of true/false factual statements across 6 topics, train a classifier on 5 sets and test on the hold-out set. This ensures the classifier learns to extract the LLM's internal signal of truth/falsehood rather than topic-specific knowledge.\nExperiments with OPT-6.7B showed that SAPLMA achieves ~70% accuracy on held-out topics, significantly outperforming 1) few-shot prompting of the LLM itself 2) classifiers trained on BERT embeddings. The 20th hidden layer works best. SAPLMA also gets 63% accuracy on statements generated by the LLM, again outperforming baselines.\n\nreasons_to_accept: - Addresses an important issue - false information generated confidently by large LLMs. Proposes a practical method to detect such false statements.\n- Thorough experiments with different LLM layer activations, different topics, and human-labeled LLM-generated statements. Compares well to baselines.\n\nreasons_to_reject: - I don't know why the author claimed that \"we expect that the BERT embeddings will perform very poorly, as they focus on the semantics of the statement rather than on whether a statement is true or false. \" In my opinion, the classifier trained on OPT-175B and the classifier trained on BERT should be serving exactly the same function. The only difference is the model size, BERT is relatively small, while OPT-6.7B is more powerful. It's weird to say that \"BERT embedding focus on the semantics, not true/falsehood\". This should be your conclusion/explanation after you see the experiment results, not your claim before doing the experiment. Please explain if I misunderstand your claim here.\n- As you didn't consider inputting any grounded documents when examining the truthfulness of the model, a better way is that you should only construct the examples using the pretraining data of OPT-6.7B. You showed that the final accuracy is around 70%, but we cannot know whether the remaining 30% accuracy gap is caused by #1) OPT-6.7B has not been trained on some of the new knowledge existing in your new dataset, so it's unsure (lying) about its prediction, even if the input sentence is truthful #2) the classifier is just not good enough. If you can construct a dataset in this way, we can leave out reason #1 and only focus on reason #2.\n- No experiments on larger models like GPT-175B or LLaMA-65B. Unclear if findings transfer to more capable LLMs.\n- The classifier architecture used seems arbitrary. No tuning of hyperparameters. More analysis is needed on optimal architectures.\n\nquestions_for_the_authors: - A. Can you explain why you \"expect that the BERT embeddings will perform very poorly, as they focus on the semantics of the statement rather than on whether a statement is true or false.\" ? It's unclear for me.\n- B. Can you provide the experiment results for larger models like OPT-175B?\n- C. Can you provide some ablation study or analysis to show that your classifier architecture is optimal?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "lC8li2PUPs",
        "length": 430,
        "human_text": "paper_topic_and_main_contributions: The paper shows evidence for the encoding of the factuality-related information in the latent representations of LMs, even when they generate nonfactual text, providing evidence for a disparity between the content of the representation space and the nature of the generated text.\n\nreasons_to_accept: The paper addresses an important question -- hallucinations in LM generation and ways to identify them. The key result is nontrivial, showing that models encode the truthfulness of an answer regardless of the text they actually generate as a continuation of the prompt. The leave-on-out design of the probe is nice and is showing good generalization to different domains (although it would be nice to *also* report numbers on the same domain).\n\nreasons_to_reject: - Testing on a single dataset and a single model. At the very least, the results should be replicated on different OPT models and on different factual datasets.\n- Several design decisions are not clearly motivated - see questions for the authors. Particularly, it is not clear whether the GPT-generated portion of the data was validated.\n- Assuming the data is balanced, the accuracy scores reported are low. This puts into questions the claims that models \"know\" when the statement is true.\n- It is not clear whether the information extracted by the probe is being used by the model (in the causal sense). It would be nice to include linear probes and perform linear erasure / amnesic probing experiments.\n\nquestions_for_the_authors: - The methods section reads \"We used a reliable source that included a table with several properties for each instance\". What is this source?\n- Did you verify the scientific-facts portion of the data, generated by ChatGPT?\n- The paper reports that trying to prompt GPT directly to decide whether the statement is true of false yielded random accuracy. This is a very surprising - is that true for all categories in the data? Did you try to look at the probabilities of the actual true and false answers, rather than at the probabilities of the tokens \"true\" and \"false\"?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "TANNXBr2Bw",
        "length": 580,
        "human_text": "paper_topic_and_main_contributions: The paper describes a method to get a model to identify the veracity of its own generations at a low additional cost. It shows that with a simple classifier on top of certain layers of a model's output hidden states, the veracity prediction of model generations can be learned with a modest amount of training data, and is generalizable to unseen domains.\n\nreasons_to_accept: The problem targeted by this paper, the hallucination issue and improving trust in LLMs, is one of great importance to the deployment of LLMs. The proposed method shows promising performance with lightweight architecture, experiments are thorough in the different conditions, generalization from artificial training data to actual model generations is also covered.\nIn the analysis of why a model would generate false claims even when they are able to discern them from true ones, the first reason discussed is also inspiring.\n\nreasons_to_reject: The method is only evaluated on a dataset that the authors created with the paper, where the exact sources of information for these datasets, as well as the construction details (e.g. how the tables are turned into sentences, etc.), are not mentioned (lines 324-325) While the authors compared with a BERT classifier and few-shot self-correction, the pool of baselines feels a bit thin, with a lack of external baselines tackling the same problem (e.g. the missing reference listed below).\nThe scope of the paper is limited to simple factoid sentences, where the veracity of factoid claims embedded in more complex utterances, or the veracity of the more complex utterances themselves are not covered.\nA number of presentation improvements could be made, for instance, the second reason-for-generation failure in lines 105-111 is not obvious, need further support.\n\nquestions_for_the_authors: I have a few questions to the authors: A. Regarding BERT classifiers and the strategy of using generation probabilities, what would the results be, if they are posed a simplified task of comparing pairs of true / false statements? ( Given the construction method of the dataset, the true and false statements should be in pairs) This would serve as a measure to the information these perplexity/pseudo-perplexity values have on the veracity of statements, if the length/frequency factors etc. could be marginalized.\nB. Given the difference in accuracy values with the change in thresholds, it feels it would ultimately be better to report AUC values instead of these accuracy values, for the AUC values are a more comprehensive measure of discriminative power along the spectrum of thresholds. Thoughts?\nC. Considering future work, is it possible for the LM to tell at which tokens did its own outputs begin to derail?\n\nmissing_references: The following paper also tackles the problem of having a LM critique on its own predictions: Language Models (Mostly) Know What They Know (https://arxiv.org/abs/2207.05221). While it mostly concerns the problem of calibration of logit likelihoods, the two papers are of similar spirits and the methods may serve as additional baselines.\n\ntypos_grammar_style_and_presentation_improvements: lines 018-020: repetitive to lines 008-010; line 118: capital M in Model; line 205: multilingual line 222: text line 267: LM/LLM, not LMM\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "49_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_49_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.7487750000000001,
      "max_similarity": 0.7761,
      "avg_coverage": 0.39692500000000003,
      "max_coverage": 0.5714
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 490,
      "avg_human_length": 534.25
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 9,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "UsE4zEGlq9",
        "similarity": 0.7322,
        "coverage": 0.2727,
        "human_length": 797,
        "human_text": "paper_topic_and_main_contributions: In the authors' words, the aim of the work is to confirm that different approaches of constructing cross-lingual summarization (CLS) datasets will lead to different degrees of translationese. Then they study how such translationese affects CLS model evaluation and performance when it appears in source documents or target summaries.\n\nreasons_to_accept: The impact of translationese in the domain of NLP is a critical issue. Here it is important to take into account its growing dependence on ML techniques, a reality associated with the need for training corpus in different fields of knowledge and languages. This has led to the application of TM tools to its generation and, consequently, the generalization of all kinds of phenomena related to the concept of \"translationese\". In turn, given the popularization of NLP-based tools, this could even affect language on a longer term. The recent irruption of GPT-like models delves into the importance of studying this type of problems, particularly in relation to summarization.\nIn this context, the work is interesting. Regardless of the fact that the authors' conclusions coincide with those intuitively expected, the truth is that the evidence corroborates them and seems to dispel any possible doubts.\n\nreasons_to_reject: A. I wonder if the fact of considering only m-BART-based MT models for testing does not introduce some kind of bias in the results.\nB. I also wonder if the best way to address the impact of translationese in the field of cross-lingual technologies would not be by focusing our attention on the basic MT technologies... and not on each of their possible practical applications (summatization, ...). In this regard, it should be remembered that there are already works published in this line as, for example: Eva Vanmassenhove, Dimitar Shterionov, and Matthew Gwilliam. 2021. Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2203\u20132213, Online. Association for Computational Linguistics.\nSicheng Yu, Qianru Sun, Hao Zhang, and Jing Jiang. 2022. Translate-Train Embracing Translationese Artifacts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 362\u2013370, Dublin, Ireland. Association for Computational Linguistics.\nthe latter of which is one of the references included in this work.\n\nquestions_for_the_authors: The authors' responses seem conditioned by the size limitations of the text, both in my case and in that of the rest of the reviewers.\nCertainly some of the modifications required imply a significant investment of time and work, but it is no less true that if this is the case, it is because it refers to aspects that should have been considered before writing the text. More precisely, we have that: A. Please, justify the authors' position in relation to the reviewer's item A in the previous section \"Reasons to reject\".\nRegarding the authors' responses, it is not clear whether or not some type of comparison with other operating models will be included in the reviewed paper, given that the authors limit themselves to stating that: \"We would like to add this discussion in Limitation section\" when I have no doubt that it is an issue that must be addressed in one way or another in an eventual final review, if only to warn of the possibility of biases associated with the use of a certain type of modeling, given that this would condition the entire experimental part of the work.\nB. Please, justify the authors' position in relation to the reviewer's item B in the previous section \"Reasons to reject\".\nRegarding the authors' answers at this point, indeed most existing translationese-relevant studies typically focus on MT, which would have precisely facilitated a comparison like the one we have just commented on. In any case, we are talking about a decision of the authors who have adequately justified.\n\ntypos_grammar_style_and_presentation_improvements: Is such a display of bibliographic references really necessary ? Their presence is overwhelming and makes it difficult to understand the text (see for example Lines 040 or 058). Such display does not at all favor the reading and comprehension of the text.\nPlease, avoid redirects in the text, particularly in the Introduction (see Line 063, 083, 084). Instead, include a \"road-map\" of the paper at the end of Section 1, including a brief look of all the other Sections.\nMost of the last paragraph (Contributions) of Section 1 retake the same contents included in the previous one as well in the Abstract. Is this really necessary.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "aFZZzDoQta",
        "similarity": 0.7369,
        "coverage": 0.5714,
        "human_length": 429,
        "human_text": "paper_topic_and_main_contributions: In this paper the authors explore the effect of translationese on the cross-lingual summarization (CLS). Since in the cross-lingual summarization task the data is often obtained via translation (often MT) this leads to a question of whether the translationese, likely introduced during this process, affects the task itself (CLS) The authors investigate how translationese, when present in source documents or target summaries, affects the modeling and evaluation of CLS. The author explore this problem through a series of experiments and give suggestions on how one may deal with this issue in the future. These include controlling translationese in test sets, developing translationese-aware models, and building mixed-quality CLS datasets.\nThe authors also include a partial human evaluation, though details are missing.\n\nreasons_to_accept: - The problem addressed in this study is very timely and likely to be an issue in the current systems; - The experiment choice is motivated by the RQs.\n\nreasons_to_reject: - I am not sure about the human evaluation employed, as there are no sufficient details in the paper raising ethical and methodological questions. For instance, I believe that the ratings should not be displayed as means as the scale employed is likely ordinal (or at least a distribution of scores should be added. This further raises a question about the IAA analysis, as the reported Fleiss Kappa should be employed to categorical data. In other words, the IAA is done for categorical data, the means are reported as if the data was interval/numerical, while most likely the data is ordinal, though it is impossible to say as the details are not reported (or somehow I have missed that).\n\nquestions_for_the_authors: Question A: How were the human evaluators selected? Were they compensated for this task?  Question B: What is the 3-point scale? What instructions were given to the annotators?\n\ntypos_grammar_style_and_presentation_improvements: Apart from Fleiss Kappa, it would be also good to report the percentage agreement. If the data is not categorical, Krippendorff's alpha should be employed. Preferably CI should be reported.\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: Details on human evaluation are missing, it is not stated how much the evaluators were paid or whether the study design was reviewed by IRB.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "sp4old3SET",
        "similarity": 0.7499,
        "coverage": 0.4167,
        "human_length": 432,
        "human_text": "paper_topic_and_main_contributions: Paper is a comprehensive look at the impact of translationese in cross-lingual summarization.  authors motivate the work based on the shortage of cross-lingual summarization resources.  After introducing some metrics to estimate translationese, they look at their impact when present in the source text, target summaries and also their impact on summarization models.  Results indicate their presence and motivates some suggestions about containing their presence with automatic and manual efforts.\n\nreasons_to_accept: 1. Interesting topic (cross-language summarization).\n2. Fairly comprehensive look at the impact of translationese in source, target, eval and models.  The comprehensive work just requires some substantial polishing and restructuring.\n\nreasons_to_reject: 1. Paper has a poor writing and it is difficult to follow.  Moreover it is not organized well (long sentences, latex mistakes, many accronyms, long repetitions of table results in the text body). Even some basic concepts like translationese are not defined well.  There's not a single example in the entire paper.\n2. poor replicability of the work.  so much details are missing in computing various translationese metrics for different languages and cls models.\n3. The conclusion and macro level impact of the work does not seem significant.\n\nquestions_for_the_authors: 1. where are details of the wide range of linguistic analysis to compute translationese metrics (parse, pos, entropy, etc).  how do factor varying levels of quality for such analysis in different languages?\n2. section 3.3 is quite confusing:  here the direction is en2x and one would expect the summaries to be in the x languages and annotation is conducted in those 4-5 languages?  also any explanation for the fair/weak agreement for fluency and also the fact that overal agreement is higher than the individual factors?\n3. Re: Table 8, the MT data is always kept at 100% in the ablation study.  do you think using a cleaner subset of the MT data (using your translationese metrics) can make the impact of that data stronger?\n\ntypos_grammar_style_and_presentation_improvements: 1.  latex errors: line 057 (long referencing at the middle of sentence),  2. Table 9 & 10 in page 8.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "f6KqcDTVzC",
        "similarity": 0.7761,
        "coverage": 0.3269,
        "human_length": 479,
        "human_text": "paper_topic_and_main_contributions: 1. This paper explores the impact of translationese on the evaluation and performance of cross-lingual summarization models. \n2. The paper outlines the process of creating different cross-lingual summarization datasets and highlights potential disparities between automated and human evaluations. \n3. A comprehensive overview of the development of current cross-lingual summarization datasets is provided. \n4. The paper also delves into the factors within translated text that contribute to the phenomenon known as 'translationese'. \n5. Experiments were conducted to measure the influence of translationese on both documents and summaries, and the resulting analysis is presented in a clear and easily understandable manner.\n\nreasons_to_accept: 1. This paper is the first attempt to analyze the impact of translationese in cross-lingual summarization tasks. \n2. The authors provided a detailed analysis of the impact of translationese concerning source documents and Target summaries for various languages and datasets. \n3. The authors highlight findings, such as the potential promise in constructing mixed-quality or semi-supervised cross-lingual summarization datasets, which could be a valuable direction for further research. \n4. This paper emphasizes how important it is to use human-translated documents and summaries to create strong cross-lingual summarization models. \n5. The analysis process used to identify the impact of translationese allows other groups to reproduce for different language families.\n\nreasons_to_reject: 1. In section 3.3, the human evaluation part isn't clear. \n         a. Were the evaluators native speakers? What was their expertise level?\n      b. What guidelines were given for human evaluation? The specifics of the 3-point scale breakdown for metrics like Informativeness,            Fluency, and Overall are missing.                                                           c.  On line 363, what does \"overall quality\" mean? \n       d. The authors mentioned selected 100 samples from the XSAMSUM test set randomly. How many of these belonged to mBART-HT             and mBART-MT? However, Table 4 indicates separate human evaluation for these subsets.\n     e. If human evaluation was done separately, I would expect additional Fleiss' kappa scores for Informativeness, Fluency, and Overall.\n     f. On line 369, mentioned that  \"good agreement between evaluators.\" But according to Fleiss' kappa score interpretation             (https://en.wikipedia.org/wiki/Fleiss%27_kappa), the scores show fair to moderate agreement. It's important to explain the reasons            for the lower agreement scores.                                                                                                                                           g. Opting for human evaluation on two different datasets would be preferable.\n2. The above raised issue apply to section 4.3 also, where Fleiss' kappa scores and metric ratings are missing.\n3. As stated in Appendix B, all reported experimental scores are averages of 3 runs. However, the corresponding standard deviation scores are absent.\n4. Regarding lines 022-023, there's no evidence or empirical study provided on how translationese might negatively affect CLS model performance in real-world applications.\n\ntypos_grammar_style_and_presentation_improvements: Minor issue: 1. line183 --> check the spacing\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "UsE4zEGlq9",
        "length": 797,
        "human_text": "paper_topic_and_main_contributions: In the authors' words, the aim of the work is to confirm that different approaches of constructing cross-lingual summarization (CLS) datasets will lead to different degrees of translationese. Then they study how such translationese affects CLS model evaluation and performance when it appears in source documents or target summaries.\n\nreasons_to_accept: The impact of translationese in the domain of NLP is a critical issue. Here it is important to take into account its growing dependence on ML techniques, a reality associated with the need for training corpus in different fields of knowledge and languages. This has led to the application of TM tools to its generation and, consequently, the generalization of all kinds of phenomena related to the concept of \"translationese\". In turn, given the popularization of NLP-based tools, this could even affect language on a longer term. The recent irruption of GPT-like models delves into the importance of studying this type of problems, particularly in relation to summarization.\nIn this context, the work is interesting. Regardless of the fact that the authors' conclusions coincide with those intuitively expected, the truth is that the evidence corroborates them and seems to dispel any possible doubts.\n\nreasons_to_reject: A. I wonder if the fact of considering only m-BART-based MT models for testing does not introduce some kind of bias in the results.\nB. I also wonder if the best way to address the impact of translationese in the field of cross-lingual technologies would not be by focusing our attention on the basic MT technologies... and not on each of their possible practical applications (summatization, ...). In this regard, it should be remembered that there are already works published in this line as, for example: Eva Vanmassenhove, Dimitar Shterionov, and Matthew Gwilliam. 2021. Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2203\u20132213, Online. Association for Computational Linguistics.\nSicheng Yu, Qianru Sun, Hao Zhang, and Jing Jiang. 2022. Translate-Train Embracing Translationese Artifacts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 362\u2013370, Dublin, Ireland. Association for Computational Linguistics.\nthe latter of which is one of the references included in this work.\n\nquestions_for_the_authors: The authors' responses seem conditioned by the size limitations of the text, both in my case and in that of the rest of the reviewers.\nCertainly some of the modifications required imply a significant investment of time and work, but it is no less true that if this is the case, it is because it refers to aspects that should have been considered before writing the text. More precisely, we have that: A. Please, justify the authors' position in relation to the reviewer's item A in the previous section \"Reasons to reject\".\nRegarding the authors' responses, it is not clear whether or not some type of comparison with other operating models will be included in the reviewed paper, given that the authors limit themselves to stating that: \"We would like to add this discussion in Limitation section\" when I have no doubt that it is an issue that must be addressed in one way or another in an eventual final review, if only to warn of the possibility of biases associated with the use of a certain type of modeling, given that this would condition the entire experimental part of the work.\nB. Please, justify the authors' position in relation to the reviewer's item B in the previous section \"Reasons to reject\".\nRegarding the authors' answers at this point, indeed most existing translationese-relevant studies typically focus on MT, which would have precisely facilitated a comparison like the one we have just commented on. In any case, we are talking about a decision of the authors who have adequately justified.\n\ntypos_grammar_style_and_presentation_improvements: Is such a display of bibliographic references really necessary ? Their presence is overwhelming and makes it difficult to understand the text (see for example Lines 040 or 058). Such display does not at all favor the reading and comprehension of the text.\nPlease, avoid redirects in the text, particularly in the Introduction (see Line 063, 083, 084). Instead, include a \"road-map\" of the paper at the end of Section 1, including a brief look of all the other Sections.\nMost of the last paragraph (Contributions) of Section 1 retake the same contents included in the previous one as well in the Abstract. Is this really necessary.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "aFZZzDoQta",
        "length": 429,
        "human_text": "paper_topic_and_main_contributions: In this paper the authors explore the effect of translationese on the cross-lingual summarization (CLS). Since in the cross-lingual summarization task the data is often obtained via translation (often MT) this leads to a question of whether the translationese, likely introduced during this process, affects the task itself (CLS) The authors investigate how translationese, when present in source documents or target summaries, affects the modeling and evaluation of CLS. The author explore this problem through a series of experiments and give suggestions on how one may deal with this issue in the future. These include controlling translationese in test sets, developing translationese-aware models, and building mixed-quality CLS datasets.\nThe authors also include a partial human evaluation, though details are missing.\n\nreasons_to_accept: - The problem addressed in this study is very timely and likely to be an issue in the current systems; - The experiment choice is motivated by the RQs.\n\nreasons_to_reject: - I am not sure about the human evaluation employed, as there are no sufficient details in the paper raising ethical and methodological questions. For instance, I believe that the ratings should not be displayed as means as the scale employed is likely ordinal (or at least a distribution of scores should be added. This further raises a question about the IAA analysis, as the reported Fleiss Kappa should be employed to categorical data. In other words, the IAA is done for categorical data, the means are reported as if the data was interval/numerical, while most likely the data is ordinal, though it is impossible to say as the details are not reported (or somehow I have missed that).\n\nquestions_for_the_authors: Question A: How were the human evaluators selected? Were they compensated for this task?  Question B: What is the 3-point scale? What instructions were given to the annotators?\n\ntypos_grammar_style_and_presentation_improvements: Apart from Fleiss Kappa, it would be also good to report the percentage agreement. If the data is not categorical, Krippendorff's alpha should be employed. Preferably CI should be reported.\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: Details on human evaluation are missing, it is not stated how much the evaluators were paid or whether the study design was reviewed by IRB.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "sp4old3SET",
        "length": 432,
        "human_text": "paper_topic_and_main_contributions: Paper is a comprehensive look at the impact of translationese in cross-lingual summarization.  authors motivate the work based on the shortage of cross-lingual summarization resources.  After introducing some metrics to estimate translationese, they look at their impact when present in the source text, target summaries and also their impact on summarization models.  Results indicate their presence and motivates some suggestions about containing their presence with automatic and manual efforts.\n\nreasons_to_accept: 1. Interesting topic (cross-language summarization).\n2. Fairly comprehensive look at the impact of translationese in source, target, eval and models.  The comprehensive work just requires some substantial polishing and restructuring.\n\nreasons_to_reject: 1. Paper has a poor writing and it is difficult to follow.  Moreover it is not organized well (long sentences, latex mistakes, many accronyms, long repetitions of table results in the text body). Even some basic concepts like translationese are not defined well.  There's not a single example in the entire paper.\n2. poor replicability of the work.  so much details are missing in computing various translationese metrics for different languages and cls models.\n3. The conclusion and macro level impact of the work does not seem significant.\n\nquestions_for_the_authors: 1. where are details of the wide range of linguistic analysis to compute translationese metrics (parse, pos, entropy, etc).  how do factor varying levels of quality for such analysis in different languages?\n2. section 3.3 is quite confusing:  here the direction is en2x and one would expect the summaries to be in the x languages and annotation is conducted in those 4-5 languages?  also any explanation for the fair/weak agreement for fluency and also the fact that overal agreement is higher than the individual factors?\n3. Re: Table 8, the MT data is always kept at 100% in the ablation study.  do you think using a cleaner subset of the MT data (using your translationese metrics) can make the impact of that data stronger?\n\ntypos_grammar_style_and_presentation_improvements: 1.  latex errors: line 057 (long referencing at the middle of sentence),  2. Table 9 & 10 in page 8.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "f6KqcDTVzC",
        "length": 479,
        "human_text": "paper_topic_and_main_contributions: 1. This paper explores the impact of translationese on the evaluation and performance of cross-lingual summarization models. \n2. The paper outlines the process of creating different cross-lingual summarization datasets and highlights potential disparities between automated and human evaluations. \n3. A comprehensive overview of the development of current cross-lingual summarization datasets is provided. \n4. The paper also delves into the factors within translated text that contribute to the phenomenon known as 'translationese'. \n5. Experiments were conducted to measure the influence of translationese on both documents and summaries, and the resulting analysis is presented in a clear and easily understandable manner.\n\nreasons_to_accept: 1. This paper is the first attempt to analyze the impact of translationese in cross-lingual summarization tasks. \n2. The authors provided a detailed analysis of the impact of translationese concerning source documents and Target summaries for various languages and datasets. \n3. The authors highlight findings, such as the potential promise in constructing mixed-quality or semi-supervised cross-lingual summarization datasets, which could be a valuable direction for further research. \n4. This paper emphasizes how important it is to use human-translated documents and summaries to create strong cross-lingual summarization models. \n5. The analysis process used to identify the impact of translationese allows other groups to reproduce for different language families.\n\nreasons_to_reject: 1. In section 3.3, the human evaluation part isn't clear. \n         a. Were the evaluators native speakers? What was their expertise level?\n      b. What guidelines were given for human evaluation? The specifics of the 3-point scale breakdown for metrics like Informativeness,            Fluency, and Overall are missing.                                                           c.  On line 363, what does \"overall quality\" mean? \n       d. The authors mentioned selected 100 samples from the XSAMSUM test set randomly. How many of these belonged to mBART-HT             and mBART-MT? However, Table 4 indicates separate human evaluation for these subsets.\n     e. If human evaluation was done separately, I would expect additional Fleiss' kappa scores for Informativeness, Fluency, and Overall.\n     f. On line 369, mentioned that  \"good agreement between evaluators.\" But according to Fleiss' kappa score interpretation             (https://en.wikipedia.org/wiki/Fleiss%27_kappa), the scores show fair to moderate agreement. It's important to explain the reasons            for the lower agreement scores.                                                                                                                                           g. Opting for human evaluation on two different datasets would be preferable.\n2. The above raised issue apply to section 4.3 also, where Fleiss' kappa scores and metric ratings are missing.\n3. As stated in Appendix B, all reported experimental scores are averages of 3 runs. However, the corresponding standard deviation scores are absent.\n4. Regarding lines 022-023, there's no evidence or empirical study provided on how translationese might negatively affect CLS model performance in real-world applications.\n\ntypos_grammar_style_and_presentation_improvements: Minor issue: 1. line183 --> check the spacing\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "49_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_49_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.750625,
      "max_similarity": 0.7814,
      "avg_coverage": 0.37735,
      "max_coverage": 0.5714
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 381,
      "avg_human_length": 534.25
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 6,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "UsE4zEGlq9",
        "similarity": 0.736,
        "coverage": 0.25,
        "human_length": 797,
        "human_text": "paper_topic_and_main_contributions: In the authors' words, the aim of the work is to confirm that different approaches of constructing cross-lingual summarization (CLS) datasets will lead to different degrees of translationese. Then they study how such translationese affects CLS model evaluation and performance when it appears in source documents or target summaries.\n\nreasons_to_accept: The impact of translationese in the domain of NLP is a critical issue. Here it is important to take into account its growing dependence on ML techniques, a reality associated with the need for training corpus in different fields of knowledge and languages. This has led to the application of TM tools to its generation and, consequently, the generalization of all kinds of phenomena related to the concept of \"translationese\". In turn, given the popularization of NLP-based tools, this could even affect language on a longer term. The recent irruption of GPT-like models delves into the importance of studying this type of problems, particularly in relation to summarization.\nIn this context, the work is interesting. Regardless of the fact that the authors' conclusions coincide with those intuitively expected, the truth is that the evidence corroborates them and seems to dispel any possible doubts.\n\nreasons_to_reject: A. I wonder if the fact of considering only m-BART-based MT models for testing does not introduce some kind of bias in the results.\nB. I also wonder if the best way to address the impact of translationese in the field of cross-lingual technologies would not be by focusing our attention on the basic MT technologies... and not on each of their possible practical applications (summatization, ...). In this regard, it should be remembered that there are already works published in this line as, for example: Eva Vanmassenhove, Dimitar Shterionov, and Matthew Gwilliam. 2021. Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2203\u20132213, Online. Association for Computational Linguistics.\nSicheng Yu, Qianru Sun, Hao Zhang, and Jing Jiang. 2022. Translate-Train Embracing Translationese Artifacts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 362\u2013370, Dublin, Ireland. Association for Computational Linguistics.\nthe latter of which is one of the references included in this work.\n\nquestions_for_the_authors: The authors' responses seem conditioned by the size limitations of the text, both in my case and in that of the rest of the reviewers.\nCertainly some of the modifications required imply a significant investment of time and work, but it is no less true that if this is the case, it is because it refers to aspects that should have been considered before writing the text. More precisely, we have that: A. Please, justify the authors' position in relation to the reviewer's item A in the previous section \"Reasons to reject\".\nRegarding the authors' responses, it is not clear whether or not some type of comparison with other operating models will be included in the reviewed paper, given that the authors limit themselves to stating that: \"We would like to add this discussion in Limitation section\" when I have no doubt that it is an issue that must be addressed in one way or another in an eventual final review, if only to warn of the possibility of biases associated with the use of a certain type of modeling, given that this would condition the entire experimental part of the work.\nB. Please, justify the authors' position in relation to the reviewer's item B in the previous section \"Reasons to reject\".\nRegarding the authors' answers at this point, indeed most existing translationese-relevant studies typically focus on MT, which would have precisely facilitated a comparison like the one we have just commented on. In any case, we are talking about a decision of the authors who have adequately justified.\n\ntypos_grammar_style_and_presentation_improvements: Is such a display of bibliographic references really necessary ? Their presence is overwhelming and makes it difficult to understand the text (see for example Lines 040 or 058). Such display does not at all favor the reading and comprehension of the text.\nPlease, avoid redirects in the text, particularly in the Introduction (see Line 063, 083, 084). Instead, include a \"road-map\" of the paper at the end of Section 1, including a brief look of all the other Sections.\nMost of the last paragraph (Contributions) of Section 1 retake the same contents included in the previous one as well in the Abstract. Is this really necessary.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "aFZZzDoQta",
        "similarity": 0.7381,
        "coverage": 0.5714,
        "human_length": 429,
        "human_text": "paper_topic_and_main_contributions: In this paper the authors explore the effect of translationese on the cross-lingual summarization (CLS). Since in the cross-lingual summarization task the data is often obtained via translation (often MT) this leads to a question of whether the translationese, likely introduced during this process, affects the task itself (CLS) The authors investigate how translationese, when present in source documents or target summaries, affects the modeling and evaluation of CLS. The author explore this problem through a series of experiments and give suggestions on how one may deal with this issue in the future. These include controlling translationese in test sets, developing translationese-aware models, and building mixed-quality CLS datasets.\nThe authors also include a partial human evaluation, though details are missing.\n\nreasons_to_accept: - The problem addressed in this study is very timely and likely to be an issue in the current systems; - The experiment choice is motivated by the RQs.\n\nreasons_to_reject: - I am not sure about the human evaluation employed, as there are no sufficient details in the paper raising ethical and methodological questions. For instance, I believe that the ratings should not be displayed as means as the scale employed is likely ordinal (or at least a distribution of scores should be added. This further raises a question about the IAA analysis, as the reported Fleiss Kappa should be employed to categorical data. In other words, the IAA is done for categorical data, the means are reported as if the data was interval/numerical, while most likely the data is ordinal, though it is impossible to say as the details are not reported (or somehow I have missed that).\n\nquestions_for_the_authors: Question A: How were the human evaluators selected? Were they compensated for this task?  Question B: What is the 3-point scale? What instructions were given to the annotators?\n\ntypos_grammar_style_and_presentation_improvements: Apart from Fleiss Kappa, it would be also good to report the percentage agreement. If the data is not categorical, Krippendorff's alpha should be employed. Preferably CI should be reported.\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: Details on human evaluation are missing, it is not stated how much the evaluators were paid or whether the study design was reviewed by IRB.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "sp4old3SET",
        "similarity": 0.747,
        "coverage": 0.3611,
        "human_length": 432,
        "human_text": "paper_topic_and_main_contributions: Paper is a comprehensive look at the impact of translationese in cross-lingual summarization.  authors motivate the work based on the shortage of cross-lingual summarization resources.  After introducing some metrics to estimate translationese, they look at their impact when present in the source text, target summaries and also their impact on summarization models.  Results indicate their presence and motivates some suggestions about containing their presence with automatic and manual efforts.\n\nreasons_to_accept: 1. Interesting topic (cross-language summarization).\n2. Fairly comprehensive look at the impact of translationese in source, target, eval and models.  The comprehensive work just requires some substantial polishing and restructuring.\n\nreasons_to_reject: 1. Paper has a poor writing and it is difficult to follow.  Moreover it is not organized well (long sentences, latex mistakes, many accronyms, long repetitions of table results in the text body). Even some basic concepts like translationese are not defined well.  There's not a single example in the entire paper.\n2. poor replicability of the work.  so much details are missing in computing various translationese metrics for different languages and cls models.\n3. The conclusion and macro level impact of the work does not seem significant.\n\nquestions_for_the_authors: 1. where are details of the wide range of linguistic analysis to compute translationese metrics (parse, pos, entropy, etc).  how do factor varying levels of quality for such analysis in different languages?\n2. section 3.3 is quite confusing:  here the direction is en2x and one would expect the summaries to be in the x languages and annotation is conducted in those 4-5 languages?  also any explanation for the fair/weak agreement for fluency and also the fact that overal agreement is higher than the individual factors?\n3. Re: Table 8, the MT data is always kept at 100% in the ablation study.  do you think using a cleaner subset of the MT data (using your translationese metrics) can make the impact of that data stronger?\n\ntypos_grammar_style_and_presentation_improvements: 1.  latex errors: line 057 (long referencing at the middle of sentence),  2. Table 9 & 10 in page 8.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "f6KqcDTVzC",
        "similarity": 0.7814,
        "coverage": 0.3269,
        "human_length": 479,
        "human_text": "paper_topic_and_main_contributions: 1. This paper explores the impact of translationese on the evaluation and performance of cross-lingual summarization models. \n2. The paper outlines the process of creating different cross-lingual summarization datasets and highlights potential disparities between automated and human evaluations. \n3. A comprehensive overview of the development of current cross-lingual summarization datasets is provided. \n4. The paper also delves into the factors within translated text that contribute to the phenomenon known as 'translationese'. \n5. Experiments were conducted to measure the influence of translationese on both documents and summaries, and the resulting analysis is presented in a clear and easily understandable manner.\n\nreasons_to_accept: 1. This paper is the first attempt to analyze the impact of translationese in cross-lingual summarization tasks. \n2. The authors provided a detailed analysis of the impact of translationese concerning source documents and Target summaries for various languages and datasets. \n3. The authors highlight findings, such as the potential promise in constructing mixed-quality or semi-supervised cross-lingual summarization datasets, which could be a valuable direction for further research. \n4. This paper emphasizes how important it is to use human-translated documents and summaries to create strong cross-lingual summarization models. \n5. The analysis process used to identify the impact of translationese allows other groups to reproduce for different language families.\n\nreasons_to_reject: 1. In section 3.3, the human evaluation part isn't clear. \n         a. Were the evaluators native speakers? What was their expertise level?\n      b. What guidelines were given for human evaluation? The specifics of the 3-point scale breakdown for metrics like Informativeness,            Fluency, and Overall are missing.                                                           c.  On line 363, what does \"overall quality\" mean? \n       d. The authors mentioned selected 100 samples from the XSAMSUM test set randomly. How many of these belonged to mBART-HT             and mBART-MT? However, Table 4 indicates separate human evaluation for these subsets.\n     e. If human evaluation was done separately, I would expect additional Fleiss' kappa scores for Informativeness, Fluency, and Overall.\n     f. On line 369, mentioned that  \"good agreement between evaluators.\" But according to Fleiss' kappa score interpretation             (https://en.wikipedia.org/wiki/Fleiss%27_kappa), the scores show fair to moderate agreement. It's important to explain the reasons            for the lower agreement scores.                                                                                                                                           g. Opting for human evaluation on two different datasets would be preferable.\n2. The above raised issue apply to section 4.3 also, where Fleiss' kappa scores and metric ratings are missing.\n3. As stated in Appendix B, all reported experimental scores are averages of 3 runs. However, the corresponding standard deviation scores are absent.\n4. Regarding lines 022-023, there's no evidence or empirical study provided on how translationese might negatively affect CLS model performance in real-world applications.\n\ntypos_grammar_style_and_presentation_improvements: Minor issue: 1. line183 --> check the spacing\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "UsE4zEGlq9",
        "length": 797,
        "human_text": "paper_topic_and_main_contributions: In the authors' words, the aim of the work is to confirm that different approaches of constructing cross-lingual summarization (CLS) datasets will lead to different degrees of translationese. Then they study how such translationese affects CLS model evaluation and performance when it appears in source documents or target summaries.\n\nreasons_to_accept: The impact of translationese in the domain of NLP is a critical issue. Here it is important to take into account its growing dependence on ML techniques, a reality associated with the need for training corpus in different fields of knowledge and languages. This has led to the application of TM tools to its generation and, consequently, the generalization of all kinds of phenomena related to the concept of \"translationese\". In turn, given the popularization of NLP-based tools, this could even affect language on a longer term. The recent irruption of GPT-like models delves into the importance of studying this type of problems, particularly in relation to summarization.\nIn this context, the work is interesting. Regardless of the fact that the authors' conclusions coincide with those intuitively expected, the truth is that the evidence corroborates them and seems to dispel any possible doubts.\n\nreasons_to_reject: A. I wonder if the fact of considering only m-BART-based MT models for testing does not introduce some kind of bias in the results.\nB. I also wonder if the best way to address the impact of translationese in the field of cross-lingual technologies would not be by focusing our attention on the basic MT technologies... and not on each of their possible practical applications (summatization, ...). In this regard, it should be remembered that there are already works published in this line as, for example: Eva Vanmassenhove, Dimitar Shterionov, and Matthew Gwilliam. 2021. Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2203\u20132213, Online. Association for Computational Linguistics.\nSicheng Yu, Qianru Sun, Hao Zhang, and Jing Jiang. 2022. Translate-Train Embracing Translationese Artifacts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 362\u2013370, Dublin, Ireland. Association for Computational Linguistics.\nthe latter of which is one of the references included in this work.\n\nquestions_for_the_authors: The authors' responses seem conditioned by the size limitations of the text, both in my case and in that of the rest of the reviewers.\nCertainly some of the modifications required imply a significant investment of time and work, but it is no less true that if this is the case, it is because it refers to aspects that should have been considered before writing the text. More precisely, we have that: A. Please, justify the authors' position in relation to the reviewer's item A in the previous section \"Reasons to reject\".\nRegarding the authors' responses, it is not clear whether or not some type of comparison with other operating models will be included in the reviewed paper, given that the authors limit themselves to stating that: \"We would like to add this discussion in Limitation section\" when I have no doubt that it is an issue that must be addressed in one way or another in an eventual final review, if only to warn of the possibility of biases associated with the use of a certain type of modeling, given that this would condition the entire experimental part of the work.\nB. Please, justify the authors' position in relation to the reviewer's item B in the previous section \"Reasons to reject\".\nRegarding the authors' answers at this point, indeed most existing translationese-relevant studies typically focus on MT, which would have precisely facilitated a comparison like the one we have just commented on. In any case, we are talking about a decision of the authors who have adequately justified.\n\ntypos_grammar_style_and_presentation_improvements: Is such a display of bibliographic references really necessary ? Their presence is overwhelming and makes it difficult to understand the text (see for example Lines 040 or 058). Such display does not at all favor the reading and comprehension of the text.\nPlease, avoid redirects in the text, particularly in the Introduction (see Line 063, 083, 084). Instead, include a \"road-map\" of the paper at the end of Section 1, including a brief look of all the other Sections.\nMost of the last paragraph (Contributions) of Section 1 retake the same contents included in the previous one as well in the Abstract. Is this really necessary.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "aFZZzDoQta",
        "length": 429,
        "human_text": "paper_topic_and_main_contributions: In this paper the authors explore the effect of translationese on the cross-lingual summarization (CLS). Since in the cross-lingual summarization task the data is often obtained via translation (often MT) this leads to a question of whether the translationese, likely introduced during this process, affects the task itself (CLS) The authors investigate how translationese, when present in source documents or target summaries, affects the modeling and evaluation of CLS. The author explore this problem through a series of experiments and give suggestions on how one may deal with this issue in the future. These include controlling translationese in test sets, developing translationese-aware models, and building mixed-quality CLS datasets.\nThe authors also include a partial human evaluation, though details are missing.\n\nreasons_to_accept: - The problem addressed in this study is very timely and likely to be an issue in the current systems; - The experiment choice is motivated by the RQs.\n\nreasons_to_reject: - I am not sure about the human evaluation employed, as there are no sufficient details in the paper raising ethical and methodological questions. For instance, I believe that the ratings should not be displayed as means as the scale employed is likely ordinal (or at least a distribution of scores should be added. This further raises a question about the IAA analysis, as the reported Fleiss Kappa should be employed to categorical data. In other words, the IAA is done for categorical data, the means are reported as if the data was interval/numerical, while most likely the data is ordinal, though it is impossible to say as the details are not reported (or somehow I have missed that).\n\nquestions_for_the_authors: Question A: How were the human evaluators selected? Were they compensated for this task?  Question B: What is the 3-point scale? What instructions were given to the annotators?\n\ntypos_grammar_style_and_presentation_improvements: Apart from Fleiss Kappa, it would be also good to report the percentage agreement. If the data is not categorical, Krippendorff's alpha should be employed. Preferably CI should be reported.\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: Details on human evaluation are missing, it is not stated how much the evaluators were paid or whether the study design was reviewed by IRB.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "sp4old3SET",
        "length": 432,
        "human_text": "paper_topic_and_main_contributions: Paper is a comprehensive look at the impact of translationese in cross-lingual summarization.  authors motivate the work based on the shortage of cross-lingual summarization resources.  After introducing some metrics to estimate translationese, they look at their impact when present in the source text, target summaries and also their impact on summarization models.  Results indicate their presence and motivates some suggestions about containing their presence with automatic and manual efforts.\n\nreasons_to_accept: 1. Interesting topic (cross-language summarization).\n2. Fairly comprehensive look at the impact of translationese in source, target, eval and models.  The comprehensive work just requires some substantial polishing and restructuring.\n\nreasons_to_reject: 1. Paper has a poor writing and it is difficult to follow.  Moreover it is not organized well (long sentences, latex mistakes, many accronyms, long repetitions of table results in the text body). Even some basic concepts like translationese are not defined well.  There's not a single example in the entire paper.\n2. poor replicability of the work.  so much details are missing in computing various translationese metrics for different languages and cls models.\n3. The conclusion and macro level impact of the work does not seem significant.\n\nquestions_for_the_authors: 1. where are details of the wide range of linguistic analysis to compute translationese metrics (parse, pos, entropy, etc).  how do factor varying levels of quality for such analysis in different languages?\n2. section 3.3 is quite confusing:  here the direction is en2x and one would expect the summaries to be in the x languages and annotation is conducted in those 4-5 languages?  also any explanation for the fair/weak agreement for fluency and also the fact that overal agreement is higher than the individual factors?\n3. Re: Table 8, the MT data is always kept at 100% in the ablation study.  do you think using a cleaner subset of the MT data (using your translationese metrics) can make the impact of that data stronger?\n\ntypos_grammar_style_and_presentation_improvements: 1.  latex errors: line 057 (long referencing at the middle of sentence),  2. Table 9 & 10 in page 8.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "f6KqcDTVzC",
        "length": 479,
        "human_text": "paper_topic_and_main_contributions: 1. This paper explores the impact of translationese on the evaluation and performance of cross-lingual summarization models. \n2. The paper outlines the process of creating different cross-lingual summarization datasets and highlights potential disparities between automated and human evaluations. \n3. A comprehensive overview of the development of current cross-lingual summarization datasets is provided. \n4. The paper also delves into the factors within translated text that contribute to the phenomenon known as 'translationese'. \n5. Experiments were conducted to measure the influence of translationese on both documents and summaries, and the resulting analysis is presented in a clear and easily understandable manner.\n\nreasons_to_accept: 1. This paper is the first attempt to analyze the impact of translationese in cross-lingual summarization tasks. \n2. The authors provided a detailed analysis of the impact of translationese concerning source documents and Target summaries for various languages and datasets. \n3. The authors highlight findings, such as the potential promise in constructing mixed-quality or semi-supervised cross-lingual summarization datasets, which could be a valuable direction for further research. \n4. This paper emphasizes how important it is to use human-translated documents and summaries to create strong cross-lingual summarization models. \n5. The analysis process used to identify the impact of translationese allows other groups to reproduce for different language families.\n\nreasons_to_reject: 1. In section 3.3, the human evaluation part isn't clear. \n         a. Were the evaluators native speakers? What was their expertise level?\n      b. What guidelines were given for human evaluation? The specifics of the 3-point scale breakdown for metrics like Informativeness,            Fluency, and Overall are missing.                                                           c.  On line 363, what does \"overall quality\" mean? \n       d. The authors mentioned selected 100 samples from the XSAMSUM test set randomly. How many of these belonged to mBART-HT             and mBART-MT? However, Table 4 indicates separate human evaluation for these subsets.\n     e. If human evaluation was done separately, I would expect additional Fleiss' kappa scores for Informativeness, Fluency, and Overall.\n     f. On line 369, mentioned that  \"good agreement between evaluators.\" But according to Fleiss' kappa score interpretation             (https://en.wikipedia.org/wiki/Fleiss%27_kappa), the scores show fair to moderate agreement. It's important to explain the reasons            for the lower agreement scores.                                                                                                                                           g. Opting for human evaluation on two different datasets would be preferable.\n2. The above raised issue apply to section 4.3 also, where Fleiss' kappa scores and metric ratings are missing.\n3. As stated in Appendix B, all reported experimental scores are averages of 3 runs. However, the corresponding standard deviation scores are absent.\n4. Regarding lines 022-023, there's no evidence or empirical study provided on how translationese might negatively affect CLS model performance in real-world applications.\n\ntypos_grammar_style_and_presentation_improvements: Minor issue: 1. line183 --> check the spacing\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "162_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_162_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7461333333333333,
      "max_similarity": 0.7543,
      "avg_coverage": 0.5046333333333334,
      "max_coverage": 0.75
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 475,
      "avg_human_length": 329.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 7,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "6Hwj61BTA4",
        "similarity": 0.7543,
        "coverage": 0.3846,
        "human_length": 303,
        "human_text": "paper_topic_and_main_contributions: This paper proposes symbolic tuning for in-context learning. \nIn symbol tuning, we replace the semantic labels of the few-shot exemplars with random irrelevant labels and remvoe the instructions in the exemplars. Then the instruction-tuned models are finetuned with the new data. \nAccording to the authors, symbolic tuning not only improves performance on few-shot classification problems, but also enhances algorithmic reasoning ability and can restore the flipped-label following ability.\n\nreasons_to_accept: 1. The symbol tuning approach is simple and effective. It seems to be complementary to instruction tuning in terms of the algorithmic reasoning ability and strict instruction-following ability (e.g. flipped labels). \n2. The extensive experiments across model scales make the results convincing. \n3. The details of the experiments are available.\n\nreasons_to_reject: 1. The authors only experiment with one class of closed-source models which are inaccessible to most researchers. The effectiveness of symbol tuning on the open-source LLMs such as Llama 1,2, is till unclear. \n2. The template does not look like EMNLP 2023 template.\n\nquestions_for_the_authors: 1) What is the performance of symbol tuning an LLM without instruction tuning? Will it still increase its ability on algorithmic reasoning and following flipped labels?   2) What is the relationship betweem instruction tuning and symbol tuning (only for classification problems)? \n3) Does symbol tuning increase algorithmic reasoning in CoT settings, like 8-shot CoT on GSM8K, which is also open-ended generation like list function tasks?\n\nmissing_references: None\n\ntypos_grammar_style_and_presentation_improvements: The font does not seem to follow EMNLP2023 template.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "7KCEi5zbr7",
        "similarity": 0.7436,
        "coverage": 0.3793,
        "human_length": 315,
        "human_text": "paper_topic_and_main_contributions: What is this paper about: This paper proposes a new method called symbol tuning to overcome the disadvantages of instruction tuning. In symbol tuning, the instructions are removed and natural language outputs are replaced with a random symbol. The LLMs are the fine-tuned on the input-masked outputs. This forces the models to reason only from examples not from the instructions. Experimental results on several NLP tasks shows the effectiveness of the proposed approach.\nMain Contributions: 1. New method symbol tuning to effectively learn from in-context examples. \n2. Proposed approach helps model to override knowledge if provided with contradictory information aka. editing knowledge in LLMs. \n3. Strong results on all benchmarks compared to IT.\n\nreasons_to_accept: Reasons to Accept: Strong motivation: The sensitiveness of prompts in the LLMs performance has been a concern in IT. This paper provided a strong motivation behind introducing symbol tuning.\nUsefulness to the NLP community: The symbol tuning approach is useful to NLP community for training LLMs using in-context examples.\nStrong experiment results: The authors performed comprehensive experiments on a broad range of NLP tasks and with a varied sizes of LLMs.\n\nreasons_to_reject: None.\n\nquestions_for_the_authors: 1. Will the authors make their code once accepted? \n2. Why does symbol tuning performs worse for smaller LMs like Flan-PaLM-8B? \n3. In Table-1 we see for the IT-LLM-62B-cont that without instructions performed better than with instructions? So, in this case even without instructions the model was able to learn from in-context examples. Does it mean we don't need symbol tuning in all the cases?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "FXVQiFCttH",
        "similarity": 0.7405,
        "coverage": 0.75,
        "human_length": 371,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors provide a novel finetuning approach for LLM via \u201csymbol tuning\u201d--finetuning LMs on input-label pairs for which the natural language labels are replaced with nonce words (\u201cfoo\u201d,\u201dbar\u201d)--and conduct experimental validation of the proposed approach for in-context learning and algorithmic reasoning tasks. The authors find that symbol tuned models demonstrate superior performance on in-context learning tasks and with underspecified prompts. They further show that symbol tuning improves performance on multiple algorithmic reasoning benchmarks (BIG-Bench, Turing Concepts). The authors also conduct tests with prompts that have flipped labels, necessitating models\u2019 overriding of prior knowledge to correctly answer prompts, showing that symbol tuning improves model performance in this domain as well.\n\nreasons_to_accept: - Novelty: the authors provide a novel approach to finetuning models, drawing from the intuition that when LMs struggle using instructions or labels to determine the task at hand, they instead resort to learning from in-context examples. This simple but effective approach provides both unique findings and a compelling explanation for model\u2019s improvement with symbol tuning.  - Robustness: the authors support their methodology with clear and logical analysis on the impact of symbol tuning in a variety of different contexts\u2013providing robust findings while also demonstrating the need for further research on the impact of symbol tuning for LLMs; this work provides promise for future work exploring more creative uses of input-label pairs during in-context learning.\n\nreasons_to_reject: Minor weaknesses/nits - Reproducibility is an issue due to the models tested being closed-source (although the process of symbol-tuning is described in detail) - Although the authors hypothesize about why symbol tuning allows models to perform better in the tested in-context learning and algorithmic reasoning tasks, the research lacks clear evidence backing these model\u2019s performance (but this may be better kept for future work)\n\ntypos_grammar_style_and_presentation_improvements: ln. 60: it \u2192 they\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "6Hwj61BTA4",
        "length": 303,
        "human_text": "paper_topic_and_main_contributions: This paper proposes symbolic tuning for in-context learning. \nIn symbol tuning, we replace the semantic labels of the few-shot exemplars with random irrelevant labels and remvoe the instructions in the exemplars. Then the instruction-tuned models are finetuned with the new data. \nAccording to the authors, symbolic tuning not only improves performance on few-shot classification problems, but also enhances algorithmic reasoning ability and can restore the flipped-label following ability.\n\nreasons_to_accept: 1. The symbol tuning approach is simple and effective. It seems to be complementary to instruction tuning in terms of the algorithmic reasoning ability and strict instruction-following ability (e.g. flipped labels). \n2. The extensive experiments across model scales make the results convincing. \n3. The details of the experiments are available.\n\nreasons_to_reject: 1. The authors only experiment with one class of closed-source models which are inaccessible to most researchers. The effectiveness of symbol tuning on the open-source LLMs such as Llama 1,2, is till unclear. \n2. The template does not look like EMNLP 2023 template.\n\nquestions_for_the_authors: 1) What is the performance of symbol tuning an LLM without instruction tuning? Will it still increase its ability on algorithmic reasoning and following flipped labels?   2) What is the relationship betweem instruction tuning and symbol tuning (only for classification problems)? \n3) Does symbol tuning increase algorithmic reasoning in CoT settings, like 8-shot CoT on GSM8K, which is also open-ended generation like list function tasks?\n\nmissing_references: None\n\ntypos_grammar_style_and_presentation_improvements: The font does not seem to follow EMNLP2023 template.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "7KCEi5zbr7",
        "length": 315,
        "human_text": "paper_topic_and_main_contributions: What is this paper about: This paper proposes a new method called symbol tuning to overcome the disadvantages of instruction tuning. In symbol tuning, the instructions are removed and natural language outputs are replaced with a random symbol. The LLMs are the fine-tuned on the input-masked outputs. This forces the models to reason only from examples not from the instructions. Experimental results on several NLP tasks shows the effectiveness of the proposed approach.\nMain Contributions: 1. New method symbol tuning to effectively learn from in-context examples. \n2. Proposed approach helps model to override knowledge if provided with contradictory information aka. editing knowledge in LLMs. \n3. Strong results on all benchmarks compared to IT.\n\nreasons_to_accept: Reasons to Accept: Strong motivation: The sensitiveness of prompts in the LLMs performance has been a concern in IT. This paper provided a strong motivation behind introducing symbol tuning.\nUsefulness to the NLP community: The symbol tuning approach is useful to NLP community for training LLMs using in-context examples.\nStrong experiment results: The authors performed comprehensive experiments on a broad range of NLP tasks and with a varied sizes of LLMs.\n\nreasons_to_reject: None.\n\nquestions_for_the_authors: 1. Will the authors make their code once accepted? \n2. Why does symbol tuning performs worse for smaller LMs like Flan-PaLM-8B? \n3. In Table-1 we see for the IT-LLM-62B-cont that without instructions performed better than with instructions? So, in this case even without instructions the model was able to learn from in-context examples. Does it mean we don't need symbol tuning in all the cases?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "FXVQiFCttH",
        "length": 371,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors provide a novel finetuning approach for LLM via \u201csymbol tuning\u201d--finetuning LMs on input-label pairs for which the natural language labels are replaced with nonce words (\u201cfoo\u201d,\u201dbar\u201d)--and conduct experimental validation of the proposed approach for in-context learning and algorithmic reasoning tasks. The authors find that symbol tuned models demonstrate superior performance on in-context learning tasks and with underspecified prompts. They further show that symbol tuning improves performance on multiple algorithmic reasoning benchmarks (BIG-Bench, Turing Concepts). The authors also conduct tests with prompts that have flipped labels, necessitating models\u2019 overriding of prior knowledge to correctly answer prompts, showing that symbol tuning improves model performance in this domain as well.\n\nreasons_to_accept: - Novelty: the authors provide a novel approach to finetuning models, drawing from the intuition that when LMs struggle using instructions or labels to determine the task at hand, they instead resort to learning from in-context examples. This simple but effective approach provides both unique findings and a compelling explanation for model\u2019s improvement with symbol tuning.  - Robustness: the authors support their methodology with clear and logical analysis on the impact of symbol tuning in a variety of different contexts\u2013providing robust findings while also demonstrating the need for further research on the impact of symbol tuning for LLMs; this work provides promise for future work exploring more creative uses of input-label pairs during in-context learning.\n\nreasons_to_reject: Minor weaknesses/nits - Reproducibility is an issue due to the models tested being closed-source (although the process of symbol-tuning is described in detail) - Although the authors hypothesize about why symbol tuning allows models to perform better in the tested in-context learning and algorithmic reasoning tasks, the research lacks clear evidence backing these model\u2019s performance (but this may be better kept for future work)\n\ntypos_grammar_style_and_presentation_improvements: ln. 60: it \u2192 they\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "162_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_162_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7485666666666667,
      "max_similarity": 0.7567,
      "avg_coverage": 0.5046333333333334,
      "max_coverage": 0.75
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 472,
      "avg_human_length": 329.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 7,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "6Hwj61BTA4",
        "similarity": 0.7567,
        "coverage": 0.3846,
        "human_length": 303,
        "human_text": "paper_topic_and_main_contributions: This paper proposes symbolic tuning for in-context learning. \nIn symbol tuning, we replace the semantic labels of the few-shot exemplars with random irrelevant labels and remvoe the instructions in the exemplars. Then the instruction-tuned models are finetuned with the new data. \nAccording to the authors, symbolic tuning not only improves performance on few-shot classification problems, but also enhances algorithmic reasoning ability and can restore the flipped-label following ability.\n\nreasons_to_accept: 1. The symbol tuning approach is simple and effective. It seems to be complementary to instruction tuning in terms of the algorithmic reasoning ability and strict instruction-following ability (e.g. flipped labels). \n2. The extensive experiments across model scales make the results convincing. \n3. The details of the experiments are available.\n\nreasons_to_reject: 1. The authors only experiment with one class of closed-source models which are inaccessible to most researchers. The effectiveness of symbol tuning on the open-source LLMs such as Llama 1,2, is till unclear. \n2. The template does not look like EMNLP 2023 template.\n\nquestions_for_the_authors: 1) What is the performance of symbol tuning an LLM without instruction tuning? Will it still increase its ability on algorithmic reasoning and following flipped labels?   2) What is the relationship betweem instruction tuning and symbol tuning (only for classification problems)? \n3) Does symbol tuning increase algorithmic reasoning in CoT settings, like 8-shot CoT on GSM8K, which is also open-ended generation like list function tasks?\n\nmissing_references: None\n\ntypos_grammar_style_and_presentation_improvements: The font does not seem to follow EMNLP2023 template.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "7KCEi5zbr7",
        "similarity": 0.7459,
        "coverage": 0.3793,
        "human_length": 315,
        "human_text": "paper_topic_and_main_contributions: What is this paper about: This paper proposes a new method called symbol tuning to overcome the disadvantages of instruction tuning. In symbol tuning, the instructions are removed and natural language outputs are replaced with a random symbol. The LLMs are the fine-tuned on the input-masked outputs. This forces the models to reason only from examples not from the instructions. Experimental results on several NLP tasks shows the effectiveness of the proposed approach.\nMain Contributions: 1. New method symbol tuning to effectively learn from in-context examples. \n2. Proposed approach helps model to override knowledge if provided with contradictory information aka. editing knowledge in LLMs. \n3. Strong results on all benchmarks compared to IT.\n\nreasons_to_accept: Reasons to Accept: Strong motivation: The sensitiveness of prompts in the LLMs performance has been a concern in IT. This paper provided a strong motivation behind introducing symbol tuning.\nUsefulness to the NLP community: The symbol tuning approach is useful to NLP community for training LLMs using in-context examples.\nStrong experiment results: The authors performed comprehensive experiments on a broad range of NLP tasks and with a varied sizes of LLMs.\n\nreasons_to_reject: None.\n\nquestions_for_the_authors: 1. Will the authors make their code once accepted? \n2. Why does symbol tuning performs worse for smaller LMs like Flan-PaLM-8B? \n3. In Table-1 we see for the IT-LLM-62B-cont that without instructions performed better than with instructions? So, in this case even without instructions the model was able to learn from in-context examples. Does it mean we don't need symbol tuning in all the cases?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "FXVQiFCttH",
        "similarity": 0.7431,
        "coverage": 0.75,
        "human_length": 371,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors provide a novel finetuning approach for LLM via \u201csymbol tuning\u201d--finetuning LMs on input-label pairs for which the natural language labels are replaced with nonce words (\u201cfoo\u201d,\u201dbar\u201d)--and conduct experimental validation of the proposed approach for in-context learning and algorithmic reasoning tasks. The authors find that symbol tuned models demonstrate superior performance on in-context learning tasks and with underspecified prompts. They further show that symbol tuning improves performance on multiple algorithmic reasoning benchmarks (BIG-Bench, Turing Concepts). The authors also conduct tests with prompts that have flipped labels, necessitating models\u2019 overriding of prior knowledge to correctly answer prompts, showing that symbol tuning improves model performance in this domain as well.\n\nreasons_to_accept: - Novelty: the authors provide a novel approach to finetuning models, drawing from the intuition that when LMs struggle using instructions or labels to determine the task at hand, they instead resort to learning from in-context examples. This simple but effective approach provides both unique findings and a compelling explanation for model\u2019s improvement with symbol tuning.  - Robustness: the authors support their methodology with clear and logical analysis on the impact of symbol tuning in a variety of different contexts\u2013providing robust findings while also demonstrating the need for further research on the impact of symbol tuning for LLMs; this work provides promise for future work exploring more creative uses of input-label pairs during in-context learning.\n\nreasons_to_reject: Minor weaknesses/nits - Reproducibility is an issue due to the models tested being closed-source (although the process of symbol-tuning is described in detail) - Although the authors hypothesize about why symbol tuning allows models to perform better in the tested in-context learning and algorithmic reasoning tasks, the research lacks clear evidence backing these model\u2019s performance (but this may be better kept for future work)\n\ntypos_grammar_style_and_presentation_improvements: ln. 60: it \u2192 they\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "6Hwj61BTA4",
        "length": 303,
        "human_text": "paper_topic_and_main_contributions: This paper proposes symbolic tuning for in-context learning. \nIn symbol tuning, we replace the semantic labels of the few-shot exemplars with random irrelevant labels and remvoe the instructions in the exemplars. Then the instruction-tuned models are finetuned with the new data. \nAccording to the authors, symbolic tuning not only improves performance on few-shot classification problems, but also enhances algorithmic reasoning ability and can restore the flipped-label following ability.\n\nreasons_to_accept: 1. The symbol tuning approach is simple and effective. It seems to be complementary to instruction tuning in terms of the algorithmic reasoning ability and strict instruction-following ability (e.g. flipped labels). \n2. The extensive experiments across model scales make the results convincing. \n3. The details of the experiments are available.\n\nreasons_to_reject: 1. The authors only experiment with one class of closed-source models which are inaccessible to most researchers. The effectiveness of symbol tuning on the open-source LLMs such as Llama 1,2, is till unclear. \n2. The template does not look like EMNLP 2023 template.\n\nquestions_for_the_authors: 1) What is the performance of symbol tuning an LLM without instruction tuning? Will it still increase its ability on algorithmic reasoning and following flipped labels?   2) What is the relationship betweem instruction tuning and symbol tuning (only for classification problems)? \n3) Does symbol tuning increase algorithmic reasoning in CoT settings, like 8-shot CoT on GSM8K, which is also open-ended generation like list function tasks?\n\nmissing_references: None\n\ntypos_grammar_style_and_presentation_improvements: The font does not seem to follow EMNLP2023 template.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "7KCEi5zbr7",
        "length": 315,
        "human_text": "paper_topic_and_main_contributions: What is this paper about: This paper proposes a new method called symbol tuning to overcome the disadvantages of instruction tuning. In symbol tuning, the instructions are removed and natural language outputs are replaced with a random symbol. The LLMs are the fine-tuned on the input-masked outputs. This forces the models to reason only from examples not from the instructions. Experimental results on several NLP tasks shows the effectiveness of the proposed approach.\nMain Contributions: 1. New method symbol tuning to effectively learn from in-context examples. \n2. Proposed approach helps model to override knowledge if provided with contradictory information aka. editing knowledge in LLMs. \n3. Strong results on all benchmarks compared to IT.\n\nreasons_to_accept: Reasons to Accept: Strong motivation: The sensitiveness of prompts in the LLMs performance has been a concern in IT. This paper provided a strong motivation behind introducing symbol tuning.\nUsefulness to the NLP community: The symbol tuning approach is useful to NLP community for training LLMs using in-context examples.\nStrong experiment results: The authors performed comprehensive experiments on a broad range of NLP tasks and with a varied sizes of LLMs.\n\nreasons_to_reject: None.\n\nquestions_for_the_authors: 1. Will the authors make their code once accepted? \n2. Why does symbol tuning performs worse for smaller LMs like Flan-PaLM-8B? \n3. In Table-1 we see for the IT-LLM-62B-cont that without instructions performed better than with instructions? So, in this case even without instructions the model was able to learn from in-context examples. Does it mean we don't need symbol tuning in all the cases?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "FXVQiFCttH",
        "length": 371,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors provide a novel finetuning approach for LLM via \u201csymbol tuning\u201d--finetuning LMs on input-label pairs for which the natural language labels are replaced with nonce words (\u201cfoo\u201d,\u201dbar\u201d)--and conduct experimental validation of the proposed approach for in-context learning and algorithmic reasoning tasks. The authors find that symbol tuned models demonstrate superior performance on in-context learning tasks and with underspecified prompts. They further show that symbol tuning improves performance on multiple algorithmic reasoning benchmarks (BIG-Bench, Turing Concepts). The authors also conduct tests with prompts that have flipped labels, necessitating models\u2019 overriding of prior knowledge to correctly answer prompts, showing that symbol tuning improves model performance in this domain as well.\n\nreasons_to_accept: - Novelty: the authors provide a novel approach to finetuning models, drawing from the intuition that when LMs struggle using instructions or labels to determine the task at hand, they instead resort to learning from in-context examples. This simple but effective approach provides both unique findings and a compelling explanation for model\u2019s improvement with symbol tuning.  - Robustness: the authors support their methodology with clear and logical analysis on the impact of symbol tuning in a variety of different contexts\u2013providing robust findings while also demonstrating the need for further research on the impact of symbol tuning for LLMs; this work provides promise for future work exploring more creative uses of input-label pairs during in-context learning.\n\nreasons_to_reject: Minor weaknesses/nits - Reproducibility is an issue due to the models tested being closed-source (although the process of symbol-tuning is described in detail) - Although the authors hypothesize about why symbol tuning allows models to perform better in the tested in-context learning and algorithmic reasoning tasks, the research lacks clear evidence backing these model\u2019s performance (but this may be better kept for future work)\n\ntypos_grammar_style_and_presentation_improvements: ln. 60: it \u2192 they\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "156_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_156_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7382333333333334,
      "max_similarity": 0.7435,
      "avg_coverage": 0.4859333333333334,
      "max_coverage": 0.6316
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 593,
      "avg_human_length": 309.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 6,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "rLgi8qsXPs",
        "similarity": 0.7378,
        "coverage": 0.4762,
        "human_length": 298,
        "human_text": "paper_topic_and_main_contributions: The paper presents a new dataset of machine translation hallucination and omission errors, including sentence-level and token-level annotations for eighteen translation directions involving nine languages. They evaluate multiple hallucination and omission detection methods on the dataset, finding that existing detection methods perform much better on high-resource language pairs. Sequence log-probability is best for sentence-level hallucination detection, and token attribution methods are best for omission detection or token-level detection.\n\nreasons_to_accept: The dataset is a useful resource for future work on hallucinations and omissions in machine translation systems. It is particularly interesting to see the differences in existing detection methods' performance between low and high resource language pairs.\n\nreasons_to_reject: The main text is longer than 8 pages.\nThe dataset only contains hallucination and coverage errors for the 600M distilled NLLB model.\nThe limitations section is quite short; it could mention other limitations, e.g. ambiguity in the definition of hallucinations and omissions, difficulties in word-level annotation (e.g. depending on different languages' morphology), focusing on pairs to/from English, etc.\n\nquestions_for_the_authors: A: How were translators recruited, particularly for the low-resource languages?\nB: How might function words and languages' morphology affect token-level hallucination/omission annotations and evaluation scores? E.g. words like \"to\" and \"the\" behave very differently across languages (e.g. sometimes as affixes to content words, or sometimes as standalone words). In many cases, it seems unclear how to annotate function words given the provided hallucination/omission guidelines. This would affect token-level detection scores.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "5vHGcemQA4",
        "similarity": 0.7435,
        "coverage": 0.35,
        "human_length": 281,
        "human_text": "paper_topic_and_main_contributions: This paper is about developing a new dataset for detecting hallucinations and omissions in machine translation. The dataset is quite general; it contains 18 low and high resourced language pair directions. The dataset is quite useful, as it provides a way for MT systems to be trained to detect such pathological translation mistakes. The paper shows how existing system which were developed for detecting such pathologies, perform on the new dataset, with interesting results.\n\nreasons_to_accept: This paper should be accepted for EMNLP as it provides a very useful dataset for the research community, which was not available before. I find the annotation guidelines clear and simple. However, I do think more annotation examples should have been provided. The existing examples in Figure 3 are probably not enough.\n\nreasons_to_reject: 1. The paper should be accepted. However, it does go beyond the 8 pages requirement (the Conclusions section crosses over to the 9th page), which is not allowed in this setting. \n2. The translations annotated in this project, were all generated by the same model. I recommend adding another MT model to make sure the data is not biased.\n\nquestions_for_the_authors: I am not sure I totally understand what is the background of the annotators. The authors do say something about it, but I think it should be discussed with more details.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "yRNyuRRslE",
        "similarity": 0.7334,
        "coverage": 0.6316,
        "human_length": 349,
        "human_text": "paper_topic_and_main_contributions: The paper presents a multilingual dataset and benchmark for the detection of hallucinations and omissions in machine translation. The dataset consists of sentence-level and token-level manual annotations for 18 translation directions providing a valuable novel and comprehensive benchmark. The paper also includes results from known detection methods and also proposes a new metric for the reliable detection of omissions.\n\nreasons_to_accept: This is a very much needed dataset that will enable systematic tests of detection methods of hallucinations in machine translations. The experiments also show that a good language coverage is necessary to draw conclusions that generalize. The manual annotation effort is important to avoid artificial settings that do not reflect the real nature of the problem.\nThe paper is very well written, clearly presented and the dataset and detection methods are clearly described and discussed. The analyses and discussion contributes valuable information to this important topic and the resources are reusable and very valuable.\n\nreasons_to_reject: There is almost no information about the annotation process and the work of the annotators. I would like to see some discussions about the difficulty of the annotation work. I can imagine that there i a lot of variation in opinions on hallucination and I would like to know how the authors dealt with discrepancies and disagreements.\n\nquestions_for_the_authors: Did you measure annotation agreement and did you consider to resolve or keep annotation variation? \nThe choice of NLLB 600: selecting the smallest model, is this because of inference costs or better chance of finding omissions and hallucinations? \nHow did you combine the 3 sampling strategies? Did you take equal amounts from each of them in the final dataset? Is it interesting to look at the impact of sampling strategy somehow?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "rLgi8qsXPs",
        "length": 298,
        "human_text": "paper_topic_and_main_contributions: The paper presents a new dataset of machine translation hallucination and omission errors, including sentence-level and token-level annotations for eighteen translation directions involving nine languages. They evaluate multiple hallucination and omission detection methods on the dataset, finding that existing detection methods perform much better on high-resource language pairs. Sequence log-probability is best for sentence-level hallucination detection, and token attribution methods are best for omission detection or token-level detection.\n\nreasons_to_accept: The dataset is a useful resource for future work on hallucinations and omissions in machine translation systems. It is particularly interesting to see the differences in existing detection methods' performance between low and high resource language pairs.\n\nreasons_to_reject: The main text is longer than 8 pages.\nThe dataset only contains hallucination and coverage errors for the 600M distilled NLLB model.\nThe limitations section is quite short; it could mention other limitations, e.g. ambiguity in the definition of hallucinations and omissions, difficulties in word-level annotation (e.g. depending on different languages' morphology), focusing on pairs to/from English, etc.\n\nquestions_for_the_authors: A: How were translators recruited, particularly for the low-resource languages?\nB: How might function words and languages' morphology affect token-level hallucination/omission annotations and evaluation scores? E.g. words like \"to\" and \"the\" behave very differently across languages (e.g. sometimes as affixes to content words, or sometimes as standalone words). In many cases, it seems unclear how to annotate function words given the provided hallucination/omission guidelines. This would affect token-level detection scores.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "5vHGcemQA4",
        "length": 281,
        "human_text": "paper_topic_and_main_contributions: This paper is about developing a new dataset for detecting hallucinations and omissions in machine translation. The dataset is quite general; it contains 18 low and high resourced language pair directions. The dataset is quite useful, as it provides a way for MT systems to be trained to detect such pathological translation mistakes. The paper shows how existing system which were developed for detecting such pathologies, perform on the new dataset, with interesting results.\n\nreasons_to_accept: This paper should be accepted for EMNLP as it provides a very useful dataset for the research community, which was not available before. I find the annotation guidelines clear and simple. However, I do think more annotation examples should have been provided. The existing examples in Figure 3 are probably not enough.\n\nreasons_to_reject: 1. The paper should be accepted. However, it does go beyond the 8 pages requirement (the Conclusions section crosses over to the 9th page), which is not allowed in this setting. \n2. The translations annotated in this project, were all generated by the same model. I recommend adding another MT model to make sure the data is not biased.\n\nquestions_for_the_authors: I am not sure I totally understand what is the background of the annotators. The authors do say something about it, but I think it should be discussed with more details.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "yRNyuRRslE",
        "length": 349,
        "human_text": "paper_topic_and_main_contributions: The paper presents a multilingual dataset and benchmark for the detection of hallucinations and omissions in machine translation. The dataset consists of sentence-level and token-level manual annotations for 18 translation directions providing a valuable novel and comprehensive benchmark. The paper also includes results from known detection methods and also proposes a new metric for the reliable detection of omissions.\n\nreasons_to_accept: This is a very much needed dataset that will enable systematic tests of detection methods of hallucinations in machine translations. The experiments also show that a good language coverage is necessary to draw conclusions that generalize. The manual annotation effort is important to avoid artificial settings that do not reflect the real nature of the problem.\nThe paper is very well written, clearly presented and the dataset and detection methods are clearly described and discussed. The analyses and discussion contributes valuable information to this important topic and the resources are reusable and very valuable.\n\nreasons_to_reject: There is almost no information about the annotation process and the work of the annotators. I would like to see some discussions about the difficulty of the annotation work. I can imagine that there i a lot of variation in opinions on hallucination and I would like to know how the authors dealt with discrepancies and disagreements.\n\nquestions_for_the_authors: Did you measure annotation agreement and did you consider to resolve or keep annotation variation? \nThe choice of NLLB 600: selecting the smallest model, is this because of inference costs or better chance of finding omissions and hallucinations? \nHow did you combine the 3 sampling strategies? Did you take equal amounts from each of them in the final dataset? Is it interesting to look at the impact of sampling strategy somehow?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "156_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_156_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7364333333333334,
      "max_similarity": 0.7416,
      "avg_coverage": 0.4859333333333334,
      "max_coverage": 0.6316
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 610,
      "avg_human_length": 309.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 6,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "rLgi8qsXPs",
        "similarity": 0.7349,
        "coverage": 0.4762,
        "human_length": 298,
        "human_text": "paper_topic_and_main_contributions: The paper presents a new dataset of machine translation hallucination and omission errors, including sentence-level and token-level annotations for eighteen translation directions involving nine languages. They evaluate multiple hallucination and omission detection methods on the dataset, finding that existing detection methods perform much better on high-resource language pairs. Sequence log-probability is best for sentence-level hallucination detection, and token attribution methods are best for omission detection or token-level detection.\n\nreasons_to_accept: The dataset is a useful resource for future work on hallucinations and omissions in machine translation systems. It is particularly interesting to see the differences in existing detection methods' performance between low and high resource language pairs.\n\nreasons_to_reject: The main text is longer than 8 pages.\nThe dataset only contains hallucination and coverage errors for the 600M distilled NLLB model.\nThe limitations section is quite short; it could mention other limitations, e.g. ambiguity in the definition of hallucinations and omissions, difficulties in word-level annotation (e.g. depending on different languages' morphology), focusing on pairs to/from English, etc.\n\nquestions_for_the_authors: A: How were translators recruited, particularly for the low-resource languages?\nB: How might function words and languages' morphology affect token-level hallucination/omission annotations and evaluation scores? E.g. words like \"to\" and \"the\" behave very differently across languages (e.g. sometimes as affixes to content words, or sometimes as standalone words). In many cases, it seems unclear how to annotate function words given the provided hallucination/omission guidelines. This would affect token-level detection scores.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "5vHGcemQA4",
        "similarity": 0.7416,
        "coverage": 0.35,
        "human_length": 281,
        "human_text": "paper_topic_and_main_contributions: This paper is about developing a new dataset for detecting hallucinations and omissions in machine translation. The dataset is quite general; it contains 18 low and high resourced language pair directions. The dataset is quite useful, as it provides a way for MT systems to be trained to detect such pathological translation mistakes. The paper shows how existing system which were developed for detecting such pathologies, perform on the new dataset, with interesting results.\n\nreasons_to_accept: This paper should be accepted for EMNLP as it provides a very useful dataset for the research community, which was not available before. I find the annotation guidelines clear and simple. However, I do think more annotation examples should have been provided. The existing examples in Figure 3 are probably not enough.\n\nreasons_to_reject: 1. The paper should be accepted. However, it does go beyond the 8 pages requirement (the Conclusions section crosses over to the 9th page), which is not allowed in this setting. \n2. The translations annotated in this project, were all generated by the same model. I recommend adding another MT model to make sure the data is not biased.\n\nquestions_for_the_authors: I am not sure I totally understand what is the background of the annotators. The authors do say something about it, but I think it should be discussed with more details.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "yRNyuRRslE",
        "similarity": 0.7328,
        "coverage": 0.6316,
        "human_length": 349,
        "human_text": "paper_topic_and_main_contributions: The paper presents a multilingual dataset and benchmark for the detection of hallucinations and omissions in machine translation. The dataset consists of sentence-level and token-level manual annotations for 18 translation directions providing a valuable novel and comprehensive benchmark. The paper also includes results from known detection methods and also proposes a new metric for the reliable detection of omissions.\n\nreasons_to_accept: This is a very much needed dataset that will enable systematic tests of detection methods of hallucinations in machine translations. The experiments also show that a good language coverage is necessary to draw conclusions that generalize. The manual annotation effort is important to avoid artificial settings that do not reflect the real nature of the problem.\nThe paper is very well written, clearly presented and the dataset and detection methods are clearly described and discussed. The analyses and discussion contributes valuable information to this important topic and the resources are reusable and very valuable.\n\nreasons_to_reject: There is almost no information about the annotation process and the work of the annotators. I would like to see some discussions about the difficulty of the annotation work. I can imagine that there i a lot of variation in opinions on hallucination and I would like to know how the authors dealt with discrepancies and disagreements.\n\nquestions_for_the_authors: Did you measure annotation agreement and did you consider to resolve or keep annotation variation? \nThe choice of NLLB 600: selecting the smallest model, is this because of inference costs or better chance of finding omissions and hallucinations? \nHow did you combine the 3 sampling strategies? Did you take equal amounts from each of them in the final dataset? Is it interesting to look at the impact of sampling strategy somehow?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "rLgi8qsXPs",
        "length": 298,
        "human_text": "paper_topic_and_main_contributions: The paper presents a new dataset of machine translation hallucination and omission errors, including sentence-level and token-level annotations for eighteen translation directions involving nine languages. They evaluate multiple hallucination and omission detection methods on the dataset, finding that existing detection methods perform much better on high-resource language pairs. Sequence log-probability is best for sentence-level hallucination detection, and token attribution methods are best for omission detection or token-level detection.\n\nreasons_to_accept: The dataset is a useful resource for future work on hallucinations and omissions in machine translation systems. It is particularly interesting to see the differences in existing detection methods' performance between low and high resource language pairs.\n\nreasons_to_reject: The main text is longer than 8 pages.\nThe dataset only contains hallucination and coverage errors for the 600M distilled NLLB model.\nThe limitations section is quite short; it could mention other limitations, e.g. ambiguity in the definition of hallucinations and omissions, difficulties in word-level annotation (e.g. depending on different languages' morphology), focusing on pairs to/from English, etc.\n\nquestions_for_the_authors: A: How were translators recruited, particularly for the low-resource languages?\nB: How might function words and languages' morphology affect token-level hallucination/omission annotations and evaluation scores? E.g. words like \"to\" and \"the\" behave very differently across languages (e.g. sometimes as affixes to content words, or sometimes as standalone words). In many cases, it seems unclear how to annotate function words given the provided hallucination/omission guidelines. This would affect token-level detection scores.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "5vHGcemQA4",
        "length": 281,
        "human_text": "paper_topic_and_main_contributions: This paper is about developing a new dataset for detecting hallucinations and omissions in machine translation. The dataset is quite general; it contains 18 low and high resourced language pair directions. The dataset is quite useful, as it provides a way for MT systems to be trained to detect such pathological translation mistakes. The paper shows how existing system which were developed for detecting such pathologies, perform on the new dataset, with interesting results.\n\nreasons_to_accept: This paper should be accepted for EMNLP as it provides a very useful dataset for the research community, which was not available before. I find the annotation guidelines clear and simple. However, I do think more annotation examples should have been provided. The existing examples in Figure 3 are probably not enough.\n\nreasons_to_reject: 1. The paper should be accepted. However, it does go beyond the 8 pages requirement (the Conclusions section crosses over to the 9th page), which is not allowed in this setting. \n2. The translations annotated in this project, were all generated by the same model. I recommend adding another MT model to make sure the data is not biased.\n\nquestions_for_the_authors: I am not sure I totally understand what is the background of the annotators. The authors do say something about it, but I think it should be discussed with more details.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "yRNyuRRslE",
        "length": 349,
        "human_text": "paper_topic_and_main_contributions: The paper presents a multilingual dataset and benchmark for the detection of hallucinations and omissions in machine translation. The dataset consists of sentence-level and token-level manual annotations for 18 translation directions providing a valuable novel and comprehensive benchmark. The paper also includes results from known detection methods and also proposes a new metric for the reliable detection of omissions.\n\nreasons_to_accept: This is a very much needed dataset that will enable systematic tests of detection methods of hallucinations in machine translations. The experiments also show that a good language coverage is necessary to draw conclusions that generalize. The manual annotation effort is important to avoid artificial settings that do not reflect the real nature of the problem.\nThe paper is very well written, clearly presented and the dataset and detection methods are clearly described and discussed. The analyses and discussion contributes valuable information to this important topic and the resources are reusable and very valuable.\n\nreasons_to_reject: There is almost no information about the annotation process and the work of the annotators. I would like to see some discussions about the difficulty of the annotation work. I can imagine that there i a lot of variation in opinions on hallucination and I would like to know how the authors dealt with discrepancies and disagreements.\n\nquestions_for_the_authors: Did you measure annotation agreement and did you consider to resolve or keep annotation variation? \nThe choice of NLLB 600: selecting the smallest model, is this because of inference costs or better chance of finding omissions and hallucinations? \nHow did you combine the 3 sampling strategies? Did you take equal amounts from each of them in the final dataset? Is it interesting to look at the impact of sampling strategy somehow?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "187_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_187_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7208333333333333,
      "max_similarity": 0.7563,
      "avg_coverage": 0.5657666666666666,
      "max_coverage": 0.6111
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 537,
      "avg_human_length": 350.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 11,
      "suggestions_count": 12
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "8zUinYzpEb",
        "similarity": 0.7124,
        "coverage": 0.6111,
        "human_length": 384,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a new method for semi-supervised entity alignment based on Teacher-Student architecture, termed MixTEA. The main contribution of this paper is that the proposed method trains alignment learning with both manually labeled mappings and probabilistic pseudo mappings to alleviate the negative influence of noisy or uncertain pseudo mappings.\n\nreasons_to_accept: This paper studies the entity alignment problem in a semi-supervised scenario which is a practical and challenging task. Existing methods mainly suffer from two problems: uncertainty of pseudo mappings and noisy pseudo-mapping learning. To alleviate the negative influence of these two problems, the author proposed a novel method that guides model learning with an end-to-end mixture teaching of manually labeled mappings and probabilistic pseudo mappings. In this part of the experiment, the author conducts extensive experiments to demonstrate the effectiveness of this method.\n-  author conducts extensive experiments to demonstrate the effectiveness of this method.\n-  The structure of the paper is reasonable and the content is sufficient.  -  The schematic diagram is concise. Readers can fully understand the method proposed in the article through the schematic diagram\n\nreasons_to_reject: - Paper proposes a bi-directional voting (BDV) strategy and a matching diversity-based rectification (MDR) module to assist the probabilistic pseudo-mapping learning but lacks the theoretical demonstration of the effectiveness of the BDV strategy and MDR model.  - I would like to see how the BDV strategy improves conventional alignment strategies.  - In addition, the effectiveness of the MDR module shown in Table 2 is not obvious, this component may be not critical.\n- Experiments do not concretely show the effectiveness of your method on alleviating the negative impact of uncertainty of pseudo mappings and noisy pseudo mapping learning.\n\nquestions_for_the_authors: See my review.\n\nmissing_references: No\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "HLZpLyWNx7",
        "similarity": 0.6938,
        "coverage": 0.5,
        "human_length": 175,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a semi-supervised EA method, which guides the model learning with an end-to-end mix-ture teaching of manually labeled mappings and probabilistic pseudo mappings.\n\nreasons_to_accept: 1. This authors introduce a bi-directional voting (BDV) strategy which utilizes the alignment decisions in different directions to estimate the uncertainty of pseudo mappings. \n2. This authors design a matching diversity-based rectification (MDR) module to adjust the pseudo mapping learning. \n3. Experimental results seem good.\n\nreasons_to_reject: 1. This paper is not well motivated 2. The proposed BDV and MDR are not clearly explained\n\nquestions_for_the_authors: 1. Why BSD works? \n2. Why MDR works? What is the meaning of Equation (12)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "npmguiJVca",
        "similarity": 0.7563,
        "coverage": 0.5862,
        "human_length": 493,
        "human_text": "paper_topic_and_main_contributions: The authors propose a new end-to-end method MixTEA for a semi-supervised entity alignment task. MixTEA has a teacher-student architecture, where both student and teacher models are knowledge graph encoders. The method is claimed to overcome two significant issues of semi-supervised EA methods: uncertainty of pseudo mapping (by switching a binary pseudo mapping to a probabilistic one and estimating the matching uncertainty via confidence score) and an increased impact of noisy pseudo mappings (by a matching diversity-based rectification).\n\nreasons_to_accept: - The paper addresses the challenging problem of using unlabeled data for entity alignment. It is well-motivated and clearly written.   - The paper proposes several novel and interesting approaches, such as BDV (bi-directional voting for more comprehensive pseudo mappings) and MDR (matching diversity-based rectification for dynamic mitigating the influence of noisy mappings).  - The authors provide an extensive and detailed ablation study and results discussion.\n\nreasons_to_reject: - Some inaccuracies in mathematical notation (e.g., line 278 - what is \"l\" in \"l-1\"?).  - In Section 4, the authors claim that the end-to-end training \"...gradually improves the quality of pseudo mapping\". It would be useful to add some example/analysis/proof that this is indeed happening and the mappings are becoming more and more accurate (e.g., the results of the model after every n-th iteration?).   - Lines 329-333: being an important part of the algorithm, this MixTEA part is not reflected in Figure 1.  - The experimental setup remains unclear to me. Was any additional unlabeled data used? How were the hyperparameters tuned? What was the search range?\n- No standard deviation for the baselines. Note that most of the baseline values are taken directly from the previous work, and some models have been trained in other settings. Therefore, a comparison of MixTEA with the baseline models should be done more carefully.  - Figure 3 is not quite clear.  - Section 6.1: the threshold values are mentioned here for the first time and are not discussed elsewhere. I guess the whole setting could be strengthened by adding more sophisticated methods for threshold tuning (e.g., a recent ACL paper [1]); apart from that, at least some discussion of the threshold values is required.  [1] Sedova and Roth. 2023. \" ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion\"\n\ntypos_grammar_style_and_presentation_improvements: - line 61, 64, 418, 545.. - unnecessary brackets in citations - line 533: \"enough\" should be removed -> \"the lack of training data\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "8zUinYzpEb",
        "length": 384,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a new method for semi-supervised entity alignment based on Teacher-Student architecture, termed MixTEA. The main contribution of this paper is that the proposed method trains alignment learning with both manually labeled mappings and probabilistic pseudo mappings to alleviate the negative influence of noisy or uncertain pseudo mappings.\n\nreasons_to_accept: This paper studies the entity alignment problem in a semi-supervised scenario which is a practical and challenging task. Existing methods mainly suffer from two problems: uncertainty of pseudo mappings and noisy pseudo-mapping learning. To alleviate the negative influence of these two problems, the author proposed a novel method that guides model learning with an end-to-end mixture teaching of manually labeled mappings and probabilistic pseudo mappings. In this part of the experiment, the author conducts extensive experiments to demonstrate the effectiveness of this method.\n-  author conducts extensive experiments to demonstrate the effectiveness of this method.\n-  The structure of the paper is reasonable and the content is sufficient.  -  The schematic diagram is concise. Readers can fully understand the method proposed in the article through the schematic diagram\n\nreasons_to_reject: - Paper proposes a bi-directional voting (BDV) strategy and a matching diversity-based rectification (MDR) module to assist the probabilistic pseudo-mapping learning but lacks the theoretical demonstration of the effectiveness of the BDV strategy and MDR model.  - I would like to see how the BDV strategy improves conventional alignment strategies.  - In addition, the effectiveness of the MDR module shown in Table 2 is not obvious, this component may be not critical.\n- Experiments do not concretely show the effectiveness of your method on alleviating the negative impact of uncertainty of pseudo mappings and noisy pseudo mapping learning.\n\nquestions_for_the_authors: See my review.\n\nmissing_references: No\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "HLZpLyWNx7",
        "length": 175,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a semi-supervised EA method, which guides the model learning with an end-to-end mix-ture teaching of manually labeled mappings and probabilistic pseudo mappings.\n\nreasons_to_accept: 1. This authors introduce a bi-directional voting (BDV) strategy which utilizes the alignment decisions in different directions to estimate the uncertainty of pseudo mappings. \n2. This authors design a matching diversity-based rectification (MDR) module to adjust the pseudo mapping learning. \n3. Experimental results seem good.\n\nreasons_to_reject: 1. This paper is not well motivated 2. The proposed BDV and MDR are not clearly explained\n\nquestions_for_the_authors: 1. Why BSD works? \n2. Why MDR works? What is the meaning of Equation (12)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "npmguiJVca",
        "length": 493,
        "human_text": "paper_topic_and_main_contributions: The authors propose a new end-to-end method MixTEA for a semi-supervised entity alignment task. MixTEA has a teacher-student architecture, where both student and teacher models are knowledge graph encoders. The method is claimed to overcome two significant issues of semi-supervised EA methods: uncertainty of pseudo mapping (by switching a binary pseudo mapping to a probabilistic one and estimating the matching uncertainty via confidence score) and an increased impact of noisy pseudo mappings (by a matching diversity-based rectification).\n\nreasons_to_accept: - The paper addresses the challenging problem of using unlabeled data for entity alignment. It is well-motivated and clearly written.   - The paper proposes several novel and interesting approaches, such as BDV (bi-directional voting for more comprehensive pseudo mappings) and MDR (matching diversity-based rectification for dynamic mitigating the influence of noisy mappings).  - The authors provide an extensive and detailed ablation study and results discussion.\n\nreasons_to_reject: - Some inaccuracies in mathematical notation (e.g., line 278 - what is \"l\" in \"l-1\"?).  - In Section 4, the authors claim that the end-to-end training \"...gradually improves the quality of pseudo mapping\". It would be useful to add some example/analysis/proof that this is indeed happening and the mappings are becoming more and more accurate (e.g., the results of the model after every n-th iteration?).   - Lines 329-333: being an important part of the algorithm, this MixTEA part is not reflected in Figure 1.  - The experimental setup remains unclear to me. Was any additional unlabeled data used? How were the hyperparameters tuned? What was the search range?\n- No standard deviation for the baselines. Note that most of the baseline values are taken directly from the previous work, and some models have been trained in other settings. Therefore, a comparison of MixTEA with the baseline models should be done more carefully.  - Figure 3 is not quite clear.  - Section 6.1: the threshold values are mentioned here for the first time and are not discussed elsewhere. I guess the whole setting could be strengthened by adding more sophisticated methods for threshold tuning (e.g., a recent ACL paper [1]); apart from that, at least some discussion of the threshold values is required.  [1] Sedova and Roth. 2023. \" ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion\"\n\ntypos_grammar_style_and_presentation_improvements: - line 61, 64, 418, 545.. - unnecessary brackets in citations - line 533: \"enough\" should be removed -> \"the lack of training data\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "187_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_187_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7207666666666667,
      "max_similarity": 0.754,
      "avg_coverage": 0.5657666666666666,
      "max_coverage": 0.6111
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 541,
      "avg_human_length": 350.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 11,
      "suggestions_count": 12
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "8zUinYzpEb",
        "similarity": 0.7126,
        "coverage": 0.6111,
        "human_length": 384,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a new method for semi-supervised entity alignment based on Teacher-Student architecture, termed MixTEA. The main contribution of this paper is that the proposed method trains alignment learning with both manually labeled mappings and probabilistic pseudo mappings to alleviate the negative influence of noisy or uncertain pseudo mappings.\n\nreasons_to_accept: This paper studies the entity alignment problem in a semi-supervised scenario which is a practical and challenging task. Existing methods mainly suffer from two problems: uncertainty of pseudo mappings and noisy pseudo-mapping learning. To alleviate the negative influence of these two problems, the author proposed a novel method that guides model learning with an end-to-end mixture teaching of manually labeled mappings and probabilistic pseudo mappings. In this part of the experiment, the author conducts extensive experiments to demonstrate the effectiveness of this method.\n-  author conducts extensive experiments to demonstrate the effectiveness of this method.\n-  The structure of the paper is reasonable and the content is sufficient.  -  The schematic diagram is concise. Readers can fully understand the method proposed in the article through the schematic diagram\n\nreasons_to_reject: - Paper proposes a bi-directional voting (BDV) strategy and a matching diversity-based rectification (MDR) module to assist the probabilistic pseudo-mapping learning but lacks the theoretical demonstration of the effectiveness of the BDV strategy and MDR model.  - I would like to see how the BDV strategy improves conventional alignment strategies.  - In addition, the effectiveness of the MDR module shown in Table 2 is not obvious, this component may be not critical.\n- Experiments do not concretely show the effectiveness of your method on alleviating the negative impact of uncertainty of pseudo mappings and noisy pseudo mapping learning.\n\nquestions_for_the_authors: See my review.\n\nmissing_references: No\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "HLZpLyWNx7",
        "similarity": 0.6957,
        "coverage": 0.5,
        "human_length": 175,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a semi-supervised EA method, which guides the model learning with an end-to-end mix-ture teaching of manually labeled mappings and probabilistic pseudo mappings.\n\nreasons_to_accept: 1. This authors introduce a bi-directional voting (BDV) strategy which utilizes the alignment decisions in different directions to estimate the uncertainty of pseudo mappings. \n2. This authors design a matching diversity-based rectification (MDR) module to adjust the pseudo mapping learning. \n3. Experimental results seem good.\n\nreasons_to_reject: 1. This paper is not well motivated 2. The proposed BDV and MDR are not clearly explained\n\nquestions_for_the_authors: 1. Why BSD works? \n2. Why MDR works? What is the meaning of Equation (12)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "npmguiJVca",
        "similarity": 0.754,
        "coverage": 0.5862,
        "human_length": 493,
        "human_text": "paper_topic_and_main_contributions: The authors propose a new end-to-end method MixTEA for a semi-supervised entity alignment task. MixTEA has a teacher-student architecture, where both student and teacher models are knowledge graph encoders. The method is claimed to overcome two significant issues of semi-supervised EA methods: uncertainty of pseudo mapping (by switching a binary pseudo mapping to a probabilistic one and estimating the matching uncertainty via confidence score) and an increased impact of noisy pseudo mappings (by a matching diversity-based rectification).\n\nreasons_to_accept: - The paper addresses the challenging problem of using unlabeled data for entity alignment. It is well-motivated and clearly written.   - The paper proposes several novel and interesting approaches, such as BDV (bi-directional voting for more comprehensive pseudo mappings) and MDR (matching diversity-based rectification for dynamic mitigating the influence of noisy mappings).  - The authors provide an extensive and detailed ablation study and results discussion.\n\nreasons_to_reject: - Some inaccuracies in mathematical notation (e.g., line 278 - what is \"l\" in \"l-1\"?).  - In Section 4, the authors claim that the end-to-end training \"...gradually improves the quality of pseudo mapping\". It would be useful to add some example/analysis/proof that this is indeed happening and the mappings are becoming more and more accurate (e.g., the results of the model after every n-th iteration?).   - Lines 329-333: being an important part of the algorithm, this MixTEA part is not reflected in Figure 1.  - The experimental setup remains unclear to me. Was any additional unlabeled data used? How were the hyperparameters tuned? What was the search range?\n- No standard deviation for the baselines. Note that most of the baseline values are taken directly from the previous work, and some models have been trained in other settings. Therefore, a comparison of MixTEA with the baseline models should be done more carefully.  - Figure 3 is not quite clear.  - Section 6.1: the threshold values are mentioned here for the first time and are not discussed elsewhere. I guess the whole setting could be strengthened by adding more sophisticated methods for threshold tuning (e.g., a recent ACL paper [1]); apart from that, at least some discussion of the threshold values is required.  [1] Sedova and Roth. 2023. \" ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion\"\n\ntypos_grammar_style_and_presentation_improvements: - line 61, 64, 418, 545.. - unnecessary brackets in citations - line 533: \"enough\" should be removed -> \"the lack of training data\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "8zUinYzpEb",
        "length": 384,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a new method for semi-supervised entity alignment based on Teacher-Student architecture, termed MixTEA. The main contribution of this paper is that the proposed method trains alignment learning with both manually labeled mappings and probabilistic pseudo mappings to alleviate the negative influence of noisy or uncertain pseudo mappings.\n\nreasons_to_accept: This paper studies the entity alignment problem in a semi-supervised scenario which is a practical and challenging task. Existing methods mainly suffer from two problems: uncertainty of pseudo mappings and noisy pseudo-mapping learning. To alleviate the negative influence of these two problems, the author proposed a novel method that guides model learning with an end-to-end mixture teaching of manually labeled mappings and probabilistic pseudo mappings. In this part of the experiment, the author conducts extensive experiments to demonstrate the effectiveness of this method.\n-  author conducts extensive experiments to demonstrate the effectiveness of this method.\n-  The structure of the paper is reasonable and the content is sufficient.  -  The schematic diagram is concise. Readers can fully understand the method proposed in the article through the schematic diagram\n\nreasons_to_reject: - Paper proposes a bi-directional voting (BDV) strategy and a matching diversity-based rectification (MDR) module to assist the probabilistic pseudo-mapping learning but lacks the theoretical demonstration of the effectiveness of the BDV strategy and MDR model.  - I would like to see how the BDV strategy improves conventional alignment strategies.  - In addition, the effectiveness of the MDR module shown in Table 2 is not obvious, this component may be not critical.\n- Experiments do not concretely show the effectiveness of your method on alleviating the negative impact of uncertainty of pseudo mappings and noisy pseudo mapping learning.\n\nquestions_for_the_authors: See my review.\n\nmissing_references: No\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "HLZpLyWNx7",
        "length": 175,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a semi-supervised EA method, which guides the model learning with an end-to-end mix-ture teaching of manually labeled mappings and probabilistic pseudo mappings.\n\nreasons_to_accept: 1. This authors introduce a bi-directional voting (BDV) strategy which utilizes the alignment decisions in different directions to estimate the uncertainty of pseudo mappings. \n2. This authors design a matching diversity-based rectification (MDR) module to adjust the pseudo mapping learning. \n3. Experimental results seem good.\n\nreasons_to_reject: 1. This paper is not well motivated 2. The proposed BDV and MDR are not clearly explained\n\nquestions_for_the_authors: 1. Why BSD works? \n2. Why MDR works? What is the meaning of Equation (12)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "npmguiJVca",
        "length": 493,
        "human_text": "paper_topic_and_main_contributions: The authors propose a new end-to-end method MixTEA for a semi-supervised entity alignment task. MixTEA has a teacher-student architecture, where both student and teacher models are knowledge graph encoders. The method is claimed to overcome two significant issues of semi-supervised EA methods: uncertainty of pseudo mapping (by switching a binary pseudo mapping to a probabilistic one and estimating the matching uncertainty via confidence score) and an increased impact of noisy pseudo mappings (by a matching diversity-based rectification).\n\nreasons_to_accept: - The paper addresses the challenging problem of using unlabeled data for entity alignment. It is well-motivated and clearly written.   - The paper proposes several novel and interesting approaches, such as BDV (bi-directional voting for more comprehensive pseudo mappings) and MDR (matching diversity-based rectification for dynamic mitigating the influence of noisy mappings).  - The authors provide an extensive and detailed ablation study and results discussion.\n\nreasons_to_reject: - Some inaccuracies in mathematical notation (e.g., line 278 - what is \"l\" in \"l-1\"?).  - In Section 4, the authors claim that the end-to-end training \"...gradually improves the quality of pseudo mapping\". It would be useful to add some example/analysis/proof that this is indeed happening and the mappings are becoming more and more accurate (e.g., the results of the model after every n-th iteration?).   - Lines 329-333: being an important part of the algorithm, this MixTEA part is not reflected in Figure 1.  - The experimental setup remains unclear to me. Was any additional unlabeled data used? How were the hyperparameters tuned? What was the search range?\n- No standard deviation for the baselines. Note that most of the baseline values are taken directly from the previous work, and some models have been trained in other settings. Therefore, a comparison of MixTEA with the baseline models should be done more carefully.  - Figure 3 is not quite clear.  - Section 6.1: the threshold values are mentioned here for the first time and are not discussed elsewhere. I guess the whole setting could be strengthened by adding more sophisticated methods for threshold tuning (e.g., a recent ACL paper [1]); apart from that, at least some discussion of the threshold values is required.  [1] Sedova and Roth. 2023. \" ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion\"\n\ntypos_grammar_style_and_presentation_improvements: - line 61, 64, 418, 545.. - unnecessary brackets in citations - line 533: \"enough\" should be removed -> \"the lack of training data\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "113_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_113_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6857333333333333,
      "max_similarity": 0.7416,
      "avg_coverage": 0.07266666666666666,
      "max_coverage": 0.1538
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 30,
      "avg_human_length": 458.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": false,
      "has_weaknesses": false,
      "has_suggestions": false
    },
    "content_count_metrics": {
      "strengths_count": 0,
      "weaknesses_count": 0,
      "suggestions_count": 0
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "mZanr9TnY2",
        "similarity": 0.626,
        "coverage": 0.1538,
        "human_length": 218,
        "human_text": "paper_topic_and_main_contributions: This paper propose to improve the SimCSE method from two perspectives, namely handling dropout noise and addressing feature corruption. They propose two strategies, namely off-dropout and dimension-wise contrastive learning, to alleviate the dropout noise and feature corruption. \nExperiment results on several benchmark datasets verify the effectiveness of this method. \nThis work also provides in-detail analysis for the proposed strategies.\n\nreasons_to_accept: 1. This work propose two important issues which have long existed in the Contrastive Learning-based representation but ignored by previous work. \n2. The proposed two strategies are simple yet effective, leading to improvement on several benchmark datasets; 3. The authors provide in-detail analysis for their methods which are insightful and inspirational;\n\nreasons_to_reject: Only experimenting on 1 million randomly sampled sentences and BERT-base/large architecture. In the context of large-language model, the scalability of the proposed method are not mentioned.\n\nquestions_for_the_authors: Can you make a comparison with the text embedding APIs provided by OpenAI?( https://platform.openai.com/docs/api-reference/embeddings)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "5jVJKq06xe",
        "similarity": 0.7416,
        "coverage": 0.0339,
        "human_length": 665,
        "human_text": "paper_topic_and_main_contributions: This paper identifies two issues with SimCSE and proposes two solutions.\nFirst, the application of dropout has not been thoroughly investigated. SimCSE only investigated the use of standard dropout as noise and only examined the influence of dropout rate. This paper further investigates the effect of dropout noise level by adding different amounts of noise to the positive and negative samples. They find that while adding some level of noise to the positive samples is beneficial, adding dropout noise to negative samples is purely harmful to performance. Therefore, they propose using off-sample for dropout noise. They avoid adding noise to the negative samples and maintain the standard dropout noise for positive samples.\nSecond, the authors argue that SimCSE has an issue with feature corruption. They describe the issue as the high similarities in dimensions of a model's representation. They propose addressing this issue by applying the recently proposed Barlow Twins method.\nBy combining these two methods, they are able to significantly increase the performance of SimCSE. The two methods are also compatible with newer SimCSE-based methods, such as DiffCSE.\n\nreasons_to_accept: I would like to extend my congratulations to the authors for successfully identifying two commonly overlooked aspects of SimCSE and for providing compelling solutions to address these issues. The authors have effectively presented the two main problems of SimCSE along with empirical evidence to support their arguments. Moreover, the suggested solutions are meticulously explained in great detail. Undoubtedly, the findings of this study will greatly benefit the research community.\n\nreasons_to_reject: Unfortunately, this paper is not well written, as there are numerous typos and grammatical errors throughout. Additionally, there is an unfinished sentence present (caption of table 1). Consequently, reading and comprehending the paper becomes quite challenging.\nMoreover, there are also numerous imprecise uses of terminology. These misuses create confusion while reading and can hinder future work.\n- [241] Eq. (2) involves 2xN _BERT structures_.  ---> BERT representations - [249] we respectively add more noise (+Noise) or reduce some noise (-Noise) from z... -> we respectively add **more** noise (+Noise) or **less** noise (-Noise) **to** z... - Is _feature corruption_ the commonly used name for the issue?  This name of the issue is not mentioned by Zbontar et al. (2021).\nThe authors also did not mention whether the source code will be released publicly.  This will make the reproduction process much more difficult.\n\nquestions_for_the_authors: Is _feature corruption_ the commonly used name for the issue?  This name of the issue is not mentioned by Zbontar et al. (2021).\nWill you release the source code upon acceptance?\n\ntypos_grammar_style_and_presentation_improvements: This paper has several grammatical mistakes.  Here are some common ones: - Typically a comma is placed after an in-sentence equation, and the following clause explaining the equation should start with the lower case.  Take equation (3) as an example: $$ \\ell = ... (3), $$ **w**here $s$... - A space should be added between a bracket and its preceding word: Just to name a few occurrences: [503] as previous studies(Gao et al., 2021; ... [510] 2021-2016(Agirre et al., 2012) - Please double check capitalisation.  The first letter of each sentence should be capitalised.\n[286] ... performance. 2) **t**he model... [264] variable $\\xi$. and thus... -> the period should be a comma.\nOne-off typos: - [266] missing . in ... Eq (2) ... - [259] extra . after footnote mark 1.\n- [083] Need a space after the bracket in \"...learning (DCL)to break\" - [265+] Missing period in the sentence in footnote 1.\nPlease proof read the paper more carefully upon acceptance.  The aforementioned typos and mistakes are by no means exhaustive.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "3VSBq7FvdX",
        "similarity": 0.6896,
        "coverage": 0.0303,
        "human_length": 493,
        "human_text": "paper_topic_and_main_contributions: This work first investigates the potential weaknesses in the common practices used in the self-supervised sentence embedding framework, SimCSE. First, they point to the dropout used in the contrastive loss. Through empirical investigation, the paper finds the noise to be useful mostly for positive pairs. Secondly, the paper addresses the \"Corruption Issue\" where the components of the representations become highly correlated. Typically, BarlowTwins objective is added to mitigate this issue. However, the paper finds an intrinsic issue (rank bottleneck) with the this method. For both of these problems, the paper proposes simple solutions that is shown to be effective in improving the SOTA. The training is done a subset of wikipedia sentences (typical in this field) and the evaluation is done on sentence similarity tasks (STS).\n\nreasons_to_accept: A1. The paper shed light on the potential issues with a common sentence embedding representation algorithm. This itself will allow future work to improve the framework or design better algorithms. Also, the investigation done by the paper provide valuable insights into the inner working of such algorithms.\nA2. Although I'm more excited about the empirical investigation part of the paper, the proposed solutions achieve improve upon the original SimCSE, moreover the these solutions seem to be general and can be applied as an auxiliary objectives on top other algorithms.\nA3. The evaluation setup includes many baselines which helps to put the results in the context.\n\nreasons_to_reject: I didn't find any major weakness with this paper. But the following could be used to improve the work. * though this is not my main area of expertise and I'm not very familiar with all the work in this line.*\nR1. The paper does not provide any runtime efficiency metrics for SimCSE++. It seems there are many reduction operation happening in the DCL objective. It's not clear if training with DCL objective is slower or not. If it's slower then one could argue that the time spend on computing DCL objective can be instead used to train on more data/iteration which can further improve the performance.  R1. One limitation of the evaluation setup could be the limited size of the training and model sizes. It's convincible that with enough compute and data the issue tackled by the paper be mitigated or not be contributing to much.\nR2. The clarity of presentation in the paper can be improved. The mathematical notation is hard to follow especially in lines 262 and 468.\n\nquestions_for_the_authors: Refer to the weaknesses section.\n\ntypos_grammar_style_and_presentation_improvements: Line 254 second $z_i^{1,+}$ -> $z_i^{2,+}$ Line 267: $\\langle z_i^1, z_i^1 \\rangle$ -> $\\langle z_i^1, z_i^2 \\rangle$\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "mZanr9TnY2",
        "length": 218,
        "human_text": "paper_topic_and_main_contributions: This paper propose to improve the SimCSE method from two perspectives, namely handling dropout noise and addressing feature corruption. They propose two strategies, namely off-dropout and dimension-wise contrastive learning, to alleviate the dropout noise and feature corruption. \nExperiment results on several benchmark datasets verify the effectiveness of this method. \nThis work also provides in-detail analysis for the proposed strategies.\n\nreasons_to_accept: 1. This work propose two important issues which have long existed in the Contrastive Learning-based representation but ignored by previous work. \n2. The proposed two strategies are simple yet effective, leading to improvement on several benchmark datasets; 3. The authors provide in-detail analysis for their methods which are insightful and inspirational;\n\nreasons_to_reject: Only experimenting on 1 million randomly sampled sentences and BERT-base/large architecture. In the context of large-language model, the scalability of the proposed method are not mentioned.\n\nquestions_for_the_authors: Can you make a comparison with the text embedding APIs provided by OpenAI?( https://platform.openai.com/docs/api-reference/embeddings)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "5jVJKq06xe",
        "length": 665,
        "human_text": "paper_topic_and_main_contributions: This paper identifies two issues with SimCSE and proposes two solutions.\nFirst, the application of dropout has not been thoroughly investigated. SimCSE only investigated the use of standard dropout as noise and only examined the influence of dropout rate. This paper further investigates the effect of dropout noise level by adding different amounts of noise to the positive and negative samples. They find that while adding some level of noise to the positive samples is beneficial, adding dropout noise to negative samples is purely harmful to performance. Therefore, they propose using off-sample for dropout noise. They avoid adding noise to the negative samples and maintain the standard dropout noise for positive samples.\nSecond, the authors argue that SimCSE has an issue with feature corruption. They describe the issue as the high similarities in dimensions of a model's representation. They propose addressing this issue by applying the recently proposed Barlow Twins method.\nBy combining these two methods, they are able to significantly increase the performance of SimCSE. The two methods are also compatible with newer SimCSE-based methods, such as DiffCSE.\n\nreasons_to_accept: I would like to extend my congratulations to the authors for successfully identifying two commonly overlooked aspects of SimCSE and for providing compelling solutions to address these issues. The authors have effectively presented the two main problems of SimCSE along with empirical evidence to support their arguments. Moreover, the suggested solutions are meticulously explained in great detail. Undoubtedly, the findings of this study will greatly benefit the research community.\n\nreasons_to_reject: Unfortunately, this paper is not well written, as there are numerous typos and grammatical errors throughout. Additionally, there is an unfinished sentence present (caption of table 1). Consequently, reading and comprehending the paper becomes quite challenging.\nMoreover, there are also numerous imprecise uses of terminology. These misuses create confusion while reading and can hinder future work.\n- [241] Eq. (2) involves 2xN _BERT structures_.  ---> BERT representations - [249] we respectively add more noise (+Noise) or reduce some noise (-Noise) from z... -> we respectively add **more** noise (+Noise) or **less** noise (-Noise) **to** z... - Is _feature corruption_ the commonly used name for the issue?  This name of the issue is not mentioned by Zbontar et al. (2021).\nThe authors also did not mention whether the source code will be released publicly.  This will make the reproduction process much more difficult.\n\nquestions_for_the_authors: Is _feature corruption_ the commonly used name for the issue?  This name of the issue is not mentioned by Zbontar et al. (2021).\nWill you release the source code upon acceptance?\n\ntypos_grammar_style_and_presentation_improvements: This paper has several grammatical mistakes.  Here are some common ones: - Typically a comma is placed after an in-sentence equation, and the following clause explaining the equation should start with the lower case.  Take equation (3) as an example: $$ \\ell = ... (3), $$ **w**here $s$... - A space should be added between a bracket and its preceding word: Just to name a few occurrences: [503] as previous studies(Gao et al., 2021; ... [510] 2021-2016(Agirre et al., 2012) - Please double check capitalisation.  The first letter of each sentence should be capitalised.\n[286] ... performance. 2) **t**he model... [264] variable $\\xi$. and thus... -> the period should be a comma.\nOne-off typos: - [266] missing . in ... Eq (2) ... - [259] extra . after footnote mark 1.\n- [083] Need a space after the bracket in \"...learning (DCL)to break\" - [265+] Missing period in the sentence in footnote 1.\nPlease proof read the paper more carefully upon acceptance.  The aforementioned typos and mistakes are by no means exhaustive.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "3VSBq7FvdX",
        "length": 493,
        "human_text": "paper_topic_and_main_contributions: This work first investigates the potential weaknesses in the common practices used in the self-supervised sentence embedding framework, SimCSE. First, they point to the dropout used in the contrastive loss. Through empirical investigation, the paper finds the noise to be useful mostly for positive pairs. Secondly, the paper addresses the \"Corruption Issue\" where the components of the representations become highly correlated. Typically, BarlowTwins objective is added to mitigate this issue. However, the paper finds an intrinsic issue (rank bottleneck) with the this method. For both of these problems, the paper proposes simple solutions that is shown to be effective in improving the SOTA. The training is done a subset of wikipedia sentences (typical in this field) and the evaluation is done on sentence similarity tasks (STS).\n\nreasons_to_accept: A1. The paper shed light on the potential issues with a common sentence embedding representation algorithm. This itself will allow future work to improve the framework or design better algorithms. Also, the investigation done by the paper provide valuable insights into the inner working of such algorithms.\nA2. Although I'm more excited about the empirical investigation part of the paper, the proposed solutions achieve improve upon the original SimCSE, moreover the these solutions seem to be general and can be applied as an auxiliary objectives on top other algorithms.\nA3. The evaluation setup includes many baselines which helps to put the results in the context.\n\nreasons_to_reject: I didn't find any major weakness with this paper. But the following could be used to improve the work. * though this is not my main area of expertise and I'm not very familiar with all the work in this line.*\nR1. The paper does not provide any runtime efficiency metrics for SimCSE++. It seems there are many reduction operation happening in the DCL objective. It's not clear if training with DCL objective is slower or not. If it's slower then one could argue that the time spend on computing DCL objective can be instead used to train on more data/iteration which can further improve the performance.  R1. One limitation of the evaluation setup could be the limited size of the training and model sizes. It's convincible that with enough compute and data the issue tackled by the paper be mitigated or not be contributing to much.\nR2. The clarity of presentation in the paper can be improved. The mathematical notation is hard to follow especially in lines 262 and 468.\n\nquestions_for_the_authors: Refer to the weaknesses section.\n\ntypos_grammar_style_and_presentation_improvements: Line 254 second $z_i^{1,+}$ -> $z_i^{2,+}$ Line 267: $\\langle z_i^1, z_i^1 \\rangle$ -> $\\langle z_i^1, z_i^2 \\rangle$\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "113_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_113_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6883333333333334,
      "max_similarity": 0.7398,
      "avg_coverage": 0.13406666666666664,
      "max_coverage": 0.3077
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 85,
      "avg_human_length": 458.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": false,
      "has_suggestions": false
    },
    "content_count_metrics": {
      "strengths_count": 1,
      "weaknesses_count": 0,
      "suggestions_count": 0
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "mZanr9TnY2",
        "similarity": 0.6554,
        "coverage": 0.3077,
        "human_length": 218,
        "human_text": "paper_topic_and_main_contributions: This paper propose to improve the SimCSE method from two perspectives, namely handling dropout noise and addressing feature corruption. They propose two strategies, namely off-dropout and dimension-wise contrastive learning, to alleviate the dropout noise and feature corruption. \nExperiment results on several benchmark datasets verify the effectiveness of this method. \nThis work also provides in-detail analysis for the proposed strategies.\n\nreasons_to_accept: 1. This work propose two important issues which have long existed in the Contrastive Learning-based representation but ignored by previous work. \n2. The proposed two strategies are simple yet effective, leading to improvement on several benchmark datasets; 3. The authors provide in-detail analysis for their methods which are insightful and inspirational;\n\nreasons_to_reject: Only experimenting on 1 million randomly sampled sentences and BERT-base/large architecture. In the context of large-language model, the scalability of the proposed method are not mentioned.\n\nquestions_for_the_authors: Can you make a comparison with the text embedding APIs provided by OpenAI?( https://platform.openai.com/docs/api-reference/embeddings)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "5jVJKq06xe",
        "similarity": 0.6698,
        "coverage": 0.0339,
        "human_length": 665,
        "human_text": "paper_topic_and_main_contributions: This paper identifies two issues with SimCSE and proposes two solutions.\nFirst, the application of dropout has not been thoroughly investigated. SimCSE only investigated the use of standard dropout as noise and only examined the influence of dropout rate. This paper further investigates the effect of dropout noise level by adding different amounts of noise to the positive and negative samples. They find that while adding some level of noise to the positive samples is beneficial, adding dropout noise to negative samples is purely harmful to performance. Therefore, they propose using off-sample for dropout noise. They avoid adding noise to the negative samples and maintain the standard dropout noise for positive samples.\nSecond, the authors argue that SimCSE has an issue with feature corruption. They describe the issue as the high similarities in dimensions of a model's representation. They propose addressing this issue by applying the recently proposed Barlow Twins method.\nBy combining these two methods, they are able to significantly increase the performance of SimCSE. The two methods are also compatible with newer SimCSE-based methods, such as DiffCSE.\n\nreasons_to_accept: I would like to extend my congratulations to the authors for successfully identifying two commonly overlooked aspects of SimCSE and for providing compelling solutions to address these issues. The authors have effectively presented the two main problems of SimCSE along with empirical evidence to support their arguments. Moreover, the suggested solutions are meticulously explained in great detail. Undoubtedly, the findings of this study will greatly benefit the research community.\n\nreasons_to_reject: Unfortunately, this paper is not well written, as there are numerous typos and grammatical errors throughout. Additionally, there is an unfinished sentence present (caption of table 1). Consequently, reading and comprehending the paper becomes quite challenging.\nMoreover, there are also numerous imprecise uses of terminology. These misuses create confusion while reading and can hinder future work.\n- [241] Eq. (2) involves 2xN _BERT structures_.  ---> BERT representations - [249] we respectively add more noise (+Noise) or reduce some noise (-Noise) from z... -> we respectively add **more** noise (+Noise) or **less** noise (-Noise) **to** z... - Is _feature corruption_ the commonly used name for the issue?  This name of the issue is not mentioned by Zbontar et al. (2021).\nThe authors also did not mention whether the source code will be released publicly.  This will make the reproduction process much more difficult.\n\nquestions_for_the_authors: Is _feature corruption_ the commonly used name for the issue?  This name of the issue is not mentioned by Zbontar et al. (2021).\nWill you release the source code upon acceptance?\n\ntypos_grammar_style_and_presentation_improvements: This paper has several grammatical mistakes.  Here are some common ones: - Typically a comma is placed after an in-sentence equation, and the following clause explaining the equation should start with the lower case.  Take equation (3) as an example: $$ \\ell = ... (3), $$ **w**here $s$... - A space should be added between a bracket and its preceding word: Just to name a few occurrences: [503] as previous studies(Gao et al., 2021; ... [510] 2021-2016(Agirre et al., 2012) - Please double check capitalisation.  The first letter of each sentence should be capitalised.\n[286] ... performance. 2) **t**he model... [264] variable $\\xi$. and thus... -> the period should be a comma.\nOne-off typos: - [266] missing . in ... Eq (2) ... - [259] extra . after footnote mark 1.\n- [083] Need a space after the bracket in \"...learning (DCL)to break\" - [265+] Missing period in the sentence in footnote 1.\nPlease proof read the paper more carefully upon acceptance.  The aforementioned typos and mistakes are by no means exhaustive.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "3VSBq7FvdX",
        "similarity": 0.7398,
        "coverage": 0.0606,
        "human_length": 493,
        "human_text": "paper_topic_and_main_contributions: This work first investigates the potential weaknesses in the common practices used in the self-supervised sentence embedding framework, SimCSE. First, they point to the dropout used in the contrastive loss. Through empirical investigation, the paper finds the noise to be useful mostly for positive pairs. Secondly, the paper addresses the \"Corruption Issue\" where the components of the representations become highly correlated. Typically, BarlowTwins objective is added to mitigate this issue. However, the paper finds an intrinsic issue (rank bottleneck) with the this method. For both of these problems, the paper proposes simple solutions that is shown to be effective in improving the SOTA. The training is done a subset of wikipedia sentences (typical in this field) and the evaluation is done on sentence similarity tasks (STS).\n\nreasons_to_accept: A1. The paper shed light on the potential issues with a common sentence embedding representation algorithm. This itself will allow future work to improve the framework or design better algorithms. Also, the investigation done by the paper provide valuable insights into the inner working of such algorithms.\nA2. Although I'm more excited about the empirical investigation part of the paper, the proposed solutions achieve improve upon the original SimCSE, moreover the these solutions seem to be general and can be applied as an auxiliary objectives on top other algorithms.\nA3. The evaluation setup includes many baselines which helps to put the results in the context.\n\nreasons_to_reject: I didn't find any major weakness with this paper. But the following could be used to improve the work. * though this is not my main area of expertise and I'm not very familiar with all the work in this line.*\nR1. The paper does not provide any runtime efficiency metrics for SimCSE++. It seems there are many reduction operation happening in the DCL objective. It's not clear if training with DCL objective is slower or not. If it's slower then one could argue that the time spend on computing DCL objective can be instead used to train on more data/iteration which can further improve the performance.  R1. One limitation of the evaluation setup could be the limited size of the training and model sizes. It's convincible that with enough compute and data the issue tackled by the paper be mitigated or not be contributing to much.\nR2. The clarity of presentation in the paper can be improved. The mathematical notation is hard to follow especially in lines 262 and 468.\n\nquestions_for_the_authors: Refer to the weaknesses section.\n\ntypos_grammar_style_and_presentation_improvements: Line 254 second $z_i^{1,+}$ -> $z_i^{2,+}$ Line 267: $\\langle z_i^1, z_i^1 \\rangle$ -> $\\langle z_i^1, z_i^2 \\rangle$\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "mZanr9TnY2",
        "length": 218,
        "human_text": "paper_topic_and_main_contributions: This paper propose to improve the SimCSE method from two perspectives, namely handling dropout noise and addressing feature corruption. They propose two strategies, namely off-dropout and dimension-wise contrastive learning, to alleviate the dropout noise and feature corruption. \nExperiment results on several benchmark datasets verify the effectiveness of this method. \nThis work also provides in-detail analysis for the proposed strategies.\n\nreasons_to_accept: 1. This work propose two important issues which have long existed in the Contrastive Learning-based representation but ignored by previous work. \n2. The proposed two strategies are simple yet effective, leading to improvement on several benchmark datasets; 3. The authors provide in-detail analysis for their methods which are insightful and inspirational;\n\nreasons_to_reject: Only experimenting on 1 million randomly sampled sentences and BERT-base/large architecture. In the context of large-language model, the scalability of the proposed method are not mentioned.\n\nquestions_for_the_authors: Can you make a comparison with the text embedding APIs provided by OpenAI?( https://platform.openai.com/docs/api-reference/embeddings)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "5jVJKq06xe",
        "length": 665,
        "human_text": "paper_topic_and_main_contributions: This paper identifies two issues with SimCSE and proposes two solutions.\nFirst, the application of dropout has not been thoroughly investigated. SimCSE only investigated the use of standard dropout as noise and only examined the influence of dropout rate. This paper further investigates the effect of dropout noise level by adding different amounts of noise to the positive and negative samples. They find that while adding some level of noise to the positive samples is beneficial, adding dropout noise to negative samples is purely harmful to performance. Therefore, they propose using off-sample for dropout noise. They avoid adding noise to the negative samples and maintain the standard dropout noise for positive samples.\nSecond, the authors argue that SimCSE has an issue with feature corruption. They describe the issue as the high similarities in dimensions of a model's representation. They propose addressing this issue by applying the recently proposed Barlow Twins method.\nBy combining these two methods, they are able to significantly increase the performance of SimCSE. The two methods are also compatible with newer SimCSE-based methods, such as DiffCSE.\n\nreasons_to_accept: I would like to extend my congratulations to the authors for successfully identifying two commonly overlooked aspects of SimCSE and for providing compelling solutions to address these issues. The authors have effectively presented the two main problems of SimCSE along with empirical evidence to support their arguments. Moreover, the suggested solutions are meticulously explained in great detail. Undoubtedly, the findings of this study will greatly benefit the research community.\n\nreasons_to_reject: Unfortunately, this paper is not well written, as there are numerous typos and grammatical errors throughout. Additionally, there is an unfinished sentence present (caption of table 1). Consequently, reading and comprehending the paper becomes quite challenging.\nMoreover, there are also numerous imprecise uses of terminology. These misuses create confusion while reading and can hinder future work.\n- [241] Eq. (2) involves 2xN _BERT structures_.  ---> BERT representations - [249] we respectively add more noise (+Noise) or reduce some noise (-Noise) from z... -> we respectively add **more** noise (+Noise) or **less** noise (-Noise) **to** z... - Is _feature corruption_ the commonly used name for the issue?  This name of the issue is not mentioned by Zbontar et al. (2021).\nThe authors also did not mention whether the source code will be released publicly.  This will make the reproduction process much more difficult.\n\nquestions_for_the_authors: Is _feature corruption_ the commonly used name for the issue?  This name of the issue is not mentioned by Zbontar et al. (2021).\nWill you release the source code upon acceptance?\n\ntypos_grammar_style_and_presentation_improvements: This paper has several grammatical mistakes.  Here are some common ones: - Typically a comma is placed after an in-sentence equation, and the following clause explaining the equation should start with the lower case.  Take equation (3) as an example: $$ \\ell = ... (3), $$ **w**here $s$... - A space should be added between a bracket and its preceding word: Just to name a few occurrences: [503] as previous studies(Gao et al., 2021; ... [510] 2021-2016(Agirre et al., 2012) - Please double check capitalisation.  The first letter of each sentence should be capitalised.\n[286] ... performance. 2) **t**he model... [264] variable $\\xi$. and thus... -> the period should be a comma.\nOne-off typos: - [266] missing . in ... Eq (2) ... - [259] extra . after footnote mark 1.\n- [083] Need a space after the bracket in \"...learning (DCL)to break\" - [265+] Missing period in the sentence in footnote 1.\nPlease proof read the paper more carefully upon acceptance.  The aforementioned typos and mistakes are by no means exhaustive.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "3VSBq7FvdX",
        "length": 493,
        "human_text": "paper_topic_and_main_contributions: This work first investigates the potential weaknesses in the common practices used in the self-supervised sentence embedding framework, SimCSE. First, they point to the dropout used in the contrastive loss. Through empirical investigation, the paper finds the noise to be useful mostly for positive pairs. Secondly, the paper addresses the \"Corruption Issue\" where the components of the representations become highly correlated. Typically, BarlowTwins objective is added to mitigate this issue. However, the paper finds an intrinsic issue (rank bottleneck) with the this method. For both of these problems, the paper proposes simple solutions that is shown to be effective in improving the SOTA. The training is done a subset of wikipedia sentences (typical in this field) and the evaluation is done on sentence similarity tasks (STS).\n\nreasons_to_accept: A1. The paper shed light on the potential issues with a common sentence embedding representation algorithm. This itself will allow future work to improve the framework or design better algorithms. Also, the investigation done by the paper provide valuable insights into the inner working of such algorithms.\nA2. Although I'm more excited about the empirical investigation part of the paper, the proposed solutions achieve improve upon the original SimCSE, moreover the these solutions seem to be general and can be applied as an auxiliary objectives on top other algorithms.\nA3. The evaluation setup includes many baselines which helps to put the results in the context.\n\nreasons_to_reject: I didn't find any major weakness with this paper. But the following could be used to improve the work. * though this is not my main area of expertise and I'm not very familiar with all the work in this line.*\nR1. The paper does not provide any runtime efficiency metrics for SimCSE++. It seems there are many reduction operation happening in the DCL objective. It's not clear if training with DCL objective is slower or not. If it's slower then one could argue that the time spend on computing DCL objective can be instead used to train on more data/iteration which can further improve the performance.  R1. One limitation of the evaluation setup could be the limited size of the training and model sizes. It's convincible that with enough compute and data the issue tackled by the paper be mitigated or not be contributing to much.\nR2. The clarity of presentation in the paper can be improved. The mathematical notation is hard to follow especially in lines 262 and 468.\n\nquestions_for_the_authors: Refer to the weaknesses section.\n\ntypos_grammar_style_and_presentation_improvements: Line 254 second $z_i^{1,+}$ -> $z_i^{2,+}$ Line 267: $\\langle z_i^1, z_i^1 \\rangle$ -> $\\langle z_i^1, z_i^2 \\rangle$\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "148_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_148_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7322333333333334,
      "max_similarity": 0.7446,
      "avg_coverage": 0.29843333333333333,
      "max_coverage": 0.3684
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 342,
      "avg_human_length": 656.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 5,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "1Sjfzf8mvP",
        "similarity": 0.7325,
        "coverage": 0.3684,
        "human_length": 346,
        "human_text": "paper_topic_and_main_contributions: In this article, authors propose to project the representation of a piece of text obtained with a deep language model into a \u201cconcept\u201d space (more precisely, computing cosine similarity between the piece of text representation and each concept representation) for improving LLM interpretability. In the experiments, these concepts are built using Wikipedia categories, for which selection depends on the required level of granularity. The method is evaluated on  7 common datasets with both human and automatic evaluations.\n\nreasons_to_accept: Simple yet effective method The approach is carefully evaluated, both with human in the loop and automatic/semi-automatic protocols. \nThe method can be adapted to many application domains, as one can tailor the concept space to the dataset at hand.\n\nreasons_to_reject: The article lacks a comparison with competitors, that are well presented on the related work section. \nThe article lacks an analysis of the impact of the chosen concepts (even on the granularity).\n\nquestions_for_the_authors: Concepts representations are not contextualized. What about ambiguous context? The method proposed in 2.3 might circumvent this issue, but it should be properly tested. \nThis approach is related with topic modeling, in the specific case of fixed topic. How this approach competes against these supervised topic modeling approaches ? \nIf there exist some correlation in the concept space (concepts are not orthogonal enough in term of semantic, e.g. in the case of colinear concept's representations) then a lot of the initial information will be lost (contrary to what is stated in 3.3). The approach seems not to lose that much on classification task, but how the chosen concepts impact the performance of the obtained representation should be carefully evaluated.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "I9abwbfJmA",
        "similarity": 0.7196,
        "coverage": 0.3571,
        "human_length": 671,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an approach for transforming the otherwise opaque dimensions of contextualized sentence embeddings into directly interpretable categories of an ontology and evaluate them in various topic classification tasks.   The proposed approach (dubbed CES) turns some concept inventory into text, transforms them into the representation space of a sentence encoder, finally constructs a transformation matrix of these vectorized concept vectors that maps the latent vectorial representations of the encoder into such a space in which the individual dimensions are directly tied to one of the concepts from the concept inventory.   The idea is simple and sound, even though the concepts seem to be converted into the representation space somewhat naively.   The representations with interpretable dimensions are evaluated in 7 different document classification tasks.   One limitation of the work is that it only works for document classification.   The quality of the transformed representations is assessed is via measuring the prediction similarity of such random forest classifiers that use either the transformed or the original representations as document features.   As the goal of the transformation was to create interpretable representations, using a more transluent classifier (other than a random forest with 100 classifiers) would be more adequate.   Could the prediction similarity be due to the large number of classifiers used in the ensemble of classifiers?   Besides the agreement rate of the classifiers relying on the original and the interpretable features, their accuracy is equally important, which results are delegated to the appendix.   It seems that the use of the transformed representations degrade classification performance noticeably.   Some amount of performance drop is fine in exchange for more interpretable representations, but this kind of tradeoff should be made explicit and clearly articulated.   The understandability of the representations from the perspective of document classification is assessed by humans as well as in an automated way.   Related to the automated evaluation of the understandability of the transformed representations, the alternative baseline approaches that assign meaning to each dimension of the original latent space are too naive, i.e., the proposed CES approach has access to C*, the concept inventory tailored for the given task, whereas the alternative approach uses either a vocabulary of the most frequent words or a concept inventory that does not enjoy the benefit of being adjusted to the domain in question.   This way, it remains unknown to what extent CES performs better due to the more useful vocabulary or the actual approach itself.   When performing the automated undestandability assessment, based on the figures in Table 4, only 20 documents per dataset was used.   Since the automated assessment is not constrained by human labor, this kind of comparison might have been performed over more documents.\n\nreasons_to_accept: Developing interpretable (document) representations is important topic, for which a simple approach is provided.\n\nreasons_to_reject: The proposed approach only works for document classification, the applied baseline is too simple and the comparison with other existing approaches is lacking.   The classification performance drops quite noticeably compared to the use of the original (yet uninterpretable) representations.\n\ntypos_grammar_style_and_presentation_improvements: The statement in the related work section that sparsification only works for static embeddings is not true, see e.g. [1,2,3].   [3], being an extension of (Senel et al., 2018) for the contextual case could serve as a stronger baseline as it is also capable of assigning human interpretable labels to the dimensions of transformed latent representations.\n[1] Berend, G\u00e1bor. [\" Sparsity makes sense: Word sense disambiguation using sparse contextualized word representations.\"](https://aclanthology.org/2020.emnlp-main.683/)   [2] Yun, Zeyu, et al. [\"Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors.\"](https://aclanthology.org/2021.deelio-1.1/)   [3] Ficsor, Tam\u00e1s, and G\u00e1bor Berend. [\" Changing the Basis of Contextual Representations with Explicit Semantics.\" ]( https://aclanthology.org/2021.acl-srw.25/)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "T0h4ObI82F",
        "similarity": 0.7446,
        "coverage": 0.1698,
        "human_length": 952,
        "human_text": "paper_topic_and_main_contributions: The authors propose to make embedding space more interpretable with respect to the model that produced them by conceptualizing the embedding space. The concept(ual) space is supposed to be more comprehensible for humans. \nThey motivate their work by the need for debugging embedding models (in this case LLMs) and for detecting biases, as well as for explaining decision made by systems incorporating LMs. \nThey propose an algorithm for conceptualization as well as a new evaluation technique to test the conceptualization. \nAs concept database, they use a Wikipedia graph. For labeling edges between concepts, they calculate sibling scores based on the similarity of the sibling concepts to detect \"is-a\" relations. \nSince concepts often are hierarchical and not all down-stream tasks might need all concepts, the conceptual space can have different granularity, depending on how \"deep\" a concept can branch out (or how many children concepts are available) and what is needed. \nThe authors accomplish this by adapting the concept space iteratively to the input text.\nFor evaluating their method, they use both human and LLM judgements.\nThe authors' main contribution is a new variant of how to make embedding spaces more interpretable, together with an interesting evaluation.\n\nreasons_to_accept: - The work is well motivated and set into context with related approaches.\n- It is definitely relevant for research in embedding spaces (wrt biases, model decisions, etc.)\n- I like the idea of having different granularity for the concept spaces.\n- It seems that the proposed method works well and it is certainly an interesting take on embedding space interpretation, but I am not 100% certain I understood everything (see questions).\n\nreasons_to_reject: - The method needs some clarification -- see questions.\n- The evaluation is interesting, but at least the results using a classifier trained on the concept space are not super convincing, since the agreement scores are not very high. When comparing humans vs. LLM ratings, the test set is very small.\n\nquestions_for_the_authors: General: - Why do we need the conceptual space in the first place? Is it not \"enough\" or maybe even equivalent to having close (i.e. highly similar) concepts in L, which describe the text vector t in L?\nAbstract: - The first sentence in the abstract is confusing to me: One of the \"main methods\" for interpreting a text is to embed it? Not really, is it? It's one of the main methods to process natural language using computers etc., but for interpreting a text, humans do not map it to vectors -- probably to concepts, though!?\nSection 1: - The embeddings of small LMs like skip-gram or GloVe were also not interpretable -- did you consider these as well? ( referring to ll. 36) - ll. 49: by \"understanding\" the embedding space, we might get some insights on those layers, but not into the rest of the model.\n2.2: - l. 181: |C\u00b9| = 37: if I understand correctly, this means that in the first level of the hierarchy, there are only 37 concepts in total? Or am I missing something -- it seems a rather small number for top-level concepts in Wikipedia to me.\n- I am not 100% certain I understand the \"selective refinement\". What does \"the concept with the largest weight\" mean exactly?\n- How is \"removeP\" determined? Is it simply a parameter you set at the beginning?\n2.1: - Can you please provide a URL to the \"Wikipedia category directed graph\" 3.:\n- For the function tau, you use the string representation of the concept, correct? Might this induce some confusion for the embedding model, since there is no context for these concepts?\n3.1: - The 10 sentences you use as context, are they preceding the sentence under consideration? How are they connected to the sentence except that they are from the same article?\n3.2: - The Kappa coefficient is \"relatively high\" on some of the datasets, but also quite low on others, e.g., 20 news group, Ohsumed. Also, the difference in accuracy on 20 news group is pretty large, especially when compared to the other datasets. Do you have an explanation for that?\n- Maybe I oversaw it -- please mention that all datasets are in English.\n3.3.2: - Please add to the table captions that the scores represent \"raw agreement\".\n4.2: - The plots in Figure 2 are really interesting! I just wonder what they actually mean -- the concept of \"weapons\" with input \"government\" in GPT-2 gets stronger with higher layers -- why? What was the context for \"government\"?\n\nmissing_references: - a citation for DMA?\nThese references are not necessarily missing, but maybe they could give some further ideas since they are based on similar idea(s), i.e., \"conceptualization of embeddings\": - Koc et al., 2018: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure - Sommerauer & Fokkens, 2018: Firearms and Tigers are Dangerous, Kitchen Knives and Zebras are Not: Testing whether Word Embeddings Can Tell - Molino et al., 2019: Parallax: Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae\n\ntypos_grammar_style_and_presentation_improvements: The paper is well structured and written.\nTypos: - l. 76: \"that latent dimensionS correspond ..\" / \"that A latent dimension corresponds ..\" Style: - The conceptual space C and the set of concepts C are hard to differentiate, maybe use CS for conceptual space?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "1Sjfzf8mvP",
        "length": 346,
        "human_text": "paper_topic_and_main_contributions: In this article, authors propose to project the representation of a piece of text obtained with a deep language model into a \u201cconcept\u201d space (more precisely, computing cosine similarity between the piece of text representation and each concept representation) for improving LLM interpretability. In the experiments, these concepts are built using Wikipedia categories, for which selection depends on the required level of granularity. The method is evaluated on  7 common datasets with both human and automatic evaluations.\n\nreasons_to_accept: Simple yet effective method The approach is carefully evaluated, both with human in the loop and automatic/semi-automatic protocols. \nThe method can be adapted to many application domains, as one can tailor the concept space to the dataset at hand.\n\nreasons_to_reject: The article lacks a comparison with competitors, that are well presented on the related work section. \nThe article lacks an analysis of the impact of the chosen concepts (even on the granularity).\n\nquestions_for_the_authors: Concepts representations are not contextualized. What about ambiguous context? The method proposed in 2.3 might circumvent this issue, but it should be properly tested. \nThis approach is related with topic modeling, in the specific case of fixed topic. How this approach competes against these supervised topic modeling approaches ? \nIf there exist some correlation in the concept space (concepts are not orthogonal enough in term of semantic, e.g. in the case of colinear concept's representations) then a lot of the initial information will be lost (contrary to what is stated in 3.3). The approach seems not to lose that much on classification task, but how the chosen concepts impact the performance of the obtained representation should be carefully evaluated.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "I9abwbfJmA",
        "length": 671,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an approach for transforming the otherwise opaque dimensions of contextualized sentence embeddings into directly interpretable categories of an ontology and evaluate them in various topic classification tasks.   The proposed approach (dubbed CES) turns some concept inventory into text, transforms them into the representation space of a sentence encoder, finally constructs a transformation matrix of these vectorized concept vectors that maps the latent vectorial representations of the encoder into such a space in which the individual dimensions are directly tied to one of the concepts from the concept inventory.   The idea is simple and sound, even though the concepts seem to be converted into the representation space somewhat naively.   The representations with interpretable dimensions are evaluated in 7 different document classification tasks.   One limitation of the work is that it only works for document classification.   The quality of the transformed representations is assessed is via measuring the prediction similarity of such random forest classifiers that use either the transformed or the original representations as document features.   As the goal of the transformation was to create interpretable representations, using a more transluent classifier (other than a random forest with 100 classifiers) would be more adequate.   Could the prediction similarity be due to the large number of classifiers used in the ensemble of classifiers?   Besides the agreement rate of the classifiers relying on the original and the interpretable features, their accuracy is equally important, which results are delegated to the appendix.   It seems that the use of the transformed representations degrade classification performance noticeably.   Some amount of performance drop is fine in exchange for more interpretable representations, but this kind of tradeoff should be made explicit and clearly articulated.   The understandability of the representations from the perspective of document classification is assessed by humans as well as in an automated way.   Related to the automated evaluation of the understandability of the transformed representations, the alternative baseline approaches that assign meaning to each dimension of the original latent space are too naive, i.e., the proposed CES approach has access to C*, the concept inventory tailored for the given task, whereas the alternative approach uses either a vocabulary of the most frequent words or a concept inventory that does not enjoy the benefit of being adjusted to the domain in question.   This way, it remains unknown to what extent CES performs better due to the more useful vocabulary or the actual approach itself.   When performing the automated undestandability assessment, based on the figures in Table 4, only 20 documents per dataset was used.   Since the automated assessment is not constrained by human labor, this kind of comparison might have been performed over more documents.\n\nreasons_to_accept: Developing interpretable (document) representations is important topic, for which a simple approach is provided.\n\nreasons_to_reject: The proposed approach only works for document classification, the applied baseline is too simple and the comparison with other existing approaches is lacking.   The classification performance drops quite noticeably compared to the use of the original (yet uninterpretable) representations.\n\ntypos_grammar_style_and_presentation_improvements: The statement in the related work section that sparsification only works for static embeddings is not true, see e.g. [1,2,3].   [3], being an extension of (Senel et al., 2018) for the contextual case could serve as a stronger baseline as it is also capable of assigning human interpretable labels to the dimensions of transformed latent representations.\n[1] Berend, G\u00e1bor. [\" Sparsity makes sense: Word sense disambiguation using sparse contextualized word representations.\"](https://aclanthology.org/2020.emnlp-main.683/)   [2] Yun, Zeyu, et al. [\"Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors.\"](https://aclanthology.org/2021.deelio-1.1/)   [3] Ficsor, Tam\u00e1s, and G\u00e1bor Berend. [\" Changing the Basis of Contextual Representations with Explicit Semantics.\" ]( https://aclanthology.org/2021.acl-srw.25/)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "T0h4ObI82F",
        "length": 952,
        "human_text": "paper_topic_and_main_contributions: The authors propose to make embedding space more interpretable with respect to the model that produced them by conceptualizing the embedding space. The concept(ual) space is supposed to be more comprehensible for humans. \nThey motivate their work by the need for debugging embedding models (in this case LLMs) and for detecting biases, as well as for explaining decision made by systems incorporating LMs. \nThey propose an algorithm for conceptualization as well as a new evaluation technique to test the conceptualization. \nAs concept database, they use a Wikipedia graph. For labeling edges between concepts, they calculate sibling scores based on the similarity of the sibling concepts to detect \"is-a\" relations. \nSince concepts often are hierarchical and not all down-stream tasks might need all concepts, the conceptual space can have different granularity, depending on how \"deep\" a concept can branch out (or how many children concepts are available) and what is needed. \nThe authors accomplish this by adapting the concept space iteratively to the input text.\nFor evaluating their method, they use both human and LLM judgements.\nThe authors' main contribution is a new variant of how to make embedding spaces more interpretable, together with an interesting evaluation.\n\nreasons_to_accept: - The work is well motivated and set into context with related approaches.\n- It is definitely relevant for research in embedding spaces (wrt biases, model decisions, etc.)\n- I like the idea of having different granularity for the concept spaces.\n- It seems that the proposed method works well and it is certainly an interesting take on embedding space interpretation, but I am not 100% certain I understood everything (see questions).\n\nreasons_to_reject: - The method needs some clarification -- see questions.\n- The evaluation is interesting, but at least the results using a classifier trained on the concept space are not super convincing, since the agreement scores are not very high. When comparing humans vs. LLM ratings, the test set is very small.\n\nquestions_for_the_authors: General: - Why do we need the conceptual space in the first place? Is it not \"enough\" or maybe even equivalent to having close (i.e. highly similar) concepts in L, which describe the text vector t in L?\nAbstract: - The first sentence in the abstract is confusing to me: One of the \"main methods\" for interpreting a text is to embed it? Not really, is it? It's one of the main methods to process natural language using computers etc., but for interpreting a text, humans do not map it to vectors -- probably to concepts, though!?\nSection 1: - The embeddings of small LMs like skip-gram or GloVe were also not interpretable -- did you consider these as well? ( referring to ll. 36) - ll. 49: by \"understanding\" the embedding space, we might get some insights on those layers, but not into the rest of the model.\n2.2: - l. 181: |C\u00b9| = 37: if I understand correctly, this means that in the first level of the hierarchy, there are only 37 concepts in total? Or am I missing something -- it seems a rather small number for top-level concepts in Wikipedia to me.\n- I am not 100% certain I understand the \"selective refinement\". What does \"the concept with the largest weight\" mean exactly?\n- How is \"removeP\" determined? Is it simply a parameter you set at the beginning?\n2.1: - Can you please provide a URL to the \"Wikipedia category directed graph\" 3.:\n- For the function tau, you use the string representation of the concept, correct? Might this induce some confusion for the embedding model, since there is no context for these concepts?\n3.1: - The 10 sentences you use as context, are they preceding the sentence under consideration? How are they connected to the sentence except that they are from the same article?\n3.2: - The Kappa coefficient is \"relatively high\" on some of the datasets, but also quite low on others, e.g., 20 news group, Ohsumed. Also, the difference in accuracy on 20 news group is pretty large, especially when compared to the other datasets. Do you have an explanation for that?\n- Maybe I oversaw it -- please mention that all datasets are in English.\n3.3.2: - Please add to the table captions that the scores represent \"raw agreement\".\n4.2: - The plots in Figure 2 are really interesting! I just wonder what they actually mean -- the concept of \"weapons\" with input \"government\" in GPT-2 gets stronger with higher layers -- why? What was the context for \"government\"?\n\nmissing_references: - a citation for DMA?\nThese references are not necessarily missing, but maybe they could give some further ideas since they are based on similar idea(s), i.e., \"conceptualization of embeddings\": - Koc et al., 2018: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure - Sommerauer & Fokkens, 2018: Firearms and Tigers are Dangerous, Kitchen Knives and Zebras are Not: Testing whether Word Embeddings Can Tell - Molino et al., 2019: Parallax: Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae\n\ntypos_grammar_style_and_presentation_improvements: The paper is well structured and written.\nTypos: - l. 76: \"that latent dimensionS correspond ..\" / \"that A latent dimension corresponds ..\" Style: - The conceptual space C and the set of concepts C are hard to differentiate, maybe use CS for conceptual space?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "148_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_148_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7308333333333333,
      "max_similarity": 0.7427,
      "avg_coverage": 0.29843333333333333,
      "max_coverage": 0.3684
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 344,
      "avg_human_length": 656.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 5,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "1Sjfzf8mvP",
        "similarity": 0.7312,
        "coverage": 0.3684,
        "human_length": 346,
        "human_text": "paper_topic_and_main_contributions: In this article, authors propose to project the representation of a piece of text obtained with a deep language model into a \u201cconcept\u201d space (more precisely, computing cosine similarity between the piece of text representation and each concept representation) for improving LLM interpretability. In the experiments, these concepts are built using Wikipedia categories, for which selection depends on the required level of granularity. The method is evaluated on  7 common datasets with both human and automatic evaluations.\n\nreasons_to_accept: Simple yet effective method The approach is carefully evaluated, both with human in the loop and automatic/semi-automatic protocols. \nThe method can be adapted to many application domains, as one can tailor the concept space to the dataset at hand.\n\nreasons_to_reject: The article lacks a comparison with competitors, that are well presented on the related work section. \nThe article lacks an analysis of the impact of the chosen concepts (even on the granularity).\n\nquestions_for_the_authors: Concepts representations are not contextualized. What about ambiguous context? The method proposed in 2.3 might circumvent this issue, but it should be properly tested. \nThis approach is related with topic modeling, in the specific case of fixed topic. How this approach competes against these supervised topic modeling approaches ? \nIf there exist some correlation in the concept space (concepts are not orthogonal enough in term of semantic, e.g. in the case of colinear concept's representations) then a lot of the initial information will be lost (contrary to what is stated in 3.3). The approach seems not to lose that much on classification task, but how the chosen concepts impact the performance of the obtained representation should be carefully evaluated.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "I9abwbfJmA",
        "similarity": 0.7186,
        "coverage": 0.3571,
        "human_length": 671,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an approach for transforming the otherwise opaque dimensions of contextualized sentence embeddings into directly interpretable categories of an ontology and evaluate them in various topic classification tasks.   The proposed approach (dubbed CES) turns some concept inventory into text, transforms them into the representation space of a sentence encoder, finally constructs a transformation matrix of these vectorized concept vectors that maps the latent vectorial representations of the encoder into such a space in which the individual dimensions are directly tied to one of the concepts from the concept inventory.   The idea is simple and sound, even though the concepts seem to be converted into the representation space somewhat naively.   The representations with interpretable dimensions are evaluated in 7 different document classification tasks.   One limitation of the work is that it only works for document classification.   The quality of the transformed representations is assessed is via measuring the prediction similarity of such random forest classifiers that use either the transformed or the original representations as document features.   As the goal of the transformation was to create interpretable representations, using a more transluent classifier (other than a random forest with 100 classifiers) would be more adequate.   Could the prediction similarity be due to the large number of classifiers used in the ensemble of classifiers?   Besides the agreement rate of the classifiers relying on the original and the interpretable features, their accuracy is equally important, which results are delegated to the appendix.   It seems that the use of the transformed representations degrade classification performance noticeably.   Some amount of performance drop is fine in exchange for more interpretable representations, but this kind of tradeoff should be made explicit and clearly articulated.   The understandability of the representations from the perspective of document classification is assessed by humans as well as in an automated way.   Related to the automated evaluation of the understandability of the transformed representations, the alternative baseline approaches that assign meaning to each dimension of the original latent space are too naive, i.e., the proposed CES approach has access to C*, the concept inventory tailored for the given task, whereas the alternative approach uses either a vocabulary of the most frequent words or a concept inventory that does not enjoy the benefit of being adjusted to the domain in question.   This way, it remains unknown to what extent CES performs better due to the more useful vocabulary or the actual approach itself.   When performing the automated undestandability assessment, based on the figures in Table 4, only 20 documents per dataset was used.   Since the automated assessment is not constrained by human labor, this kind of comparison might have been performed over more documents.\n\nreasons_to_accept: Developing interpretable (document) representations is important topic, for which a simple approach is provided.\n\nreasons_to_reject: The proposed approach only works for document classification, the applied baseline is too simple and the comparison with other existing approaches is lacking.   The classification performance drops quite noticeably compared to the use of the original (yet uninterpretable) representations.\n\ntypos_grammar_style_and_presentation_improvements: The statement in the related work section that sparsification only works for static embeddings is not true, see e.g. [1,2,3].   [3], being an extension of (Senel et al., 2018) for the contextual case could serve as a stronger baseline as it is also capable of assigning human interpretable labels to the dimensions of transformed latent representations.\n[1] Berend, G\u00e1bor. [\" Sparsity makes sense: Word sense disambiguation using sparse contextualized word representations.\"](https://aclanthology.org/2020.emnlp-main.683/)   [2] Yun, Zeyu, et al. [\"Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors.\"](https://aclanthology.org/2021.deelio-1.1/)   [3] Ficsor, Tam\u00e1s, and G\u00e1bor Berend. [\" Changing the Basis of Contextual Representations with Explicit Semantics.\" ]( https://aclanthology.org/2021.acl-srw.25/)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "T0h4ObI82F",
        "similarity": 0.7427,
        "coverage": 0.1698,
        "human_length": 952,
        "human_text": "paper_topic_and_main_contributions: The authors propose to make embedding space more interpretable with respect to the model that produced them by conceptualizing the embedding space. The concept(ual) space is supposed to be more comprehensible for humans. \nThey motivate their work by the need for debugging embedding models (in this case LLMs) and for detecting biases, as well as for explaining decision made by systems incorporating LMs. \nThey propose an algorithm for conceptualization as well as a new evaluation technique to test the conceptualization. \nAs concept database, they use a Wikipedia graph. For labeling edges between concepts, they calculate sibling scores based on the similarity of the sibling concepts to detect \"is-a\" relations. \nSince concepts often are hierarchical and not all down-stream tasks might need all concepts, the conceptual space can have different granularity, depending on how \"deep\" a concept can branch out (or how many children concepts are available) and what is needed. \nThe authors accomplish this by adapting the concept space iteratively to the input text.\nFor evaluating their method, they use both human and LLM judgements.\nThe authors' main contribution is a new variant of how to make embedding spaces more interpretable, together with an interesting evaluation.\n\nreasons_to_accept: - The work is well motivated and set into context with related approaches.\n- It is definitely relevant for research in embedding spaces (wrt biases, model decisions, etc.)\n- I like the idea of having different granularity for the concept spaces.\n- It seems that the proposed method works well and it is certainly an interesting take on embedding space interpretation, but I am not 100% certain I understood everything (see questions).\n\nreasons_to_reject: - The method needs some clarification -- see questions.\n- The evaluation is interesting, but at least the results using a classifier trained on the concept space are not super convincing, since the agreement scores are not very high. When comparing humans vs. LLM ratings, the test set is very small.\n\nquestions_for_the_authors: General: - Why do we need the conceptual space in the first place? Is it not \"enough\" or maybe even equivalent to having close (i.e. highly similar) concepts in L, which describe the text vector t in L?\nAbstract: - The first sentence in the abstract is confusing to me: One of the \"main methods\" for interpreting a text is to embed it? Not really, is it? It's one of the main methods to process natural language using computers etc., but for interpreting a text, humans do not map it to vectors -- probably to concepts, though!?\nSection 1: - The embeddings of small LMs like skip-gram or GloVe were also not interpretable -- did you consider these as well? ( referring to ll. 36) - ll. 49: by \"understanding\" the embedding space, we might get some insights on those layers, but not into the rest of the model.\n2.2: - l. 181: |C\u00b9| = 37: if I understand correctly, this means that in the first level of the hierarchy, there are only 37 concepts in total? Or am I missing something -- it seems a rather small number for top-level concepts in Wikipedia to me.\n- I am not 100% certain I understand the \"selective refinement\". What does \"the concept with the largest weight\" mean exactly?\n- How is \"removeP\" determined? Is it simply a parameter you set at the beginning?\n2.1: - Can you please provide a URL to the \"Wikipedia category directed graph\" 3.:\n- For the function tau, you use the string representation of the concept, correct? Might this induce some confusion for the embedding model, since there is no context for these concepts?\n3.1: - The 10 sentences you use as context, are they preceding the sentence under consideration? How are they connected to the sentence except that they are from the same article?\n3.2: - The Kappa coefficient is \"relatively high\" on some of the datasets, but also quite low on others, e.g., 20 news group, Ohsumed. Also, the difference in accuracy on 20 news group is pretty large, especially when compared to the other datasets. Do you have an explanation for that?\n- Maybe I oversaw it -- please mention that all datasets are in English.\n3.3.2: - Please add to the table captions that the scores represent \"raw agreement\".\n4.2: - The plots in Figure 2 are really interesting! I just wonder what they actually mean -- the concept of \"weapons\" with input \"government\" in GPT-2 gets stronger with higher layers -- why? What was the context for \"government\"?\n\nmissing_references: - a citation for DMA?\nThese references are not necessarily missing, but maybe they could give some further ideas since they are based on similar idea(s), i.e., \"conceptualization of embeddings\": - Koc et al., 2018: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure - Sommerauer & Fokkens, 2018: Firearms and Tigers are Dangerous, Kitchen Knives and Zebras are Not: Testing whether Word Embeddings Can Tell - Molino et al., 2019: Parallax: Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae\n\ntypos_grammar_style_and_presentation_improvements: The paper is well structured and written.\nTypos: - l. 76: \"that latent dimensionS correspond ..\" / \"that A latent dimension corresponds ..\" Style: - The conceptual space C and the set of concepts C are hard to differentiate, maybe use CS for conceptual space?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "1Sjfzf8mvP",
        "length": 346,
        "human_text": "paper_topic_and_main_contributions: In this article, authors propose to project the representation of a piece of text obtained with a deep language model into a \u201cconcept\u201d space (more precisely, computing cosine similarity between the piece of text representation and each concept representation) for improving LLM interpretability. In the experiments, these concepts are built using Wikipedia categories, for which selection depends on the required level of granularity. The method is evaluated on  7 common datasets with both human and automatic evaluations.\n\nreasons_to_accept: Simple yet effective method The approach is carefully evaluated, both with human in the loop and automatic/semi-automatic protocols. \nThe method can be adapted to many application domains, as one can tailor the concept space to the dataset at hand.\n\nreasons_to_reject: The article lacks a comparison with competitors, that are well presented on the related work section. \nThe article lacks an analysis of the impact of the chosen concepts (even on the granularity).\n\nquestions_for_the_authors: Concepts representations are not contextualized. What about ambiguous context? The method proposed in 2.3 might circumvent this issue, but it should be properly tested. \nThis approach is related with topic modeling, in the specific case of fixed topic. How this approach competes against these supervised topic modeling approaches ? \nIf there exist some correlation in the concept space (concepts are not orthogonal enough in term of semantic, e.g. in the case of colinear concept's representations) then a lot of the initial information will be lost (contrary to what is stated in 3.3). The approach seems not to lose that much on classification task, but how the chosen concepts impact the performance of the obtained representation should be carefully evaluated.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "I9abwbfJmA",
        "length": 671,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an approach for transforming the otherwise opaque dimensions of contextualized sentence embeddings into directly interpretable categories of an ontology and evaluate them in various topic classification tasks.   The proposed approach (dubbed CES) turns some concept inventory into text, transforms them into the representation space of a sentence encoder, finally constructs a transformation matrix of these vectorized concept vectors that maps the latent vectorial representations of the encoder into such a space in which the individual dimensions are directly tied to one of the concepts from the concept inventory.   The idea is simple and sound, even though the concepts seem to be converted into the representation space somewhat naively.   The representations with interpretable dimensions are evaluated in 7 different document classification tasks.   One limitation of the work is that it only works for document classification.   The quality of the transformed representations is assessed is via measuring the prediction similarity of such random forest classifiers that use either the transformed or the original representations as document features.   As the goal of the transformation was to create interpretable representations, using a more transluent classifier (other than a random forest with 100 classifiers) would be more adequate.   Could the prediction similarity be due to the large number of classifiers used in the ensemble of classifiers?   Besides the agreement rate of the classifiers relying on the original and the interpretable features, their accuracy is equally important, which results are delegated to the appendix.   It seems that the use of the transformed representations degrade classification performance noticeably.   Some amount of performance drop is fine in exchange for more interpretable representations, but this kind of tradeoff should be made explicit and clearly articulated.   The understandability of the representations from the perspective of document classification is assessed by humans as well as in an automated way.   Related to the automated evaluation of the understandability of the transformed representations, the alternative baseline approaches that assign meaning to each dimension of the original latent space are too naive, i.e., the proposed CES approach has access to C*, the concept inventory tailored for the given task, whereas the alternative approach uses either a vocabulary of the most frequent words or a concept inventory that does not enjoy the benefit of being adjusted to the domain in question.   This way, it remains unknown to what extent CES performs better due to the more useful vocabulary or the actual approach itself.   When performing the automated undestandability assessment, based on the figures in Table 4, only 20 documents per dataset was used.   Since the automated assessment is not constrained by human labor, this kind of comparison might have been performed over more documents.\n\nreasons_to_accept: Developing interpretable (document) representations is important topic, for which a simple approach is provided.\n\nreasons_to_reject: The proposed approach only works for document classification, the applied baseline is too simple and the comparison with other existing approaches is lacking.   The classification performance drops quite noticeably compared to the use of the original (yet uninterpretable) representations.\n\ntypos_grammar_style_and_presentation_improvements: The statement in the related work section that sparsification only works for static embeddings is not true, see e.g. [1,2,3].   [3], being an extension of (Senel et al., 2018) for the contextual case could serve as a stronger baseline as it is also capable of assigning human interpretable labels to the dimensions of transformed latent representations.\n[1] Berend, G\u00e1bor. [\" Sparsity makes sense: Word sense disambiguation using sparse contextualized word representations.\"](https://aclanthology.org/2020.emnlp-main.683/)   [2] Yun, Zeyu, et al. [\"Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors.\"](https://aclanthology.org/2021.deelio-1.1/)   [3] Ficsor, Tam\u00e1s, and G\u00e1bor Berend. [\" Changing the Basis of Contextual Representations with Explicit Semantics.\" ]( https://aclanthology.org/2021.acl-srw.25/)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "T0h4ObI82F",
        "length": 952,
        "human_text": "paper_topic_and_main_contributions: The authors propose to make embedding space more interpretable with respect to the model that produced them by conceptualizing the embedding space. The concept(ual) space is supposed to be more comprehensible for humans. \nThey motivate their work by the need for debugging embedding models (in this case LLMs) and for detecting biases, as well as for explaining decision made by systems incorporating LMs. \nThey propose an algorithm for conceptualization as well as a new evaluation technique to test the conceptualization. \nAs concept database, they use a Wikipedia graph. For labeling edges between concepts, they calculate sibling scores based on the similarity of the sibling concepts to detect \"is-a\" relations. \nSince concepts often are hierarchical and not all down-stream tasks might need all concepts, the conceptual space can have different granularity, depending on how \"deep\" a concept can branch out (or how many children concepts are available) and what is needed. \nThe authors accomplish this by adapting the concept space iteratively to the input text.\nFor evaluating their method, they use both human and LLM judgements.\nThe authors' main contribution is a new variant of how to make embedding spaces more interpretable, together with an interesting evaluation.\n\nreasons_to_accept: - The work is well motivated and set into context with related approaches.\n- It is definitely relevant for research in embedding spaces (wrt biases, model decisions, etc.)\n- I like the idea of having different granularity for the concept spaces.\n- It seems that the proposed method works well and it is certainly an interesting take on embedding space interpretation, but I am not 100% certain I understood everything (see questions).\n\nreasons_to_reject: - The method needs some clarification -- see questions.\n- The evaluation is interesting, but at least the results using a classifier trained on the concept space are not super convincing, since the agreement scores are not very high. When comparing humans vs. LLM ratings, the test set is very small.\n\nquestions_for_the_authors: General: - Why do we need the conceptual space in the first place? Is it not \"enough\" or maybe even equivalent to having close (i.e. highly similar) concepts in L, which describe the text vector t in L?\nAbstract: - The first sentence in the abstract is confusing to me: One of the \"main methods\" for interpreting a text is to embed it? Not really, is it? It's one of the main methods to process natural language using computers etc., but for interpreting a text, humans do not map it to vectors -- probably to concepts, though!?\nSection 1: - The embeddings of small LMs like skip-gram or GloVe were also not interpretable -- did you consider these as well? ( referring to ll. 36) - ll. 49: by \"understanding\" the embedding space, we might get some insights on those layers, but not into the rest of the model.\n2.2: - l. 181: |C\u00b9| = 37: if I understand correctly, this means that in the first level of the hierarchy, there are only 37 concepts in total? Or am I missing something -- it seems a rather small number for top-level concepts in Wikipedia to me.\n- I am not 100% certain I understand the \"selective refinement\". What does \"the concept with the largest weight\" mean exactly?\n- How is \"removeP\" determined? Is it simply a parameter you set at the beginning?\n2.1: - Can you please provide a URL to the \"Wikipedia category directed graph\" 3.:\n- For the function tau, you use the string representation of the concept, correct? Might this induce some confusion for the embedding model, since there is no context for these concepts?\n3.1: - The 10 sentences you use as context, are they preceding the sentence under consideration? How are they connected to the sentence except that they are from the same article?\n3.2: - The Kappa coefficient is \"relatively high\" on some of the datasets, but also quite low on others, e.g., 20 news group, Ohsumed. Also, the difference in accuracy on 20 news group is pretty large, especially when compared to the other datasets. Do you have an explanation for that?\n- Maybe I oversaw it -- please mention that all datasets are in English.\n3.3.2: - Please add to the table captions that the scores represent \"raw agreement\".\n4.2: - The plots in Figure 2 are really interesting! I just wonder what they actually mean -- the concept of \"weapons\" with input \"government\" in GPT-2 gets stronger with higher layers -- why? What was the context for \"government\"?\n\nmissing_references: - a citation for DMA?\nThese references are not necessarily missing, but maybe they could give some further ideas since they are based on similar idea(s), i.e., \"conceptualization of embeddings\": - Koc et al., 2018: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure - Sommerauer & Fokkens, 2018: Firearms and Tigers are Dangerous, Kitchen Knives and Zebras are Not: Testing whether Word Embeddings Can Tell - Molino et al., 2019: Parallax: Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae\n\ntypos_grammar_style_and_presentation_improvements: The paper is well structured and written.\nTypos: - l. 76: \"that latent dimensionS correspond ..\" / \"that A latent dimension corresponds ..\" Style: - The conceptual space C and the set of concepts C are hard to differentiate, maybe use CS for conceptual space?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "38_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_38_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7084,
      "max_similarity": 0.727,
      "avg_coverage": 0.42893333333333333,
      "max_coverage": 0.7333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 393,
      "avg_human_length": 352.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 6,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "03ddnktwqI",
        "similarity": 0.7089,
        "coverage": 0.2353,
        "human_length": 512,
        "human_text": "paper_topic_and_main_contributions: This paper is about Open-Vocabulary Named Entity Recognition (NER), which is the task of identifying and classifying named entities in text, where the set of entity types is not fixed in advance. The paper proposes a novel and scalable two-stage method called Context-Type SemAntiC Alignment and FusiOn (CACAO) to recognize entities in novel types by their textual names or descriptions. The method is formulated as a semantic matching task and pre-trained on 80M context-type pairs, making it easily accessible through natural supervision. The paper addresses the challenge of open-vocabulary named entity recognition and makes several contributions towards a solution, including proposing the CACAO method, evaluating it on multiple datasets, and releasing the pre-trained models and code for reproducibility. The paper also provides a thorough analysis of the proposed method and its limitations, as well as directions for future work.\n\nreasons_to_accept: 1. The strengths of this paper include proposing a novel and scalable two-stage method for open-vocabulary named entity recognition , which is a challenging and interesting task in natural language processing. \n2. The paper provides a thorough analysis of the proposed method and its limitations, as well as directions for future work. \n3. The paper also evaluates the proposed method on multiple datasets and releases the pre-trained models and code for reproducibility.\n\nreasons_to_reject: 1. The paper mainly focuses on the English language, and it is unclear how well the proposed method would generalize to other languages. \n2. The paper does not provide a detailed analysis of the computational complexity and efficiency of the proposed method, which could limit its practical applicability in some scenarios. \n3. The paper does not provide a detailed analysis of the impact of different hyperparameters on the performance of the proposed method, which could limit the understanding of the robustness of the method. \n4. It is better to compare with other LLM-based NER methods, which seems to be more suitable for zero-shot and open-vocabulary.\n\nquestions_for_the_authors: Could you provide some examples of cases where the proposed method fails or makes errors, and how the method could be improved to handle such cases?\n\nmissing_references: Li B, Yin W, Chen M. Ultra-fine entity typing with indirect supervision from natural language inference[J]. Transactions of the Association for Computational Linguistics, 2022, 10: 607-622.\nLi D, Hu B, Chen Q. Prompt-based Text Entailment for Low-Resource Named Entity Recognition[C]//Proceedings of the 29th International Conference on Computational Linguistics. 2022: 1896-1903.\nJiao Y, Li S, Xie Y, et al. Open-vocabulary argument role prediction for event extraction[J]. arXiv preprint arXiv:2211.01577, 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "uUh2CxobHE",
        "similarity": 0.6893,
        "coverage": 0.7333,
        "human_length": 273,
        "human_text": "paper_topic_and_main_contributions: This paper tackles a realistic open-vocabulary NER problem, i.e., recognizing novel types of entities given only their surface names/descriptions and annotation of base types. \nThe paper formulates OVNER as a semantic matching task and proposes a two-stage method. It first pretrains dual encoders on distantly-annotated context-type pairs for alignment based on contrastive learning, then it finetunes the cross-encoder on base type supervision.\n\nreasons_to_accept: The paper is easy to follow and well-motivated. \nAbundant experimental results verify the effectiveness of the approach.\n\nreasons_to_reject: The paper only compares with discriminative methods, it is expected to compare with generative methods or LLMs which might perform better in some cases.\n\nquestions_for_the_authors: Question A: Continual learning NER requires sufficient annotated data for new type, while continual few-shot NER does not. Comparing OVNER with this paradigm makes the discussion more complete in the introduction (Line 064).\nQuestion B: It seems that the performance of novel types remains largely lower than that of base types. Discussion about future efforts to improve the performance and error analysis are expected.\n\nethical_concerns: No\n\ntypos_grammar_style_and_presentation_improvements: A: \u201cOur Proposed Method\u201d in Table 1 can be omitted.\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "CVJYRBJh8W",
        "similarity": 0.727,
        "coverage": 0.3182,
        "human_length": 271,
        "human_text": "paper_topic_and_main_contributions: This work proposes a more challenging task called Open Vocabulary Named Entity Recognition(OVNER) in response to a real-world need. The authors started with the motivation that the current NER approach does not work well to discover novel entity types. The goal of the newly proposed OVNER is to automatically recognize new types when given a description beside the name. For OVNER, the work also proposed a framework called CACAO and experimentally validated its effectiveness.\n\nreasons_to_accept: 1. This work proposes a new Setting in an attempt to address an important challenge for NER in recent years, which is how to discover new entity types. Acceptance of this work may be useful for subsequent research in the NLP community. \n2. The work is relatively complete and somewhat sound. While proposing OVNER, the CACAO framework is proposed to address it. The design and results of the experiment can relatively support their claims.\n\nreasons_to_reject: 1. The work does not bridge well to past related research work in the community. This work criticizes past related research directions. OVNER should also have certain limitations, such as the provision of descriptions being necessary. \n2. Does OVNER cover Domain-Adapted NER scenarios? Does it include scenarios with large domain differences?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "03ddnktwqI",
        "length": 512,
        "human_text": "paper_topic_and_main_contributions: This paper is about Open-Vocabulary Named Entity Recognition (NER), which is the task of identifying and classifying named entities in text, where the set of entity types is not fixed in advance. The paper proposes a novel and scalable two-stage method called Context-Type SemAntiC Alignment and FusiOn (CACAO) to recognize entities in novel types by their textual names or descriptions. The method is formulated as a semantic matching task and pre-trained on 80M context-type pairs, making it easily accessible through natural supervision. The paper addresses the challenge of open-vocabulary named entity recognition and makes several contributions towards a solution, including proposing the CACAO method, evaluating it on multiple datasets, and releasing the pre-trained models and code for reproducibility. The paper also provides a thorough analysis of the proposed method and its limitations, as well as directions for future work.\n\nreasons_to_accept: 1. The strengths of this paper include proposing a novel and scalable two-stage method for open-vocabulary named entity recognition , which is a challenging and interesting task in natural language processing. \n2. The paper provides a thorough analysis of the proposed method and its limitations, as well as directions for future work. \n3. The paper also evaluates the proposed method on multiple datasets and releases the pre-trained models and code for reproducibility.\n\nreasons_to_reject: 1. The paper mainly focuses on the English language, and it is unclear how well the proposed method would generalize to other languages. \n2. The paper does not provide a detailed analysis of the computational complexity and efficiency of the proposed method, which could limit its practical applicability in some scenarios. \n3. The paper does not provide a detailed analysis of the impact of different hyperparameters on the performance of the proposed method, which could limit the understanding of the robustness of the method. \n4. It is better to compare with other LLM-based NER methods, which seems to be more suitable for zero-shot and open-vocabulary.\n\nquestions_for_the_authors: Could you provide some examples of cases where the proposed method fails or makes errors, and how the method could be improved to handle such cases?\n\nmissing_references: Li B, Yin W, Chen M. Ultra-fine entity typing with indirect supervision from natural language inference[J]. Transactions of the Association for Computational Linguistics, 2022, 10: 607-622.\nLi D, Hu B, Chen Q. Prompt-based Text Entailment for Low-Resource Named Entity Recognition[C]//Proceedings of the 29th International Conference on Computational Linguistics. 2022: 1896-1903.\nJiao Y, Li S, Xie Y, et al. Open-vocabulary argument role prediction for event extraction[J]. arXiv preprint arXiv:2211.01577, 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "uUh2CxobHE",
        "length": 273,
        "human_text": "paper_topic_and_main_contributions: This paper tackles a realistic open-vocabulary NER problem, i.e., recognizing novel types of entities given only their surface names/descriptions and annotation of base types. \nThe paper formulates OVNER as a semantic matching task and proposes a two-stage method. It first pretrains dual encoders on distantly-annotated context-type pairs for alignment based on contrastive learning, then it finetunes the cross-encoder on base type supervision.\n\nreasons_to_accept: The paper is easy to follow and well-motivated. \nAbundant experimental results verify the effectiveness of the approach.\n\nreasons_to_reject: The paper only compares with discriminative methods, it is expected to compare with generative methods or LLMs which might perform better in some cases.\n\nquestions_for_the_authors: Question A: Continual learning NER requires sufficient annotated data for new type, while continual few-shot NER does not. Comparing OVNER with this paradigm makes the discussion more complete in the introduction (Line 064).\nQuestion B: It seems that the performance of novel types remains largely lower than that of base types. Discussion about future efforts to improve the performance and error analysis are expected.\n\nethical_concerns: No\n\ntypos_grammar_style_and_presentation_improvements: A: \u201cOur Proposed Method\u201d in Table 1 can be omitted.\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "CVJYRBJh8W",
        "length": 271,
        "human_text": "paper_topic_and_main_contributions: This work proposes a more challenging task called Open Vocabulary Named Entity Recognition(OVNER) in response to a real-world need. The authors started with the motivation that the current NER approach does not work well to discover novel entity types. The goal of the newly proposed OVNER is to automatically recognize new types when given a description beside the name. For OVNER, the work also proposed a framework called CACAO and experimentally validated its effectiveness.\n\nreasons_to_accept: 1. This work proposes a new Setting in an attempt to address an important challenge for NER in recent years, which is how to discover new entity types. Acceptance of this work may be useful for subsequent research in the NLP community. \n2. The work is relatively complete and somewhat sound. While proposing OVNER, the CACAO framework is proposed to address it. The design and results of the experiment can relatively support their claims.\n\nreasons_to_reject: 1. The work does not bridge well to past related research work in the community. This work criticizes past related research directions. OVNER should also have certain limitations, such as the provision of descriptions being necessary. \n2. Does OVNER cover Domain-Adapted NER scenarios? Does it include scenarios with large domain differences?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "38_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_38_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7097333333333333,
      "max_similarity": 0.7316,
      "avg_coverage": 0.43166666666666664,
      "max_coverage": 0.6667
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 359,
      "avg_human_length": 352.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 6,
      "suggestions_count": 3
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "03ddnktwqI",
        "similarity": 0.7037,
        "coverage": 0.2647,
        "human_length": 512,
        "human_text": "paper_topic_and_main_contributions: This paper is about Open-Vocabulary Named Entity Recognition (NER), which is the task of identifying and classifying named entities in text, where the set of entity types is not fixed in advance. The paper proposes a novel and scalable two-stage method called Context-Type SemAntiC Alignment and FusiOn (CACAO) to recognize entities in novel types by their textual names or descriptions. The method is formulated as a semantic matching task and pre-trained on 80M context-type pairs, making it easily accessible through natural supervision. The paper addresses the challenge of open-vocabulary named entity recognition and makes several contributions towards a solution, including proposing the CACAO method, evaluating it on multiple datasets, and releasing the pre-trained models and code for reproducibility. The paper also provides a thorough analysis of the proposed method and its limitations, as well as directions for future work.\n\nreasons_to_accept: 1. The strengths of this paper include proposing a novel and scalable two-stage method for open-vocabulary named entity recognition , which is a challenging and interesting task in natural language processing. \n2. The paper provides a thorough analysis of the proposed method and its limitations, as well as directions for future work. \n3. The paper also evaluates the proposed method on multiple datasets and releases the pre-trained models and code for reproducibility.\n\nreasons_to_reject: 1. The paper mainly focuses on the English language, and it is unclear how well the proposed method would generalize to other languages. \n2. The paper does not provide a detailed analysis of the computational complexity and efficiency of the proposed method, which could limit its practical applicability in some scenarios. \n3. The paper does not provide a detailed analysis of the impact of different hyperparameters on the performance of the proposed method, which could limit the understanding of the robustness of the method. \n4. It is better to compare with other LLM-based NER methods, which seems to be more suitable for zero-shot and open-vocabulary.\n\nquestions_for_the_authors: Could you provide some examples of cases where the proposed method fails or makes errors, and how the method could be improved to handle such cases?\n\nmissing_references: Li B, Yin W, Chen M. Ultra-fine entity typing with indirect supervision from natural language inference[J]. Transactions of the Association for Computational Linguistics, 2022, 10: 607-622.\nLi D, Hu B, Chen Q. Prompt-based Text Entailment for Low-Resource Named Entity Recognition[C]//Proceedings of the 29th International Conference on Computational Linguistics. 2022: 1896-1903.\nJiao Y, Li S, Xie Y, et al. Open-vocabulary argument role prediction for event extraction[J]. arXiv preprint arXiv:2211.01577, 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "uUh2CxobHE",
        "similarity": 0.6939,
        "coverage": 0.6667,
        "human_length": 273,
        "human_text": "paper_topic_and_main_contributions: This paper tackles a realistic open-vocabulary NER problem, i.e., recognizing novel types of entities given only their surface names/descriptions and annotation of base types. \nThe paper formulates OVNER as a semantic matching task and proposes a two-stage method. It first pretrains dual encoders on distantly-annotated context-type pairs for alignment based on contrastive learning, then it finetunes the cross-encoder on base type supervision.\n\nreasons_to_accept: The paper is easy to follow and well-motivated. \nAbundant experimental results verify the effectiveness of the approach.\n\nreasons_to_reject: The paper only compares with discriminative methods, it is expected to compare with generative methods or LLMs which might perform better in some cases.\n\nquestions_for_the_authors: Question A: Continual learning NER requires sufficient annotated data for new type, while continual few-shot NER does not. Comparing OVNER with this paradigm makes the discussion more complete in the introduction (Line 064).\nQuestion B: It seems that the performance of novel types remains largely lower than that of base types. Discussion about future efforts to improve the performance and error analysis are expected.\n\nethical_concerns: No\n\ntypos_grammar_style_and_presentation_improvements: A: \u201cOur Proposed Method\u201d in Table 1 can be omitted.\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "CVJYRBJh8W",
        "similarity": 0.7316,
        "coverage": 0.3636,
        "human_length": 271,
        "human_text": "paper_topic_and_main_contributions: This work proposes a more challenging task called Open Vocabulary Named Entity Recognition(OVNER) in response to a real-world need. The authors started with the motivation that the current NER approach does not work well to discover novel entity types. The goal of the newly proposed OVNER is to automatically recognize new types when given a description beside the name. For OVNER, the work also proposed a framework called CACAO and experimentally validated its effectiveness.\n\nreasons_to_accept: 1. This work proposes a new Setting in an attempt to address an important challenge for NER in recent years, which is how to discover new entity types. Acceptance of this work may be useful for subsequent research in the NLP community. \n2. The work is relatively complete and somewhat sound. While proposing OVNER, the CACAO framework is proposed to address it. The design and results of the experiment can relatively support their claims.\n\nreasons_to_reject: 1. The work does not bridge well to past related research work in the community. This work criticizes past related research directions. OVNER should also have certain limitations, such as the provision of descriptions being necessary. \n2. Does OVNER cover Domain-Adapted NER scenarios? Does it include scenarios with large domain differences?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "03ddnktwqI",
        "length": 512,
        "human_text": "paper_topic_and_main_contributions: This paper is about Open-Vocabulary Named Entity Recognition (NER), which is the task of identifying and classifying named entities in text, where the set of entity types is not fixed in advance. The paper proposes a novel and scalable two-stage method called Context-Type SemAntiC Alignment and FusiOn (CACAO) to recognize entities in novel types by their textual names or descriptions. The method is formulated as a semantic matching task and pre-trained on 80M context-type pairs, making it easily accessible through natural supervision. The paper addresses the challenge of open-vocabulary named entity recognition and makes several contributions towards a solution, including proposing the CACAO method, evaluating it on multiple datasets, and releasing the pre-trained models and code for reproducibility. The paper also provides a thorough analysis of the proposed method and its limitations, as well as directions for future work.\n\nreasons_to_accept: 1. The strengths of this paper include proposing a novel and scalable two-stage method for open-vocabulary named entity recognition , which is a challenging and interesting task in natural language processing. \n2. The paper provides a thorough analysis of the proposed method and its limitations, as well as directions for future work. \n3. The paper also evaluates the proposed method on multiple datasets and releases the pre-trained models and code for reproducibility.\n\nreasons_to_reject: 1. The paper mainly focuses on the English language, and it is unclear how well the proposed method would generalize to other languages. \n2. The paper does not provide a detailed analysis of the computational complexity and efficiency of the proposed method, which could limit its practical applicability in some scenarios. \n3. The paper does not provide a detailed analysis of the impact of different hyperparameters on the performance of the proposed method, which could limit the understanding of the robustness of the method. \n4. It is better to compare with other LLM-based NER methods, which seems to be more suitable for zero-shot and open-vocabulary.\n\nquestions_for_the_authors: Could you provide some examples of cases where the proposed method fails or makes errors, and how the method could be improved to handle such cases?\n\nmissing_references: Li B, Yin W, Chen M. Ultra-fine entity typing with indirect supervision from natural language inference[J]. Transactions of the Association for Computational Linguistics, 2022, 10: 607-622.\nLi D, Hu B, Chen Q. Prompt-based Text Entailment for Low-Resource Named Entity Recognition[C]//Proceedings of the 29th International Conference on Computational Linguistics. 2022: 1896-1903.\nJiao Y, Li S, Xie Y, et al. Open-vocabulary argument role prediction for event extraction[J]. arXiv preprint arXiv:2211.01577, 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "uUh2CxobHE",
        "length": 273,
        "human_text": "paper_topic_and_main_contributions: This paper tackles a realistic open-vocabulary NER problem, i.e., recognizing novel types of entities given only their surface names/descriptions and annotation of base types. \nThe paper formulates OVNER as a semantic matching task and proposes a two-stage method. It first pretrains dual encoders on distantly-annotated context-type pairs for alignment based on contrastive learning, then it finetunes the cross-encoder on base type supervision.\n\nreasons_to_accept: The paper is easy to follow and well-motivated. \nAbundant experimental results verify the effectiveness of the approach.\n\nreasons_to_reject: The paper only compares with discriminative methods, it is expected to compare with generative methods or LLMs which might perform better in some cases.\n\nquestions_for_the_authors: Question A: Continual learning NER requires sufficient annotated data for new type, while continual few-shot NER does not. Comparing OVNER with this paradigm makes the discussion more complete in the introduction (Line 064).\nQuestion B: It seems that the performance of novel types remains largely lower than that of base types. Discussion about future efforts to improve the performance and error analysis are expected.\n\nethical_concerns: No\n\ntypos_grammar_style_and_presentation_improvements: A: \u201cOur Proposed Method\u201d in Table 1 can be omitted.\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "CVJYRBJh8W",
        "length": 271,
        "human_text": "paper_topic_and_main_contributions: This work proposes a more challenging task called Open Vocabulary Named Entity Recognition(OVNER) in response to a real-world need. The authors started with the motivation that the current NER approach does not work well to discover novel entity types. The goal of the newly proposed OVNER is to automatically recognize new types when given a description beside the name. For OVNER, the work also proposed a framework called CACAO and experimentally validated its effectiveness.\n\nreasons_to_accept: 1. This work proposes a new Setting in an attempt to address an important challenge for NER in recent years, which is how to discover new entity types. Acceptance of this work may be useful for subsequent research in the NLP community. \n2. The work is relatively complete and somewhat sound. While proposing OVNER, the CACAO framework is proposed to address it. The design and results of the experiment can relatively support their claims.\n\nreasons_to_reject: 1. The work does not bridge well to past related research work in the community. This work criticizes past related research directions. OVNER should also have certain limitations, such as the provision of descriptions being necessary. \n2. Does OVNER cover Domain-Adapted NER scenarios? Does it include scenarios with large domain differences?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "173_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_173_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7064333333333334,
      "max_similarity": 0.7318,
      "avg_coverage": 0.6915999999999999,
      "max_coverage": 0.8
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 861,
      "avg_human_length": 445.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 16,
      "suggestions_count": 19
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "6CzNgmsIqj",
        "similarity": 0.7081,
        "coverage": 0.7222,
        "human_length": 315,
        "human_text": "paper_topic_and_main_contributions: The authors propose a knowledge graph enhanced language model (KAPALM) that fuses coarse- and fine-grained representations of entity knowledge from knowledge graphs to conduct fake news detection. They design a method to prune subgraphs, and then use attention mechanism to obtain knowledge representation. By combining it with a textual representation, KAPALM could further improve the detection performance.\n\nreasons_to_accept: \u2022This work copes with the fake news classification task, which is useful in practice.  \u2022The author proposed an effective model for the fake news detection task and verified its effectiveness on the public test set\n\nreasons_to_reject: \u2022The motivation stated in the abstract is not solid enough, and the method proposed in the paper is not novel enough. For the statement \" majority of these methods focus on news entity information and ignore the structured knowledge among news entities\" in the abstract, some existing methods already do this, like CompareNet[1].\n\u2022The comparison of the proposed method with the baseline is not fair enough. The Bert-base-with-adapter used in the article is inconsistent with the Roberta used by KPL, and it is difficult to prove that KAPALM is more effective than KPL. What is the motivation for Bert-base-with-adapter?\n\u2022The coarse-grained and fine-grained knowledge mentioned in the article is not clear enough. The knowledge pruning method proposed in the article is intuitive.\n[1]. Hu L, Yang T, Zhang L, et al. Compare to the knowledge: Graph neural fake news detection with external knowledge[C], in ACL2021.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 1: Poor: I cannot identify the contributions of this paper, or I believe the claims are not sufficiently backed up by evidence. I would fight to have it rejected.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "UT8FMj1NCh",
        "similarity": 0.7318,
        "coverage": 0.5526,
        "human_length": 821,
        "human_text": "paper_topic_and_main_contributions: Identifying fake news is a highly relevant topic and one that is at the core of NLP given the importance that written text contributes to solving the task. As such it presents a great topical match for EMNLP.  The authors argue that identifying entities in some text and incorporating relationships between entities drawn from some external knowledge graphs might help in this classification task. The architecture that is being proposed is one that generates embeddings from three different angles: the actual text, an entity graph based on entities in the text and related entities in some external graph, a pruned version of this graph. The three embeddings resulting from this process are concatenated before being fed into a binary classifier.\nExperiments on the FakeNewsNet dataset are reported.\n\nreasons_to_accept: - This is a highly relevant research area, and any work that pushes our understanding as to how to identify misinformation of any form is a step forward.\n- The work is conducted on a well-known benchmark dataset and is therefore easily contextualisable with other work in the field.  - Conducting an ablation study is a strong aspect of the experimental work as it offers more nuanced insights into what components of the architecture contribute how much  to the overall performance.\n\nreasons_to_reject: There are a number of weaknesses in this work. Among the biggest issues I consider the following:  - Weak baselines: Given the chosen dataset contains much more information than the textual content I would want to see the results of this work compared to the state of the art reported in the literature that looks at the same dataset. To pick just one recent example [1], the results reported (such as F1) appear to be way higher than anything in Table 2 (and that is also true for the baselines reported in [1]). What is needed is a comparison against what the current state of the art is as reported in the literature (ideally reproduced to conduct significance tests where appropriate).  - Only a single dataset is used to explore the problem (well, it is two different parts but in the end it is one fairly specific dataset). There are many more benchmark datasets for text classification (including fake news detection) that could be included to provide more confidence in the findings.  - There are many missing details (and no supplementary material such as code) making it impossible to replicate the work. For example, unless I have overlooked it I cannot see what knowledge graphs are actually being used.  - There are no statistical significance tests (and terms such as \"outperform\" should therefore not be used)  [1] Donabauer \"Exploring Fake News Detection with Heterogeneous Social Media Context Graphs\". ECIR 2023.\n\nquestions_for_the_authors: (1) It is unclear to me that text properties alone (together with what text properties of external sources add to that) would be sufficient to distinguish fabricated news from real news. This may be true for some datasets but I imagine that with more effort put into creating stories (and more sophisticated tools such as those from the GPT family) it would be impossible to distinguish the two classes without incorporating more signals (such as contextual features). What is the underlying intuition in your work? Do you argue that even with the widepread adoption of ChatGPT et al. it will be possible to distinguish fake news from real news based on text (+ entities) alone?   (2) Given you adopt a dataset that includes many more signals I wonder why you do not include all these and THEN try to push forward the state of the art by going beyond what you can achieve. What is the reason to only try to improve on the text classification alone?  (3) Is there a need to include all the baselines? I have the impression that none of 1 - 7 are actually needed as we can assume for BERT to be a stronger baseline (it is also not a surprise that the results demonstrate yet again that traditional methods do not appear to be competitive with neural methods).\n(4) Why did you not include a discussion of ethical issues? Given the topic this would have been a helpful addition.\n\nmissing_references: You should include references that represent the state of the art in respect to whatever dataset(s) you have chosen.\nMake sure you include peer-reviewed versions of a paper where this is appropriate (e.g. the BERT paper).\n\ntypos_grammar_style_and_presentation_improvements: The paper needs to be properly proofread. It has many language problems throughout.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "m18NpFwGiw",
        "similarity": 0.6794,
        "coverage": 0.8,
        "human_length": 201,
        "human_text": "paper_topic_and_main_contributions: The paper investigate to integrate structured knowledge among entities in the news article to enhance the fake news detection. The proposed method outperforms previous KG related approaches that do not utilise structured information. It also shows promising results in the few-shot setting.\n\nreasons_to_accept: The approach is novel and achieves SOTA on two datasets with full scale training, and show promising performance in the few-shot setting.\n\nreasons_to_reject: Lack statistical significance tests.\n\nquestions_for_the_authors: Did you conduct error analysis to explore the reason why your proposed method is worse than KPL in the few shot setting on Politifact, but better than KPL on Gossipcop?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "6CzNgmsIqj",
        "length": 315,
        "human_text": "paper_topic_and_main_contributions: The authors propose a knowledge graph enhanced language model (KAPALM) that fuses coarse- and fine-grained representations of entity knowledge from knowledge graphs to conduct fake news detection. They design a method to prune subgraphs, and then use attention mechanism to obtain knowledge representation. By combining it with a textual representation, KAPALM could further improve the detection performance.\n\nreasons_to_accept: \u2022This work copes with the fake news classification task, which is useful in practice.  \u2022The author proposed an effective model for the fake news detection task and verified its effectiveness on the public test set\n\nreasons_to_reject: \u2022The motivation stated in the abstract is not solid enough, and the method proposed in the paper is not novel enough. For the statement \" majority of these methods focus on news entity information and ignore the structured knowledge among news entities\" in the abstract, some existing methods already do this, like CompareNet[1].\n\u2022The comparison of the proposed method with the baseline is not fair enough. The Bert-base-with-adapter used in the article is inconsistent with the Roberta used by KPL, and it is difficult to prove that KAPALM is more effective than KPL. What is the motivation for Bert-base-with-adapter?\n\u2022The coarse-grained and fine-grained knowledge mentioned in the article is not clear enough. The knowledge pruning method proposed in the article is intuitive.\n[1]. Hu L, Yang T, Zhang L, et al. Compare to the knowledge: Graph neural fake news detection with external knowledge[C], in ACL2021.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 1: Poor: I cannot identify the contributions of this paper, or I believe the claims are not sufficiently backed up by evidence. I would fight to have it rejected.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "UT8FMj1NCh",
        "length": 821,
        "human_text": "paper_topic_and_main_contributions: Identifying fake news is a highly relevant topic and one that is at the core of NLP given the importance that written text contributes to solving the task. As such it presents a great topical match for EMNLP.  The authors argue that identifying entities in some text and incorporating relationships between entities drawn from some external knowledge graphs might help in this classification task. The architecture that is being proposed is one that generates embeddings from three different angles: the actual text, an entity graph based on entities in the text and related entities in some external graph, a pruned version of this graph. The three embeddings resulting from this process are concatenated before being fed into a binary classifier.\nExperiments on the FakeNewsNet dataset are reported.\n\nreasons_to_accept: - This is a highly relevant research area, and any work that pushes our understanding as to how to identify misinformation of any form is a step forward.\n- The work is conducted on a well-known benchmark dataset and is therefore easily contextualisable with other work in the field.  - Conducting an ablation study is a strong aspect of the experimental work as it offers more nuanced insights into what components of the architecture contribute how much  to the overall performance.\n\nreasons_to_reject: There are a number of weaknesses in this work. Among the biggest issues I consider the following:  - Weak baselines: Given the chosen dataset contains much more information than the textual content I would want to see the results of this work compared to the state of the art reported in the literature that looks at the same dataset. To pick just one recent example [1], the results reported (such as F1) appear to be way higher than anything in Table 2 (and that is also true for the baselines reported in [1]). What is needed is a comparison against what the current state of the art is as reported in the literature (ideally reproduced to conduct significance tests where appropriate).  - Only a single dataset is used to explore the problem (well, it is two different parts but in the end it is one fairly specific dataset). There are many more benchmark datasets for text classification (including fake news detection) that could be included to provide more confidence in the findings.  - There are many missing details (and no supplementary material such as code) making it impossible to replicate the work. For example, unless I have overlooked it I cannot see what knowledge graphs are actually being used.  - There are no statistical significance tests (and terms such as \"outperform\" should therefore not be used)  [1] Donabauer \"Exploring Fake News Detection with Heterogeneous Social Media Context Graphs\". ECIR 2023.\n\nquestions_for_the_authors: (1) It is unclear to me that text properties alone (together with what text properties of external sources add to that) would be sufficient to distinguish fabricated news from real news. This may be true for some datasets but I imagine that with more effort put into creating stories (and more sophisticated tools such as those from the GPT family) it would be impossible to distinguish the two classes without incorporating more signals (such as contextual features). What is the underlying intuition in your work? Do you argue that even with the widepread adoption of ChatGPT et al. it will be possible to distinguish fake news from real news based on text (+ entities) alone?   (2) Given you adopt a dataset that includes many more signals I wonder why you do not include all these and THEN try to push forward the state of the art by going beyond what you can achieve. What is the reason to only try to improve on the text classification alone?  (3) Is there a need to include all the baselines? I have the impression that none of 1 - 7 are actually needed as we can assume for BERT to be a stronger baseline (it is also not a surprise that the results demonstrate yet again that traditional methods do not appear to be competitive with neural methods).\n(4) Why did you not include a discussion of ethical issues? Given the topic this would have been a helpful addition.\n\nmissing_references: You should include references that represent the state of the art in respect to whatever dataset(s) you have chosen.\nMake sure you include peer-reviewed versions of a paper where this is appropriate (e.g. the BERT paper).\n\ntypos_grammar_style_and_presentation_improvements: The paper needs to be properly proofread. It has many language problems throughout.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "m18NpFwGiw",
        "length": 201,
        "human_text": "paper_topic_and_main_contributions: The paper investigate to integrate structured knowledge among entities in the news article to enhance the fake news detection. The proposed method outperforms previous KG related approaches that do not utilise structured information. It also shows promising results in the few-shot setting.\n\nreasons_to_accept: The approach is novel and achieves SOTA on two datasets with full scale training, and show promising performance in the few-shot setting.\n\nreasons_to_reject: Lack statistical significance tests.\n\nquestions_for_the_authors: Did you conduct error analysis to explore the reason why your proposed method is worse than KPL in the few shot setting on Politifact, but better than KPL on Gossipcop?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "173_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_173_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7059000000000001,
      "max_similarity": 0.7307,
      "avg_coverage": 0.7425,
      "max_coverage": 0.9
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 910,
      "avg_human_length": 445.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 16,
      "suggestions_count": 19
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "6CzNgmsIqj",
        "similarity": 0.7072,
        "coverage": 0.7222,
        "human_length": 315,
        "human_text": "paper_topic_and_main_contributions: The authors propose a knowledge graph enhanced language model (KAPALM) that fuses coarse- and fine-grained representations of entity knowledge from knowledge graphs to conduct fake news detection. They design a method to prune subgraphs, and then use attention mechanism to obtain knowledge representation. By combining it with a textual representation, KAPALM could further improve the detection performance.\n\nreasons_to_accept: \u2022This work copes with the fake news classification task, which is useful in practice.  \u2022The author proposed an effective model for the fake news detection task and verified its effectiveness on the public test set\n\nreasons_to_reject: \u2022The motivation stated in the abstract is not solid enough, and the method proposed in the paper is not novel enough. For the statement \" majority of these methods focus on news entity information and ignore the structured knowledge among news entities\" in the abstract, some existing methods already do this, like CompareNet[1].\n\u2022The comparison of the proposed method with the baseline is not fair enough. The Bert-base-with-adapter used in the article is inconsistent with the Roberta used by KPL, and it is difficult to prove that KAPALM is more effective than KPL. What is the motivation for Bert-base-with-adapter?\n\u2022The coarse-grained and fine-grained knowledge mentioned in the article is not clear enough. The knowledge pruning method proposed in the article is intuitive.\n[1]. Hu L, Yang T, Zhang L, et al. Compare to the knowledge: Graph neural fake news detection with external knowledge[C], in ACL2021.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 1: Poor: I cannot identify the contributions of this paper, or I believe the claims are not sufficiently backed up by evidence. I would fight to have it rejected.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "UT8FMj1NCh",
        "similarity": 0.7307,
        "coverage": 0.6053,
        "human_length": 821,
        "human_text": "paper_topic_and_main_contributions: Identifying fake news is a highly relevant topic and one that is at the core of NLP given the importance that written text contributes to solving the task. As such it presents a great topical match for EMNLP.  The authors argue that identifying entities in some text and incorporating relationships between entities drawn from some external knowledge graphs might help in this classification task. The architecture that is being proposed is one that generates embeddings from three different angles: the actual text, an entity graph based on entities in the text and related entities in some external graph, a pruned version of this graph. The three embeddings resulting from this process are concatenated before being fed into a binary classifier.\nExperiments on the FakeNewsNet dataset are reported.\n\nreasons_to_accept: - This is a highly relevant research area, and any work that pushes our understanding as to how to identify misinformation of any form is a step forward.\n- The work is conducted on a well-known benchmark dataset and is therefore easily contextualisable with other work in the field.  - Conducting an ablation study is a strong aspect of the experimental work as it offers more nuanced insights into what components of the architecture contribute how much  to the overall performance.\n\nreasons_to_reject: There are a number of weaknesses in this work. Among the biggest issues I consider the following:  - Weak baselines: Given the chosen dataset contains much more information than the textual content I would want to see the results of this work compared to the state of the art reported in the literature that looks at the same dataset. To pick just one recent example [1], the results reported (such as F1) appear to be way higher than anything in Table 2 (and that is also true for the baselines reported in [1]). What is needed is a comparison against what the current state of the art is as reported in the literature (ideally reproduced to conduct significance tests where appropriate).  - Only a single dataset is used to explore the problem (well, it is two different parts but in the end it is one fairly specific dataset). There are many more benchmark datasets for text classification (including fake news detection) that could be included to provide more confidence in the findings.  - There are many missing details (and no supplementary material such as code) making it impossible to replicate the work. For example, unless I have overlooked it I cannot see what knowledge graphs are actually being used.  - There are no statistical significance tests (and terms such as \"outperform\" should therefore not be used)  [1] Donabauer \"Exploring Fake News Detection with Heterogeneous Social Media Context Graphs\". ECIR 2023.\n\nquestions_for_the_authors: (1) It is unclear to me that text properties alone (together with what text properties of external sources add to that) would be sufficient to distinguish fabricated news from real news. This may be true for some datasets but I imagine that with more effort put into creating stories (and more sophisticated tools such as those from the GPT family) it would be impossible to distinguish the two classes without incorporating more signals (such as contextual features). What is the underlying intuition in your work? Do you argue that even with the widepread adoption of ChatGPT et al. it will be possible to distinguish fake news from real news based on text (+ entities) alone?   (2) Given you adopt a dataset that includes many more signals I wonder why you do not include all these and THEN try to push forward the state of the art by going beyond what you can achieve. What is the reason to only try to improve on the text classification alone?  (3) Is there a need to include all the baselines? I have the impression that none of 1 - 7 are actually needed as we can assume for BERT to be a stronger baseline (it is also not a surprise that the results demonstrate yet again that traditional methods do not appear to be competitive with neural methods).\n(4) Why did you not include a discussion of ethical issues? Given the topic this would have been a helpful addition.\n\nmissing_references: You should include references that represent the state of the art in respect to whatever dataset(s) you have chosen.\nMake sure you include peer-reviewed versions of a paper where this is appropriate (e.g. the BERT paper).\n\ntypos_grammar_style_and_presentation_improvements: The paper needs to be properly proofread. It has many language problems throughout.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "m18NpFwGiw",
        "similarity": 0.6798,
        "coverage": 0.9,
        "human_length": 201,
        "human_text": "paper_topic_and_main_contributions: The paper investigate to integrate structured knowledge among entities in the news article to enhance the fake news detection. The proposed method outperforms previous KG related approaches that do not utilise structured information. It also shows promising results in the few-shot setting.\n\nreasons_to_accept: The approach is novel and achieves SOTA on two datasets with full scale training, and show promising performance in the few-shot setting.\n\nreasons_to_reject: Lack statistical significance tests.\n\nquestions_for_the_authors: Did you conduct error analysis to explore the reason why your proposed method is worse than KPL in the few shot setting on Politifact, but better than KPL on Gossipcop?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "6CzNgmsIqj",
        "length": 315,
        "human_text": "paper_topic_and_main_contributions: The authors propose a knowledge graph enhanced language model (KAPALM) that fuses coarse- and fine-grained representations of entity knowledge from knowledge graphs to conduct fake news detection. They design a method to prune subgraphs, and then use attention mechanism to obtain knowledge representation. By combining it with a textual representation, KAPALM could further improve the detection performance.\n\nreasons_to_accept: \u2022This work copes with the fake news classification task, which is useful in practice.  \u2022The author proposed an effective model for the fake news detection task and verified its effectiveness on the public test set\n\nreasons_to_reject: \u2022The motivation stated in the abstract is not solid enough, and the method proposed in the paper is not novel enough. For the statement \" majority of these methods focus on news entity information and ignore the structured knowledge among news entities\" in the abstract, some existing methods already do this, like CompareNet[1].\n\u2022The comparison of the proposed method with the baseline is not fair enough. The Bert-base-with-adapter used in the article is inconsistent with the Roberta used by KPL, and it is difficult to prove that KAPALM is more effective than KPL. What is the motivation for Bert-base-with-adapter?\n\u2022The coarse-grained and fine-grained knowledge mentioned in the article is not clear enough. The knowledge pruning method proposed in the article is intuitive.\n[1]. Hu L, Yang T, Zhang L, et al. Compare to the knowledge: Graph neural fake news detection with external knowledge[C], in ACL2021.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 1: Poor: I cannot identify the contributions of this paper, or I believe the claims are not sufficiently backed up by evidence. I would fight to have it rejected.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "UT8FMj1NCh",
        "length": 821,
        "human_text": "paper_topic_and_main_contributions: Identifying fake news is a highly relevant topic and one that is at the core of NLP given the importance that written text contributes to solving the task. As such it presents a great topical match for EMNLP.  The authors argue that identifying entities in some text and incorporating relationships between entities drawn from some external knowledge graphs might help in this classification task. The architecture that is being proposed is one that generates embeddings from three different angles: the actual text, an entity graph based on entities in the text and related entities in some external graph, a pruned version of this graph. The three embeddings resulting from this process are concatenated before being fed into a binary classifier.\nExperiments on the FakeNewsNet dataset are reported.\n\nreasons_to_accept: - This is a highly relevant research area, and any work that pushes our understanding as to how to identify misinformation of any form is a step forward.\n- The work is conducted on a well-known benchmark dataset and is therefore easily contextualisable with other work in the field.  - Conducting an ablation study is a strong aspect of the experimental work as it offers more nuanced insights into what components of the architecture contribute how much  to the overall performance.\n\nreasons_to_reject: There are a number of weaknesses in this work. Among the biggest issues I consider the following:  - Weak baselines: Given the chosen dataset contains much more information than the textual content I would want to see the results of this work compared to the state of the art reported in the literature that looks at the same dataset. To pick just one recent example [1], the results reported (such as F1) appear to be way higher than anything in Table 2 (and that is also true for the baselines reported in [1]). What is needed is a comparison against what the current state of the art is as reported in the literature (ideally reproduced to conduct significance tests where appropriate).  - Only a single dataset is used to explore the problem (well, it is two different parts but in the end it is one fairly specific dataset). There are many more benchmark datasets for text classification (including fake news detection) that could be included to provide more confidence in the findings.  - There are many missing details (and no supplementary material such as code) making it impossible to replicate the work. For example, unless I have overlooked it I cannot see what knowledge graphs are actually being used.  - There are no statistical significance tests (and terms such as \"outperform\" should therefore not be used)  [1] Donabauer \"Exploring Fake News Detection with Heterogeneous Social Media Context Graphs\". ECIR 2023.\n\nquestions_for_the_authors: (1) It is unclear to me that text properties alone (together with what text properties of external sources add to that) would be sufficient to distinguish fabricated news from real news. This may be true for some datasets but I imagine that with more effort put into creating stories (and more sophisticated tools such as those from the GPT family) it would be impossible to distinguish the two classes without incorporating more signals (such as contextual features). What is the underlying intuition in your work? Do you argue that even with the widepread adoption of ChatGPT et al. it will be possible to distinguish fake news from real news based on text (+ entities) alone?   (2) Given you adopt a dataset that includes many more signals I wonder why you do not include all these and THEN try to push forward the state of the art by going beyond what you can achieve. What is the reason to only try to improve on the text classification alone?  (3) Is there a need to include all the baselines? I have the impression that none of 1 - 7 are actually needed as we can assume for BERT to be a stronger baseline (it is also not a surprise that the results demonstrate yet again that traditional methods do not appear to be competitive with neural methods).\n(4) Why did you not include a discussion of ethical issues? Given the topic this would have been a helpful addition.\n\nmissing_references: You should include references that represent the state of the art in respect to whatever dataset(s) you have chosen.\nMake sure you include peer-reviewed versions of a paper where this is appropriate (e.g. the BERT paper).\n\ntypos_grammar_style_and_presentation_improvements: The paper needs to be properly proofread. It has many language problems throughout.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "m18NpFwGiw",
        "length": 201,
        "human_text": "paper_topic_and_main_contributions: The paper investigate to integrate structured knowledge among entities in the news article to enhance the fake news detection. The proposed method outperforms previous KG related approaches that do not utilise structured information. It also shows promising results in the few-shot setting.\n\nreasons_to_accept: The approach is novel and achieves SOTA on two datasets with full scale training, and show promising performance in the few-shot setting.\n\nreasons_to_reject: Lack statistical significance tests.\n\nquestions_for_the_authors: Did you conduct error analysis to explore the reason why your proposed method is worse than KPL in the few shot setting on Politifact, but better than KPL on Gossipcop?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "89_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_89_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7289333333333333,
      "max_similarity": 0.7387,
      "avg_coverage": 0.5890666666666666,
      "max_coverage": 0.6667
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 774,
      "avg_human_length": 378.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 11,
      "suggestions_count": 12
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "7kuXn7zbc2",
        "similarity": 0.7139,
        "coverage": 0.6667,
        "human_length": 322,
        "human_text": "paper_topic_and_main_contributions: This paper explores the performance gap of LMs between answering compositional questions and their sub-questions. The authors discover that the growing size of LMs could not narrow the gap. The authors then find that the elicitive prompting (e.g., chain-of-thought prompting) could narrow the gap, and propose a better version of it (using search engines to answer sub-questions) for improvement. In experiments, two existing QA datasets and one newly proposed dataset (around hundreds of questions) are used for evaluation. Results show that the proposed prompting method can improve the performance in answering compositional questions more or less.\nThe main contributions here are two findings mentioned above, a newly proposed elecitive prompting method, and a new multi-hop QA dataset.\n\nreasons_to_accept: This paper discusses an interesting and important topic for LMs: whether pretrained LLMs can solve compositional tasks well. The findings indicate that enlarging the size of LMs would not help them much to achieve higher compositional ability (i.e., narrowing the compositional gap). It can only let LMs learn to answer the one-hop sub-questions better. But designing better elecitive prompt could help more or less. \nThese findings are interesting and could be inspiring for the community.\n\nreasons_to_reject: Based on the findings, the contribution of the proposed prompting method is marginal. First of all, decomposing compositional questions and using search engines is not a new idea. Second, experiment results cannot clearly indicate that the proposed prompting method can effectively narrow the compositional gap. They only show that it could bring slight performance improvement.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "bwSwcSyIMX",
        "similarity": 0.7387,
        "coverage": 0.4815,
        "human_length": 424,
        "human_text": "paper_topic_and_main_contributions: This work focuses on the compositional reasoning ability of large language models, and makes contributions in both evaluation and method. For evaluation, the main idea of this work is to evaluate the model's ability on two-hop questions where each individual hop is easy to understand. This work also constructs two new datasets for such evaluation, one is CC, and the other is Bamboogle. Method-wise, this work proposes the self-ask prompting method. Specifically, it encourages the model to decompose the original question and ask follow-up questions when needed. This process can also be extended to include an external search step. In the experiments on multi-hop reasoning datasets, self-ask shows better performance than chain-of-thought (further gain when using external search), and similar performance to least-to-most with better speed.\n\nreasons_to_accept: 1. This is a straightforward idea and it shows decent improvement in the evaluation. Its compatibility with external searching steps brings huge potential to this method. \n2. The evaluation idea to evaluate compositionally with 2-hop questions is neat. This work also provides two new datasets for this type of evaluation.\n\nreasons_to_reject: 1. All the analyses and experiments are conducted with GPT-3 models. This is fine for the self-ask experiments, but having results from more models can make the observation of \"the compositional gap\" more convincing, as the current results may be influenced by some e GPT-3 specific training data or tricks. \n2. While the self-ask prompting style really fits multi-hop reasoning questions, it's unclear whether these ideas will work for implicit multi-hop questions (i.e., the multi-hop decomposition is not clear just from the question) or more generally, implicit multi-step reasoning questions. Having those evaluations can further strengthen this paper. Also I find it pretty strange that as the first dataset mentioned in this draft, self-ask results on CC can only be seen in the Appendix and is not compared to other baselines.\n\nquestions_for_the_authors: 1. In Table 6, why do the right/wrong probabilities do not sum up to one? And in general, why does the model performance vary significantly across different question types? Can the difference be explained by different confidences (as in line 222-226)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "sqJ29Tesoo",
        "similarity": 0.7342,
        "coverage": 0.619,
        "human_length": 389,
        "human_text": "paper_topic_and_main_contributions: This paper studies the compositional reasoning ability of LLMs, especially the GPT-3 family. The paper introduces the notion of the compositionality gap to study whether LLMs can obtain stronger compositional ability as the scale increases. The authors also proposed a new method, i.e., self-ask, that achieves improvement over chain-of-thought.\n\nreasons_to_accept: - This work studies the compositionality gap in multi-hop reasoning tasks. The authors present several interesting findings that can shed light on future research on compositional reasoning and emergent abilities in LLMs.\n- This work introduces Compositional Celebrities (CC) and Bamboogle datasets to study the compositionality gap and they can serve as useful resources for the community.\n- The paper proposed the self-ask elicitive prompting which effectively narrows down the compositionality gap in larger LMs.\n\nreasons_to_reject: - More qualitative analysis on why and how elicitive prompting narrows down the compositionality gap can shed more light on related research.\n- The study mainly focuses on 2-hop reasoning and it would be more interesting to look into the compositionality that involves more than 2 hops.\n- The paper only validates the problem in GPT models. It\u2019s not clear if the conclusions drawn in this paper can still hold true in other LLMs, such as Flan-T5.\n- The novelty of self-ask prompting is somewhat limited as there\u2019s much recent literature proposing similar ideas.\n\nquestions_for_the_authors: - Do you have any analysis on how many follow-up questions the LLMs ask? Will this statistic relate to the size of language models?\n- Do you have any analysis or evaluation that compares the quality of intermediate reasoning steps between CoT and self-ask?\n\ntypos_grammar_style_and_presentation_improvements: - The authors can consider putting the important experiment results into the main body (e.g., how self-ask narrows the compositionality gap in Figure 6). The current presentation in the main body looks lack of enough empirical analysis.\n- The writing can be more concise and has a more clear flow.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "7kuXn7zbc2",
        "length": 322,
        "human_text": "paper_topic_and_main_contributions: This paper explores the performance gap of LMs between answering compositional questions and their sub-questions. The authors discover that the growing size of LMs could not narrow the gap. The authors then find that the elicitive prompting (e.g., chain-of-thought prompting) could narrow the gap, and propose a better version of it (using search engines to answer sub-questions) for improvement. In experiments, two existing QA datasets and one newly proposed dataset (around hundreds of questions) are used for evaluation. Results show that the proposed prompting method can improve the performance in answering compositional questions more or less.\nThe main contributions here are two findings mentioned above, a newly proposed elecitive prompting method, and a new multi-hop QA dataset.\n\nreasons_to_accept: This paper discusses an interesting and important topic for LMs: whether pretrained LLMs can solve compositional tasks well. The findings indicate that enlarging the size of LMs would not help them much to achieve higher compositional ability (i.e., narrowing the compositional gap). It can only let LMs learn to answer the one-hop sub-questions better. But designing better elecitive prompt could help more or less. \nThese findings are interesting and could be inspiring for the community.\n\nreasons_to_reject: Based on the findings, the contribution of the proposed prompting method is marginal. First of all, decomposing compositional questions and using search engines is not a new idea. Second, experiment results cannot clearly indicate that the proposed prompting method can effectively narrow the compositional gap. They only show that it could bring slight performance improvement.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "bwSwcSyIMX",
        "length": 424,
        "human_text": "paper_topic_and_main_contributions: This work focuses on the compositional reasoning ability of large language models, and makes contributions in both evaluation and method. For evaluation, the main idea of this work is to evaluate the model's ability on two-hop questions where each individual hop is easy to understand. This work also constructs two new datasets for such evaluation, one is CC, and the other is Bamboogle. Method-wise, this work proposes the self-ask prompting method. Specifically, it encourages the model to decompose the original question and ask follow-up questions when needed. This process can also be extended to include an external search step. In the experiments on multi-hop reasoning datasets, self-ask shows better performance than chain-of-thought (further gain when using external search), and similar performance to least-to-most with better speed.\n\nreasons_to_accept: 1. This is a straightforward idea and it shows decent improvement in the evaluation. Its compatibility with external searching steps brings huge potential to this method. \n2. The evaluation idea to evaluate compositionally with 2-hop questions is neat. This work also provides two new datasets for this type of evaluation.\n\nreasons_to_reject: 1. All the analyses and experiments are conducted with GPT-3 models. This is fine for the self-ask experiments, but having results from more models can make the observation of \"the compositional gap\" more convincing, as the current results may be influenced by some e GPT-3 specific training data or tricks. \n2. While the self-ask prompting style really fits multi-hop reasoning questions, it's unclear whether these ideas will work for implicit multi-hop questions (i.e., the multi-hop decomposition is not clear just from the question) or more generally, implicit multi-step reasoning questions. Having those evaluations can further strengthen this paper. Also I find it pretty strange that as the first dataset mentioned in this draft, self-ask results on CC can only be seen in the Appendix and is not compared to other baselines.\n\nquestions_for_the_authors: 1. In Table 6, why do the right/wrong probabilities do not sum up to one? And in general, why does the model performance vary significantly across different question types? Can the difference be explained by different confidences (as in line 222-226)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "sqJ29Tesoo",
        "length": 389,
        "human_text": "paper_topic_and_main_contributions: This paper studies the compositional reasoning ability of LLMs, especially the GPT-3 family. The paper introduces the notion of the compositionality gap to study whether LLMs can obtain stronger compositional ability as the scale increases. The authors also proposed a new method, i.e., self-ask, that achieves improvement over chain-of-thought.\n\nreasons_to_accept: - This work studies the compositionality gap in multi-hop reasoning tasks. The authors present several interesting findings that can shed light on future research on compositional reasoning and emergent abilities in LLMs.\n- This work introduces Compositional Celebrities (CC) and Bamboogle datasets to study the compositionality gap and they can serve as useful resources for the community.\n- The paper proposed the self-ask elicitive prompting which effectively narrows down the compositionality gap in larger LMs.\n\nreasons_to_reject: - More qualitative analysis on why and how elicitive prompting narrows down the compositionality gap can shed more light on related research.\n- The study mainly focuses on 2-hop reasoning and it would be more interesting to look into the compositionality that involves more than 2 hops.\n- The paper only validates the problem in GPT models. It\u2019s not clear if the conclusions drawn in this paper can still hold true in other LLMs, such as Flan-T5.\n- The novelty of self-ask prompting is somewhat limited as there\u2019s much recent literature proposing similar ideas.\n\nquestions_for_the_authors: - Do you have any analysis on how many follow-up questions the LLMs ask? Will this statistic relate to the size of language models?\n- Do you have any analysis or evaluation that compares the quality of intermediate reasoning steps between CoT and self-ask?\n\ntypos_grammar_style_and_presentation_improvements: - The authors can consider putting the important experiment results into the main body (e.g., how self-ask narrows the compositionality gap in Figure 6). The current presentation in the main body looks lack of enough empirical analysis.\n- The writing can be more concise and has a more clear flow.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "89_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_89_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7266333333333334,
      "max_similarity": 0.7363,
      "avg_coverage": 0.6075666666666667,
      "max_coverage": 0.7222
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 650,
      "avg_human_length": 378.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 7,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "7kuXn7zbc2",
        "similarity": 0.7135,
        "coverage": 0.7222,
        "human_length": 322,
        "human_text": "paper_topic_and_main_contributions: This paper explores the performance gap of LMs between answering compositional questions and their sub-questions. The authors discover that the growing size of LMs could not narrow the gap. The authors then find that the elicitive prompting (e.g., chain-of-thought prompting) could narrow the gap, and propose a better version of it (using search engines to answer sub-questions) for improvement. In experiments, two existing QA datasets and one newly proposed dataset (around hundreds of questions) are used for evaluation. Results show that the proposed prompting method can improve the performance in answering compositional questions more or less.\nThe main contributions here are two findings mentioned above, a newly proposed elecitive prompting method, and a new multi-hop QA dataset.\n\nreasons_to_accept: This paper discusses an interesting and important topic for LMs: whether pretrained LLMs can solve compositional tasks well. The findings indicate that enlarging the size of LMs would not help them much to achieve higher compositional ability (i.e., narrowing the compositional gap). It can only let LMs learn to answer the one-hop sub-questions better. But designing better elecitive prompt could help more or less. \nThese findings are interesting and could be inspiring for the community.\n\nreasons_to_reject: Based on the findings, the contribution of the proposed prompting method is marginal. First of all, decomposing compositional questions and using search engines is not a new idea. Second, experiment results cannot clearly indicate that the proposed prompting method can effectively narrow the compositional gap. They only show that it could bring slight performance improvement.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "bwSwcSyIMX",
        "similarity": 0.7363,
        "coverage": 0.4815,
        "human_length": 424,
        "human_text": "paper_topic_and_main_contributions: This work focuses on the compositional reasoning ability of large language models, and makes contributions in both evaluation and method. For evaluation, the main idea of this work is to evaluate the model's ability on two-hop questions where each individual hop is easy to understand. This work also constructs two new datasets for such evaluation, one is CC, and the other is Bamboogle. Method-wise, this work proposes the self-ask prompting method. Specifically, it encourages the model to decompose the original question and ask follow-up questions when needed. This process can also be extended to include an external search step. In the experiments on multi-hop reasoning datasets, self-ask shows better performance than chain-of-thought (further gain when using external search), and similar performance to least-to-most with better speed.\n\nreasons_to_accept: 1. This is a straightforward idea and it shows decent improvement in the evaluation. Its compatibility with external searching steps brings huge potential to this method. \n2. The evaluation idea to evaluate compositionally with 2-hop questions is neat. This work also provides two new datasets for this type of evaluation.\n\nreasons_to_reject: 1. All the analyses and experiments are conducted with GPT-3 models. This is fine for the self-ask experiments, but having results from more models can make the observation of \"the compositional gap\" more convincing, as the current results may be influenced by some e GPT-3 specific training data or tricks. \n2. While the self-ask prompting style really fits multi-hop reasoning questions, it's unclear whether these ideas will work for implicit multi-hop questions (i.e., the multi-hop decomposition is not clear just from the question) or more generally, implicit multi-step reasoning questions. Having those evaluations can further strengthen this paper. Also I find it pretty strange that as the first dataset mentioned in this draft, self-ask results on CC can only be seen in the Appendix and is not compared to other baselines.\n\nquestions_for_the_authors: 1. In Table 6, why do the right/wrong probabilities do not sum up to one? And in general, why does the model performance vary significantly across different question types? Can the difference be explained by different confidences (as in line 222-226)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "sqJ29Tesoo",
        "similarity": 0.7301,
        "coverage": 0.619,
        "human_length": 389,
        "human_text": "paper_topic_and_main_contributions: This paper studies the compositional reasoning ability of LLMs, especially the GPT-3 family. The paper introduces the notion of the compositionality gap to study whether LLMs can obtain stronger compositional ability as the scale increases. The authors also proposed a new method, i.e., self-ask, that achieves improvement over chain-of-thought.\n\nreasons_to_accept: - This work studies the compositionality gap in multi-hop reasoning tasks. The authors present several interesting findings that can shed light on future research on compositional reasoning and emergent abilities in LLMs.\n- This work introduces Compositional Celebrities (CC) and Bamboogle datasets to study the compositionality gap and they can serve as useful resources for the community.\n- The paper proposed the self-ask elicitive prompting which effectively narrows down the compositionality gap in larger LMs.\n\nreasons_to_reject: - More qualitative analysis on why and how elicitive prompting narrows down the compositionality gap can shed more light on related research.\n- The study mainly focuses on 2-hop reasoning and it would be more interesting to look into the compositionality that involves more than 2 hops.\n- The paper only validates the problem in GPT models. It\u2019s not clear if the conclusions drawn in this paper can still hold true in other LLMs, such as Flan-T5.\n- The novelty of self-ask prompting is somewhat limited as there\u2019s much recent literature proposing similar ideas.\n\nquestions_for_the_authors: - Do you have any analysis on how many follow-up questions the LLMs ask? Will this statistic relate to the size of language models?\n- Do you have any analysis or evaluation that compares the quality of intermediate reasoning steps between CoT and self-ask?\n\ntypos_grammar_style_and_presentation_improvements: - The authors can consider putting the important experiment results into the main body (e.g., how self-ask narrows the compositionality gap in Figure 6). The current presentation in the main body looks lack of enough empirical analysis.\n- The writing can be more concise and has a more clear flow.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "7kuXn7zbc2",
        "length": 322,
        "human_text": "paper_topic_and_main_contributions: This paper explores the performance gap of LMs between answering compositional questions and their sub-questions. The authors discover that the growing size of LMs could not narrow the gap. The authors then find that the elicitive prompting (e.g., chain-of-thought prompting) could narrow the gap, and propose a better version of it (using search engines to answer sub-questions) for improvement. In experiments, two existing QA datasets and one newly proposed dataset (around hundreds of questions) are used for evaluation. Results show that the proposed prompting method can improve the performance in answering compositional questions more or less.\nThe main contributions here are two findings mentioned above, a newly proposed elecitive prompting method, and a new multi-hop QA dataset.\n\nreasons_to_accept: This paper discusses an interesting and important topic for LMs: whether pretrained LLMs can solve compositional tasks well. The findings indicate that enlarging the size of LMs would not help them much to achieve higher compositional ability (i.e., narrowing the compositional gap). It can only let LMs learn to answer the one-hop sub-questions better. But designing better elecitive prompt could help more or less. \nThese findings are interesting and could be inspiring for the community.\n\nreasons_to_reject: Based on the findings, the contribution of the proposed prompting method is marginal. First of all, decomposing compositional questions and using search engines is not a new idea. Second, experiment results cannot clearly indicate that the proposed prompting method can effectively narrow the compositional gap. They only show that it could bring slight performance improvement.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "bwSwcSyIMX",
        "length": 424,
        "human_text": "paper_topic_and_main_contributions: This work focuses on the compositional reasoning ability of large language models, and makes contributions in both evaluation and method. For evaluation, the main idea of this work is to evaluate the model's ability on two-hop questions where each individual hop is easy to understand. This work also constructs two new datasets for such evaluation, one is CC, and the other is Bamboogle. Method-wise, this work proposes the self-ask prompting method. Specifically, it encourages the model to decompose the original question and ask follow-up questions when needed. This process can also be extended to include an external search step. In the experiments on multi-hop reasoning datasets, self-ask shows better performance than chain-of-thought (further gain when using external search), and similar performance to least-to-most with better speed.\n\nreasons_to_accept: 1. This is a straightforward idea and it shows decent improvement in the evaluation. Its compatibility with external searching steps brings huge potential to this method. \n2. The evaluation idea to evaluate compositionally with 2-hop questions is neat. This work also provides two new datasets for this type of evaluation.\n\nreasons_to_reject: 1. All the analyses and experiments are conducted with GPT-3 models. This is fine for the self-ask experiments, but having results from more models can make the observation of \"the compositional gap\" more convincing, as the current results may be influenced by some e GPT-3 specific training data or tricks. \n2. While the self-ask prompting style really fits multi-hop reasoning questions, it's unclear whether these ideas will work for implicit multi-hop questions (i.e., the multi-hop decomposition is not clear just from the question) or more generally, implicit multi-step reasoning questions. Having those evaluations can further strengthen this paper. Also I find it pretty strange that as the first dataset mentioned in this draft, self-ask results on CC can only be seen in the Appendix and is not compared to other baselines.\n\nquestions_for_the_authors: 1. In Table 6, why do the right/wrong probabilities do not sum up to one? And in general, why does the model performance vary significantly across different question types? Can the difference be explained by different confidences (as in line 222-226)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "sqJ29Tesoo",
        "length": 389,
        "human_text": "paper_topic_and_main_contributions: This paper studies the compositional reasoning ability of LLMs, especially the GPT-3 family. The paper introduces the notion of the compositionality gap to study whether LLMs can obtain stronger compositional ability as the scale increases. The authors also proposed a new method, i.e., self-ask, that achieves improvement over chain-of-thought.\n\nreasons_to_accept: - This work studies the compositionality gap in multi-hop reasoning tasks. The authors present several interesting findings that can shed light on future research on compositional reasoning and emergent abilities in LLMs.\n- This work introduces Compositional Celebrities (CC) and Bamboogle datasets to study the compositionality gap and they can serve as useful resources for the community.\n- The paper proposed the self-ask elicitive prompting which effectively narrows down the compositionality gap in larger LMs.\n\nreasons_to_reject: - More qualitative analysis on why and how elicitive prompting narrows down the compositionality gap can shed more light on related research.\n- The study mainly focuses on 2-hop reasoning and it would be more interesting to look into the compositionality that involves more than 2 hops.\n- The paper only validates the problem in GPT models. It\u2019s not clear if the conclusions drawn in this paper can still hold true in other LLMs, such as Flan-T5.\n- The novelty of self-ask prompting is somewhat limited as there\u2019s much recent literature proposing similar ideas.\n\nquestions_for_the_authors: - Do you have any analysis on how many follow-up questions the LLMs ask? Will this statistic relate to the size of language models?\n- Do you have any analysis or evaluation that compares the quality of intermediate reasoning steps between CoT and self-ask?\n\ntypos_grammar_style_and_presentation_improvements: - The authors can consider putting the important experiment results into the main body (e.g., how self-ask narrows the compositionality gap in Figure 6). The current presentation in the main body looks lack of enough empirical analysis.\n- The writing can be more concise and has a more clear flow.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "204_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_204_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7248,
      "max_similarity": 0.7334,
      "avg_coverage": 0.543,
      "max_coverage": 0.6207
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 524,
      "avg_human_length": 476.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "StwZBvrzNp",
        "similarity": 0.7334,
        "coverage": 0.5833,
        "human_length": 396,
        "human_text": "paper_topic_and_main_contributions: The work conducted a noise audit of nine offensive speech classifiers on a dataset of more than 92 million YouTube comments, revealing considerable variations in the results. Also, it annotated a dataset with views from people from different political parties regarding their perception of offense and also about vicarious offense, that is, to predict offense for others who do not share the same political belief. Finally, it analyzed human and machine moderators' agreement on what is offensive.\n\nreasons_to_accept: - The release of a novel dataset that can be valuable for further research regarding vicarious offense.\n- The annotation process is well explained.\n- Experimental evaluation is complete: comparing machine and human moderators, generating informative results, and discussing the impact of what was achieved.\n\nreasons_to_reject: - Since each machine moderator is trained on a different dataset, they are expected to have a low agreement. Training on a concatenation of datasets or evaluating more public APIs, such as Perspective API, would generate more informative noise audit results.\n- Some presentation problems make the last two sections difficult to follow. I suggested improvements in the corresponding section from review.\n\ntypos_grammar_style_and_presentation_improvements: - Please cite the Appendix subsections in the paper so we can refer easily to them.\n- Table 1 is not cited in the paper.\n- Line 381: The cited result (0.43) is extracted from the Appendix. Consider reorganizing so the result appears in the main paper.\n- Figure 3 caption is incorrect. A cell [i, j] is not referring to machine moderators' agreement since the figure is also showing human moderators' agreement.\n- Line 478: Figure reference is missing.\n- Figure 4 uses a different vicarious offense notation from what was previously explained in the paper (in lines 401-404).\n- The results using ChatGPT to predict the vicarious offense are in the Discussion and Conclusion section, which should be in the Results section.\n- In the Appendix, Figures 5, 6, and 7 are not cited.\n- In the Appendix, Section B.1 is empty.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "2nnMLbf8jr",
        "similarity": 0.7261,
        "coverage": 0.425,
        "human_length": 535,
        "human_text": "paper_topic_and_main_contributions: This work operates in the domain of political discourse, performing a large-scale study on the perception of offense from across the political spectra as well as from machine moderators. Further, a dataset produced by their study is released, labeled additionally for \"vicarious offense\".\n\nreasons_to_accept: 1. A novel study analyzing perceptions of offense from different political spectra, as well as alignment with machine learning models. This study is vital in current political discourse and discussion on hate speech.\n2. Description of crowdsourced annotators is thorough and gives a solid picture of the annotator pool.\n3. The list of machine moderators is extensive, leading to a thorough analysis.\n4. Findings are interesting and the experiments conducted robustly.\n\nreasons_to_reject: Nothing significant. See questions for comments/suggestions for improvement.\n\nquestions_for_the_authors: 1. I was wondering whether it would be useful to add an example case of vicarious offense in the introduction to better showcase this concept. Even though the definition is adequate, this is a new concept and would therefore benefit from a readily available example. Maybe you could repurpose the Fig. 1 caption to specifically point out what vicarious offense is.\n2. Can this study have a temporal element as well? It would be interesting to see how the findings evolve throughout the years.\n3. Is there a reason you did not use the Perspective API? You did use TCC (model 8 in your list), but I am wondering why the Perspective API was avoided. While I am not advocating for its use, a lot of contemporary work uses it so I found this decision interesting. If there is an explicit reason behind this, including it in the paper would be informative.\n4. For RQ2, I would have liked to have seen self-alignment as a baseline: what is the alignment within Democrats (Dem^Dem)? How well are Democrats at predicting what other Democrats find offensive? This would be interesting as a comparison.\n\nmissing_references: There are two works that perform studies on offense in political discourse: 1. Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection, Lara Grimminger, Roman Klinger, 2021. ( work collects offensive tweets from democrats and republicans targeting the other community) 2. Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments, Antonis Maronikolakis, Axel Wisiorek, Leah Nann, Haris Jabbar, Sahana Udupa, Hinrich Schuetze, 2022. ( one of the domains examined is hate speech in political discourse, collecting data targeting politicians, state, civil rights advocates, etc.)\nWhile these two differ from this work since they only collect data for hate speech in political discourse and do not perform a noise audit, I find they are still pertinent (slightly contradicting lines 167-169).\n\ntypos_grammar_style_and_presentation_improvements: lines 193 and 210 and 232: Usually footnotes are added after punctuation for more compact text. Such as this: text.1\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "LEA1pFGI0Q",
        "similarity": 0.7149,
        "coverage": 0.6207,
        "human_length": 497,
        "human_text": "paper_topic_and_main_contributions: The paper focuses on first-person offense and vicarious offense in US-based political conversations. The paper then investigates 3 research questions based on the alignment between machine-machine, human-human, and machine-human moderators to identify first-person and vicarious offense present in the conversations.\n\nreasons_to_accept: The idea is interesting and exciting. \nThe need to focus on vicarious offense is indeed important from the societal point of view. \nResults and discussions are properly backed by political theories. \nThe comparison of human-human moderators is well discussed and presented. \nOverall, the writing of the paper is impressive as it is well written and understood easily.\n\nreasons_to_reject: I am not convinced how authors have claimed about machine moderators. I feel that not enough importance is given to the preparation of machine moderators. Similar architectures of BERT /RoBERTa models are trained on different datasets and used as machine moderators to evaluate their performance on unseen political data. Due to the very simple architecture and the fact that I think these datasets are general datasets (with high imbalance) that are not entirely based on political data, it is possible that these models do not perform well on first-person offense detection and therefore have not been well aligned. I like that the authors in the discussion section used ChatGPT for identifying vicarious offense, but they have not shown how ChatGPT performs in identifying first-person offense compared to the models used in the study. Nevertheless, major claims are made about the inability of machine moderators to identify offense. In my opinion, these claims need to be backed up by better models, APIs such as Perspective API, or recent open-source/APIs-based large language models.\n\nquestions_for_the_authors: A. What is the motivation to use the generic datasets for the special case of identifying offenses in US-based political conversations? It is possible models may not learn well because the datasets do not provide enough information related to democrats, republicans, and independent groups based offense.\nB. Do the authors try to use few-shot ChatGPT/Perspective API or other open-source APIs for vicarious and first-person offense identification? It is possible the results might be changed greatly. However, it is still a possibility that can be understood after running the experiments.\n\nmissing_references: None in my opinion.\n\ntypos_grammar_style_and_presentation_improvements: I think that the illustrations and tables should be better placed. The figures that are mentioned on one page are located after 2 pages, which sometimes makes it difficult to follow them. It is always better to place the tables and figures before you mention them.\nTypos: Figure ?? in section 5.3 A.4, B.1 only headings are mentioned\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "StwZBvrzNp",
        "length": 396,
        "human_text": "paper_topic_and_main_contributions: The work conducted a noise audit of nine offensive speech classifiers on a dataset of more than 92 million YouTube comments, revealing considerable variations in the results. Also, it annotated a dataset with views from people from different political parties regarding their perception of offense and also about vicarious offense, that is, to predict offense for others who do not share the same political belief. Finally, it analyzed human and machine moderators' agreement on what is offensive.\n\nreasons_to_accept: - The release of a novel dataset that can be valuable for further research regarding vicarious offense.\n- The annotation process is well explained.\n- Experimental evaluation is complete: comparing machine and human moderators, generating informative results, and discussing the impact of what was achieved.\n\nreasons_to_reject: - Since each machine moderator is trained on a different dataset, they are expected to have a low agreement. Training on a concatenation of datasets or evaluating more public APIs, such as Perspective API, would generate more informative noise audit results.\n- Some presentation problems make the last two sections difficult to follow. I suggested improvements in the corresponding section from review.\n\ntypos_grammar_style_and_presentation_improvements: - Please cite the Appendix subsections in the paper so we can refer easily to them.\n- Table 1 is not cited in the paper.\n- Line 381: The cited result (0.43) is extracted from the Appendix. Consider reorganizing so the result appears in the main paper.\n- Figure 3 caption is incorrect. A cell [i, j] is not referring to machine moderators' agreement since the figure is also showing human moderators' agreement.\n- Line 478: Figure reference is missing.\n- Figure 4 uses a different vicarious offense notation from what was previously explained in the paper (in lines 401-404).\n- The results using ChatGPT to predict the vicarious offense are in the Discussion and Conclusion section, which should be in the Results section.\n- In the Appendix, Figures 5, 6, and 7 are not cited.\n- In the Appendix, Section B.1 is empty.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "2nnMLbf8jr",
        "length": 535,
        "human_text": "paper_topic_and_main_contributions: This work operates in the domain of political discourse, performing a large-scale study on the perception of offense from across the political spectra as well as from machine moderators. Further, a dataset produced by their study is released, labeled additionally for \"vicarious offense\".\n\nreasons_to_accept: 1. A novel study analyzing perceptions of offense from different political spectra, as well as alignment with machine learning models. This study is vital in current political discourse and discussion on hate speech.\n2. Description of crowdsourced annotators is thorough and gives a solid picture of the annotator pool.\n3. The list of machine moderators is extensive, leading to a thorough analysis.\n4. Findings are interesting and the experiments conducted robustly.\n\nreasons_to_reject: Nothing significant. See questions for comments/suggestions for improvement.\n\nquestions_for_the_authors: 1. I was wondering whether it would be useful to add an example case of vicarious offense in the introduction to better showcase this concept. Even though the definition is adequate, this is a new concept and would therefore benefit from a readily available example. Maybe you could repurpose the Fig. 1 caption to specifically point out what vicarious offense is.\n2. Can this study have a temporal element as well? It would be interesting to see how the findings evolve throughout the years.\n3. Is there a reason you did not use the Perspective API? You did use TCC (model 8 in your list), but I am wondering why the Perspective API was avoided. While I am not advocating for its use, a lot of contemporary work uses it so I found this decision interesting. If there is an explicit reason behind this, including it in the paper would be informative.\n4. For RQ2, I would have liked to have seen self-alignment as a baseline: what is the alignment within Democrats (Dem^Dem)? How well are Democrats at predicting what other Democrats find offensive? This would be interesting as a comparison.\n\nmissing_references: There are two works that perform studies on offense in political discourse: 1. Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection, Lara Grimminger, Roman Klinger, 2021. ( work collects offensive tweets from democrats and republicans targeting the other community) 2. Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments, Antonis Maronikolakis, Axel Wisiorek, Leah Nann, Haris Jabbar, Sahana Udupa, Hinrich Schuetze, 2022. ( one of the domains examined is hate speech in political discourse, collecting data targeting politicians, state, civil rights advocates, etc.)\nWhile these two differ from this work since they only collect data for hate speech in political discourse and do not perform a noise audit, I find they are still pertinent (slightly contradicting lines 167-169).\n\ntypos_grammar_style_and_presentation_improvements: lines 193 and 210 and 232: Usually footnotes are added after punctuation for more compact text. Such as this: text.1\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "LEA1pFGI0Q",
        "length": 497,
        "human_text": "paper_topic_and_main_contributions: The paper focuses on first-person offense and vicarious offense in US-based political conversations. The paper then investigates 3 research questions based on the alignment between machine-machine, human-human, and machine-human moderators to identify first-person and vicarious offense present in the conversations.\n\nreasons_to_accept: The idea is interesting and exciting. \nThe need to focus on vicarious offense is indeed important from the societal point of view. \nResults and discussions are properly backed by political theories. \nThe comparison of human-human moderators is well discussed and presented. \nOverall, the writing of the paper is impressive as it is well written and understood easily.\n\nreasons_to_reject: I am not convinced how authors have claimed about machine moderators. I feel that not enough importance is given to the preparation of machine moderators. Similar architectures of BERT /RoBERTa models are trained on different datasets and used as machine moderators to evaluate their performance on unseen political data. Due to the very simple architecture and the fact that I think these datasets are general datasets (with high imbalance) that are not entirely based on political data, it is possible that these models do not perform well on first-person offense detection and therefore have not been well aligned. I like that the authors in the discussion section used ChatGPT for identifying vicarious offense, but they have not shown how ChatGPT performs in identifying first-person offense compared to the models used in the study. Nevertheless, major claims are made about the inability of machine moderators to identify offense. In my opinion, these claims need to be backed up by better models, APIs such as Perspective API, or recent open-source/APIs-based large language models.\n\nquestions_for_the_authors: A. What is the motivation to use the generic datasets for the special case of identifying offenses in US-based political conversations? It is possible models may not learn well because the datasets do not provide enough information related to democrats, republicans, and independent groups based offense.\nB. Do the authors try to use few-shot ChatGPT/Perspective API or other open-source APIs for vicarious and first-person offense identification? It is possible the results might be changed greatly. However, it is still a possibility that can be understood after running the experiments.\n\nmissing_references: None in my opinion.\n\ntypos_grammar_style_and_presentation_improvements: I think that the illustrations and tables should be better placed. The figures that are mentioned on one page are located after 2 pages, which sometimes makes it difficult to follow them. It is always better to place the tables and figures before you mention them.\nTypos: Figure ?? in section 5.3 A.4, B.1 only headings are mentioned\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "204_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_204_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7260666666666666,
      "max_similarity": 0.7338,
      "avg_coverage": 0.4807333333333334,
      "max_coverage": 0.5172
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 427,
      "avg_human_length": 476.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 3
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "StwZBvrzNp",
        "similarity": 0.7338,
        "coverage": 0.5,
        "human_length": 396,
        "human_text": "paper_topic_and_main_contributions: The work conducted a noise audit of nine offensive speech classifiers on a dataset of more than 92 million YouTube comments, revealing considerable variations in the results. Also, it annotated a dataset with views from people from different political parties regarding their perception of offense and also about vicarious offense, that is, to predict offense for others who do not share the same political belief. Finally, it analyzed human and machine moderators' agreement on what is offensive.\n\nreasons_to_accept: - The release of a novel dataset that can be valuable for further research regarding vicarious offense.\n- The annotation process is well explained.\n- Experimental evaluation is complete: comparing machine and human moderators, generating informative results, and discussing the impact of what was achieved.\n\nreasons_to_reject: - Since each machine moderator is trained on a different dataset, they are expected to have a low agreement. Training on a concatenation of datasets or evaluating more public APIs, such as Perspective API, would generate more informative noise audit results.\n- Some presentation problems make the last two sections difficult to follow. I suggested improvements in the corresponding section from review.\n\ntypos_grammar_style_and_presentation_improvements: - Please cite the Appendix subsections in the paper so we can refer easily to them.\n- Table 1 is not cited in the paper.\n- Line 381: The cited result (0.43) is extracted from the Appendix. Consider reorganizing so the result appears in the main paper.\n- Figure 3 caption is incorrect. A cell [i, j] is not referring to machine moderators' agreement since the figure is also showing human moderators' agreement.\n- Line 478: Figure reference is missing.\n- Figure 4 uses a different vicarious offense notation from what was previously explained in the paper (in lines 401-404).\n- The results using ChatGPT to predict the vicarious offense are in the Discussion and Conclusion section, which should be in the Results section.\n- In the Appendix, Figures 5, 6, and 7 are not cited.\n- In the Appendix, Section B.1 is empty.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "2nnMLbf8jr",
        "similarity": 0.7266,
        "coverage": 0.425,
        "human_length": 535,
        "human_text": "paper_topic_and_main_contributions: This work operates in the domain of political discourse, performing a large-scale study on the perception of offense from across the political spectra as well as from machine moderators. Further, a dataset produced by their study is released, labeled additionally for \"vicarious offense\".\n\nreasons_to_accept: 1. A novel study analyzing perceptions of offense from different political spectra, as well as alignment with machine learning models. This study is vital in current political discourse and discussion on hate speech.\n2. Description of crowdsourced annotators is thorough and gives a solid picture of the annotator pool.\n3. The list of machine moderators is extensive, leading to a thorough analysis.\n4. Findings are interesting and the experiments conducted robustly.\n\nreasons_to_reject: Nothing significant. See questions for comments/suggestions for improvement.\n\nquestions_for_the_authors: 1. I was wondering whether it would be useful to add an example case of vicarious offense in the introduction to better showcase this concept. Even though the definition is adequate, this is a new concept and would therefore benefit from a readily available example. Maybe you could repurpose the Fig. 1 caption to specifically point out what vicarious offense is.\n2. Can this study have a temporal element as well? It would be interesting to see how the findings evolve throughout the years.\n3. Is there a reason you did not use the Perspective API? You did use TCC (model 8 in your list), but I am wondering why the Perspective API was avoided. While I am not advocating for its use, a lot of contemporary work uses it so I found this decision interesting. If there is an explicit reason behind this, including it in the paper would be informative.\n4. For RQ2, I would have liked to have seen self-alignment as a baseline: what is the alignment within Democrats (Dem^Dem)? How well are Democrats at predicting what other Democrats find offensive? This would be interesting as a comparison.\n\nmissing_references: There are two works that perform studies on offense in political discourse: 1. Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection, Lara Grimminger, Roman Klinger, 2021. ( work collects offensive tweets from democrats and republicans targeting the other community) 2. Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments, Antonis Maronikolakis, Axel Wisiorek, Leah Nann, Haris Jabbar, Sahana Udupa, Hinrich Schuetze, 2022. ( one of the domains examined is hate speech in political discourse, collecting data targeting politicians, state, civil rights advocates, etc.)\nWhile these two differ from this work since they only collect data for hate speech in political discourse and do not perform a noise audit, I find they are still pertinent (slightly contradicting lines 167-169).\n\ntypos_grammar_style_and_presentation_improvements: lines 193 and 210 and 232: Usually footnotes are added after punctuation for more compact text. Such as this: text.1\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "LEA1pFGI0Q",
        "similarity": 0.7178,
        "coverage": 0.5172,
        "human_length": 497,
        "human_text": "paper_topic_and_main_contributions: The paper focuses on first-person offense and vicarious offense in US-based political conversations. The paper then investigates 3 research questions based on the alignment between machine-machine, human-human, and machine-human moderators to identify first-person and vicarious offense present in the conversations.\n\nreasons_to_accept: The idea is interesting and exciting. \nThe need to focus on vicarious offense is indeed important from the societal point of view. \nResults and discussions are properly backed by political theories. \nThe comparison of human-human moderators is well discussed and presented. \nOverall, the writing of the paper is impressive as it is well written and understood easily.\n\nreasons_to_reject: I am not convinced how authors have claimed about machine moderators. I feel that not enough importance is given to the preparation of machine moderators. Similar architectures of BERT /RoBERTa models are trained on different datasets and used as machine moderators to evaluate their performance on unseen political data. Due to the very simple architecture and the fact that I think these datasets are general datasets (with high imbalance) that are not entirely based on political data, it is possible that these models do not perform well on first-person offense detection and therefore have not been well aligned. I like that the authors in the discussion section used ChatGPT for identifying vicarious offense, but they have not shown how ChatGPT performs in identifying first-person offense compared to the models used in the study. Nevertheless, major claims are made about the inability of machine moderators to identify offense. In my opinion, these claims need to be backed up by better models, APIs such as Perspective API, or recent open-source/APIs-based large language models.\n\nquestions_for_the_authors: A. What is the motivation to use the generic datasets for the special case of identifying offenses in US-based political conversations? It is possible models may not learn well because the datasets do not provide enough information related to democrats, republicans, and independent groups based offense.\nB. Do the authors try to use few-shot ChatGPT/Perspective API or other open-source APIs for vicarious and first-person offense identification? It is possible the results might be changed greatly. However, it is still a possibility that can be understood after running the experiments.\n\nmissing_references: None in my opinion.\n\ntypos_grammar_style_and_presentation_improvements: I think that the illustrations and tables should be better placed. The figures that are mentioned on one page are located after 2 pages, which sometimes makes it difficult to follow them. It is always better to place the tables and figures before you mention them.\nTypos: Figure ?? in section 5.3 A.4, B.1 only headings are mentioned\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "StwZBvrzNp",
        "length": 396,
        "human_text": "paper_topic_and_main_contributions: The work conducted a noise audit of nine offensive speech classifiers on a dataset of more than 92 million YouTube comments, revealing considerable variations in the results. Also, it annotated a dataset with views from people from different political parties regarding their perception of offense and also about vicarious offense, that is, to predict offense for others who do not share the same political belief. Finally, it analyzed human and machine moderators' agreement on what is offensive.\n\nreasons_to_accept: - The release of a novel dataset that can be valuable for further research regarding vicarious offense.\n- The annotation process is well explained.\n- Experimental evaluation is complete: comparing machine and human moderators, generating informative results, and discussing the impact of what was achieved.\n\nreasons_to_reject: - Since each machine moderator is trained on a different dataset, they are expected to have a low agreement. Training on a concatenation of datasets or evaluating more public APIs, such as Perspective API, would generate more informative noise audit results.\n- Some presentation problems make the last two sections difficult to follow. I suggested improvements in the corresponding section from review.\n\ntypos_grammar_style_and_presentation_improvements: - Please cite the Appendix subsections in the paper so we can refer easily to them.\n- Table 1 is not cited in the paper.\n- Line 381: The cited result (0.43) is extracted from the Appendix. Consider reorganizing so the result appears in the main paper.\n- Figure 3 caption is incorrect. A cell [i, j] is not referring to machine moderators' agreement since the figure is also showing human moderators' agreement.\n- Line 478: Figure reference is missing.\n- Figure 4 uses a different vicarious offense notation from what was previously explained in the paper (in lines 401-404).\n- The results using ChatGPT to predict the vicarious offense are in the Discussion and Conclusion section, which should be in the Results section.\n- In the Appendix, Figures 5, 6, and 7 are not cited.\n- In the Appendix, Section B.1 is empty.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "2nnMLbf8jr",
        "length": 535,
        "human_text": "paper_topic_and_main_contributions: This work operates in the domain of political discourse, performing a large-scale study on the perception of offense from across the political spectra as well as from machine moderators. Further, a dataset produced by their study is released, labeled additionally for \"vicarious offense\".\n\nreasons_to_accept: 1. A novel study analyzing perceptions of offense from different political spectra, as well as alignment with machine learning models. This study is vital in current political discourse and discussion on hate speech.\n2. Description of crowdsourced annotators is thorough and gives a solid picture of the annotator pool.\n3. The list of machine moderators is extensive, leading to a thorough analysis.\n4. Findings are interesting and the experiments conducted robustly.\n\nreasons_to_reject: Nothing significant. See questions for comments/suggestions for improvement.\n\nquestions_for_the_authors: 1. I was wondering whether it would be useful to add an example case of vicarious offense in the introduction to better showcase this concept. Even though the definition is adequate, this is a new concept and would therefore benefit from a readily available example. Maybe you could repurpose the Fig. 1 caption to specifically point out what vicarious offense is.\n2. Can this study have a temporal element as well? It would be interesting to see how the findings evolve throughout the years.\n3. Is there a reason you did not use the Perspective API? You did use TCC (model 8 in your list), but I am wondering why the Perspective API was avoided. While I am not advocating for its use, a lot of contemporary work uses it so I found this decision interesting. If there is an explicit reason behind this, including it in the paper would be informative.\n4. For RQ2, I would have liked to have seen self-alignment as a baseline: what is the alignment within Democrats (Dem^Dem)? How well are Democrats at predicting what other Democrats find offensive? This would be interesting as a comparison.\n\nmissing_references: There are two works that perform studies on offense in political discourse: 1. Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection, Lara Grimminger, Roman Klinger, 2021. ( work collects offensive tweets from democrats and republicans targeting the other community) 2. Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments, Antonis Maronikolakis, Axel Wisiorek, Leah Nann, Haris Jabbar, Sahana Udupa, Hinrich Schuetze, 2022. ( one of the domains examined is hate speech in political discourse, collecting data targeting politicians, state, civil rights advocates, etc.)\nWhile these two differ from this work since they only collect data for hate speech in political discourse and do not perform a noise audit, I find they are still pertinent (slightly contradicting lines 167-169).\n\ntypos_grammar_style_and_presentation_improvements: lines 193 and 210 and 232: Usually footnotes are added after punctuation for more compact text. Such as this: text.1\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "LEA1pFGI0Q",
        "length": 497,
        "human_text": "paper_topic_and_main_contributions: The paper focuses on first-person offense and vicarious offense in US-based political conversations. The paper then investigates 3 research questions based on the alignment between machine-machine, human-human, and machine-human moderators to identify first-person and vicarious offense present in the conversations.\n\nreasons_to_accept: The idea is interesting and exciting. \nThe need to focus on vicarious offense is indeed important from the societal point of view. \nResults and discussions are properly backed by political theories. \nThe comparison of human-human moderators is well discussed and presented. \nOverall, the writing of the paper is impressive as it is well written and understood easily.\n\nreasons_to_reject: I am not convinced how authors have claimed about machine moderators. I feel that not enough importance is given to the preparation of machine moderators. Similar architectures of BERT /RoBERTa models are trained on different datasets and used as machine moderators to evaluate their performance on unseen political data. Due to the very simple architecture and the fact that I think these datasets are general datasets (with high imbalance) that are not entirely based on political data, it is possible that these models do not perform well on first-person offense detection and therefore have not been well aligned. I like that the authors in the discussion section used ChatGPT for identifying vicarious offense, but they have not shown how ChatGPT performs in identifying first-person offense compared to the models used in the study. Nevertheless, major claims are made about the inability of machine moderators to identify offense. In my opinion, these claims need to be backed up by better models, APIs such as Perspective API, or recent open-source/APIs-based large language models.\n\nquestions_for_the_authors: A. What is the motivation to use the generic datasets for the special case of identifying offenses in US-based political conversations? It is possible models may not learn well because the datasets do not provide enough information related to democrats, republicans, and independent groups based offense.\nB. Do the authors try to use few-shot ChatGPT/Perspective API or other open-source APIs for vicarious and first-person offense identification? It is possible the results might be changed greatly. However, it is still a possibility that can be understood after running the experiments.\n\nmissing_references: None in my opinion.\n\ntypos_grammar_style_and_presentation_improvements: I think that the illustrations and tables should be better placed. The figures that are mentioned on one page are located after 2 pages, which sometimes makes it difficult to follow them. It is always better to place the tables and figures before you mention them.\nTypos: Figure ?? in section 5.3 A.4, B.1 only headings are mentioned\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "46_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_46_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7361666666666666,
      "max_similarity": 0.7482,
      "avg_coverage": 0.49990000000000007,
      "max_coverage": 0.6429
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 601,
      "avg_human_length": 320.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "ERc78IhebS",
        "similarity": 0.715,
        "coverage": 0.5333,
        "human_length": 234,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a new approach called Structured Semantic Alignment (SSA) to evaluate the accuracy and robustness of text-to-image generation models. The main contribution of the paper is the development of a new evaluation method that measures the semantic consistency between textual descriptions and generated images. The authors argue that existing metrics lack insights into the assessment of fine-grained semantic concepts, such as object attributes, context details, and semantic relationships. The experimental results demonstrate the effectiveness of the SSA approach in evaluating text-to-image generation models.\n\nreasons_to_accept: 1. The paper presents a well-motivated and novel approach to evaluate text-to-image generation models. \n2. The proposed approach is supported by experimental results that demonstrate its effectiveness in measuring semantic consistency between textual descriptions and generated images. \n3. The paper is well-written and clearly presents the motivation, approach, and experimental results.\n\nreasons_to_reject: The paper could benefit from a more detailed discussion of how it compares to other evaluation methods and a more detailed analysis of the experimental results.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "nb52FLzBo2",
        "similarity": 0.7482,
        "coverage": 0.6429,
        "human_length": 268,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on evaluating text-to-image generation models. Specifically, this paper propose a new evaluation metric named Structured Semantic Alignment (SSA) to evaluate the semantic consistency between text and generated images. SSA first projects text/image features into a shared embedding space by text parsing and scene graph generation respectively. Then SSA is learned by contrastive learning. The positive and negative samples are generated by substituting words with semantically equivalent or nonequivalent alternatives.\n\nreasons_to_accept: The idea of using mutated prompts to construct sontrastive pair makes sence.\n\nreasons_to_reject: Human evaluation is necessary to access whether the evaluation of the proposed SSA could better align with human judgement. Otherwise, how to prove the proposed SSA is better than existing metrics like CLIP-score?\nThe accuacy of SSA is hignly depend on the scene graph generation of image. Therefore, I doubt about the practical effectiveness since existing scene graph generation models are not very preceise in practical use.\nComparison with human-based evaluation suce as \"ImageReward\" is necessary.\n\nmissing_references: ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "GocYEBmo5b",
        "similarity": 0.7453,
        "coverage": 0.3235,
        "human_length": 459,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a new method for evaluation of text-to-image generation models. The key idea of the paper is to learn structured semantic embeddings across different modalities and aligning them in a joint space. Experiments demonstrate that the proposed evaluation allows for comprehensive and improved measurement of semantic consistency of text-to-image generation models.\n\nreasons_to_accept: The proposed SSA framework is very interesting and I enjoyed reading this work. Experiments sufficiently demonstrate the utility of the proposed framework (although it could be further improved, please see reasons to reject). The notion of structured embeddings and the underlying principle of SSA presented in section 3.4 could be potentially useful in other tasks/domains. Overall the paper is solid with interesting and important contributions in T2I.\n\nreasons_to_reject: I do not find any major weaknesses in the proposed work. One aspect that could be further improved to make the contribution strong is that the paper could have shown how well the proposed evaluation method could handle long prompts. As the main contribution/utility of the paper is to provide a better way to evaluate complex or long prompts, it would be useful to show how well the existing methods and the proposed SSA method evaluate the generated images with long descriptions. It\u2019s not very clear to me on the average length of prompts that were drawn from MS COCO. Please clarify. ( I assume mutations do not change the length of the prompt). Another suggestion would be to add some comparisons of existing methods and SSA on prompts of varying lengths. That would be a really interesting analysis to perform. Finally, the paper compares only one compositional T2I baseline (i.e. composable diffusion). For evaluating SSA, it would have been better to also consider more compositional generation baselines such as below: Feng, Weixi, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. \" Training-free structured diffusion guidance for compositional text-to-image synthesis.\" arXiv preprint arXiv:2212.05032 (2022).\nJim\u00e9nez, \u00c1. B. (2023). Mixture of diffusers for scene composition and high resolution image generation. arXiv preprint arXiv:2302.02412.\nKumari, N., Zhang, B., Zhang, R., Shechtman, E., & Zhu, J. Y. (2023). Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 1931-1941).\n\nquestions_for_the_authors: Please see Reasons To Reject\n\nmissing_references: Please see Reasons To Reject\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "ERc78IhebS",
        "length": 234,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a new approach called Structured Semantic Alignment (SSA) to evaluate the accuracy and robustness of text-to-image generation models. The main contribution of the paper is the development of a new evaluation method that measures the semantic consistency between textual descriptions and generated images. The authors argue that existing metrics lack insights into the assessment of fine-grained semantic concepts, such as object attributes, context details, and semantic relationships. The experimental results demonstrate the effectiveness of the SSA approach in evaluating text-to-image generation models.\n\nreasons_to_accept: 1. The paper presents a well-motivated and novel approach to evaluate text-to-image generation models. \n2. The proposed approach is supported by experimental results that demonstrate its effectiveness in measuring semantic consistency between textual descriptions and generated images. \n3. The paper is well-written and clearly presents the motivation, approach, and experimental results.\n\nreasons_to_reject: The paper could benefit from a more detailed discussion of how it compares to other evaluation methods and a more detailed analysis of the experimental results.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "nb52FLzBo2",
        "length": 268,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on evaluating text-to-image generation models. Specifically, this paper propose a new evaluation metric named Structured Semantic Alignment (SSA) to evaluate the semantic consistency between text and generated images. SSA first projects text/image features into a shared embedding space by text parsing and scene graph generation respectively. Then SSA is learned by contrastive learning. The positive and negative samples are generated by substituting words with semantically equivalent or nonequivalent alternatives.\n\nreasons_to_accept: The idea of using mutated prompts to construct sontrastive pair makes sence.\n\nreasons_to_reject: Human evaluation is necessary to access whether the evaluation of the proposed SSA could better align with human judgement. Otherwise, how to prove the proposed SSA is better than existing metrics like CLIP-score?\nThe accuacy of SSA is hignly depend on the scene graph generation of image. Therefore, I doubt about the practical effectiveness since existing scene graph generation models are not very preceise in practical use.\nComparison with human-based evaluation suce as \"ImageReward\" is necessary.\n\nmissing_references: ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "GocYEBmo5b",
        "length": 459,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a new method for evaluation of text-to-image generation models. The key idea of the paper is to learn structured semantic embeddings across different modalities and aligning them in a joint space. Experiments demonstrate that the proposed evaluation allows for comprehensive and improved measurement of semantic consistency of text-to-image generation models.\n\nreasons_to_accept: The proposed SSA framework is very interesting and I enjoyed reading this work. Experiments sufficiently demonstrate the utility of the proposed framework (although it could be further improved, please see reasons to reject). The notion of structured embeddings and the underlying principle of SSA presented in section 3.4 could be potentially useful in other tasks/domains. Overall the paper is solid with interesting and important contributions in T2I.\n\nreasons_to_reject: I do not find any major weaknesses in the proposed work. One aspect that could be further improved to make the contribution strong is that the paper could have shown how well the proposed evaluation method could handle long prompts. As the main contribution/utility of the paper is to provide a better way to evaluate complex or long prompts, it would be useful to show how well the existing methods and the proposed SSA method evaluate the generated images with long descriptions. It\u2019s not very clear to me on the average length of prompts that were drawn from MS COCO. Please clarify. ( I assume mutations do not change the length of the prompt). Another suggestion would be to add some comparisons of existing methods and SSA on prompts of varying lengths. That would be a really interesting analysis to perform. Finally, the paper compares only one compositional T2I baseline (i.e. composable diffusion). For evaluating SSA, it would have been better to also consider more compositional generation baselines such as below: Feng, Weixi, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. \" Training-free structured diffusion guidance for compositional text-to-image synthesis.\" arXiv preprint arXiv:2212.05032 (2022).\nJim\u00e9nez, \u00c1. B. (2023). Mixture of diffusers for scene composition and high resolution image generation. arXiv preprint arXiv:2302.02412.\nKumari, N., Zhang, B., Zhang, R., Shechtman, E., & Zhu, J. Y. (2023). Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 1931-1941).\n\nquestions_for_the_authors: Please see Reasons To Reject\n\nmissing_references: Please see Reasons To Reject\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "46_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_46_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7307,
      "max_similarity": 0.7451,
      "avg_coverage": 0.5557333333333333,
      "max_coverage": 0.7143
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 661,
      "avg_human_length": 320.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "ERc78IhebS",
        "similarity": 0.7149,
        "coverage": 0.6,
        "human_length": 234,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a new approach called Structured Semantic Alignment (SSA) to evaluate the accuracy and robustness of text-to-image generation models. The main contribution of the paper is the development of a new evaluation method that measures the semantic consistency between textual descriptions and generated images. The authors argue that existing metrics lack insights into the assessment of fine-grained semantic concepts, such as object attributes, context details, and semantic relationships. The experimental results demonstrate the effectiveness of the SSA approach in evaluating text-to-image generation models.\n\nreasons_to_accept: 1. The paper presents a well-motivated and novel approach to evaluate text-to-image generation models. \n2. The proposed approach is supported by experimental results that demonstrate its effectiveness in measuring semantic consistency between textual descriptions and generated images. \n3. The paper is well-written and clearly presents the motivation, approach, and experimental results.\n\nreasons_to_reject: The paper could benefit from a more detailed discussion of how it compares to other evaluation methods and a more detailed analysis of the experimental results.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "nb52FLzBo2",
        "similarity": 0.7321,
        "coverage": 0.7143,
        "human_length": 268,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on evaluating text-to-image generation models. Specifically, this paper propose a new evaluation metric named Structured Semantic Alignment (SSA) to evaluate the semantic consistency between text and generated images. SSA first projects text/image features into a shared embedding space by text parsing and scene graph generation respectively. Then SSA is learned by contrastive learning. The positive and negative samples are generated by substituting words with semantically equivalent or nonequivalent alternatives.\n\nreasons_to_accept: The idea of using mutated prompts to construct sontrastive pair makes sence.\n\nreasons_to_reject: Human evaluation is necessary to access whether the evaluation of the proposed SSA could better align with human judgement. Otherwise, how to prove the proposed SSA is better than existing metrics like CLIP-score?\nThe accuacy of SSA is hignly depend on the scene graph generation of image. Therefore, I doubt about the practical effectiveness since existing scene graph generation models are not very preceise in practical use.\nComparison with human-based evaluation suce as \"ImageReward\" is necessary.\n\nmissing_references: ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "GocYEBmo5b",
        "similarity": 0.7451,
        "coverage": 0.3529,
        "human_length": 459,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a new method for evaluation of text-to-image generation models. The key idea of the paper is to learn structured semantic embeddings across different modalities and aligning them in a joint space. Experiments demonstrate that the proposed evaluation allows for comprehensive and improved measurement of semantic consistency of text-to-image generation models.\n\nreasons_to_accept: The proposed SSA framework is very interesting and I enjoyed reading this work. Experiments sufficiently demonstrate the utility of the proposed framework (although it could be further improved, please see reasons to reject). The notion of structured embeddings and the underlying principle of SSA presented in section 3.4 could be potentially useful in other tasks/domains. Overall the paper is solid with interesting and important contributions in T2I.\n\nreasons_to_reject: I do not find any major weaknesses in the proposed work. One aspect that could be further improved to make the contribution strong is that the paper could have shown how well the proposed evaluation method could handle long prompts. As the main contribution/utility of the paper is to provide a better way to evaluate complex or long prompts, it would be useful to show how well the existing methods and the proposed SSA method evaluate the generated images with long descriptions. It\u2019s not very clear to me on the average length of prompts that were drawn from MS COCO. Please clarify. ( I assume mutations do not change the length of the prompt). Another suggestion would be to add some comparisons of existing methods and SSA on prompts of varying lengths. That would be a really interesting analysis to perform. Finally, the paper compares only one compositional T2I baseline (i.e. composable diffusion). For evaluating SSA, it would have been better to also consider more compositional generation baselines such as below: Feng, Weixi, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. \" Training-free structured diffusion guidance for compositional text-to-image synthesis.\" arXiv preprint arXiv:2212.05032 (2022).\nJim\u00e9nez, \u00c1. B. (2023). Mixture of diffusers for scene composition and high resolution image generation. arXiv preprint arXiv:2302.02412.\nKumari, N., Zhang, B., Zhang, R., Shechtman, E., & Zhu, J. Y. (2023). Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 1931-1941).\n\nquestions_for_the_authors: Please see Reasons To Reject\n\nmissing_references: Please see Reasons To Reject\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "ERc78IhebS",
        "length": 234,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a new approach called Structured Semantic Alignment (SSA) to evaluate the accuracy and robustness of text-to-image generation models. The main contribution of the paper is the development of a new evaluation method that measures the semantic consistency between textual descriptions and generated images. The authors argue that existing metrics lack insights into the assessment of fine-grained semantic concepts, such as object attributes, context details, and semantic relationships. The experimental results demonstrate the effectiveness of the SSA approach in evaluating text-to-image generation models.\n\nreasons_to_accept: 1. The paper presents a well-motivated and novel approach to evaluate text-to-image generation models. \n2. The proposed approach is supported by experimental results that demonstrate its effectiveness in measuring semantic consistency between textual descriptions and generated images. \n3. The paper is well-written and clearly presents the motivation, approach, and experimental results.\n\nreasons_to_reject: The paper could benefit from a more detailed discussion of how it compares to other evaluation methods and a more detailed analysis of the experimental results.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "nb52FLzBo2",
        "length": 268,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on evaluating text-to-image generation models. Specifically, this paper propose a new evaluation metric named Structured Semantic Alignment (SSA) to evaluate the semantic consistency between text and generated images. SSA first projects text/image features into a shared embedding space by text parsing and scene graph generation respectively. Then SSA is learned by contrastive learning. The positive and negative samples are generated by substituting words with semantically equivalent or nonequivalent alternatives.\n\nreasons_to_accept: The idea of using mutated prompts to construct sontrastive pair makes sence.\n\nreasons_to_reject: Human evaluation is necessary to access whether the evaluation of the proposed SSA could better align with human judgement. Otherwise, how to prove the proposed SSA is better than existing metrics like CLIP-score?\nThe accuacy of SSA is hignly depend on the scene graph generation of image. Therefore, I doubt about the practical effectiveness since existing scene graph generation models are not very preceise in practical use.\nComparison with human-based evaluation suce as \"ImageReward\" is necessary.\n\nmissing_references: ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "GocYEBmo5b",
        "length": 459,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a new method for evaluation of text-to-image generation models. The key idea of the paper is to learn structured semantic embeddings across different modalities and aligning them in a joint space. Experiments demonstrate that the proposed evaluation allows for comprehensive and improved measurement of semantic consistency of text-to-image generation models.\n\nreasons_to_accept: The proposed SSA framework is very interesting and I enjoyed reading this work. Experiments sufficiently demonstrate the utility of the proposed framework (although it could be further improved, please see reasons to reject). The notion of structured embeddings and the underlying principle of SSA presented in section 3.4 could be potentially useful in other tasks/domains. Overall the paper is solid with interesting and important contributions in T2I.\n\nreasons_to_reject: I do not find any major weaknesses in the proposed work. One aspect that could be further improved to make the contribution strong is that the paper could have shown how well the proposed evaluation method could handle long prompts. As the main contribution/utility of the paper is to provide a better way to evaluate complex or long prompts, it would be useful to show how well the existing methods and the proposed SSA method evaluate the generated images with long descriptions. It\u2019s not very clear to me on the average length of prompts that were drawn from MS COCO. Please clarify. ( I assume mutations do not change the length of the prompt). Another suggestion would be to add some comparisons of existing methods and SSA on prompts of varying lengths. That would be a really interesting analysis to perform. Finally, the paper compares only one compositional T2I baseline (i.e. composable diffusion). For evaluating SSA, it would have been better to also consider more compositional generation baselines such as below: Feng, Weixi, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. \" Training-free structured diffusion guidance for compositional text-to-image synthesis.\" arXiv preprint arXiv:2212.05032 (2022).\nJim\u00e9nez, \u00c1. B. (2023). Mixture of diffusers for scene composition and high resolution image generation. arXiv preprint arXiv:2302.02412.\nKumari, N., Zhang, B., Zhang, R., Shechtman, E., & Zhu, J. Y. (2023). Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 1931-1941).\n\nquestions_for_the_authors: Please see Reasons To Reject\n\nmissing_references: Please see Reasons To Reject\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "175_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_175_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6995666666666667,
      "max_similarity": 0.7256,
      "avg_coverage": 0.5234333333333333,
      "max_coverage": 0.5625
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 452,
      "avg_human_length": 390.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 7,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "RgLkzPfsqW",
        "similarity": 0.7256,
        "coverage": 0.4815,
        "human_length": 434,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a method called SeqXGPT for detecting AI-generated text (AIGT) at the sentence level. The method utilizes log probability lists from white-box language models as features for detection. Experimental results show that SeqXGPT outperforms baseline methods in both sentence and document-level detection challenges and exhibits strong generalization capabilities.\n\nreasons_to_accept: 1. Novel Contribution: The paper addresses the important and challenging task of fine-grained AI-generated text (AIGT) detection at the sentence level, which is a significant advancement over existing document-level AIGT detection methods. The authors propose a new approach called SeqXGPT, which demonstrates promising results in both sentence and document-level AIGT detection challenges. This novel contribution fills a gap in the literature and provides valuable insights into fine-grained AIGT detection.\n2. Performance and Generalization: The experimental results show that SeqXGPT outperforms existing methods, such as DetectGPT and Sniffer, in sentence-level AIGT detection. SeqXGPT exhibits excellent performance not only in discriminating human-generated sentences but also in detecting AI-generated sentences. Furthermore, SeqXGPT demonstrates strong generalization capabilities on out-of-distribution datasets, indicating its robustness and potential for real-world applications.\n3. Dataset Construction: The authors synthesize a sentence-level AIGT detection dataset, which is crucial for studying fine-grained AIGT detection\n\nreasons_to_reject: 1. Lack of Novelty: The paper does not present a significant advancement or novel contribution to the field of fine-grained AI-generated text (AIGT) detection. The proposed approach, SeqXGPT, is similar to existing methods such as DetectGPT and Sniffer. The paper fails to demonstrate how SeqXGPT significantly outperforms or improves upon these existing methods.\n2. Insufficient Experimental Evaluation: The experimental results provided in the paper are limited and do not provide a comprehensive evaluation of the proposed approach. The paper lacks a thorough comparison with state-of-the-art methods and fails to provide statistical significance tests to support the claimed performance improvements. Additionally, the evaluation is primarily focused on synthetic datasets, which may not accurately reflect real-world scenarios.\n3. Incomplete Analysis and Discussion: The paper lacks a thorough analysis and discussion of the limitations and potential drawbacks of the proposed approach. For example, the authors do not explore the impact of incorporating semantic features or investigate the influence of diversified instructions on AIGT detection. The paper also does not address more complex scenarios where a document contains sentences.\n\nquestions_for_the_authors: Please address the concerns in \"reasons to reject\"\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 1: Could not reproduce the results here no matter how hard they tried."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "1T77OByRWS",
        "similarity": 0.6911,
        "coverage": 0.5625,
        "human_length": 295,
        "human_text": "paper_topic_and_main_contributions: Different from the document-level Artificial Intelligence Generated Text (AIGT) detection considered by current workers, this paper introduces the sentence-level AIGT detection challenge and proposes a method SeqXGPT based on convolutional neural networks and self-attention networks by utilizing log probability lists of white-box Large Language Models (LLMs). This paper also constructs a sentence-level AIGT detection dataset. The experimental results show that the proposed method achieves SOTA in sentence and document-levels detection.\n\nreasons_to_accept: 1) The method proposed in this paper can effectively solve the difficulties of sentence-level AIGT detection. \n2) The experiments designed in this paper cover 3 different sentence-level AIGT detection settings, all of which achieve SOTA. \n3) This article is written smoothly and helps readers understand.\n\nreasons_to_reject: 1. In the setting of Particular-Model Binary AIGT Detection, the table of experimental results only includes the results of GPT-2 and GPT-Neo, lacking test results on other LLMs.\n\nquestions_for_the_authors: Question A: For each model, the author sets the maximum sequence length given the maximum GPU allowance. What is the maximum sequence length for each model in the experiment? Will the difference in maximum sequence length have a significant impact on the performance of each model? Why not set the same maximum sequence length for each model?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "pHSePBU55u",
        "similarity": 0.682,
        "coverage": 0.5263,
        "human_length": 443,
        "human_text": "paper_topic_and_main_contributions: This paper addreses sentence-level AI-generated text detection task. The proposed solution for this task is to utilize a collection of (L)LMs to produce a collection of per-token likelihood, and use CNN-transformer-FCN structure to determine per-token label whether the token is AI-generated or not. The paper also provides a reasonable set of empirical evidences of the proposed model being effective in solving the addressed task.\n\nreasons_to_accept: This paper will provide a practical guide to construct an AI-generated text detection software. In particular, this method can be generally applied to any collection of LMs (already or to-be available), which makes the method presented in this paper a nice addition to the practical armory of any NLP researcher or industrial programmer.\n\nreasons_to_reject: The dataset used to train and test the proposed method appears to be somewhat narrow in how it is produced. As bootstrapping is the first-step for any new task, so OOD test results the authors provided are a must-and-nice addition, but the intensity of the OOD test should have been greater in my opinion (as, once the paper published, interested people will be doing their own OOD tests by implementing their own versions).\n\nquestions_for_the_authors: A. Would there be any other ways to create the dataset, other than seeding the human-generated sentence in the beginning and then using different LMs to fill up the rest?  My worries are related to possible systematic bias of the dataset due to how the dataset is created --- monotonically increasing tendency to be more likely to be AI-generated as more sentences appear? Would you consider your proposed SeqXGPT-Bench is guarded against this kind of bias?  B. From a similar vein, how wide or extensive does the currently evaluated OOD dataset cover the possibilities of AI-generated texts? This may be topic-wise or format-wise. I raise this point from the perspective of a potentially interested human being to implement this type of detector for a similar use case, and I would be greatly appreciative if this approach would work like a charm in my use case.\n\ntypos_grammar_style_and_presentation_improvements: - line 360: wihte -> white - line 869: trivaQA -> triviaQA\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "RgLkzPfsqW",
        "length": 434,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a method called SeqXGPT for detecting AI-generated text (AIGT) at the sentence level. The method utilizes log probability lists from white-box language models as features for detection. Experimental results show that SeqXGPT outperforms baseline methods in both sentence and document-level detection challenges and exhibits strong generalization capabilities.\n\nreasons_to_accept: 1. Novel Contribution: The paper addresses the important and challenging task of fine-grained AI-generated text (AIGT) detection at the sentence level, which is a significant advancement over existing document-level AIGT detection methods. The authors propose a new approach called SeqXGPT, which demonstrates promising results in both sentence and document-level AIGT detection challenges. This novel contribution fills a gap in the literature and provides valuable insights into fine-grained AIGT detection.\n2. Performance and Generalization: The experimental results show that SeqXGPT outperforms existing methods, such as DetectGPT and Sniffer, in sentence-level AIGT detection. SeqXGPT exhibits excellent performance not only in discriminating human-generated sentences but also in detecting AI-generated sentences. Furthermore, SeqXGPT demonstrates strong generalization capabilities on out-of-distribution datasets, indicating its robustness and potential for real-world applications.\n3. Dataset Construction: The authors synthesize a sentence-level AIGT detection dataset, which is crucial for studying fine-grained AIGT detection\n\nreasons_to_reject: 1. Lack of Novelty: The paper does not present a significant advancement or novel contribution to the field of fine-grained AI-generated text (AIGT) detection. The proposed approach, SeqXGPT, is similar to existing methods such as DetectGPT and Sniffer. The paper fails to demonstrate how SeqXGPT significantly outperforms or improves upon these existing methods.\n2. Insufficient Experimental Evaluation: The experimental results provided in the paper are limited and do not provide a comprehensive evaluation of the proposed approach. The paper lacks a thorough comparison with state-of-the-art methods and fails to provide statistical significance tests to support the claimed performance improvements. Additionally, the evaluation is primarily focused on synthetic datasets, which may not accurately reflect real-world scenarios.\n3. Incomplete Analysis and Discussion: The paper lacks a thorough analysis and discussion of the limitations and potential drawbacks of the proposed approach. For example, the authors do not explore the impact of incorporating semantic features or investigate the influence of diversified instructions on AIGT detection. The paper also does not address more complex scenarios where a document contains sentences.\n\nquestions_for_the_authors: Please address the concerns in \"reasons to reject\"\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 1: Could not reproduce the results here no matter how hard they tried.",
        "has_content": true
      },
      {
        "reviewer_id": "1T77OByRWS",
        "length": 295,
        "human_text": "paper_topic_and_main_contributions: Different from the document-level Artificial Intelligence Generated Text (AIGT) detection considered by current workers, this paper introduces the sentence-level AIGT detection challenge and proposes a method SeqXGPT based on convolutional neural networks and self-attention networks by utilizing log probability lists of white-box Large Language Models (LLMs). This paper also constructs a sentence-level AIGT detection dataset. The experimental results show that the proposed method achieves SOTA in sentence and document-levels detection.\n\nreasons_to_accept: 1) The method proposed in this paper can effectively solve the difficulties of sentence-level AIGT detection. \n2) The experiments designed in this paper cover 3 different sentence-level AIGT detection settings, all of which achieve SOTA. \n3) This article is written smoothly and helps readers understand.\n\nreasons_to_reject: 1. In the setting of Particular-Model Binary AIGT Detection, the table of experimental results only includes the results of GPT-2 and GPT-Neo, lacking test results on other LLMs.\n\nquestions_for_the_authors: Question A: For each model, the author sets the maximum sequence length given the maximum GPU allowance. What is the maximum sequence length for each model in the experiment? Will the difference in maximum sequence length have a significant impact on the performance of each model? Why not set the same maximum sequence length for each model?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "pHSePBU55u",
        "length": 443,
        "human_text": "paper_topic_and_main_contributions: This paper addreses sentence-level AI-generated text detection task. The proposed solution for this task is to utilize a collection of (L)LMs to produce a collection of per-token likelihood, and use CNN-transformer-FCN structure to determine per-token label whether the token is AI-generated or not. The paper also provides a reasonable set of empirical evidences of the proposed model being effective in solving the addressed task.\n\nreasons_to_accept: This paper will provide a practical guide to construct an AI-generated text detection software. In particular, this method can be generally applied to any collection of LMs (already or to-be available), which makes the method presented in this paper a nice addition to the practical armory of any NLP researcher or industrial programmer.\n\nreasons_to_reject: The dataset used to train and test the proposed method appears to be somewhat narrow in how it is produced. As bootstrapping is the first-step for any new task, so OOD test results the authors provided are a must-and-nice addition, but the intensity of the OOD test should have been greater in my opinion (as, once the paper published, interested people will be doing their own OOD tests by implementing their own versions).\n\nquestions_for_the_authors: A. Would there be any other ways to create the dataset, other than seeding the human-generated sentence in the beginning and then using different LMs to fill up the rest?  My worries are related to possible systematic bias of the dataset due to how the dataset is created --- monotonically increasing tendency to be more likely to be AI-generated as more sentences appear? Would you consider your proposed SeqXGPT-Bench is guarded against this kind of bias?  B. From a similar vein, how wide or extensive does the currently evaluated OOD dataset cover the possibilities of AI-generated texts? This may be topic-wise or format-wise. I raise this point from the perspective of a potentially interested human being to implement this type of detector for a similar use case, and I would be greatly appreciative if this approach would work like a charm in my use case.\n\ntypos_grammar_style_and_presentation_improvements: - line 360: wihte -> white - line 869: trivaQA -> triviaQA\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "175_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_175_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7005,
      "max_similarity": 0.7264,
      "avg_coverage": 0.5618,
      "max_coverage": 0.625
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 464,
      "avg_human_length": 390.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 7,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "RgLkzPfsqW",
        "similarity": 0.7264,
        "coverage": 0.4815,
        "human_length": 434,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a method called SeqXGPT for detecting AI-generated text (AIGT) at the sentence level. The method utilizes log probability lists from white-box language models as features for detection. Experimental results show that SeqXGPT outperforms baseline methods in both sentence and document-level detection challenges and exhibits strong generalization capabilities.\n\nreasons_to_accept: 1. Novel Contribution: The paper addresses the important and challenging task of fine-grained AI-generated text (AIGT) detection at the sentence level, which is a significant advancement over existing document-level AIGT detection methods. The authors propose a new approach called SeqXGPT, which demonstrates promising results in both sentence and document-level AIGT detection challenges. This novel contribution fills a gap in the literature and provides valuable insights into fine-grained AIGT detection.\n2. Performance and Generalization: The experimental results show that SeqXGPT outperforms existing methods, such as DetectGPT and Sniffer, in sentence-level AIGT detection. SeqXGPT exhibits excellent performance not only in discriminating human-generated sentences but also in detecting AI-generated sentences. Furthermore, SeqXGPT demonstrates strong generalization capabilities on out-of-distribution datasets, indicating its robustness and potential for real-world applications.\n3. Dataset Construction: The authors synthesize a sentence-level AIGT detection dataset, which is crucial for studying fine-grained AIGT detection\n\nreasons_to_reject: 1. Lack of Novelty: The paper does not present a significant advancement or novel contribution to the field of fine-grained AI-generated text (AIGT) detection. The proposed approach, SeqXGPT, is similar to existing methods such as DetectGPT and Sniffer. The paper fails to demonstrate how SeqXGPT significantly outperforms or improves upon these existing methods.\n2. Insufficient Experimental Evaluation: The experimental results provided in the paper are limited and do not provide a comprehensive evaluation of the proposed approach. The paper lacks a thorough comparison with state-of-the-art methods and fails to provide statistical significance tests to support the claimed performance improvements. Additionally, the evaluation is primarily focused on synthetic datasets, which may not accurately reflect real-world scenarios.\n3. Incomplete Analysis and Discussion: The paper lacks a thorough analysis and discussion of the limitations and potential drawbacks of the proposed approach. For example, the authors do not explore the impact of incorporating semantic features or investigate the influence of diversified instructions on AIGT detection. The paper also does not address more complex scenarios where a document contains sentences.\n\nquestions_for_the_authors: Please address the concerns in \"reasons to reject\"\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 1: Could not reproduce the results here no matter how hard they tried."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "1T77OByRWS",
        "similarity": 0.692,
        "coverage": 0.625,
        "human_length": 295,
        "human_text": "paper_topic_and_main_contributions: Different from the document-level Artificial Intelligence Generated Text (AIGT) detection considered by current workers, this paper introduces the sentence-level AIGT detection challenge and proposes a method SeqXGPT based on convolutional neural networks and self-attention networks by utilizing log probability lists of white-box Large Language Models (LLMs). This paper also constructs a sentence-level AIGT detection dataset. The experimental results show that the proposed method achieves SOTA in sentence and document-levels detection.\n\nreasons_to_accept: 1) The method proposed in this paper can effectively solve the difficulties of sentence-level AIGT detection. \n2) The experiments designed in this paper cover 3 different sentence-level AIGT detection settings, all of which achieve SOTA. \n3) This article is written smoothly and helps readers understand.\n\nreasons_to_reject: 1. In the setting of Particular-Model Binary AIGT Detection, the table of experimental results only includes the results of GPT-2 and GPT-Neo, lacking test results on other LLMs.\n\nquestions_for_the_authors: Question A: For each model, the author sets the maximum sequence length given the maximum GPU allowance. What is the maximum sequence length for each model in the experiment? Will the difference in maximum sequence length have a significant impact on the performance of each model? Why not set the same maximum sequence length for each model?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "pHSePBU55u",
        "similarity": 0.6831,
        "coverage": 0.5789,
        "human_length": 443,
        "human_text": "paper_topic_and_main_contributions: This paper addreses sentence-level AI-generated text detection task. The proposed solution for this task is to utilize a collection of (L)LMs to produce a collection of per-token likelihood, and use CNN-transformer-FCN structure to determine per-token label whether the token is AI-generated or not. The paper also provides a reasonable set of empirical evidences of the proposed model being effective in solving the addressed task.\n\nreasons_to_accept: This paper will provide a practical guide to construct an AI-generated text detection software. In particular, this method can be generally applied to any collection of LMs (already or to-be available), which makes the method presented in this paper a nice addition to the practical armory of any NLP researcher or industrial programmer.\n\nreasons_to_reject: The dataset used to train and test the proposed method appears to be somewhat narrow in how it is produced. As bootstrapping is the first-step for any new task, so OOD test results the authors provided are a must-and-nice addition, but the intensity of the OOD test should have been greater in my opinion (as, once the paper published, interested people will be doing their own OOD tests by implementing their own versions).\n\nquestions_for_the_authors: A. Would there be any other ways to create the dataset, other than seeding the human-generated sentence in the beginning and then using different LMs to fill up the rest?  My worries are related to possible systematic bias of the dataset due to how the dataset is created --- monotonically increasing tendency to be more likely to be AI-generated as more sentences appear? Would you consider your proposed SeqXGPT-Bench is guarded against this kind of bias?  B. From a similar vein, how wide or extensive does the currently evaluated OOD dataset cover the possibilities of AI-generated texts? This may be topic-wise or format-wise. I raise this point from the perspective of a potentially interested human being to implement this type of detector for a similar use case, and I would be greatly appreciative if this approach would work like a charm in my use case.\n\ntypos_grammar_style_and_presentation_improvements: - line 360: wihte -> white - line 869: trivaQA -> triviaQA\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "RgLkzPfsqW",
        "length": 434,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a method called SeqXGPT for detecting AI-generated text (AIGT) at the sentence level. The method utilizes log probability lists from white-box language models as features for detection. Experimental results show that SeqXGPT outperforms baseline methods in both sentence and document-level detection challenges and exhibits strong generalization capabilities.\n\nreasons_to_accept: 1. Novel Contribution: The paper addresses the important and challenging task of fine-grained AI-generated text (AIGT) detection at the sentence level, which is a significant advancement over existing document-level AIGT detection methods. The authors propose a new approach called SeqXGPT, which demonstrates promising results in both sentence and document-level AIGT detection challenges. This novel contribution fills a gap in the literature and provides valuable insights into fine-grained AIGT detection.\n2. Performance and Generalization: The experimental results show that SeqXGPT outperforms existing methods, such as DetectGPT and Sniffer, in sentence-level AIGT detection. SeqXGPT exhibits excellent performance not only in discriminating human-generated sentences but also in detecting AI-generated sentences. Furthermore, SeqXGPT demonstrates strong generalization capabilities on out-of-distribution datasets, indicating its robustness and potential for real-world applications.\n3. Dataset Construction: The authors synthesize a sentence-level AIGT detection dataset, which is crucial for studying fine-grained AIGT detection\n\nreasons_to_reject: 1. Lack of Novelty: The paper does not present a significant advancement or novel contribution to the field of fine-grained AI-generated text (AIGT) detection. The proposed approach, SeqXGPT, is similar to existing methods such as DetectGPT and Sniffer. The paper fails to demonstrate how SeqXGPT significantly outperforms or improves upon these existing methods.\n2. Insufficient Experimental Evaluation: The experimental results provided in the paper are limited and do not provide a comprehensive evaluation of the proposed approach. The paper lacks a thorough comparison with state-of-the-art methods and fails to provide statistical significance tests to support the claimed performance improvements. Additionally, the evaluation is primarily focused on synthetic datasets, which may not accurately reflect real-world scenarios.\n3. Incomplete Analysis and Discussion: The paper lacks a thorough analysis and discussion of the limitations and potential drawbacks of the proposed approach. For example, the authors do not explore the impact of incorporating semantic features or investigate the influence of diversified instructions on AIGT detection. The paper also does not address more complex scenarios where a document contains sentences.\n\nquestions_for_the_authors: Please address the concerns in \"reasons to reject\"\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 1: Could not reproduce the results here no matter how hard they tried.",
        "has_content": true
      },
      {
        "reviewer_id": "1T77OByRWS",
        "length": 295,
        "human_text": "paper_topic_and_main_contributions: Different from the document-level Artificial Intelligence Generated Text (AIGT) detection considered by current workers, this paper introduces the sentence-level AIGT detection challenge and proposes a method SeqXGPT based on convolutional neural networks and self-attention networks by utilizing log probability lists of white-box Large Language Models (LLMs). This paper also constructs a sentence-level AIGT detection dataset. The experimental results show that the proposed method achieves SOTA in sentence and document-levels detection.\n\nreasons_to_accept: 1) The method proposed in this paper can effectively solve the difficulties of sentence-level AIGT detection. \n2) The experiments designed in this paper cover 3 different sentence-level AIGT detection settings, all of which achieve SOTA. \n3) This article is written smoothly and helps readers understand.\n\nreasons_to_reject: 1. In the setting of Particular-Model Binary AIGT Detection, the table of experimental results only includes the results of GPT-2 and GPT-Neo, lacking test results on other LLMs.\n\nquestions_for_the_authors: Question A: For each model, the author sets the maximum sequence length given the maximum GPU allowance. What is the maximum sequence length for each model in the experiment? Will the difference in maximum sequence length have a significant impact on the performance of each model? Why not set the same maximum sequence length for each model?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "pHSePBU55u",
        "length": 443,
        "human_text": "paper_topic_and_main_contributions: This paper addreses sentence-level AI-generated text detection task. The proposed solution for this task is to utilize a collection of (L)LMs to produce a collection of per-token likelihood, and use CNN-transformer-FCN structure to determine per-token label whether the token is AI-generated or not. The paper also provides a reasonable set of empirical evidences of the proposed model being effective in solving the addressed task.\n\nreasons_to_accept: This paper will provide a practical guide to construct an AI-generated text detection software. In particular, this method can be generally applied to any collection of LMs (already or to-be available), which makes the method presented in this paper a nice addition to the practical armory of any NLP researcher or industrial programmer.\n\nreasons_to_reject: The dataset used to train and test the proposed method appears to be somewhat narrow in how it is produced. As bootstrapping is the first-step for any new task, so OOD test results the authors provided are a must-and-nice addition, but the intensity of the OOD test should have been greater in my opinion (as, once the paper published, interested people will be doing their own OOD tests by implementing their own versions).\n\nquestions_for_the_authors: A. Would there be any other ways to create the dataset, other than seeding the human-generated sentence in the beginning and then using different LMs to fill up the rest?  My worries are related to possible systematic bias of the dataset due to how the dataset is created --- monotonically increasing tendency to be more likely to be AI-generated as more sentences appear? Would you consider your proposed SeqXGPT-Bench is guarded against this kind of bias?  B. From a similar vein, how wide or extensive does the currently evaluated OOD dataset cover the possibilities of AI-generated texts? This may be topic-wise or format-wise. I raise this point from the perspective of a potentially interested human being to implement this type of detector for a similar use case, and I would be greatly appreciative if this approach would work like a charm in my use case.\n\ntypos_grammar_style_and_presentation_improvements: - line 360: wihte -> white - line 869: trivaQA -> triviaQA\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "130_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_130_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7147666666666667,
      "max_similarity": 0.7302,
      "avg_coverage": 0.5931666666666667,
      "max_coverage": 0.8333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 566,
      "avg_human_length": 320.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "S24OOltABM",
        "similarity": 0.7182,
        "coverage": 0.8333,
        "human_length": 253,
        "human_text": "paper_topic_and_main_contributions: - This paper proposes a pipeline that automatically generates a multi-turn chat corpus using ChatGPT.\n- Based on collected dataset, the authors present a chatting model named Baize, trained with parameter-efficient tuning based on LLaMA.\n- They additionally apply reinforcement learning with self-feedback to improve the performance of Baize.\n\nreasons_to_accept: This paper presents valuable resources (e.g. dataset and model) for research community.\n\nreasons_to_reject: - The novelty of the proposed pipeline is limited, which has been already explored in several works [1], [2]. It is popular approach to generate dataset using LLMs and train smaller LMs on the collected dataset nowadays, and there is no other contributions to be considered in the proposed framework.\n- There is no quality control (e.g. filtering) or manual evaluation on the quality of data, which is collected through automatic model-based approach.\n- It is unclear which chatting model the authors aim to develop, and the motivation for developing such model is also unclear.\n[1] Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (NAACL 2022) [2] SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization (ACL 2023 Findings)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "DxCQOcNH1q",
        "similarity": 0.6959,
        "coverage": 0.6,
        "human_length": 303,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a framework for efficiently performing Instruction-tuning on large language models (LLMs), utilizing ChatGPT. The framework involves conducting Self-chat using ChatGPT for collecting dialogues for the first stage of training. Subsequently, the obtained model is further trained using ChatGPT's Feedback as an alternative to RLHF, successfully enhancing the model's performance.\n\nreasons_to_accept: - The paper proposes an efficient and practical method for training high-quality LLM without the intervention of actual humans. It is considered an efficient and reproducible method compared to Vicuna, which is learned from actual human interactions with ShareGPT.\n- The evaluation has been conducted at each step of the proposed method, clearly demonstrating the utility of each part.\n- The paper is written clearly throughout, making it easy for readers to understand.\n\nreasons_to_reject: - The evaluation is limited to automated assessments, including those based on GPT, and lacks human evaluations.\n- As the learning is based on ChatGPT, the experimental conditions seem to favor the proposed method that uses the same model for training data; however, there is no discussion regarding this aspect.\n- Although multiple generation examples are shown, a detailed analysis of the model's performance is lacking, and the differences compared to other models like Vicuna are not clearly illustrated.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "R5XA0LJ0lb",
        "similarity": 0.7302,
        "coverage": 0.3462,
        "human_length": 406,
        "human_text": "paper_topic_and_main_contributions: The paper proposed an efficient pipeline (low cost, low compute) to imitate proprietary chat models. The paper utilized self-chat and self-distill techniques to generate chat data and human feedback data, in comparision with SFT and RLHF proposed by OpenAI's InstructGPT. Self-chat avoids human annotation, and self-distill avoids human feedback. Authors then trained a series of models based on such pipeline, such as (Baize v1, Baize v1.5 Baize v2) with (7B, 13B) parameters.  The self-distill process further added an extra LoRA module to the base model. The paper evaluate Baize using LM Evaluation Harness library and GPT-4 evaluator. The pipeline highlights efficency.\n\nreasons_to_accept: The paper proposed an efficient pipeline to imitate state-of-the-art proprietary chat models such as ChatGPT.\nThe paper proposed a novel self-chat technique, which avoids human annotation.\nThe paper proposed a novel self-distill technique, which avoids human feedback.\nThe pipeline highlights efficency.\n\nreasons_to_reject: I noticed that author stated that \"Different from these attempts, our work focuses on developing an affordable and reproducible pipeline to efficiently tune a general-purpose language model for multi-turn chat.\" Despite this, Baize's current results are not competitive on leaderboards like Huggingface OpenLLM. Considering running Baize requires the same resource as other high-ranking Llama-13B models, and the author failed to point out any directions to further make Baize competitive with other 13B models. So I suspect that the proposed pipeline not work. I think authors should further improve the proposed pipeline to get a competitive result.\nThe generated data may contain hallucination, finetuning on such data seems to cause model generate random response.\n\nquestions_for_the_authors: Have you ever tried using full parameter finetuning, what is the difference between full-parameter finetuning and LoRA on your data?  The generated data may contain hallucination, finetuning on such data seems to cause model generate random response. Have you ever tried some methods to avoid that?\nDo you have any future plan to improve Baize's pipeline?\nDo you think if you have a base model of ChatGPT (gpt-3.5), using self-chat and self-distill can yield a similar model with ChatGPT? If not, what is the reason?\nAre you confident on your proposed pipeline?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "S24OOltABM",
        "length": 253,
        "human_text": "paper_topic_and_main_contributions: - This paper proposes a pipeline that automatically generates a multi-turn chat corpus using ChatGPT.\n- Based on collected dataset, the authors present a chatting model named Baize, trained with parameter-efficient tuning based on LLaMA.\n- They additionally apply reinforcement learning with self-feedback to improve the performance of Baize.\n\nreasons_to_accept: This paper presents valuable resources (e.g. dataset and model) for research community.\n\nreasons_to_reject: - The novelty of the proposed pipeline is limited, which has been already explored in several works [1], [2]. It is popular approach to generate dataset using LLMs and train smaller LMs on the collected dataset nowadays, and there is no other contributions to be considered in the proposed framework.\n- There is no quality control (e.g. filtering) or manual evaluation on the quality of data, which is collected through automatic model-based approach.\n- It is unclear which chatting model the authors aim to develop, and the motivation for developing such model is also unclear.\n[1] Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (NAACL 2022) [2] SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization (ACL 2023 Findings)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "DxCQOcNH1q",
        "length": 303,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a framework for efficiently performing Instruction-tuning on large language models (LLMs), utilizing ChatGPT. The framework involves conducting Self-chat using ChatGPT for collecting dialogues for the first stage of training. Subsequently, the obtained model is further trained using ChatGPT's Feedback as an alternative to RLHF, successfully enhancing the model's performance.\n\nreasons_to_accept: - The paper proposes an efficient and practical method for training high-quality LLM without the intervention of actual humans. It is considered an efficient and reproducible method compared to Vicuna, which is learned from actual human interactions with ShareGPT.\n- The evaluation has been conducted at each step of the proposed method, clearly demonstrating the utility of each part.\n- The paper is written clearly throughout, making it easy for readers to understand.\n\nreasons_to_reject: - The evaluation is limited to automated assessments, including those based on GPT, and lacks human evaluations.\n- As the learning is based on ChatGPT, the experimental conditions seem to favor the proposed method that uses the same model for training data; however, there is no discussion regarding this aspect.\n- Although multiple generation examples are shown, a detailed analysis of the model's performance is lacking, and the differences compared to other models like Vicuna are not clearly illustrated.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "R5XA0LJ0lb",
        "length": 406,
        "human_text": "paper_topic_and_main_contributions: The paper proposed an efficient pipeline (low cost, low compute) to imitate proprietary chat models. The paper utilized self-chat and self-distill techniques to generate chat data and human feedback data, in comparision with SFT and RLHF proposed by OpenAI's InstructGPT. Self-chat avoids human annotation, and self-distill avoids human feedback. Authors then trained a series of models based on such pipeline, such as (Baize v1, Baize v1.5 Baize v2) with (7B, 13B) parameters.  The self-distill process further added an extra LoRA module to the base model. The paper evaluate Baize using LM Evaluation Harness library and GPT-4 evaluator. The pipeline highlights efficency.\n\nreasons_to_accept: The paper proposed an efficient pipeline to imitate state-of-the-art proprietary chat models such as ChatGPT.\nThe paper proposed a novel self-chat technique, which avoids human annotation.\nThe paper proposed a novel self-distill technique, which avoids human feedback.\nThe pipeline highlights efficency.\n\nreasons_to_reject: I noticed that author stated that \"Different from these attempts, our work focuses on developing an affordable and reproducible pipeline to efficiently tune a general-purpose language model for multi-turn chat.\" Despite this, Baize's current results are not competitive on leaderboards like Huggingface OpenLLM. Considering running Baize requires the same resource as other high-ranking Llama-13B models, and the author failed to point out any directions to further make Baize competitive with other 13B models. So I suspect that the proposed pipeline not work. I think authors should further improve the proposed pipeline to get a competitive result.\nThe generated data may contain hallucination, finetuning on such data seems to cause model generate random response.\n\nquestions_for_the_authors: Have you ever tried using full parameter finetuning, what is the difference between full-parameter finetuning and LoRA on your data?  The generated data may contain hallucination, finetuning on such data seems to cause model generate random response. Have you ever tried some methods to avoid that?\nDo you have any future plan to improve Baize's pipeline?\nDo you think if you have a base model of ChatGPT (gpt-3.5), using self-chat and self-distill can yield a similar model with ChatGPT? If not, what is the reason?\nAre you confident on your proposed pipeline?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "130_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_130_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7131,
      "max_similarity": 0.7268,
      "avg_coverage": 0.5709333333333334,
      "max_coverage": 0.8333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 689,
      "avg_human_length": 320.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "S24OOltABM",
        "similarity": 0.711,
        "coverage": 0.8333,
        "human_length": 253,
        "human_text": "paper_topic_and_main_contributions: - This paper proposes a pipeline that automatically generates a multi-turn chat corpus using ChatGPT.\n- Based on collected dataset, the authors present a chatting model named Baize, trained with parameter-efficient tuning based on LLaMA.\n- They additionally apply reinforcement learning with self-feedback to improve the performance of Baize.\n\nreasons_to_accept: This paper presents valuable resources (e.g. dataset and model) for research community.\n\nreasons_to_reject: - The novelty of the proposed pipeline is limited, which has been already explored in several works [1], [2]. It is popular approach to generate dataset using LLMs and train smaller LMs on the collected dataset nowadays, and there is no other contributions to be considered in the proposed framework.\n- There is no quality control (e.g. filtering) or manual evaluation on the quality of data, which is collected through automatic model-based approach.\n- It is unclear which chatting model the authors aim to develop, and the motivation for developing such model is also unclear.\n[1] Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (NAACL 2022) [2] SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization (ACL 2023 Findings)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "DxCQOcNH1q",
        "similarity": 0.7015,
        "coverage": 0.5333,
        "human_length": 303,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a framework for efficiently performing Instruction-tuning on large language models (LLMs), utilizing ChatGPT. The framework involves conducting Self-chat using ChatGPT for collecting dialogues for the first stage of training. Subsequently, the obtained model is further trained using ChatGPT's Feedback as an alternative to RLHF, successfully enhancing the model's performance.\n\nreasons_to_accept: - The paper proposes an efficient and practical method for training high-quality LLM without the intervention of actual humans. It is considered an efficient and reproducible method compared to Vicuna, which is learned from actual human interactions with ShareGPT.\n- The evaluation has been conducted at each step of the proposed method, clearly demonstrating the utility of each part.\n- The paper is written clearly throughout, making it easy for readers to understand.\n\nreasons_to_reject: - The evaluation is limited to automated assessments, including those based on GPT, and lacks human evaluations.\n- As the learning is based on ChatGPT, the experimental conditions seem to favor the proposed method that uses the same model for training data; however, there is no discussion regarding this aspect.\n- Although multiple generation examples are shown, a detailed analysis of the model's performance is lacking, and the differences compared to other models like Vicuna are not clearly illustrated.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "R5XA0LJ0lb",
        "similarity": 0.7268,
        "coverage": 0.3462,
        "human_length": 406,
        "human_text": "paper_topic_and_main_contributions: The paper proposed an efficient pipeline (low cost, low compute) to imitate proprietary chat models. The paper utilized self-chat and self-distill techniques to generate chat data and human feedback data, in comparision with SFT and RLHF proposed by OpenAI's InstructGPT. Self-chat avoids human annotation, and self-distill avoids human feedback. Authors then trained a series of models based on such pipeline, such as (Baize v1, Baize v1.5 Baize v2) with (7B, 13B) parameters.  The self-distill process further added an extra LoRA module to the base model. The paper evaluate Baize using LM Evaluation Harness library and GPT-4 evaluator. The pipeline highlights efficency.\n\nreasons_to_accept: The paper proposed an efficient pipeline to imitate state-of-the-art proprietary chat models such as ChatGPT.\nThe paper proposed a novel self-chat technique, which avoids human annotation.\nThe paper proposed a novel self-distill technique, which avoids human feedback.\nThe pipeline highlights efficency.\n\nreasons_to_reject: I noticed that author stated that \"Different from these attempts, our work focuses on developing an affordable and reproducible pipeline to efficiently tune a general-purpose language model for multi-turn chat.\" Despite this, Baize's current results are not competitive on leaderboards like Huggingface OpenLLM. Considering running Baize requires the same resource as other high-ranking Llama-13B models, and the author failed to point out any directions to further make Baize competitive with other 13B models. So I suspect that the proposed pipeline not work. I think authors should further improve the proposed pipeline to get a competitive result.\nThe generated data may contain hallucination, finetuning on such data seems to cause model generate random response.\n\nquestions_for_the_authors: Have you ever tried using full parameter finetuning, what is the difference between full-parameter finetuning and LoRA on your data?  The generated data may contain hallucination, finetuning on such data seems to cause model generate random response. Have you ever tried some methods to avoid that?\nDo you have any future plan to improve Baize's pipeline?\nDo you think if you have a base model of ChatGPT (gpt-3.5), using self-chat and self-distill can yield a similar model with ChatGPT? If not, what is the reason?\nAre you confident on your proposed pipeline?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "S24OOltABM",
        "length": 253,
        "human_text": "paper_topic_and_main_contributions: - This paper proposes a pipeline that automatically generates a multi-turn chat corpus using ChatGPT.\n- Based on collected dataset, the authors present a chatting model named Baize, trained with parameter-efficient tuning based on LLaMA.\n- They additionally apply reinforcement learning with self-feedback to improve the performance of Baize.\n\nreasons_to_accept: This paper presents valuable resources (e.g. dataset and model) for research community.\n\nreasons_to_reject: - The novelty of the proposed pipeline is limited, which has been already explored in several works [1], [2]. It is popular approach to generate dataset using LLMs and train smaller LMs on the collected dataset nowadays, and there is no other contributions to be considered in the proposed framework.\n- There is no quality control (e.g. filtering) or manual evaluation on the quality of data, which is collected through automatic model-based approach.\n- It is unclear which chatting model the authors aim to develop, and the motivation for developing such model is also unclear.\n[1] Symbolic Knowledge Distillation: from General Language Models to Commonsense Models (NAACL 2022) [2] SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization (ACL 2023 Findings)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "DxCQOcNH1q",
        "length": 303,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a framework for efficiently performing Instruction-tuning on large language models (LLMs), utilizing ChatGPT. The framework involves conducting Self-chat using ChatGPT for collecting dialogues for the first stage of training. Subsequently, the obtained model is further trained using ChatGPT's Feedback as an alternative to RLHF, successfully enhancing the model's performance.\n\nreasons_to_accept: - The paper proposes an efficient and practical method for training high-quality LLM without the intervention of actual humans. It is considered an efficient and reproducible method compared to Vicuna, which is learned from actual human interactions with ShareGPT.\n- The evaluation has been conducted at each step of the proposed method, clearly demonstrating the utility of each part.\n- The paper is written clearly throughout, making it easy for readers to understand.\n\nreasons_to_reject: - The evaluation is limited to automated assessments, including those based on GPT, and lacks human evaluations.\n- As the learning is based on ChatGPT, the experimental conditions seem to favor the proposed method that uses the same model for training data; however, there is no discussion regarding this aspect.\n- Although multiple generation examples are shown, a detailed analysis of the model's performance is lacking, and the differences compared to other models like Vicuna are not clearly illustrated.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "R5XA0LJ0lb",
        "length": 406,
        "human_text": "paper_topic_and_main_contributions: The paper proposed an efficient pipeline (low cost, low compute) to imitate proprietary chat models. The paper utilized self-chat and self-distill techniques to generate chat data and human feedback data, in comparision with SFT and RLHF proposed by OpenAI's InstructGPT. Self-chat avoids human annotation, and self-distill avoids human feedback. Authors then trained a series of models based on such pipeline, such as (Baize v1, Baize v1.5 Baize v2) with (7B, 13B) parameters.  The self-distill process further added an extra LoRA module to the base model. The paper evaluate Baize using LM Evaluation Harness library and GPT-4 evaluator. The pipeline highlights efficency.\n\nreasons_to_accept: The paper proposed an efficient pipeline to imitate state-of-the-art proprietary chat models such as ChatGPT.\nThe paper proposed a novel self-chat technique, which avoids human annotation.\nThe paper proposed a novel self-distill technique, which avoids human feedback.\nThe pipeline highlights efficency.\n\nreasons_to_reject: I noticed that author stated that \"Different from these attempts, our work focuses on developing an affordable and reproducible pipeline to efficiently tune a general-purpose language model for multi-turn chat.\" Despite this, Baize's current results are not competitive on leaderboards like Huggingface OpenLLM. Considering running Baize requires the same resource as other high-ranking Llama-13B models, and the author failed to point out any directions to further make Baize competitive with other 13B models. So I suspect that the proposed pipeline not work. I think authors should further improve the proposed pipeline to get a competitive result.\nThe generated data may contain hallucination, finetuning on such data seems to cause model generate random response.\n\nquestions_for_the_authors: Have you ever tried using full parameter finetuning, what is the difference between full-parameter finetuning and LoRA on your data?  The generated data may contain hallucination, finetuning on such data seems to cause model generate random response. Have you ever tried some methods to avoid that?\nDo you have any future plan to improve Baize's pipeline?\nDo you think if you have a base model of ChatGPT (gpt-3.5), using self-chat and self-distill can yield a similar model with ChatGPT? If not, what is the reason?\nAre you confident on your proposed pipeline?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "202_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_202_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7091,
      "max_similarity": 0.7288,
      "avg_coverage": 0.5219,
      "max_coverage": 0.6
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 466,
      "avg_human_length": 403.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 7,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "9eEVDw2FHU",
        "similarity": 0.6949,
        "coverage": 0.6,
        "human_length": 380,
        "human_text": "paper_topic_and_main_contributions: This is an experimental paper that aims to investigate the following research question:  Do linguistic variations in prompts have an effect in performance over final NLP tasks for LLMs? \nTo pursue its goal, the paper examined the performance of LLMs on different classification tasks mainly contained in GLUE and SuperGLUE. Linguistic variations are characterized by different features: mood, aspect, tense, modality, and synonymy.\n\nreasons_to_accept: - The paper aims to make a significant contribution to the understanding of how prompts should be phrased in order to obtain the best results over linguistic tasks.  - The paper performs extensive experimental analysis\n\nreasons_to_reject: - The large experimental analysis does not reveal any clear pattern and does not help in choosing what are the best ways to write a prompt for a classification task - The analysis is confined to classification tasks - It is even questionable if it is important to perform a linguistic classification task with LLMs. Indeed, solving many of the tasks is useless since we have LLMs that make decisions and generate responses without using explicit solvers for these tasks.\n\nquestions_for_the_authors: - Can you please elaborate on the reason why you have decided to not apply synonymy to the main verb of the prompt?\n\nmissing_references: No missing references\n\ntypos_grammar_style_and_presentation_improvements: Writing the introduction: - please reorganize the introduction such that there is a clear paragraph describing the work of your paper, which is: studying the effect on performances of syntactic variations on semantically equivalent prompts.\n- \"To test ***this hypothesis***, we examine prompting ... \" (line 056). It is weird to start a paragraph with an anaphoric word referring to an idea expressed in the previous paragraph. This sentence needs rephrasing.  - Make clear that the \"hypothesis\" is expressed in lines 051-055 (that is **this hypothesis***) and it is part of your intuition in building the paper\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "396OXbrxNc",
        "similarity": 0.7288,
        "coverage": 0.4324,
        "human_length": 566,
        "human_text": "paper_topic_and_main_contributions: This paper conducts comprehensive experiments to show the prompt instability of LLMs. Unlike previous work, this paper investigates the prompt robustness of the model from a lexical perspective, where the authors slightly shift the linguistic properties (e.g., mood, tense) of prompt and find a significant performance variation of LLMs, including instruction-tuned LLMs. Some results are also different from the previous works, suggesting the vulnerability of numerous research findings in current prompt-related research. Thus, the author also concludes with some empirical advice for future research.\n\nreasons_to_accept: 1. ** A novel perspective**. The author utilizes a lexico-level perturbation and linguistic perspective to investigate the prompt robustness, which differs from the previous sentence-level paraphrasing or simple synonymy replacing. \n2. ** Useful proposal**. The proposal concluded by this paper might also benefit future research.\n\nreasons_to_reject: 1. ** Limited contribution**. Though this paper provides some useful suggestions, most conclusions have already been proposed by a bunch of previous works or are just commonsense for the community (e.g., prompt transfer). During the whole section 5, the authors simply introduce the results and compare scores, but there are no more insights, analysis, or explanations. After reading the whole paper, I didn't seem to learn much from it.\n2. ** Concern about the experiment setting.** In the experiment, the authors adopt five models, but essentially, there are only two model categories --- vanilla LMs and instruction-tuned LMs. Similarly, all of these models (OPT, LLaMA) are decoder-only LMs. I am worried about the expandability of the experimental conclusion. Similarly, the datasets seem only cover the cross-dataset generalization setting; a better choice is to adopt a more challenging cross-task setting [1], which is a popular trend in the current community.\n3. ** Some viewpoints are subjective.** For example, in line 537, the authors suggest \"include estimates of performance mean and variance based on a large set of prompts\". Personally, I think the prompt is just a fundamental \"feature\" for LLMs to deal with a specific task, \"prompt engineering\" is just similar to \"feature engineering\"; we have to tune an optimal (or sub-optimal) prompt for one specific task, but we cannot anticipate the LMs will be robust to the change of the prompt. So reporting statistic scores on large-set prompts for one task seems unrealistic. But I agree with \"treating prompts as hyperparameters\" --- tune the prompt on dev set, fix it, and then repot scores on the test set.\n4. ** Writing.** I found it hard to follow some specific sections of this paper. For example, the title of section 5.3 --- \"Robustness and instruction-tuning\" --- sounds a little bit confusing.\n--- References: [1]. Mishra S, Khashabi D, Baral C, et al. Cross-task generalization via natural language crowdsourcing instructions[J]. arXiv preprint arXiv:2104.08773, 2021.\n\nquestions_for_the_authors: In your experiments, did you design all the prompts by yourself? Since you want to investigate the prompt robustness of instruction-tuned LMs, it should be better to choose the prompts used in tuning OPT-IML (seen by LMs during training), which can more effectively demonstrate your motivations.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "ZqwCNyulIu",
        "similarity": 0.7036,
        "coverage": 0.5333,
        "human_length": 263,
        "human_text": "paper_topic_and_main_contributions: This paper presents a large-scale systematic analysis of how the variations in prompts influence downstream performances. The authors conduct empirical studies on multiple tasks using different LMs under various settings. In these studies, the authors control (i) grammatical properties such as mood, tense and modality; and (ii)  lexico-semantic variation by replacing a word with its synonyms. Experimental results show that prompts transfer rather poorly across datasets and LMs.\n\nreasons_to_accept: - A very comprehensive and systematic empirical study of the instability of prompt choices. Those experimental results could potentially be utilized in many future studies.\n- The authors present several interesting conclusions, such as the existence of the correlation between LM perplexity of prompts and performance.\n\nreasons_to_reject: - Though it is not a serious flaw, it would make this paper stronger if the authors could give some explanations on why they choose certain types of linguistic variations, such as mood and tense. What if the prompts are paraphrased to a larger extent, e.g., using a constituency parser to generate sub-tree structures and making changes on the sub-tree level?\n\nmissing_references: - Do Prompts Solve NLP Tasks Using Natural Language? Sen Yang, Yunchen Zhang, Leyang Cui and Yue Zhang. ArXiv 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "9eEVDw2FHU",
        "length": 380,
        "human_text": "paper_topic_and_main_contributions: This is an experimental paper that aims to investigate the following research question:  Do linguistic variations in prompts have an effect in performance over final NLP tasks for LLMs? \nTo pursue its goal, the paper examined the performance of LLMs on different classification tasks mainly contained in GLUE and SuperGLUE. Linguistic variations are characterized by different features: mood, aspect, tense, modality, and synonymy.\n\nreasons_to_accept: - The paper aims to make a significant contribution to the understanding of how prompts should be phrased in order to obtain the best results over linguistic tasks.  - The paper performs extensive experimental analysis\n\nreasons_to_reject: - The large experimental analysis does not reveal any clear pattern and does not help in choosing what are the best ways to write a prompt for a classification task - The analysis is confined to classification tasks - It is even questionable if it is important to perform a linguistic classification task with LLMs. Indeed, solving many of the tasks is useless since we have LLMs that make decisions and generate responses without using explicit solvers for these tasks.\n\nquestions_for_the_authors: - Can you please elaborate on the reason why you have decided to not apply synonymy to the main verb of the prompt?\n\nmissing_references: No missing references\n\ntypos_grammar_style_and_presentation_improvements: Writing the introduction: - please reorganize the introduction such that there is a clear paragraph describing the work of your paper, which is: studying the effect on performances of syntactic variations on semantically equivalent prompts.\n- \"To test ***this hypothesis***, we examine prompting ... \" (line 056). It is weird to start a paragraph with an anaphoric word referring to an idea expressed in the previous paragraph. This sentence needs rephrasing.  - Make clear that the \"hypothesis\" is expressed in lines 051-055 (that is **this hypothesis***) and it is part of your intuition in building the paper\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "396OXbrxNc",
        "length": 566,
        "human_text": "paper_topic_and_main_contributions: This paper conducts comprehensive experiments to show the prompt instability of LLMs. Unlike previous work, this paper investigates the prompt robustness of the model from a lexical perspective, where the authors slightly shift the linguistic properties (e.g., mood, tense) of prompt and find a significant performance variation of LLMs, including instruction-tuned LLMs. Some results are also different from the previous works, suggesting the vulnerability of numerous research findings in current prompt-related research. Thus, the author also concludes with some empirical advice for future research.\n\nreasons_to_accept: 1. ** A novel perspective**. The author utilizes a lexico-level perturbation and linguistic perspective to investigate the prompt robustness, which differs from the previous sentence-level paraphrasing or simple synonymy replacing. \n2. ** Useful proposal**. The proposal concluded by this paper might also benefit future research.\n\nreasons_to_reject: 1. ** Limited contribution**. Though this paper provides some useful suggestions, most conclusions have already been proposed by a bunch of previous works or are just commonsense for the community (e.g., prompt transfer). During the whole section 5, the authors simply introduce the results and compare scores, but there are no more insights, analysis, or explanations. After reading the whole paper, I didn't seem to learn much from it.\n2. ** Concern about the experiment setting.** In the experiment, the authors adopt five models, but essentially, there are only two model categories --- vanilla LMs and instruction-tuned LMs. Similarly, all of these models (OPT, LLaMA) are decoder-only LMs. I am worried about the expandability of the experimental conclusion. Similarly, the datasets seem only cover the cross-dataset generalization setting; a better choice is to adopt a more challenging cross-task setting [1], which is a popular trend in the current community.\n3. ** Some viewpoints are subjective.** For example, in line 537, the authors suggest \"include estimates of performance mean and variance based on a large set of prompts\". Personally, I think the prompt is just a fundamental \"feature\" for LLMs to deal with a specific task, \"prompt engineering\" is just similar to \"feature engineering\"; we have to tune an optimal (or sub-optimal) prompt for one specific task, but we cannot anticipate the LMs will be robust to the change of the prompt. So reporting statistic scores on large-set prompts for one task seems unrealistic. But I agree with \"treating prompts as hyperparameters\" --- tune the prompt on dev set, fix it, and then repot scores on the test set.\n4. ** Writing.** I found it hard to follow some specific sections of this paper. For example, the title of section 5.3 --- \"Robustness and instruction-tuning\" --- sounds a little bit confusing.\n--- References: [1]. Mishra S, Khashabi D, Baral C, et al. Cross-task generalization via natural language crowdsourcing instructions[J]. arXiv preprint arXiv:2104.08773, 2021.\n\nquestions_for_the_authors: In your experiments, did you design all the prompts by yourself? Since you want to investigate the prompt robustness of instruction-tuned LMs, it should be better to choose the prompts used in tuning OPT-IML (seen by LMs during training), which can more effectively demonstrate your motivations.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ZqwCNyulIu",
        "length": 263,
        "human_text": "paper_topic_and_main_contributions: This paper presents a large-scale systematic analysis of how the variations in prompts influence downstream performances. The authors conduct empirical studies on multiple tasks using different LMs under various settings. In these studies, the authors control (i) grammatical properties such as mood, tense and modality; and (ii)  lexico-semantic variation by replacing a word with its synonyms. Experimental results show that prompts transfer rather poorly across datasets and LMs.\n\nreasons_to_accept: - A very comprehensive and systematic empirical study of the instability of prompt choices. Those experimental results could potentially be utilized in many future studies.\n- The authors present several interesting conclusions, such as the existence of the correlation between LM perplexity of prompts and performance.\n\nreasons_to_reject: - Though it is not a serious flaw, it would make this paper stronger if the authors could give some explanations on why they choose certain types of linguistic variations, such as mood and tense. What if the prompts are paraphrased to a larger extent, e.g., using a constituency parser to generate sub-tree structures and making changes on the sub-tree level?\n\nmissing_references: - Do Prompts Solve NLP Tasks Using Natural Language? Sen Yang, Yunchen Zhang, Leyang Cui and Yue Zhang. ArXiv 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "202_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_202_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7204666666666667,
      "max_similarity": 0.7396,
      "avg_coverage": 0.4504666666666666,
      "max_coverage": 0.5333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 294,
      "avg_human_length": 403.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 7,
      "suggestions_count": 2
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "9eEVDw2FHU",
        "similarity": 0.7097,
        "coverage": 0.5333,
        "human_length": 380,
        "human_text": "paper_topic_and_main_contributions: This is an experimental paper that aims to investigate the following research question:  Do linguistic variations in prompts have an effect in performance over final NLP tasks for LLMs? \nTo pursue its goal, the paper examined the performance of LLMs on different classification tasks mainly contained in GLUE and SuperGLUE. Linguistic variations are characterized by different features: mood, aspect, tense, modality, and synonymy.\n\nreasons_to_accept: - The paper aims to make a significant contribution to the understanding of how prompts should be phrased in order to obtain the best results over linguistic tasks.  - The paper performs extensive experimental analysis\n\nreasons_to_reject: - The large experimental analysis does not reveal any clear pattern and does not help in choosing what are the best ways to write a prompt for a classification task - The analysis is confined to classification tasks - It is even questionable if it is important to perform a linguistic classification task with LLMs. Indeed, solving many of the tasks is useless since we have LLMs that make decisions and generate responses without using explicit solvers for these tasks.\n\nquestions_for_the_authors: - Can you please elaborate on the reason why you have decided to not apply synonymy to the main verb of the prompt?\n\nmissing_references: No missing references\n\ntypos_grammar_style_and_presentation_improvements: Writing the introduction: - please reorganize the introduction such that there is a clear paragraph describing the work of your paper, which is: studying the effect on performances of syntactic variations on semantically equivalent prompts.\n- \"To test ***this hypothesis***, we examine prompting ... \" (line 056). It is weird to start a paragraph with an anaphoric word referring to an idea expressed in the previous paragraph. This sentence needs rephrasing.  - Make clear that the \"hypothesis\" is expressed in lines 051-055 (that is **this hypothesis***) and it is part of your intuition in building the paper\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "396OXbrxNc",
        "similarity": 0.7396,
        "coverage": 0.3514,
        "human_length": 566,
        "human_text": "paper_topic_and_main_contributions: This paper conducts comprehensive experiments to show the prompt instability of LLMs. Unlike previous work, this paper investigates the prompt robustness of the model from a lexical perspective, where the authors slightly shift the linguistic properties (e.g., mood, tense) of prompt and find a significant performance variation of LLMs, including instruction-tuned LLMs. Some results are also different from the previous works, suggesting the vulnerability of numerous research findings in current prompt-related research. Thus, the author also concludes with some empirical advice for future research.\n\nreasons_to_accept: 1. ** A novel perspective**. The author utilizes a lexico-level perturbation and linguistic perspective to investigate the prompt robustness, which differs from the previous sentence-level paraphrasing or simple synonymy replacing. \n2. ** Useful proposal**. The proposal concluded by this paper might also benefit future research.\n\nreasons_to_reject: 1. ** Limited contribution**. Though this paper provides some useful suggestions, most conclusions have already been proposed by a bunch of previous works or are just commonsense for the community (e.g., prompt transfer). During the whole section 5, the authors simply introduce the results and compare scores, but there are no more insights, analysis, or explanations. After reading the whole paper, I didn't seem to learn much from it.\n2. ** Concern about the experiment setting.** In the experiment, the authors adopt five models, but essentially, there are only two model categories --- vanilla LMs and instruction-tuned LMs. Similarly, all of these models (OPT, LLaMA) are decoder-only LMs. I am worried about the expandability of the experimental conclusion. Similarly, the datasets seem only cover the cross-dataset generalization setting; a better choice is to adopt a more challenging cross-task setting [1], which is a popular trend in the current community.\n3. ** Some viewpoints are subjective.** For example, in line 537, the authors suggest \"include estimates of performance mean and variance based on a large set of prompts\". Personally, I think the prompt is just a fundamental \"feature\" for LLMs to deal with a specific task, \"prompt engineering\" is just similar to \"feature engineering\"; we have to tune an optimal (or sub-optimal) prompt for one specific task, but we cannot anticipate the LMs will be robust to the change of the prompt. So reporting statistic scores on large-set prompts for one task seems unrealistic. But I agree with \"treating prompts as hyperparameters\" --- tune the prompt on dev set, fix it, and then repot scores on the test set.\n4. ** Writing.** I found it hard to follow some specific sections of this paper. For example, the title of section 5.3 --- \"Robustness and instruction-tuning\" --- sounds a little bit confusing.\n--- References: [1]. Mishra S, Khashabi D, Baral C, et al. Cross-task generalization via natural language crowdsourcing instructions[J]. arXiv preprint arXiv:2104.08773, 2021.\n\nquestions_for_the_authors: In your experiments, did you design all the prompts by yourself? Since you want to investigate the prompt robustness of instruction-tuned LMs, it should be better to choose the prompts used in tuning OPT-IML (seen by LMs during training), which can more effectively demonstrate your motivations.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "ZqwCNyulIu",
        "similarity": 0.7121,
        "coverage": 0.4667,
        "human_length": 263,
        "human_text": "paper_topic_and_main_contributions: This paper presents a large-scale systematic analysis of how the variations in prompts influence downstream performances. The authors conduct empirical studies on multiple tasks using different LMs under various settings. In these studies, the authors control (i) grammatical properties such as mood, tense and modality; and (ii)  lexico-semantic variation by replacing a word with its synonyms. Experimental results show that prompts transfer rather poorly across datasets and LMs.\n\nreasons_to_accept: - A very comprehensive and systematic empirical study of the instability of prompt choices. Those experimental results could potentially be utilized in many future studies.\n- The authors present several interesting conclusions, such as the existence of the correlation between LM perplexity of prompts and performance.\n\nreasons_to_reject: - Though it is not a serious flaw, it would make this paper stronger if the authors could give some explanations on why they choose certain types of linguistic variations, such as mood and tense. What if the prompts are paraphrased to a larger extent, e.g., using a constituency parser to generate sub-tree structures and making changes on the sub-tree level?\n\nmissing_references: - Do Prompts Solve NLP Tasks Using Natural Language? Sen Yang, Yunchen Zhang, Leyang Cui and Yue Zhang. ArXiv 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "9eEVDw2FHU",
        "length": 380,
        "human_text": "paper_topic_and_main_contributions: This is an experimental paper that aims to investigate the following research question:  Do linguistic variations in prompts have an effect in performance over final NLP tasks for LLMs? \nTo pursue its goal, the paper examined the performance of LLMs on different classification tasks mainly contained in GLUE and SuperGLUE. Linguistic variations are characterized by different features: mood, aspect, tense, modality, and synonymy.\n\nreasons_to_accept: - The paper aims to make a significant contribution to the understanding of how prompts should be phrased in order to obtain the best results over linguistic tasks.  - The paper performs extensive experimental analysis\n\nreasons_to_reject: - The large experimental analysis does not reveal any clear pattern and does not help in choosing what are the best ways to write a prompt for a classification task - The analysis is confined to classification tasks - It is even questionable if it is important to perform a linguistic classification task with LLMs. Indeed, solving many of the tasks is useless since we have LLMs that make decisions and generate responses without using explicit solvers for these tasks.\n\nquestions_for_the_authors: - Can you please elaborate on the reason why you have decided to not apply synonymy to the main verb of the prompt?\n\nmissing_references: No missing references\n\ntypos_grammar_style_and_presentation_improvements: Writing the introduction: - please reorganize the introduction such that there is a clear paragraph describing the work of your paper, which is: studying the effect on performances of syntactic variations on semantically equivalent prompts.\n- \"To test ***this hypothesis***, we examine prompting ... \" (line 056). It is weird to start a paragraph with an anaphoric word referring to an idea expressed in the previous paragraph. This sentence needs rephrasing.  - Make clear that the \"hypothesis\" is expressed in lines 051-055 (that is **this hypothesis***) and it is part of your intuition in building the paper\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "396OXbrxNc",
        "length": 566,
        "human_text": "paper_topic_and_main_contributions: This paper conducts comprehensive experiments to show the prompt instability of LLMs. Unlike previous work, this paper investigates the prompt robustness of the model from a lexical perspective, where the authors slightly shift the linguistic properties (e.g., mood, tense) of prompt and find a significant performance variation of LLMs, including instruction-tuned LLMs. Some results are also different from the previous works, suggesting the vulnerability of numerous research findings in current prompt-related research. Thus, the author also concludes with some empirical advice for future research.\n\nreasons_to_accept: 1. ** A novel perspective**. The author utilizes a lexico-level perturbation and linguistic perspective to investigate the prompt robustness, which differs from the previous sentence-level paraphrasing or simple synonymy replacing. \n2. ** Useful proposal**. The proposal concluded by this paper might also benefit future research.\n\nreasons_to_reject: 1. ** Limited contribution**. Though this paper provides some useful suggestions, most conclusions have already been proposed by a bunch of previous works or are just commonsense for the community (e.g., prompt transfer). During the whole section 5, the authors simply introduce the results and compare scores, but there are no more insights, analysis, or explanations. After reading the whole paper, I didn't seem to learn much from it.\n2. ** Concern about the experiment setting.** In the experiment, the authors adopt five models, but essentially, there are only two model categories --- vanilla LMs and instruction-tuned LMs. Similarly, all of these models (OPT, LLaMA) are decoder-only LMs. I am worried about the expandability of the experimental conclusion. Similarly, the datasets seem only cover the cross-dataset generalization setting; a better choice is to adopt a more challenging cross-task setting [1], which is a popular trend in the current community.\n3. ** Some viewpoints are subjective.** For example, in line 537, the authors suggest \"include estimates of performance mean and variance based on a large set of prompts\". Personally, I think the prompt is just a fundamental \"feature\" for LLMs to deal with a specific task, \"prompt engineering\" is just similar to \"feature engineering\"; we have to tune an optimal (or sub-optimal) prompt for one specific task, but we cannot anticipate the LMs will be robust to the change of the prompt. So reporting statistic scores on large-set prompts for one task seems unrealistic. But I agree with \"treating prompts as hyperparameters\" --- tune the prompt on dev set, fix it, and then repot scores on the test set.\n4. ** Writing.** I found it hard to follow some specific sections of this paper. For example, the title of section 5.3 --- \"Robustness and instruction-tuning\" --- sounds a little bit confusing.\n--- References: [1]. Mishra S, Khashabi D, Baral C, et al. Cross-task generalization via natural language crowdsourcing instructions[J]. arXiv preprint arXiv:2104.08773, 2021.\n\nquestions_for_the_authors: In your experiments, did you design all the prompts by yourself? Since you want to investigate the prompt robustness of instruction-tuned LMs, it should be better to choose the prompts used in tuning OPT-IML (seen by LMs during training), which can more effectively demonstrate your motivations.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ZqwCNyulIu",
        "length": 263,
        "human_text": "paper_topic_and_main_contributions: This paper presents a large-scale systematic analysis of how the variations in prompts influence downstream performances. The authors conduct empirical studies on multiple tasks using different LMs under various settings. In these studies, the authors control (i) grammatical properties such as mood, tense and modality; and (ii)  lexico-semantic variation by replacing a word with its synonyms. Experimental results show that prompts transfer rather poorly across datasets and LMs.\n\nreasons_to_accept: - A very comprehensive and systematic empirical study of the instability of prompt choices. Those experimental results could potentially be utilized in many future studies.\n- The authors present several interesting conclusions, such as the existence of the correlation between LM perplexity of prompts and performance.\n\nreasons_to_reject: - Though it is not a serious flaw, it would make this paper stronger if the authors could give some explanations on why they choose certain types of linguistic variations, such as mood and tense. What if the prompts are paraphrased to a larger extent, e.g., using a constituency parser to generate sub-tree structures and making changes on the sub-tree level?\n\nmissing_references: - Do Prompts Solve NLP Tasks Using Natural Language? Sen Yang, Yunchen Zhang, Leyang Cui and Yue Zhang. ArXiv 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "181_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_181_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7339333333333333,
      "max_similarity": 0.756,
      "avg_coverage": 0.4973,
      "max_coverage": 0.5833
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 869,
      "avg_human_length": 532.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "oqVy0moOXf",
        "similarity": 0.714,
        "coverage": 0.5833,
        "human_length": 426,
        "human_text": "paper_topic_and_main_contributions: This paper investigates the impact of influence-functions in the context of identifying self-influential examples (i.e. examples that are difficult to predict from other examples and are thus hypothesised to be memorised). The paper investigates properties of such self-influential examples, suggesting that they are properties of data rather than dependant on model architecture and initialisation. Finally, the paper shows how knowledge about such examples can be used to improve training and out-of-distribution generalisation of models.\n\nreasons_to_accept: The contribution seems novel, the experiments are (for the most part) well executed and motivated, the research questions are clear and the results support the findings.\nI appreciate the fact that some of the less well-known techniques, metrics, etc by examples.\n\nreasons_to_reject: I have two concerns with this paper.\nFirstly, I don't think the results regarding the stability of influence functions would withstand a statistical test - only two results are being compared here (although for different settings). Perhaps the chosen different initialisation/model happened to produce a similar result by chance. I believe for more robust conclusions, the sample set of observations should be more than a pair. I appreciate that these experiments are computationally expensive, perhaps a bigger sample could be investigated for a subset of the investigated settings (e.g. 'first' only in Table 2) Secondly, I find the paper's content arrangement rather odd, see my comments in the presentation section. But this is only a minor concern and could be addressed by an additional page of content, should the paper be accepted.\n\nquestions_for_the_authors: Question 1: Are you going to release the experimental setup to the public?\n\nmissing_references: None that I'm aware of.\n\ntypos_grammar_style_and_presentation_improvements: I find it odd to move the related work section into the appendix, while spending a whole page on Sections 2 and 3 and 4.1, which are essentially background. Sections 3 and 4.1 could definitely be moved to the appendix, since they're already summarised in Table 1.\nFurthermore, the introduction set's the scene rather awkwardly. I believe the concept of self-influence should be (at least intuitively) introduced earlier and followed up by the research questions this paper is addressing. I am not sure why the paragraphs in lines 64 to 96 are so prominent.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Hosf0hdEW6",
        "similarity": 0.7318,
        "coverage": 0.4667,
        "human_length": 643,
        "human_text": "paper_topic_and_main_contributions: The paper studied whether self-influence scores based on Arnoldi-based In\ufb02uence Function (ABIF) can be used to filter out low-quality data (outliers, e.g. noises) so as to improve the test performance. Specifically, the authors explored 1) the stability of computed self-influence scores against different a) model states (i.e. initialization, data ordering, and batch size) and b) model size and architecture, 2) whether self-influence scores can be used to select noises, and 3) whether one can use self-influence scores to divide datasets into different subsets, and then use Automated Curriculum Learning (AutoCL) to improve training performance. The results show that 1) self-influence scores are more robust against model states than sizes and architectures, 2) self-influence scores are better at capturing synthetic data noises than natural noises, and 3) AutoCL help learning better than filtering, based on self-influence scores.\n\nreasons_to_accept: - It is an interesting idea to use self-influence score to filter data.  - Understanding instability is important for such filtering to put into practice.  - Using AutoCL instead of filtering is intriging, and the performance shown is encouraging.\n\nreasons_to_reject: - The experiment setups are not well-explained, and thus not reproducible/convincing     * Lines 360-362 should be more specific: what are the specific choices of hyper-parameters (for reproducibility)? How would that change performance (to better ground the stability)? \n    * In table 4, how did the authors choose different percentage of data to filter out?  - The writing of the background/methods is hard to follow, important points include:      * Is the IF approximation method proposed by Koh and Liang (2017)? I think the HVP method was proposed quite early. \n    * Without reading Section 7, it is hard to understand what are the different subsets of data in AutoCL     * Math notation system in AutoCL is used without introduction: what are y/Y? Also, in pgnorm (I think \\mathcal L is loss?), on which data the loss should be computed; and what are the gradient and reward batch?   - The results of synthetic noises vs. natural noises detection confuse me: on the one hand, data filtering based on self-influence improved OOD performance (Table 4); however, on the other hand, such filtering failed to filter out natural noises (Figure 1) \u2014 what are the implications then? Reading the full results, I feel some pieces of evidence are missing here: self-influence scores can help filter out noisy data and help with AutoCL, however, it cannot really detect natural noises, which is a bit puzzling.\n\nquestions_for_the_authors: - Do you think high self-influence data instances always harmful for models? I am thinking of large language models --- one would need LLMs to memorize certain facts to give correct answers; in this case, such memorization/high self-influence can become an advantage?  - Do you have any hypothesis why it is this case, that first/all are less sensitive to size/attention, but more sensitive to model states?\n- What are the fundamental difference between synthetic and natural noises that leads to the successful filtering only on synthetic data?\n\nmissing_references: Second-Order Stochastic Optimization for Machine Learning in Linear Time, Agarwal et al., 2017.\n\ntypos_grammar_style_and_presentation_improvements: A bit strange to mention a section (5.3) and put everything in the appendix. It would be better give some more information in the main text for the purpose of being self-contained.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "6wH6pSCrmE",
        "similarity": 0.756,
        "coverage": 0.4419,
        "human_length": 529,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on identifying noisy data/outliers and filtering out harmful instances to improve model performance. The author proposes using self-influence scores for data filtering and incorporating bandit curriculum learning to automate the filtering process instead of using a fixed filtering threshold. The stability and effectiveness of the self-influence scores are analyzed.\n\nreasons_to_accept: 1. The idea of using a self-influence score to identify outliers or harmful samples is inspiring. If removing a sample deteriorates the loss value on itself, then this sample should be different enough from the rest of the training data. Such samples could be mislabeled, ambiguous, or difficult samples, or out-of-distribution samples. \n2. Automated filtering using bandit curriculum learning seems effective and avoids manually tuning the hyperparameters of the previous fixed threshold. \n3. Both the stability and efficacy of self-influence are analyzed on diverse datasets, tasks, and models, demonstrating the generality of the proposed method.\n\nreasons_to_reject: 1. The claim that the proposed self-influence score is stable with respect to training and model hyperparameters across architecture variations is not actually supported by the evidence in Table 2 and Table 3. It can be observed that the self-influence score varies greatly for different model architectures and training hyperparameters. \n2. The effectiveness of the proposed self-influence score is doubtful. When demonstrating the efficacy of the self-influence score in Table 4, nearly half of the results are missing and replaced with '-'. This may raise doubts that only favorable results are picked, especially for the out-of-distribution test. It would be appreciated if the author could provide those missing results and justify why they were missed in the original table. \n3. There is no comparison with other data filtering techniques, such as AUM, Data Cartography, or PVI. How does the performance compare with those existing data filtering techniques? \n4. I agree with the other two reviewers that the paper is hard to read. A better presentation style and more clarity would be appreciated. Specific suggestions were well given by the other two reviewers.\nR[1] Pleiss, Geoff, Tianyi Zhang, Ethan Elenberg, and Kilian Q. Weinberger. \" Identifying mislabeled data using the area under the margin ranking.\" \u00a0*Advances in Neural Information Processing Systems*\u00a033 (2020): 17044-17056.\nR[2] Swayamdipta, Swabha, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and Yejin Choi. \" Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics.\" In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9275-9293. 2020.\nR[3] Ethayarajh, Kawin, Yejin Choi, and Swabha Swayamdipta. \" Understanding Dataset Difficulty with $\\mathcalV $-Usable Information.\" In International Conference on Machine Learning, pp. 5988-6008. PMLR, 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "oqVy0moOXf",
        "length": 426,
        "human_text": "paper_topic_and_main_contributions: This paper investigates the impact of influence-functions in the context of identifying self-influential examples (i.e. examples that are difficult to predict from other examples and are thus hypothesised to be memorised). The paper investigates properties of such self-influential examples, suggesting that they are properties of data rather than dependant on model architecture and initialisation. Finally, the paper shows how knowledge about such examples can be used to improve training and out-of-distribution generalisation of models.\n\nreasons_to_accept: The contribution seems novel, the experiments are (for the most part) well executed and motivated, the research questions are clear and the results support the findings.\nI appreciate the fact that some of the less well-known techniques, metrics, etc by examples.\n\nreasons_to_reject: I have two concerns with this paper.\nFirstly, I don't think the results regarding the stability of influence functions would withstand a statistical test - only two results are being compared here (although for different settings). Perhaps the chosen different initialisation/model happened to produce a similar result by chance. I believe for more robust conclusions, the sample set of observations should be more than a pair. I appreciate that these experiments are computationally expensive, perhaps a bigger sample could be investigated for a subset of the investigated settings (e.g. 'first' only in Table 2) Secondly, I find the paper's content arrangement rather odd, see my comments in the presentation section. But this is only a minor concern and could be addressed by an additional page of content, should the paper be accepted.\n\nquestions_for_the_authors: Question 1: Are you going to release the experimental setup to the public?\n\nmissing_references: None that I'm aware of.\n\ntypos_grammar_style_and_presentation_improvements: I find it odd to move the related work section into the appendix, while spending a whole page on Sections 2 and 3 and 4.1, which are essentially background. Sections 3 and 4.1 could definitely be moved to the appendix, since they're already summarised in Table 1.\nFurthermore, the introduction set's the scene rather awkwardly. I believe the concept of self-influence should be (at least intuitively) introduced earlier and followed up by the research questions this paper is addressing. I am not sure why the paragraphs in lines 64 to 96 are so prominent.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "Hosf0hdEW6",
        "length": 643,
        "human_text": "paper_topic_and_main_contributions: The paper studied whether self-influence scores based on Arnoldi-based In\ufb02uence Function (ABIF) can be used to filter out low-quality data (outliers, e.g. noises) so as to improve the test performance. Specifically, the authors explored 1) the stability of computed self-influence scores against different a) model states (i.e. initialization, data ordering, and batch size) and b) model size and architecture, 2) whether self-influence scores can be used to select noises, and 3) whether one can use self-influence scores to divide datasets into different subsets, and then use Automated Curriculum Learning (AutoCL) to improve training performance. The results show that 1) self-influence scores are more robust against model states than sizes and architectures, 2) self-influence scores are better at capturing synthetic data noises than natural noises, and 3) AutoCL help learning better than filtering, based on self-influence scores.\n\nreasons_to_accept: - It is an interesting idea to use self-influence score to filter data.  - Understanding instability is important for such filtering to put into practice.  - Using AutoCL instead of filtering is intriging, and the performance shown is encouraging.\n\nreasons_to_reject: - The experiment setups are not well-explained, and thus not reproducible/convincing     * Lines 360-362 should be more specific: what are the specific choices of hyper-parameters (for reproducibility)? How would that change performance (to better ground the stability)? \n    * In table 4, how did the authors choose different percentage of data to filter out?  - The writing of the background/methods is hard to follow, important points include:      * Is the IF approximation method proposed by Koh and Liang (2017)? I think the HVP method was proposed quite early. \n    * Without reading Section 7, it is hard to understand what are the different subsets of data in AutoCL     * Math notation system in AutoCL is used without introduction: what are y/Y? Also, in pgnorm (I think \\mathcal L is loss?), on which data the loss should be computed; and what are the gradient and reward batch?   - The results of synthetic noises vs. natural noises detection confuse me: on the one hand, data filtering based on self-influence improved OOD performance (Table 4); however, on the other hand, such filtering failed to filter out natural noises (Figure 1) \u2014 what are the implications then? Reading the full results, I feel some pieces of evidence are missing here: self-influence scores can help filter out noisy data and help with AutoCL, however, it cannot really detect natural noises, which is a bit puzzling.\n\nquestions_for_the_authors: - Do you think high self-influence data instances always harmful for models? I am thinking of large language models --- one would need LLMs to memorize certain facts to give correct answers; in this case, such memorization/high self-influence can become an advantage?  - Do you have any hypothesis why it is this case, that first/all are less sensitive to size/attention, but more sensitive to model states?\n- What are the fundamental difference between synthetic and natural noises that leads to the successful filtering only on synthetic data?\n\nmissing_references: Second-Order Stochastic Optimization for Machine Learning in Linear Time, Agarwal et al., 2017.\n\ntypos_grammar_style_and_presentation_improvements: A bit strange to mention a section (5.3) and put everything in the appendix. It would be better give some more information in the main text for the purpose of being self-contained.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "6wH6pSCrmE",
        "length": 529,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on identifying noisy data/outliers and filtering out harmful instances to improve model performance. The author proposes using self-influence scores for data filtering and incorporating bandit curriculum learning to automate the filtering process instead of using a fixed filtering threshold. The stability and effectiveness of the self-influence scores are analyzed.\n\nreasons_to_accept: 1. The idea of using a self-influence score to identify outliers or harmful samples is inspiring. If removing a sample deteriorates the loss value on itself, then this sample should be different enough from the rest of the training data. Such samples could be mislabeled, ambiguous, or difficult samples, or out-of-distribution samples. \n2. Automated filtering using bandit curriculum learning seems effective and avoids manually tuning the hyperparameters of the previous fixed threshold. \n3. Both the stability and efficacy of self-influence are analyzed on diverse datasets, tasks, and models, demonstrating the generality of the proposed method.\n\nreasons_to_reject: 1. The claim that the proposed self-influence score is stable with respect to training and model hyperparameters across architecture variations is not actually supported by the evidence in Table 2 and Table 3. It can be observed that the self-influence score varies greatly for different model architectures and training hyperparameters. \n2. The effectiveness of the proposed self-influence score is doubtful. When demonstrating the efficacy of the self-influence score in Table 4, nearly half of the results are missing and replaced with '-'. This may raise doubts that only favorable results are picked, especially for the out-of-distribution test. It would be appreciated if the author could provide those missing results and justify why they were missed in the original table. \n3. There is no comparison with other data filtering techniques, such as AUM, Data Cartography, or PVI. How does the performance compare with those existing data filtering techniques? \n4. I agree with the other two reviewers that the paper is hard to read. A better presentation style and more clarity would be appreciated. Specific suggestions were well given by the other two reviewers.\nR[1] Pleiss, Geoff, Tianyi Zhang, Ethan Elenberg, and Kilian Q. Weinberger. \" Identifying mislabeled data using the area under the margin ranking.\" \u00a0*Advances in Neural Information Processing Systems*\u00a033 (2020): 17044-17056.\nR[2] Swayamdipta, Swabha, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and Yejin Choi. \" Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics.\" In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9275-9293. 2020.\nR[3] Ethayarajh, Kawin, Yejin Choi, and Swabha Swayamdipta. \" Understanding Dataset Difficulty with $\\mathcalV $-Usable Information.\" In International Conference on Machine Learning, pp. 5988-6008. PMLR, 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "181_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_181_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7344666666666667,
      "max_similarity": 0.7558,
      "avg_coverage": 0.4895333333333334,
      "max_coverage": 0.5833
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 901,
      "avg_human_length": 532.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "oqVy0moOXf",
        "similarity": 0.7146,
        "coverage": 0.5833,
        "human_length": 426,
        "human_text": "paper_topic_and_main_contributions: This paper investigates the impact of influence-functions in the context of identifying self-influential examples (i.e. examples that are difficult to predict from other examples and are thus hypothesised to be memorised). The paper investigates properties of such self-influential examples, suggesting that they are properties of data rather than dependant on model architecture and initialisation. Finally, the paper shows how knowledge about such examples can be used to improve training and out-of-distribution generalisation of models.\n\nreasons_to_accept: The contribution seems novel, the experiments are (for the most part) well executed and motivated, the research questions are clear and the results support the findings.\nI appreciate the fact that some of the less well-known techniques, metrics, etc by examples.\n\nreasons_to_reject: I have two concerns with this paper.\nFirstly, I don't think the results regarding the stability of influence functions would withstand a statistical test - only two results are being compared here (although for different settings). Perhaps the chosen different initialisation/model happened to produce a similar result by chance. I believe for more robust conclusions, the sample set of observations should be more than a pair. I appreciate that these experiments are computationally expensive, perhaps a bigger sample could be investigated for a subset of the investigated settings (e.g. 'first' only in Table 2) Secondly, I find the paper's content arrangement rather odd, see my comments in the presentation section. But this is only a minor concern and could be addressed by an additional page of content, should the paper be accepted.\n\nquestions_for_the_authors: Question 1: Are you going to release the experimental setup to the public?\n\nmissing_references: None that I'm aware of.\n\ntypos_grammar_style_and_presentation_improvements: I find it odd to move the related work section into the appendix, while spending a whole page on Sections 2 and 3 and 4.1, which are essentially background. Sections 3 and 4.1 could definitely be moved to the appendix, since they're already summarised in Table 1.\nFurthermore, the introduction set's the scene rather awkwardly. I believe the concept of self-influence should be (at least intuitively) introduced earlier and followed up by the research questions this paper is addressing. I am not sure why the paragraphs in lines 64 to 96 are so prominent.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Hosf0hdEW6",
        "similarity": 0.733,
        "coverage": 0.4667,
        "human_length": 643,
        "human_text": "paper_topic_and_main_contributions: The paper studied whether self-influence scores based on Arnoldi-based In\ufb02uence Function (ABIF) can be used to filter out low-quality data (outliers, e.g. noises) so as to improve the test performance. Specifically, the authors explored 1) the stability of computed self-influence scores against different a) model states (i.e. initialization, data ordering, and batch size) and b) model size and architecture, 2) whether self-influence scores can be used to select noises, and 3) whether one can use self-influence scores to divide datasets into different subsets, and then use Automated Curriculum Learning (AutoCL) to improve training performance. The results show that 1) self-influence scores are more robust against model states than sizes and architectures, 2) self-influence scores are better at capturing synthetic data noises than natural noises, and 3) AutoCL help learning better than filtering, based on self-influence scores.\n\nreasons_to_accept: - It is an interesting idea to use self-influence score to filter data.  - Understanding instability is important for such filtering to put into practice.  - Using AutoCL instead of filtering is intriging, and the performance shown is encouraging.\n\nreasons_to_reject: - The experiment setups are not well-explained, and thus not reproducible/convincing     * Lines 360-362 should be more specific: what are the specific choices of hyper-parameters (for reproducibility)? How would that change performance (to better ground the stability)? \n    * In table 4, how did the authors choose different percentage of data to filter out?  - The writing of the background/methods is hard to follow, important points include:      * Is the IF approximation method proposed by Koh and Liang (2017)? I think the HVP method was proposed quite early. \n    * Without reading Section 7, it is hard to understand what are the different subsets of data in AutoCL     * Math notation system in AutoCL is used without introduction: what are y/Y? Also, in pgnorm (I think \\mathcal L is loss?), on which data the loss should be computed; and what are the gradient and reward batch?   - The results of synthetic noises vs. natural noises detection confuse me: on the one hand, data filtering based on self-influence improved OOD performance (Table 4); however, on the other hand, such filtering failed to filter out natural noises (Figure 1) \u2014 what are the implications then? Reading the full results, I feel some pieces of evidence are missing here: self-influence scores can help filter out noisy data and help with AutoCL, however, it cannot really detect natural noises, which is a bit puzzling.\n\nquestions_for_the_authors: - Do you think high self-influence data instances always harmful for models? I am thinking of large language models --- one would need LLMs to memorize certain facts to give correct answers; in this case, such memorization/high self-influence can become an advantage?  - Do you have any hypothesis why it is this case, that first/all are less sensitive to size/attention, but more sensitive to model states?\n- What are the fundamental difference between synthetic and natural noises that leads to the successful filtering only on synthetic data?\n\nmissing_references: Second-Order Stochastic Optimization for Machine Learning in Linear Time, Agarwal et al., 2017.\n\ntypos_grammar_style_and_presentation_improvements: A bit strange to mention a section (5.3) and put everything in the appendix. It would be better give some more information in the main text for the purpose of being self-contained.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "6wH6pSCrmE",
        "similarity": 0.7558,
        "coverage": 0.4186,
        "human_length": 529,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on identifying noisy data/outliers and filtering out harmful instances to improve model performance. The author proposes using self-influence scores for data filtering and incorporating bandit curriculum learning to automate the filtering process instead of using a fixed filtering threshold. The stability and effectiveness of the self-influence scores are analyzed.\n\nreasons_to_accept: 1. The idea of using a self-influence score to identify outliers or harmful samples is inspiring. If removing a sample deteriorates the loss value on itself, then this sample should be different enough from the rest of the training data. Such samples could be mislabeled, ambiguous, or difficult samples, or out-of-distribution samples. \n2. Automated filtering using bandit curriculum learning seems effective and avoids manually tuning the hyperparameters of the previous fixed threshold. \n3. Both the stability and efficacy of self-influence are analyzed on diverse datasets, tasks, and models, demonstrating the generality of the proposed method.\n\nreasons_to_reject: 1. The claim that the proposed self-influence score is stable with respect to training and model hyperparameters across architecture variations is not actually supported by the evidence in Table 2 and Table 3. It can be observed that the self-influence score varies greatly for different model architectures and training hyperparameters. \n2. The effectiveness of the proposed self-influence score is doubtful. When demonstrating the efficacy of the self-influence score in Table 4, nearly half of the results are missing and replaced with '-'. This may raise doubts that only favorable results are picked, especially for the out-of-distribution test. It would be appreciated if the author could provide those missing results and justify why they were missed in the original table. \n3. There is no comparison with other data filtering techniques, such as AUM, Data Cartography, or PVI. How does the performance compare with those existing data filtering techniques? \n4. I agree with the other two reviewers that the paper is hard to read. A better presentation style and more clarity would be appreciated. Specific suggestions were well given by the other two reviewers.\nR[1] Pleiss, Geoff, Tianyi Zhang, Ethan Elenberg, and Kilian Q. Weinberger. \" Identifying mislabeled data using the area under the margin ranking.\" \u00a0*Advances in Neural Information Processing Systems*\u00a033 (2020): 17044-17056.\nR[2] Swayamdipta, Swabha, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and Yejin Choi. \" Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics.\" In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9275-9293. 2020.\nR[3] Ethayarajh, Kawin, Yejin Choi, and Swabha Swayamdipta. \" Understanding Dataset Difficulty with $\\mathcalV $-Usable Information.\" In International Conference on Machine Learning, pp. 5988-6008. PMLR, 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "oqVy0moOXf",
        "length": 426,
        "human_text": "paper_topic_and_main_contributions: This paper investigates the impact of influence-functions in the context of identifying self-influential examples (i.e. examples that are difficult to predict from other examples and are thus hypothesised to be memorised). The paper investigates properties of such self-influential examples, suggesting that they are properties of data rather than dependant on model architecture and initialisation. Finally, the paper shows how knowledge about such examples can be used to improve training and out-of-distribution generalisation of models.\n\nreasons_to_accept: The contribution seems novel, the experiments are (for the most part) well executed and motivated, the research questions are clear and the results support the findings.\nI appreciate the fact that some of the less well-known techniques, metrics, etc by examples.\n\nreasons_to_reject: I have two concerns with this paper.\nFirstly, I don't think the results regarding the stability of influence functions would withstand a statistical test - only two results are being compared here (although for different settings). Perhaps the chosen different initialisation/model happened to produce a similar result by chance. I believe for more robust conclusions, the sample set of observations should be more than a pair. I appreciate that these experiments are computationally expensive, perhaps a bigger sample could be investigated for a subset of the investigated settings (e.g. 'first' only in Table 2) Secondly, I find the paper's content arrangement rather odd, see my comments in the presentation section. But this is only a minor concern and could be addressed by an additional page of content, should the paper be accepted.\n\nquestions_for_the_authors: Question 1: Are you going to release the experimental setup to the public?\n\nmissing_references: None that I'm aware of.\n\ntypos_grammar_style_and_presentation_improvements: I find it odd to move the related work section into the appendix, while spending a whole page on Sections 2 and 3 and 4.1, which are essentially background. Sections 3 and 4.1 could definitely be moved to the appendix, since they're already summarised in Table 1.\nFurthermore, the introduction set's the scene rather awkwardly. I believe the concept of self-influence should be (at least intuitively) introduced earlier and followed up by the research questions this paper is addressing. I am not sure why the paragraphs in lines 64 to 96 are so prominent.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "Hosf0hdEW6",
        "length": 643,
        "human_text": "paper_topic_and_main_contributions: The paper studied whether self-influence scores based on Arnoldi-based In\ufb02uence Function (ABIF) can be used to filter out low-quality data (outliers, e.g. noises) so as to improve the test performance. Specifically, the authors explored 1) the stability of computed self-influence scores against different a) model states (i.e. initialization, data ordering, and batch size) and b) model size and architecture, 2) whether self-influence scores can be used to select noises, and 3) whether one can use self-influence scores to divide datasets into different subsets, and then use Automated Curriculum Learning (AutoCL) to improve training performance. The results show that 1) self-influence scores are more robust against model states than sizes and architectures, 2) self-influence scores are better at capturing synthetic data noises than natural noises, and 3) AutoCL help learning better than filtering, based on self-influence scores.\n\nreasons_to_accept: - It is an interesting idea to use self-influence score to filter data.  - Understanding instability is important for such filtering to put into practice.  - Using AutoCL instead of filtering is intriging, and the performance shown is encouraging.\n\nreasons_to_reject: - The experiment setups are not well-explained, and thus not reproducible/convincing     * Lines 360-362 should be more specific: what are the specific choices of hyper-parameters (for reproducibility)? How would that change performance (to better ground the stability)? \n    * In table 4, how did the authors choose different percentage of data to filter out?  - The writing of the background/methods is hard to follow, important points include:      * Is the IF approximation method proposed by Koh and Liang (2017)? I think the HVP method was proposed quite early. \n    * Without reading Section 7, it is hard to understand what are the different subsets of data in AutoCL     * Math notation system in AutoCL is used without introduction: what are y/Y? Also, in pgnorm (I think \\mathcal L is loss?), on which data the loss should be computed; and what are the gradient and reward batch?   - The results of synthetic noises vs. natural noises detection confuse me: on the one hand, data filtering based on self-influence improved OOD performance (Table 4); however, on the other hand, such filtering failed to filter out natural noises (Figure 1) \u2014 what are the implications then? Reading the full results, I feel some pieces of evidence are missing here: self-influence scores can help filter out noisy data and help with AutoCL, however, it cannot really detect natural noises, which is a bit puzzling.\n\nquestions_for_the_authors: - Do you think high self-influence data instances always harmful for models? I am thinking of large language models --- one would need LLMs to memorize certain facts to give correct answers; in this case, such memorization/high self-influence can become an advantage?  - Do you have any hypothesis why it is this case, that first/all are less sensitive to size/attention, but more sensitive to model states?\n- What are the fundamental difference between synthetic and natural noises that leads to the successful filtering only on synthetic data?\n\nmissing_references: Second-Order Stochastic Optimization for Machine Learning in Linear Time, Agarwal et al., 2017.\n\ntypos_grammar_style_and_presentation_improvements: A bit strange to mention a section (5.3) and put everything in the appendix. It would be better give some more information in the main text for the purpose of being self-contained.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "6wH6pSCrmE",
        "length": 529,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on identifying noisy data/outliers and filtering out harmful instances to improve model performance. The author proposes using self-influence scores for data filtering and incorporating bandit curriculum learning to automate the filtering process instead of using a fixed filtering threshold. The stability and effectiveness of the self-influence scores are analyzed.\n\nreasons_to_accept: 1. The idea of using a self-influence score to identify outliers or harmful samples is inspiring. If removing a sample deteriorates the loss value on itself, then this sample should be different enough from the rest of the training data. Such samples could be mislabeled, ambiguous, or difficult samples, or out-of-distribution samples. \n2. Automated filtering using bandit curriculum learning seems effective and avoids manually tuning the hyperparameters of the previous fixed threshold. \n3. Both the stability and efficacy of self-influence are analyzed on diverse datasets, tasks, and models, demonstrating the generality of the proposed method.\n\nreasons_to_reject: 1. The claim that the proposed self-influence score is stable with respect to training and model hyperparameters across architecture variations is not actually supported by the evidence in Table 2 and Table 3. It can be observed that the self-influence score varies greatly for different model architectures and training hyperparameters. \n2. The effectiveness of the proposed self-influence score is doubtful. When demonstrating the efficacy of the self-influence score in Table 4, nearly half of the results are missing and replaced with '-'. This may raise doubts that only favorable results are picked, especially for the out-of-distribution test. It would be appreciated if the author could provide those missing results and justify why they were missed in the original table. \n3. There is no comparison with other data filtering techniques, such as AUM, Data Cartography, or PVI. How does the performance compare with those existing data filtering techniques? \n4. I agree with the other two reviewers that the paper is hard to read. A better presentation style and more clarity would be appreciated. Specific suggestions were well given by the other two reviewers.\nR[1] Pleiss, Geoff, Tianyi Zhang, Ethan Elenberg, and Kilian Q. Weinberger. \" Identifying mislabeled data using the area under the margin ranking.\" \u00a0*Advances in Neural Information Processing Systems*\u00a033 (2020): 17044-17056.\nR[2] Swayamdipta, Swabha, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and Yejin Choi. \" Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics.\" In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9275-9293. 2020.\nR[3] Ethayarajh, Kawin, Yejin Choi, and Swabha Swayamdipta. \" Understanding Dataset Difficulty with $\\mathcalV $-Usable Information.\" In International Conference on Machine Learning, pp. 5988-6008. PMLR, 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "65_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_65_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7298666666666667,
      "max_similarity": 0.7408,
      "avg_coverage": 0.46340000000000003,
      "max_coverage": 0.6667
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 421,
      "avg_human_length": 719.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "l1wmwaRRVf",
        "similarity": 0.7244,
        "coverage": 0.4444,
        "human_length": 677,
        "human_text": "paper_topic_and_main_contributions: The paper proposes to use policies like wait-k for simultaneous MT on word level rather than on subword level. The authors additionally suggest to use a language model (possibly with a different subword segmentation than the translation model) to further improve the quality of the translation; operating on word level makes it possible to apply the LM despite the differences in subword segmentation.\n\nreasons_to_accept: One of the main claims of the paper that so far, the average lagging, one of the main ways of evaluating simultaneous MT systems, has happened on subword level. This claim is not true, and the idea to go from subword level to word level when computing the policy is not new at all either. The remaining part about using the LM is not enough for publication.\nSo overall, I see no reasons to accept the paper in its current form.\n\nreasons_to_reject: The main claim of the paper that so far the average lagging was computed on subword level and not on word level is simply not true. In fact, SimEval by default uses word level.  This is absolutely necessary to do to have fair comparison between submissions to competitive evaluations/shared tasks like Workshop on Simultaneous Translation, and also Simultaneous speech translation track at the IWSLT conference.  Also, prior research definitely used word-level adaptive policies, like the phrases/chunks in one of the papers that I listed below - these were defined on the word level.  Your use of word-level \"chunk attention\" is misleading - you seem to just attend to multiple subwords of the same word - this is not a chunk in any of the established meaning of this word.  Overall, it is clear that the output of words, not subwords is expected in simultaneous MT, as they have to be presented to a user in form of words. How you read the input, in subwords, words, or directly from speech, or in characters - this is up to the architecture of the system, so there is nothing novel about going from subwords to words there.  So overall, I see no merit in the paper, as its main claims have no ground.\n\nmissing_references: @inproceedings{papi2022does, title={Does Simultaneous Speech Translation need Simultaneous Models?}, author={Papi, Sara and Gaido, Marco and Negri, Matteo and Turchi, Marco}, booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022}, pages={141--153}, year={2022} } @article{wilken2020neural, title={Neural Simultaneous Speech Translation Using Alignment-Based Chunking}, author={Wilken, Patrick and Alkhouli, Tamer and Matusov, Evgeny and Golik, Pavel}, journal={IWSLT 2020}, pages={237}, year={2020} }\n\ntypos_grammar_style_and_presentation_improvements: Line 493: \"all tested latency settings\": according to Fig. 6b, this is not true: in the lowest latency setting of 2, the system without \"chunk attention\" is better.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: From Line 42: \"The performance analysis and implementation of most SiMT systems, to the best of our knowledge, have been carried out on encoded sequences of source and target subwords, rather than on the original source and target sentences. This has led to two critical issues that need to be addressed.\"\nThis is simply not true! And because of this, the paper has no value! The authors did add \"to the best of our knowledge\", but actually, this is a known fact for everyone working on simultaneous MT.  === After the author's rebuttal, I acknowledge that some papers still use BPE-level evaluation of simultaneous MT systems. Still, I believe that it is clear that this is not the right thing to do - yet I don't have any ethical concerns anymore after the authors re-worded the paper.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Vx1614fGcC",
        "similarity": 0.7408,
        "coverage": 0.2791,
        "human_length": 1124,
        "human_text": "paper_topic_and_main_contributions: This paper shows that moving from token-based to word-level policies in Simultaneous Machine Translation (SiMT) brings about improvements in translation quality leveraging all subwords that are part of the word being read and not writing in the middle of a word. In addition, and more importantly, it eases the integration of external LMs in order to improve SiMT models, and this is shown applying LM-fused attention previously applied in NMT. Finally, word-level policies allows for a fair comparative evaluation independently of the subword algorithm being applied. The results show a consistent improvements up to approximately 15%.\n\nreasons_to_accept: The improvements in translation quality are consistent and significant across k values in wait-k policies.\n\nreasons_to_reject: The presentation and discussion of results need to be improved to put into perspective what contributions are having a more significant impact than others and under which conditions. For example, Figure 5 shows too many plots and too many curves, I would discard Transformer base plots and I would select those curves that clearly show the main conclusions that can be extracted from the experiments: word-level policies improve the results and adding a LM does much more. Those conditions that do not help much can be mentioned in the discussion. However, it would be nice to know what the contribution is for the word-level policies and the LMs and under which conditions one may contribute more than the other, if this is the case.\nWord-level chunk attention is not clear to me since little detail is provided about how chunks are defined.\nTo sum up, the results are good and extensive, but are not easy to read in order to draw the main conclusions. The authors should have devoted more time to prepare the presentation of the results to ease the task of the reader.\n\nquestions_for_the_authors: Question A: How are the chunk boundaries defined? what are the parameters governing the model that takes the splitting decision? Obviously, the chunk size affects the quality of the bidirectional encoding, have you explored this point in your experiments? I guess bidirectional encoding is applied to all chunks, except for the last one in which unidirectional encoding would be applied, am I right?\n\nmissing_references: When discussing the encoding of the input sequence in lines 227-232, I think it is worth mentioning two additional references: @inproceedings{iranzo-sanchez-etal-2022-simultaneous,     title = \"From Simultaneous to Streaming Machine Translation by Leveraging Streaming History\",     author = \"Iranzo Sanchez, Javier  and       Civera, Jorge  and       Juan-C{\\'\\i}scar, Alfons\",     booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",     month = may,     year = \"2022\",     address = \"Dublin, Ireland\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2022.acl-long.480\",     doi = \"10.18653/v1/2022.acl-long.480\",     pages = \"6972--6985\",     abstract = \"Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed. However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario. Indeed, these sentence-level latency measures are not well suited for continuous stream translation, resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed. This work proposes a stream-level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation, that is successfully evaluated on streaming conditions for a reference IWSLT task\", } @inproceedings{kahardipraja-etal-2021-towards,     title = \"Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental {NLU}\",     author = \"Kahardipraja, Patrick  and       Madureira, Brielen  and       Schlangen, David\",     booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",     month = nov,     year = \"2021\",     address = \"Online and Punta Cana, Dominican Republic\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2021.emnlp-main.90\",     doi = \"10.18653/v1/2021.emnlp-main.90\",     pages = \"1178--1189\",     abstract = \"Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.\", \n}\n\ntypos_grammar_style_and_presentation_improvements: Typos: L. 142: attetnion -> attention L. 216: , The goal -> , the goal L. 232: unidirectionally**.** (Elbayad et al., 2020) L. 147: missing words in \"Zhang and Feng (2022a) model to predict the alignment...\" L.422: BLEU calculation**.** (Post, 2018) Table 2: An Eexample case -> An example case Presentation improvements: Tables 4-12 in the appendix could be more compacted and elaborated to ease the comparison across k values (since explored k values are different), relative improvements over the baseline would help to clearly see the impact of the different factors.\nFigure 6 (a) and its description in the text need to be improved to make the description more consistent, you are using WW, TW, WT, but Read-k-Write-k to refer to TT policy.\nI would discard Figure 6 (b), since it can be described with a sentence. In addition, this contribution is not clear to me since little detail is provided about how chunks are defined.\nFigure 8 (a) contains too many curves that cannot be read properly. I would reduce the number of curves discarding those less significant in terms of impact in the results and describe these minor effects in the text. In addition, from an aesthetic viewpoint, I would try to use same font size across figures in page8 and shorthen legend description to not occlude the curves (see Figure 8 (a)).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "gy961FhTpg",
        "similarity": 0.7244,
        "coverage": 0.6667,
        "human_length": 357,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of the assumption in many existing studies that operations are carried out at the subword level, even though the standard unit for input and output in most practical scenarios is typically at the word level. The paper demonstrates that policies devised and validated at the subword level are surpassed by those operating at the word level, which process multiple subwords to form a complete word in a single step. The main contributions of this paper are the demonstration of the superiority of word-level policies over subword-level policies in SiMT and the proposal of a method to boost SiMT models using language models.\n\nreasons_to_accept: Strengths of the paper: - Proposes a novel method of integrating language models (LM) into Simultaneous Machine Translation (SiMT) systems - Offers a more versatile solution that can be integrated into most existing neural SiMT models - LM-fused attention proves effective in enhancing translation quality across all latency levels - Proposed word-level policy plays a crucial role in effectively managing the vocabulary mismatch between the LM and model\n\nreasons_to_reject: Weaknesses of the paper: - The proposed method may not be applicable to languages with a writing style that lacks spaces or other delimiters between words or sentences, such as Chinese. Experiments like translation direction of En-Zh should be included in this paper.\n- Integrating a large LM may require a faster compute capability to fulfill the low-latency demands of the SiMT task. Speed experiments should be involved in this paper.\n\nquestions_for_the_authors: 1. What are some potential solutions or alternative approaches that could be explored to address the limitation of the proposed method in SiMT systems for languages with a writing style that lacks spaces or other delimiters between words or sentences, such as Chinese?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "l1wmwaRRVf",
        "length": 677,
        "human_text": "paper_topic_and_main_contributions: The paper proposes to use policies like wait-k for simultaneous MT on word level rather than on subword level. The authors additionally suggest to use a language model (possibly with a different subword segmentation than the translation model) to further improve the quality of the translation; operating on word level makes it possible to apply the LM despite the differences in subword segmentation.\n\nreasons_to_accept: One of the main claims of the paper that so far, the average lagging, one of the main ways of evaluating simultaneous MT systems, has happened on subword level. This claim is not true, and the idea to go from subword level to word level when computing the policy is not new at all either. The remaining part about using the LM is not enough for publication.\nSo overall, I see no reasons to accept the paper in its current form.\n\nreasons_to_reject: The main claim of the paper that so far the average lagging was computed on subword level and not on word level is simply not true. In fact, SimEval by default uses word level.  This is absolutely necessary to do to have fair comparison between submissions to competitive evaluations/shared tasks like Workshop on Simultaneous Translation, and also Simultaneous speech translation track at the IWSLT conference.  Also, prior research definitely used word-level adaptive policies, like the phrases/chunks in one of the papers that I listed below - these were defined on the word level.  Your use of word-level \"chunk attention\" is misleading - you seem to just attend to multiple subwords of the same word - this is not a chunk in any of the established meaning of this word.  Overall, it is clear that the output of words, not subwords is expected in simultaneous MT, as they have to be presented to a user in form of words. How you read the input, in subwords, words, or directly from speech, or in characters - this is up to the architecture of the system, so there is nothing novel about going from subwords to words there.  So overall, I see no merit in the paper, as its main claims have no ground.\n\nmissing_references: @inproceedings{papi2022does, title={Does Simultaneous Speech Translation need Simultaneous Models?}, author={Papi, Sara and Gaido, Marco and Negri, Matteo and Turchi, Marco}, booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022}, pages={141--153}, year={2022} } @article{wilken2020neural, title={Neural Simultaneous Speech Translation Using Alignment-Based Chunking}, author={Wilken, Patrick and Alkhouli, Tamer and Matusov, Evgeny and Golik, Pavel}, journal={IWSLT 2020}, pages={237}, year={2020} }\n\ntypos_grammar_style_and_presentation_improvements: Line 493: \"all tested latency settings\": according to Fig. 6b, this is not true: in the lowest latency setting of 2, the system without \"chunk attention\" is better.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: From Line 42: \"The performance analysis and implementation of most SiMT systems, to the best of our knowledge, have been carried out on encoded sequences of source and target subwords, rather than on the original source and target sentences. This has led to two critical issues that need to be addressed.\"\nThis is simply not true! And because of this, the paper has no value! The authors did add \"to the best of our knowledge\", but actually, this is a known fact for everyone working on simultaneous MT.  === After the author's rebuttal, I acknowledge that some papers still use BPE-level evaluation of simultaneous MT systems. Still, I believe that it is clear that this is not the right thing to do - yet I don't have any ethical concerns anymore after the authors re-worded the paper.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "Vx1614fGcC",
        "length": 1124,
        "human_text": "paper_topic_and_main_contributions: This paper shows that moving from token-based to word-level policies in Simultaneous Machine Translation (SiMT) brings about improvements in translation quality leveraging all subwords that are part of the word being read and not writing in the middle of a word. In addition, and more importantly, it eases the integration of external LMs in order to improve SiMT models, and this is shown applying LM-fused attention previously applied in NMT. Finally, word-level policies allows for a fair comparative evaluation independently of the subword algorithm being applied. The results show a consistent improvements up to approximately 15%.\n\nreasons_to_accept: The improvements in translation quality are consistent and significant across k values in wait-k policies.\n\nreasons_to_reject: The presentation and discussion of results need to be improved to put into perspective what contributions are having a more significant impact than others and under which conditions. For example, Figure 5 shows too many plots and too many curves, I would discard Transformer base plots and I would select those curves that clearly show the main conclusions that can be extracted from the experiments: word-level policies improve the results and adding a LM does much more. Those conditions that do not help much can be mentioned in the discussion. However, it would be nice to know what the contribution is for the word-level policies and the LMs and under which conditions one may contribute more than the other, if this is the case.\nWord-level chunk attention is not clear to me since little detail is provided about how chunks are defined.\nTo sum up, the results are good and extensive, but are not easy to read in order to draw the main conclusions. The authors should have devoted more time to prepare the presentation of the results to ease the task of the reader.\n\nquestions_for_the_authors: Question A: How are the chunk boundaries defined? what are the parameters governing the model that takes the splitting decision? Obviously, the chunk size affects the quality of the bidirectional encoding, have you explored this point in your experiments? I guess bidirectional encoding is applied to all chunks, except for the last one in which unidirectional encoding would be applied, am I right?\n\nmissing_references: When discussing the encoding of the input sequence in lines 227-232, I think it is worth mentioning two additional references: @inproceedings{iranzo-sanchez-etal-2022-simultaneous,     title = \"From Simultaneous to Streaming Machine Translation by Leveraging Streaming History\",     author = \"Iranzo Sanchez, Javier  and       Civera, Jorge  and       Juan-C{\\'\\i}scar, Alfons\",     booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",     month = may,     year = \"2022\",     address = \"Dublin, Ireland\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2022.acl-long.480\",     doi = \"10.18653/v1/2022.acl-long.480\",     pages = \"6972--6985\",     abstract = \"Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed. However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario. Indeed, these sentence-level latency measures are not well suited for continuous stream translation, resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed. This work proposes a stream-level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation, that is successfully evaluated on streaming conditions for a reference IWSLT task\", } @inproceedings{kahardipraja-etal-2021-towards,     title = \"Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental {NLU}\",     author = \"Kahardipraja, Patrick  and       Madureira, Brielen  and       Schlangen, David\",     booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",     month = nov,     year = \"2021\",     address = \"Online and Punta Cana, Dominican Republic\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2021.emnlp-main.90\",     doi = \"10.18653/v1/2021.emnlp-main.90\",     pages = \"1178--1189\",     abstract = \"Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.\", \n}\n\ntypos_grammar_style_and_presentation_improvements: Typos: L. 142: attetnion -> attention L. 216: , The goal -> , the goal L. 232: unidirectionally**.** (Elbayad et al., 2020) L. 147: missing words in \"Zhang and Feng (2022a) model to predict the alignment...\" L.422: BLEU calculation**.** (Post, 2018) Table 2: An Eexample case -> An example case Presentation improvements: Tables 4-12 in the appendix could be more compacted and elaborated to ease the comparison across k values (since explored k values are different), relative improvements over the baseline would help to clearly see the impact of the different factors.\nFigure 6 (a) and its description in the text need to be improved to make the description more consistent, you are using WW, TW, WT, but Read-k-Write-k to refer to TT policy.\nI would discard Figure 6 (b), since it can be described with a sentence. In addition, this contribution is not clear to me since little detail is provided about how chunks are defined.\nFigure 8 (a) contains too many curves that cannot be read properly. I would reduce the number of curves discarding those less significant in terms of impact in the results and describe these minor effects in the text. In addition, from an aesthetic viewpoint, I would try to use same font size across figures in page8 and shorthen legend description to not occlude the curves (see Figure 8 (a)).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "gy961FhTpg",
        "length": 357,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of the assumption in many existing studies that operations are carried out at the subword level, even though the standard unit for input and output in most practical scenarios is typically at the word level. The paper demonstrates that policies devised and validated at the subword level are surpassed by those operating at the word level, which process multiple subwords to form a complete word in a single step. The main contributions of this paper are the demonstration of the superiority of word-level policies over subword-level policies in SiMT and the proposal of a method to boost SiMT models using language models.\n\nreasons_to_accept: Strengths of the paper: - Proposes a novel method of integrating language models (LM) into Simultaneous Machine Translation (SiMT) systems - Offers a more versatile solution that can be integrated into most existing neural SiMT models - LM-fused attention proves effective in enhancing translation quality across all latency levels - Proposed word-level policy plays a crucial role in effectively managing the vocabulary mismatch between the LM and model\n\nreasons_to_reject: Weaknesses of the paper: - The proposed method may not be applicable to languages with a writing style that lacks spaces or other delimiters between words or sentences, such as Chinese. Experiments like translation direction of En-Zh should be included in this paper.\n- Integrating a large LM may require a faster compute capability to fulfill the low-latency demands of the SiMT task. Speed experiments should be involved in this paper.\n\nquestions_for_the_authors: 1. What are some potential solutions or alternative approaches that could be explored to address the limitation of the proposed method in SiMT systems for languages with a writing style that lacks spaces or other delimiters between words or sentences, such as Chinese?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "65_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_65_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7319999999999999,
      "max_similarity": 0.749,
      "avg_coverage": 0.44329999999999997,
      "max_coverage": 0.6667
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 473,
      "avg_human_length": 719.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "l1wmwaRRVf",
        "similarity": 0.7231,
        "coverage": 0.4074,
        "human_length": 677,
        "human_text": "paper_topic_and_main_contributions: The paper proposes to use policies like wait-k for simultaneous MT on word level rather than on subword level. The authors additionally suggest to use a language model (possibly with a different subword segmentation than the translation model) to further improve the quality of the translation; operating on word level makes it possible to apply the LM despite the differences in subword segmentation.\n\nreasons_to_accept: One of the main claims of the paper that so far, the average lagging, one of the main ways of evaluating simultaneous MT systems, has happened on subword level. This claim is not true, and the idea to go from subword level to word level when computing the policy is not new at all either. The remaining part about using the LM is not enough for publication.\nSo overall, I see no reasons to accept the paper in its current form.\n\nreasons_to_reject: The main claim of the paper that so far the average lagging was computed on subword level and not on word level is simply not true. In fact, SimEval by default uses word level.  This is absolutely necessary to do to have fair comparison between submissions to competitive evaluations/shared tasks like Workshop on Simultaneous Translation, and also Simultaneous speech translation track at the IWSLT conference.  Also, prior research definitely used word-level adaptive policies, like the phrases/chunks in one of the papers that I listed below - these were defined on the word level.  Your use of word-level \"chunk attention\" is misleading - you seem to just attend to multiple subwords of the same word - this is not a chunk in any of the established meaning of this word.  Overall, it is clear that the output of words, not subwords is expected in simultaneous MT, as they have to be presented to a user in form of words. How you read the input, in subwords, words, or directly from speech, or in characters - this is up to the architecture of the system, so there is nothing novel about going from subwords to words there.  So overall, I see no merit in the paper, as its main claims have no ground.\n\nmissing_references: @inproceedings{papi2022does, title={Does Simultaneous Speech Translation need Simultaneous Models?}, author={Papi, Sara and Gaido, Marco and Negri, Matteo and Turchi, Marco}, booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022}, pages={141--153}, year={2022} } @article{wilken2020neural, title={Neural Simultaneous Speech Translation Using Alignment-Based Chunking}, author={Wilken, Patrick and Alkhouli, Tamer and Matusov, Evgeny and Golik, Pavel}, journal={IWSLT 2020}, pages={237}, year={2020} }\n\ntypos_grammar_style_and_presentation_improvements: Line 493: \"all tested latency settings\": according to Fig. 6b, this is not true: in the lowest latency setting of 2, the system without \"chunk attention\" is better.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: From Line 42: \"The performance analysis and implementation of most SiMT systems, to the best of our knowledge, have been carried out on encoded sequences of source and target subwords, rather than on the original source and target sentences. This has led to two critical issues that need to be addressed.\"\nThis is simply not true! And because of this, the paper has no value! The authors did add \"to the best of our knowledge\", but actually, this is a known fact for everyone working on simultaneous MT.  === After the author's rebuttal, I acknowledge that some papers still use BPE-level evaluation of simultaneous MT systems. Still, I believe that it is clear that this is not the right thing to do - yet I don't have any ethical concerns anymore after the authors re-worded the paper.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Vx1614fGcC",
        "similarity": 0.749,
        "coverage": 0.2558,
        "human_length": 1124,
        "human_text": "paper_topic_and_main_contributions: This paper shows that moving from token-based to word-level policies in Simultaneous Machine Translation (SiMT) brings about improvements in translation quality leveraging all subwords that are part of the word being read and not writing in the middle of a word. In addition, and more importantly, it eases the integration of external LMs in order to improve SiMT models, and this is shown applying LM-fused attention previously applied in NMT. Finally, word-level policies allows for a fair comparative evaluation independently of the subword algorithm being applied. The results show a consistent improvements up to approximately 15%.\n\nreasons_to_accept: The improvements in translation quality are consistent and significant across k values in wait-k policies.\n\nreasons_to_reject: The presentation and discussion of results need to be improved to put into perspective what contributions are having a more significant impact than others and under which conditions. For example, Figure 5 shows too many plots and too many curves, I would discard Transformer base plots and I would select those curves that clearly show the main conclusions that can be extracted from the experiments: word-level policies improve the results and adding a LM does much more. Those conditions that do not help much can be mentioned in the discussion. However, it would be nice to know what the contribution is for the word-level policies and the LMs and under which conditions one may contribute more than the other, if this is the case.\nWord-level chunk attention is not clear to me since little detail is provided about how chunks are defined.\nTo sum up, the results are good and extensive, but are not easy to read in order to draw the main conclusions. The authors should have devoted more time to prepare the presentation of the results to ease the task of the reader.\n\nquestions_for_the_authors: Question A: How are the chunk boundaries defined? what are the parameters governing the model that takes the splitting decision? Obviously, the chunk size affects the quality of the bidirectional encoding, have you explored this point in your experiments? I guess bidirectional encoding is applied to all chunks, except for the last one in which unidirectional encoding would be applied, am I right?\n\nmissing_references: When discussing the encoding of the input sequence in lines 227-232, I think it is worth mentioning two additional references: @inproceedings{iranzo-sanchez-etal-2022-simultaneous,     title = \"From Simultaneous to Streaming Machine Translation by Leveraging Streaming History\",     author = \"Iranzo Sanchez, Javier  and       Civera, Jorge  and       Juan-C{\\'\\i}scar, Alfons\",     booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",     month = may,     year = \"2022\",     address = \"Dublin, Ireland\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2022.acl-long.480\",     doi = \"10.18653/v1/2022.acl-long.480\",     pages = \"6972--6985\",     abstract = \"Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed. However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario. Indeed, these sentence-level latency measures are not well suited for continuous stream translation, resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed. This work proposes a stream-level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation, that is successfully evaluated on streaming conditions for a reference IWSLT task\", } @inproceedings{kahardipraja-etal-2021-towards,     title = \"Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental {NLU}\",     author = \"Kahardipraja, Patrick  and       Madureira, Brielen  and       Schlangen, David\",     booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",     month = nov,     year = \"2021\",     address = \"Online and Punta Cana, Dominican Republic\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2021.emnlp-main.90\",     doi = \"10.18653/v1/2021.emnlp-main.90\",     pages = \"1178--1189\",     abstract = \"Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.\", \n}\n\ntypos_grammar_style_and_presentation_improvements: Typos: L. 142: attetnion -> attention L. 216: , The goal -> , the goal L. 232: unidirectionally**.** (Elbayad et al., 2020) L. 147: missing words in \"Zhang and Feng (2022a) model to predict the alignment...\" L.422: BLEU calculation**.** (Post, 2018) Table 2: An Eexample case -> An example case Presentation improvements: Tables 4-12 in the appendix could be more compacted and elaborated to ease the comparison across k values (since explored k values are different), relative improvements over the baseline would help to clearly see the impact of the different factors.\nFigure 6 (a) and its description in the text need to be improved to make the description more consistent, you are using WW, TW, WT, but Read-k-Write-k to refer to TT policy.\nI would discard Figure 6 (b), since it can be described with a sentence. In addition, this contribution is not clear to me since little detail is provided about how chunks are defined.\nFigure 8 (a) contains too many curves that cannot be read properly. I would reduce the number of curves discarding those less significant in terms of impact in the results and describe these minor effects in the text. In addition, from an aesthetic viewpoint, I would try to use same font size across figures in page8 and shorthen legend description to not occlude the curves (see Figure 8 (a)).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "gy961FhTpg",
        "similarity": 0.7239,
        "coverage": 0.6667,
        "human_length": 357,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of the assumption in many existing studies that operations are carried out at the subword level, even though the standard unit for input and output in most practical scenarios is typically at the word level. The paper demonstrates that policies devised and validated at the subword level are surpassed by those operating at the word level, which process multiple subwords to form a complete word in a single step. The main contributions of this paper are the demonstration of the superiority of word-level policies over subword-level policies in SiMT and the proposal of a method to boost SiMT models using language models.\n\nreasons_to_accept: Strengths of the paper: - Proposes a novel method of integrating language models (LM) into Simultaneous Machine Translation (SiMT) systems - Offers a more versatile solution that can be integrated into most existing neural SiMT models - LM-fused attention proves effective in enhancing translation quality across all latency levels - Proposed word-level policy plays a crucial role in effectively managing the vocabulary mismatch between the LM and model\n\nreasons_to_reject: Weaknesses of the paper: - The proposed method may not be applicable to languages with a writing style that lacks spaces or other delimiters between words or sentences, such as Chinese. Experiments like translation direction of En-Zh should be included in this paper.\n- Integrating a large LM may require a faster compute capability to fulfill the low-latency demands of the SiMT task. Speed experiments should be involved in this paper.\n\nquestions_for_the_authors: 1. What are some potential solutions or alternative approaches that could be explored to address the limitation of the proposed method in SiMT systems for languages with a writing style that lacks spaces or other delimiters between words or sentences, such as Chinese?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "l1wmwaRRVf",
        "length": 677,
        "human_text": "paper_topic_and_main_contributions: The paper proposes to use policies like wait-k for simultaneous MT on word level rather than on subword level. The authors additionally suggest to use a language model (possibly with a different subword segmentation than the translation model) to further improve the quality of the translation; operating on word level makes it possible to apply the LM despite the differences in subword segmentation.\n\nreasons_to_accept: One of the main claims of the paper that so far, the average lagging, one of the main ways of evaluating simultaneous MT systems, has happened on subword level. This claim is not true, and the idea to go from subword level to word level when computing the policy is not new at all either. The remaining part about using the LM is not enough for publication.\nSo overall, I see no reasons to accept the paper in its current form.\n\nreasons_to_reject: The main claim of the paper that so far the average lagging was computed on subword level and not on word level is simply not true. In fact, SimEval by default uses word level.  This is absolutely necessary to do to have fair comparison between submissions to competitive evaluations/shared tasks like Workshop on Simultaneous Translation, and also Simultaneous speech translation track at the IWSLT conference.  Also, prior research definitely used word-level adaptive policies, like the phrases/chunks in one of the papers that I listed below - these were defined on the word level.  Your use of word-level \"chunk attention\" is misleading - you seem to just attend to multiple subwords of the same word - this is not a chunk in any of the established meaning of this word.  Overall, it is clear that the output of words, not subwords is expected in simultaneous MT, as they have to be presented to a user in form of words. How you read the input, in subwords, words, or directly from speech, or in characters - this is up to the architecture of the system, so there is nothing novel about going from subwords to words there.  So overall, I see no merit in the paper, as its main claims have no ground.\n\nmissing_references: @inproceedings{papi2022does, title={Does Simultaneous Speech Translation need Simultaneous Models?}, author={Papi, Sara and Gaido, Marco and Negri, Matteo and Turchi, Marco}, booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022}, pages={141--153}, year={2022} } @article{wilken2020neural, title={Neural Simultaneous Speech Translation Using Alignment-Based Chunking}, author={Wilken, Patrick and Alkhouli, Tamer and Matusov, Evgeny and Golik, Pavel}, journal={IWSLT 2020}, pages={237}, year={2020} }\n\ntypos_grammar_style_and_presentation_improvements: Line 493: \"all tested latency settings\": according to Fig. 6b, this is not true: in the lowest latency setting of 2, the system without \"chunk attention\" is better.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: From Line 42: \"The performance analysis and implementation of most SiMT systems, to the best of our knowledge, have been carried out on encoded sequences of source and target subwords, rather than on the original source and target sentences. This has led to two critical issues that need to be addressed.\"\nThis is simply not true! And because of this, the paper has no value! The authors did add \"to the best of our knowledge\", but actually, this is a known fact for everyone working on simultaneous MT.  === After the author's rebuttal, I acknowledge that some papers still use BPE-level evaluation of simultaneous MT systems. Still, I believe that it is clear that this is not the right thing to do - yet I don't have any ethical concerns anymore after the authors re-worded the paper.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "Vx1614fGcC",
        "length": 1124,
        "human_text": "paper_topic_and_main_contributions: This paper shows that moving from token-based to word-level policies in Simultaneous Machine Translation (SiMT) brings about improvements in translation quality leveraging all subwords that are part of the word being read and not writing in the middle of a word. In addition, and more importantly, it eases the integration of external LMs in order to improve SiMT models, and this is shown applying LM-fused attention previously applied in NMT. Finally, word-level policies allows for a fair comparative evaluation independently of the subword algorithm being applied. The results show a consistent improvements up to approximately 15%.\n\nreasons_to_accept: The improvements in translation quality are consistent and significant across k values in wait-k policies.\n\nreasons_to_reject: The presentation and discussion of results need to be improved to put into perspective what contributions are having a more significant impact than others and under which conditions. For example, Figure 5 shows too many plots and too many curves, I would discard Transformer base plots and I would select those curves that clearly show the main conclusions that can be extracted from the experiments: word-level policies improve the results and adding a LM does much more. Those conditions that do not help much can be mentioned in the discussion. However, it would be nice to know what the contribution is for the word-level policies and the LMs and under which conditions one may contribute more than the other, if this is the case.\nWord-level chunk attention is not clear to me since little detail is provided about how chunks are defined.\nTo sum up, the results are good and extensive, but are not easy to read in order to draw the main conclusions. The authors should have devoted more time to prepare the presentation of the results to ease the task of the reader.\n\nquestions_for_the_authors: Question A: How are the chunk boundaries defined? what are the parameters governing the model that takes the splitting decision? Obviously, the chunk size affects the quality of the bidirectional encoding, have you explored this point in your experiments? I guess bidirectional encoding is applied to all chunks, except for the last one in which unidirectional encoding would be applied, am I right?\n\nmissing_references: When discussing the encoding of the input sequence in lines 227-232, I think it is worth mentioning two additional references: @inproceedings{iranzo-sanchez-etal-2022-simultaneous,     title = \"From Simultaneous to Streaming Machine Translation by Leveraging Streaming History\",     author = \"Iranzo Sanchez, Javier  and       Civera, Jorge  and       Juan-C{\\'\\i}scar, Alfons\",     booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",     month = may,     year = \"2022\",     address = \"Dublin, Ireland\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2022.acl-long.480\",     doi = \"10.18653/v1/2022.acl-long.480\",     pages = \"6972--6985\",     abstract = \"Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed. However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario. Indeed, these sentence-level latency measures are not well suited for continuous stream translation, resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed. This work proposes a stream-level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation, that is successfully evaluated on streaming conditions for a reference IWSLT task\", } @inproceedings{kahardipraja-etal-2021-towards,     title = \"Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental {NLU}\",     author = \"Kahardipraja, Patrick  and       Madureira, Brielen  and       Schlangen, David\",     booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",     month = nov,     year = \"2021\",     address = \"Online and Punta Cana, Dominican Republic\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2021.emnlp-main.90\",     doi = \"10.18653/v1/2021.emnlp-main.90\",     pages = \"1178--1189\",     abstract = \"Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.\", \n}\n\ntypos_grammar_style_and_presentation_improvements: Typos: L. 142: attetnion -> attention L. 216: , The goal -> , the goal L. 232: unidirectionally**.** (Elbayad et al., 2020) L. 147: missing words in \"Zhang and Feng (2022a) model to predict the alignment...\" L.422: BLEU calculation**.** (Post, 2018) Table 2: An Eexample case -> An example case Presentation improvements: Tables 4-12 in the appendix could be more compacted and elaborated to ease the comparison across k values (since explored k values are different), relative improvements over the baseline would help to clearly see the impact of the different factors.\nFigure 6 (a) and its description in the text need to be improved to make the description more consistent, you are using WW, TW, WT, but Read-k-Write-k to refer to TT policy.\nI would discard Figure 6 (b), since it can be described with a sentence. In addition, this contribution is not clear to me since little detail is provided about how chunks are defined.\nFigure 8 (a) contains too many curves that cannot be read properly. I would reduce the number of curves discarding those less significant in terms of impact in the results and describe these minor effects in the text. In addition, from an aesthetic viewpoint, I would try to use same font size across figures in page8 and shorthen legend description to not occlude the curves (see Figure 8 (a)).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "gy961FhTpg",
        "length": 357,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of the assumption in many existing studies that operations are carried out at the subword level, even though the standard unit for input and output in most practical scenarios is typically at the word level. The paper demonstrates that policies devised and validated at the subword level are surpassed by those operating at the word level, which process multiple subwords to form a complete word in a single step. The main contributions of this paper are the demonstration of the superiority of word-level policies over subword-level policies in SiMT and the proposal of a method to boost SiMT models using language models.\n\nreasons_to_accept: Strengths of the paper: - Proposes a novel method of integrating language models (LM) into Simultaneous Machine Translation (SiMT) systems - Offers a more versatile solution that can be integrated into most existing neural SiMT models - LM-fused attention proves effective in enhancing translation quality across all latency levels - Proposed word-level policy plays a crucial role in effectively managing the vocabulary mismatch between the LM and model\n\nreasons_to_reject: Weaknesses of the paper: - The proposed method may not be applicable to languages with a writing style that lacks spaces or other delimiters between words or sentences, such as Chinese. Experiments like translation direction of En-Zh should be included in this paper.\n- Integrating a large LM may require a faster compute capability to fulfill the low-latency demands of the SiMT task. Speed experiments should be involved in this paper.\n\nquestions_for_the_authors: 1. What are some potential solutions or alternative approaches that could be explored to address the limitation of the proposed method in SiMT systems for languages with a writing style that lacks spaces or other delimiters between words or sentences, such as Chinese?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "14_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_14_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7176999999999999,
      "max_similarity": 0.7372,
      "avg_coverage": 0.43776666666666664,
      "max_coverage": 0.5417
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 663,
      "avg_human_length": 536.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 11,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "4bpRycQnwk",
        "similarity": 0.7135,
        "coverage": 0.5417,
        "human_length": 456,
        "human_text": "paper_topic_and_main_contributions: This paper tackles the task of speech query based referring video object segmentation (R-VOS). To handle the noisy nature of speech queries, the authors proposed two modules, namely noise-aware semantic adjustment, and semantic jitter suppression. Experiments show that the proposed modules are effective, and their approach achieves the state-of-the-art performance on three referring video object segmentation benchmarks, outperforming both speech R-VOS models, and ASR + text R-VOS models, closing the gap between speech based models and text based models using ground truth text.\n\nreasons_to_accept: two modules - noise-aware semantic adjustment, and semantic jitter suppression are proposed to handle the noisy speech input for semantic information extraction. Extensive ablation studies showed that the proposed modules are effective. The proposed approach is effective even when no noise is injected (table1). The impact of different noise types are also investigated, and they show that \"sustained and loud noises can lead to a severe performance drop compared to short-lived and faint noises\"\n\nreasons_to_reject: 1. The type of noise is limited to only background noise (and from AudioSet specifically), but the definition of noise in speech is much broader - reverberation, far-field, low bandwidth, missing segments, accents. Accents in particular appeared in the abstract as a motivating factor, but never mentioned in the main text of the paper. \nThe approach contains a background noise class prediction module, which could mean that the approach only works for background noise. If this is the case, I suggest the authors modify the paper to be clearer on what kind of noise is being tackled here.\n2. The generalization ability. Only a limited pool of noise categories (21 kinds in total) are used as injected noise, and therefore not sure how this approach generalize when OOD background noise appear in the speech query.\n3. The info regarding the ASR model are largely missing. The only info the author presented in the paper is that it's W2V2, but the size, pretraining/finetuning data are missing. It might be worth considering whether the proposed approach is still competitive compared to a stronger ASR model, e.g. Whisper\n\nquestions_for_the_authors: Another benefit of this end2end approach compared to using ASR, is inference speed. I'm curious of the quantitative comparison.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "XDr3eDjgH4",
        "similarity": 0.7372,
        "coverage": 0.32,
        "human_length": 789,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of referring video object segmentation (R-VOS) using speech input, which is more challenging compared to using text input due to noise and potential information loss. The paper makes several contributions to the field of R-VOS and speech-referring segmentation: Firstly, it addresses the gap between text and speech input, which is an underexplored area in the field. It introduces a method to align the semantic spaces between speech and text, enabling the adaptation of text-input R-VOS models to handle noisy speech input effectively. This allows for a more natural and convenient way for humans to refer to objects in videos using speech.\nSecondly, the paper proposes the Noise-Aware Semantic Adjustment (NSA) module, which enhances the R-VOS models to handle incomplete and distorted information from noisy speech. It focuses on extracting accurate semantics from noisy speech, enabling effective segmentation even with perturbed referring queries.\nThirdly, the paper introduces the Semantic Jitter Suppression (SJS) module, which helps the R-VOS models tolerate noisy queries by generating perturbations and suppressing noise in the textual features. This module allows the models to learn from incomplete referring guidance and adapt to noisy speech input.\nThe paper provides comprehensive experiments on three challenging benchmarks, demonstrating the superiority of the proposed method compared to state-of-the-art approaches. The experiments cover both clean and noisy speech input scenarios, showcasing the noise-tolerant capabilities of the proposed method.\n\nreasons_to_accept: Strengths: 1. Novel Approach: The paper proposes a new approach, STBridge, to bridge the gap between speech and text in R-VOS tasks. The incorporation of two key modules, NSA and SJS, effectively addresses the challenges of noisy speech input and aligning the semantic spaces between speech and text. \n2. Robust Performance: The comprehensive experiments conducted on three challenging benchmarks demonstrate that the proposed method outperforms state-of-the-art approaches. STBridge achieves significant improvements in both clean and noisy speech input scenarios, highlighting the effectiveness of the proposed modules.\nBenefits to the NLP Community: 1. Expanded Scope: The paper addresses a gap in the existing literature by exploring the underexplored area of speech-input R-VOS models. By adapting text-input R-VOS models to accommodate noisy speech input effectively, the proposed method expands the scope of R-VOS research and opens up new possibilities for multimodal HCI tasks. \n2. Noise-Tolerant Speech Understanding: The robustness of the proposed method to handle noisy speech queries provides valuable insights for researchers working on speech understanding tasks. This can contribute to the development of more accurate and reliable speech-based models, benefiting various NLP applications beyond video object segmentation. \n3. Practical Applications: The practical applications of the proposed method, such as video editing and augmented reality, have significant implications for the NLP community. The successful adaptation of text-input R-VOS models to handle noisy speech input enables the development of more user-friendly and natural human-computer interaction systems, enhancing the user experience in real-world scenarios.\n\nreasons_to_reject: Weakness: Lack of analysis of failure cases: The paper focuses on the performance improvements achieved by the proposed method but does not analyze the failure cases where the method might not perform well. Understanding the limitations and failure cases of the approach would provide a more comprehensive evaluation.\nRisk: Lack of real-world scenarios: The evaluation is conducted on benchmark datasets, and it would be beneficial to include real-world scenarios or scenarios with more complex visual scenes. This would provide a more realistic evaluation of the approach.\n\nquestions_for_the_authors: A. How does the proposed method handle cases where the spoken descriptions are ambiguous or inconsistent with the video content? Are there any strategies or techniques to handle such cases?\nB. Have you considered using noise reduction techniques to first reduce the noise in speech and then extract text through ASR to perform the R-VOS task? If so, how effective is this approach?\nC. How sensitive is the proposed method to variations in speech quality or the levels of noise in the speech? Did you observe any significant performance degradation with extremely noisy speech (e.g. SNR higher than 40 dB) or low-quality recordings?\nD. Have you tested the proposed method on languages other than English? If so, what were the challenges and how did the method perform in those cases?\nE. Have you considered incorporating other modalities (e.g. visual cues or lip movements) along with speech to improve the performance of the proposed method? If so, what were the challenges and potential benefits of such an approach?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "yvTPGBGzGY",
        "similarity": 0.7024,
        "coverage": 0.4516,
        "human_length": 365,
        "human_text": "paper_topic_and_main_contributions: This paper proposed a novel approach that adapts R-VOS models trained on clean text-video pairs to noisy speech input.  To align the semantic spaces between speech and text by incorporating two key modules: 1) Noise-Aware Semantic Adjustment for clear semantics extraction from noisy speech; and 2) Semantic Jitter Suppression enabling R-VOS models to tolerate noisy queries.\n\nreasons_to_accept: 1. The method provides solution to the important field of speech driven video object segmentation with many application scenarios. \n2. The novelty is sufficient though the solution is a modularized approach which provides step-by-step functions to deal with a challenging task. Hopefully this could lead to a more end-to-end solution from audio signals to VOS. \n3. The experiments are thorough with good results and ablation studies. \n4. The reference looks comprehensive to me. \n5. The writing is clear and easy to follow.\n\nreasons_to_reject: Please refer to my questions below for possible improvement, mostly in the approach, or at least in the writing to clarify a few details.\n\nquestions_for_the_authors: 1. What are the essential difference between noise in query and noise in ASR (line 067 vs line 076). From my understanding, the first noise is due to ASR errors which is mostly caused by the second noise. \n2. Following the above question, can you leverage the background noise from audio signals as contextual information for better queries? \n3. Is it possible to extend your idea to ASR-free queries, i.e., taking raw audio signal as input to the R-VOS model? \n4. Can you use entity/keywords extraction from speech instead of ASR? \n5. The approach relies on training data with the triplet set of video, text and audio, all synced. Is there enough data like this? How could you leverage the data with only aligned video & audio, or text and audio, etc?\n\nethical_concerns: No\n\njustification_for_ethical_concerns: No concerns.\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "4bpRycQnwk",
        "length": 456,
        "human_text": "paper_topic_and_main_contributions: This paper tackles the task of speech query based referring video object segmentation (R-VOS). To handle the noisy nature of speech queries, the authors proposed two modules, namely noise-aware semantic adjustment, and semantic jitter suppression. Experiments show that the proposed modules are effective, and their approach achieves the state-of-the-art performance on three referring video object segmentation benchmarks, outperforming both speech R-VOS models, and ASR + text R-VOS models, closing the gap between speech based models and text based models using ground truth text.\n\nreasons_to_accept: two modules - noise-aware semantic adjustment, and semantic jitter suppression are proposed to handle the noisy speech input for semantic information extraction. Extensive ablation studies showed that the proposed modules are effective. The proposed approach is effective even when no noise is injected (table1). The impact of different noise types are also investigated, and they show that \"sustained and loud noises can lead to a severe performance drop compared to short-lived and faint noises\"\n\nreasons_to_reject: 1. The type of noise is limited to only background noise (and from AudioSet specifically), but the definition of noise in speech is much broader - reverberation, far-field, low bandwidth, missing segments, accents. Accents in particular appeared in the abstract as a motivating factor, but never mentioned in the main text of the paper. \nThe approach contains a background noise class prediction module, which could mean that the approach only works for background noise. If this is the case, I suggest the authors modify the paper to be clearer on what kind of noise is being tackled here.\n2. The generalization ability. Only a limited pool of noise categories (21 kinds in total) are used as injected noise, and therefore not sure how this approach generalize when OOD background noise appear in the speech query.\n3. The info regarding the ASR model are largely missing. The only info the author presented in the paper is that it's W2V2, but the size, pretraining/finetuning data are missing. It might be worth considering whether the proposed approach is still competitive compared to a stronger ASR model, e.g. Whisper\n\nquestions_for_the_authors: Another benefit of this end2end approach compared to using ASR, is inference speed. I'm curious of the quantitative comparison.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "XDr3eDjgH4",
        "length": 789,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of referring video object segmentation (R-VOS) using speech input, which is more challenging compared to using text input due to noise and potential information loss. The paper makes several contributions to the field of R-VOS and speech-referring segmentation: Firstly, it addresses the gap between text and speech input, which is an underexplored area in the field. It introduces a method to align the semantic spaces between speech and text, enabling the adaptation of text-input R-VOS models to handle noisy speech input effectively. This allows for a more natural and convenient way for humans to refer to objects in videos using speech.\nSecondly, the paper proposes the Noise-Aware Semantic Adjustment (NSA) module, which enhances the R-VOS models to handle incomplete and distorted information from noisy speech. It focuses on extracting accurate semantics from noisy speech, enabling effective segmentation even with perturbed referring queries.\nThirdly, the paper introduces the Semantic Jitter Suppression (SJS) module, which helps the R-VOS models tolerate noisy queries by generating perturbations and suppressing noise in the textual features. This module allows the models to learn from incomplete referring guidance and adapt to noisy speech input.\nThe paper provides comprehensive experiments on three challenging benchmarks, demonstrating the superiority of the proposed method compared to state-of-the-art approaches. The experiments cover both clean and noisy speech input scenarios, showcasing the noise-tolerant capabilities of the proposed method.\n\nreasons_to_accept: Strengths: 1. Novel Approach: The paper proposes a new approach, STBridge, to bridge the gap between speech and text in R-VOS tasks. The incorporation of two key modules, NSA and SJS, effectively addresses the challenges of noisy speech input and aligning the semantic spaces between speech and text. \n2. Robust Performance: The comprehensive experiments conducted on three challenging benchmarks demonstrate that the proposed method outperforms state-of-the-art approaches. STBridge achieves significant improvements in both clean and noisy speech input scenarios, highlighting the effectiveness of the proposed modules.\nBenefits to the NLP Community: 1. Expanded Scope: The paper addresses a gap in the existing literature by exploring the underexplored area of speech-input R-VOS models. By adapting text-input R-VOS models to accommodate noisy speech input effectively, the proposed method expands the scope of R-VOS research and opens up new possibilities for multimodal HCI tasks. \n2. Noise-Tolerant Speech Understanding: The robustness of the proposed method to handle noisy speech queries provides valuable insights for researchers working on speech understanding tasks. This can contribute to the development of more accurate and reliable speech-based models, benefiting various NLP applications beyond video object segmentation. \n3. Practical Applications: The practical applications of the proposed method, such as video editing and augmented reality, have significant implications for the NLP community. The successful adaptation of text-input R-VOS models to handle noisy speech input enables the development of more user-friendly and natural human-computer interaction systems, enhancing the user experience in real-world scenarios.\n\nreasons_to_reject: Weakness: Lack of analysis of failure cases: The paper focuses on the performance improvements achieved by the proposed method but does not analyze the failure cases where the method might not perform well. Understanding the limitations and failure cases of the approach would provide a more comprehensive evaluation.\nRisk: Lack of real-world scenarios: The evaluation is conducted on benchmark datasets, and it would be beneficial to include real-world scenarios or scenarios with more complex visual scenes. This would provide a more realistic evaluation of the approach.\n\nquestions_for_the_authors: A. How does the proposed method handle cases where the spoken descriptions are ambiguous or inconsistent with the video content? Are there any strategies or techniques to handle such cases?\nB. Have you considered using noise reduction techniques to first reduce the noise in speech and then extract text through ASR to perform the R-VOS task? If so, how effective is this approach?\nC. How sensitive is the proposed method to variations in speech quality or the levels of noise in the speech? Did you observe any significant performance degradation with extremely noisy speech (e.g. SNR higher than 40 dB) or low-quality recordings?\nD. Have you tested the proposed method on languages other than English? If so, what were the challenges and how did the method perform in those cases?\nE. Have you considered incorporating other modalities (e.g. visual cues or lip movements) along with speech to improve the performance of the proposed method? If so, what were the challenges and potential benefits of such an approach?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "yvTPGBGzGY",
        "length": 365,
        "human_text": "paper_topic_and_main_contributions: This paper proposed a novel approach that adapts R-VOS models trained on clean text-video pairs to noisy speech input.  To align the semantic spaces between speech and text by incorporating two key modules: 1) Noise-Aware Semantic Adjustment for clear semantics extraction from noisy speech; and 2) Semantic Jitter Suppression enabling R-VOS models to tolerate noisy queries.\n\nreasons_to_accept: 1. The method provides solution to the important field of speech driven video object segmentation with many application scenarios. \n2. The novelty is sufficient though the solution is a modularized approach which provides step-by-step functions to deal with a challenging task. Hopefully this could lead to a more end-to-end solution from audio signals to VOS. \n3. The experiments are thorough with good results and ablation studies. \n4. The reference looks comprehensive to me. \n5. The writing is clear and easy to follow.\n\nreasons_to_reject: Please refer to my questions below for possible improvement, mostly in the approach, or at least in the writing to clarify a few details.\n\nquestions_for_the_authors: 1. What are the essential difference between noise in query and noise in ASR (line 067 vs line 076). From my understanding, the first noise is due to ASR errors which is mostly caused by the second noise. \n2. Following the above question, can you leverage the background noise from audio signals as contextual information for better queries? \n3. Is it possible to extend your idea to ASR-free queries, i.e., taking raw audio signal as input to the R-VOS model? \n4. Can you use entity/keywords extraction from speech instead of ASR? \n5. The approach relies on training data with the triplet set of video, text and audio, all synced. Is there enough data like this? How could you leverage the data with only aligned video & audio, or text and audio, etc?\n\nethical_concerns: No\n\njustification_for_ethical_concerns: No concerns.\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "14_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_14_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7193999999999999,
      "max_similarity": 0.7382,
      "avg_coverage": 0.4444333333333333,
      "max_coverage": 0.5417
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 653,
      "avg_human_length": 536.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 11,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "4bpRycQnwk",
        "similarity": 0.7158,
        "coverage": 0.5417,
        "human_length": 456,
        "human_text": "paper_topic_and_main_contributions: This paper tackles the task of speech query based referring video object segmentation (R-VOS). To handle the noisy nature of speech queries, the authors proposed two modules, namely noise-aware semantic adjustment, and semantic jitter suppression. Experiments show that the proposed modules are effective, and their approach achieves the state-of-the-art performance on three referring video object segmentation benchmarks, outperforming both speech R-VOS models, and ASR + text R-VOS models, closing the gap between speech based models and text based models using ground truth text.\n\nreasons_to_accept: two modules - noise-aware semantic adjustment, and semantic jitter suppression are proposed to handle the noisy speech input for semantic information extraction. Extensive ablation studies showed that the proposed modules are effective. The proposed approach is effective even when no noise is injected (table1). The impact of different noise types are also investigated, and they show that \"sustained and loud noises can lead to a severe performance drop compared to short-lived and faint noises\"\n\nreasons_to_reject: 1. The type of noise is limited to only background noise (and from AudioSet specifically), but the definition of noise in speech is much broader - reverberation, far-field, low bandwidth, missing segments, accents. Accents in particular appeared in the abstract as a motivating factor, but never mentioned in the main text of the paper. \nThe approach contains a background noise class prediction module, which could mean that the approach only works for background noise. If this is the case, I suggest the authors modify the paper to be clearer on what kind of noise is being tackled here.\n2. The generalization ability. Only a limited pool of noise categories (21 kinds in total) are used as injected noise, and therefore not sure how this approach generalize when OOD background noise appear in the speech query.\n3. The info regarding the ASR model are largely missing. The only info the author presented in the paper is that it's W2V2, but the size, pretraining/finetuning data are missing. It might be worth considering whether the proposed approach is still competitive compared to a stronger ASR model, e.g. Whisper\n\nquestions_for_the_authors: Another benefit of this end2end approach compared to using ASR, is inference speed. I'm curious of the quantitative comparison.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "XDr3eDjgH4",
        "similarity": 0.7382,
        "coverage": 0.34,
        "human_length": 789,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of referring video object segmentation (R-VOS) using speech input, which is more challenging compared to using text input due to noise and potential information loss. The paper makes several contributions to the field of R-VOS and speech-referring segmentation: Firstly, it addresses the gap between text and speech input, which is an underexplored area in the field. It introduces a method to align the semantic spaces between speech and text, enabling the adaptation of text-input R-VOS models to handle noisy speech input effectively. This allows for a more natural and convenient way for humans to refer to objects in videos using speech.\nSecondly, the paper proposes the Noise-Aware Semantic Adjustment (NSA) module, which enhances the R-VOS models to handle incomplete and distorted information from noisy speech. It focuses on extracting accurate semantics from noisy speech, enabling effective segmentation even with perturbed referring queries.\nThirdly, the paper introduces the Semantic Jitter Suppression (SJS) module, which helps the R-VOS models tolerate noisy queries by generating perturbations and suppressing noise in the textual features. This module allows the models to learn from incomplete referring guidance and adapt to noisy speech input.\nThe paper provides comprehensive experiments on three challenging benchmarks, demonstrating the superiority of the proposed method compared to state-of-the-art approaches. The experiments cover both clean and noisy speech input scenarios, showcasing the noise-tolerant capabilities of the proposed method.\n\nreasons_to_accept: Strengths: 1. Novel Approach: The paper proposes a new approach, STBridge, to bridge the gap between speech and text in R-VOS tasks. The incorporation of two key modules, NSA and SJS, effectively addresses the challenges of noisy speech input and aligning the semantic spaces between speech and text. \n2. Robust Performance: The comprehensive experiments conducted on three challenging benchmarks demonstrate that the proposed method outperforms state-of-the-art approaches. STBridge achieves significant improvements in both clean and noisy speech input scenarios, highlighting the effectiveness of the proposed modules.\nBenefits to the NLP Community: 1. Expanded Scope: The paper addresses a gap in the existing literature by exploring the underexplored area of speech-input R-VOS models. By adapting text-input R-VOS models to accommodate noisy speech input effectively, the proposed method expands the scope of R-VOS research and opens up new possibilities for multimodal HCI tasks. \n2. Noise-Tolerant Speech Understanding: The robustness of the proposed method to handle noisy speech queries provides valuable insights for researchers working on speech understanding tasks. This can contribute to the development of more accurate and reliable speech-based models, benefiting various NLP applications beyond video object segmentation. \n3. Practical Applications: The practical applications of the proposed method, such as video editing and augmented reality, have significant implications for the NLP community. The successful adaptation of text-input R-VOS models to handle noisy speech input enables the development of more user-friendly and natural human-computer interaction systems, enhancing the user experience in real-world scenarios.\n\nreasons_to_reject: Weakness: Lack of analysis of failure cases: The paper focuses on the performance improvements achieved by the proposed method but does not analyze the failure cases where the method might not perform well. Understanding the limitations and failure cases of the approach would provide a more comprehensive evaluation.\nRisk: Lack of real-world scenarios: The evaluation is conducted on benchmark datasets, and it would be beneficial to include real-world scenarios or scenarios with more complex visual scenes. This would provide a more realistic evaluation of the approach.\n\nquestions_for_the_authors: A. How does the proposed method handle cases where the spoken descriptions are ambiguous or inconsistent with the video content? Are there any strategies or techniques to handle such cases?\nB. Have you considered using noise reduction techniques to first reduce the noise in speech and then extract text through ASR to perform the R-VOS task? If so, how effective is this approach?\nC. How sensitive is the proposed method to variations in speech quality or the levels of noise in the speech? Did you observe any significant performance degradation with extremely noisy speech (e.g. SNR higher than 40 dB) or low-quality recordings?\nD. Have you tested the proposed method on languages other than English? If so, what were the challenges and how did the method perform in those cases?\nE. Have you considered incorporating other modalities (e.g. visual cues or lip movements) along with speech to improve the performance of the proposed method? If so, what were the challenges and potential benefits of such an approach?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "yvTPGBGzGY",
        "similarity": 0.7042,
        "coverage": 0.4516,
        "human_length": 365,
        "human_text": "paper_topic_and_main_contributions: This paper proposed a novel approach that adapts R-VOS models trained on clean text-video pairs to noisy speech input.  To align the semantic spaces between speech and text by incorporating two key modules: 1) Noise-Aware Semantic Adjustment for clear semantics extraction from noisy speech; and 2) Semantic Jitter Suppression enabling R-VOS models to tolerate noisy queries.\n\nreasons_to_accept: 1. The method provides solution to the important field of speech driven video object segmentation with many application scenarios. \n2. The novelty is sufficient though the solution is a modularized approach which provides step-by-step functions to deal with a challenging task. Hopefully this could lead to a more end-to-end solution from audio signals to VOS. \n3. The experiments are thorough with good results and ablation studies. \n4. The reference looks comprehensive to me. \n5. The writing is clear and easy to follow.\n\nreasons_to_reject: Please refer to my questions below for possible improvement, mostly in the approach, or at least in the writing to clarify a few details.\n\nquestions_for_the_authors: 1. What are the essential difference between noise in query and noise in ASR (line 067 vs line 076). From my understanding, the first noise is due to ASR errors which is mostly caused by the second noise. \n2. Following the above question, can you leverage the background noise from audio signals as contextual information for better queries? \n3. Is it possible to extend your idea to ASR-free queries, i.e., taking raw audio signal as input to the R-VOS model? \n4. Can you use entity/keywords extraction from speech instead of ASR? \n5. The approach relies on training data with the triplet set of video, text and audio, all synced. Is there enough data like this? How could you leverage the data with only aligned video & audio, or text and audio, etc?\n\nethical_concerns: No\n\njustification_for_ethical_concerns: No concerns.\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "4bpRycQnwk",
        "length": 456,
        "human_text": "paper_topic_and_main_contributions: This paper tackles the task of speech query based referring video object segmentation (R-VOS). To handle the noisy nature of speech queries, the authors proposed two modules, namely noise-aware semantic adjustment, and semantic jitter suppression. Experiments show that the proposed modules are effective, and their approach achieves the state-of-the-art performance on three referring video object segmentation benchmarks, outperforming both speech R-VOS models, and ASR + text R-VOS models, closing the gap between speech based models and text based models using ground truth text.\n\nreasons_to_accept: two modules - noise-aware semantic adjustment, and semantic jitter suppression are proposed to handle the noisy speech input for semantic information extraction. Extensive ablation studies showed that the proposed modules are effective. The proposed approach is effective even when no noise is injected (table1). The impact of different noise types are also investigated, and they show that \"sustained and loud noises can lead to a severe performance drop compared to short-lived and faint noises\"\n\nreasons_to_reject: 1. The type of noise is limited to only background noise (and from AudioSet specifically), but the definition of noise in speech is much broader - reverberation, far-field, low bandwidth, missing segments, accents. Accents in particular appeared in the abstract as a motivating factor, but never mentioned in the main text of the paper. \nThe approach contains a background noise class prediction module, which could mean that the approach only works for background noise. If this is the case, I suggest the authors modify the paper to be clearer on what kind of noise is being tackled here.\n2. The generalization ability. Only a limited pool of noise categories (21 kinds in total) are used as injected noise, and therefore not sure how this approach generalize when OOD background noise appear in the speech query.\n3. The info regarding the ASR model are largely missing. The only info the author presented in the paper is that it's W2V2, but the size, pretraining/finetuning data are missing. It might be worth considering whether the proposed approach is still competitive compared to a stronger ASR model, e.g. Whisper\n\nquestions_for_the_authors: Another benefit of this end2end approach compared to using ASR, is inference speed. I'm curious of the quantitative comparison.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "XDr3eDjgH4",
        "length": 789,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of referring video object segmentation (R-VOS) using speech input, which is more challenging compared to using text input due to noise and potential information loss. The paper makes several contributions to the field of R-VOS and speech-referring segmentation: Firstly, it addresses the gap between text and speech input, which is an underexplored area in the field. It introduces a method to align the semantic spaces between speech and text, enabling the adaptation of text-input R-VOS models to handle noisy speech input effectively. This allows for a more natural and convenient way for humans to refer to objects in videos using speech.\nSecondly, the paper proposes the Noise-Aware Semantic Adjustment (NSA) module, which enhances the R-VOS models to handle incomplete and distorted information from noisy speech. It focuses on extracting accurate semantics from noisy speech, enabling effective segmentation even with perturbed referring queries.\nThirdly, the paper introduces the Semantic Jitter Suppression (SJS) module, which helps the R-VOS models tolerate noisy queries by generating perturbations and suppressing noise in the textual features. This module allows the models to learn from incomplete referring guidance and adapt to noisy speech input.\nThe paper provides comprehensive experiments on three challenging benchmarks, demonstrating the superiority of the proposed method compared to state-of-the-art approaches. The experiments cover both clean and noisy speech input scenarios, showcasing the noise-tolerant capabilities of the proposed method.\n\nreasons_to_accept: Strengths: 1. Novel Approach: The paper proposes a new approach, STBridge, to bridge the gap between speech and text in R-VOS tasks. The incorporation of two key modules, NSA and SJS, effectively addresses the challenges of noisy speech input and aligning the semantic spaces between speech and text. \n2. Robust Performance: The comprehensive experiments conducted on three challenging benchmarks demonstrate that the proposed method outperforms state-of-the-art approaches. STBridge achieves significant improvements in both clean and noisy speech input scenarios, highlighting the effectiveness of the proposed modules.\nBenefits to the NLP Community: 1. Expanded Scope: The paper addresses a gap in the existing literature by exploring the underexplored area of speech-input R-VOS models. By adapting text-input R-VOS models to accommodate noisy speech input effectively, the proposed method expands the scope of R-VOS research and opens up new possibilities for multimodal HCI tasks. \n2. Noise-Tolerant Speech Understanding: The robustness of the proposed method to handle noisy speech queries provides valuable insights for researchers working on speech understanding tasks. This can contribute to the development of more accurate and reliable speech-based models, benefiting various NLP applications beyond video object segmentation. \n3. Practical Applications: The practical applications of the proposed method, such as video editing and augmented reality, have significant implications for the NLP community. The successful adaptation of text-input R-VOS models to handle noisy speech input enables the development of more user-friendly and natural human-computer interaction systems, enhancing the user experience in real-world scenarios.\n\nreasons_to_reject: Weakness: Lack of analysis of failure cases: The paper focuses on the performance improvements achieved by the proposed method but does not analyze the failure cases where the method might not perform well. Understanding the limitations and failure cases of the approach would provide a more comprehensive evaluation.\nRisk: Lack of real-world scenarios: The evaluation is conducted on benchmark datasets, and it would be beneficial to include real-world scenarios or scenarios with more complex visual scenes. This would provide a more realistic evaluation of the approach.\n\nquestions_for_the_authors: A. How does the proposed method handle cases where the spoken descriptions are ambiguous or inconsistent with the video content? Are there any strategies or techniques to handle such cases?\nB. Have you considered using noise reduction techniques to first reduce the noise in speech and then extract text through ASR to perform the R-VOS task? If so, how effective is this approach?\nC. How sensitive is the proposed method to variations in speech quality or the levels of noise in the speech? Did you observe any significant performance degradation with extremely noisy speech (e.g. SNR higher than 40 dB) or low-quality recordings?\nD. Have you tested the proposed method on languages other than English? If so, what were the challenges and how did the method perform in those cases?\nE. Have you considered incorporating other modalities (e.g. visual cues or lip movements) along with speech to improve the performance of the proposed method? If so, what were the challenges and potential benefits of such an approach?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "yvTPGBGzGY",
        "length": 365,
        "human_text": "paper_topic_and_main_contributions: This paper proposed a novel approach that adapts R-VOS models trained on clean text-video pairs to noisy speech input.  To align the semantic spaces between speech and text by incorporating two key modules: 1) Noise-Aware Semantic Adjustment for clear semantics extraction from noisy speech; and 2) Semantic Jitter Suppression enabling R-VOS models to tolerate noisy queries.\n\nreasons_to_accept: 1. The method provides solution to the important field of speech driven video object segmentation with many application scenarios. \n2. The novelty is sufficient though the solution is a modularized approach which provides step-by-step functions to deal with a challenging task. Hopefully this could lead to a more end-to-end solution from audio signals to VOS. \n3. The experiments are thorough with good results and ablation studies. \n4. The reference looks comprehensive to me. \n5. The writing is clear and easy to follow.\n\nreasons_to_reject: Please refer to my questions below for possible improvement, mostly in the approach, or at least in the writing to clarify a few details.\n\nquestions_for_the_authors: 1. What are the essential difference between noise in query and noise in ASR (line 067 vs line 076). From my understanding, the first noise is due to ASR errors which is mostly caused by the second noise. \n2. Following the above question, can you leverage the background noise from audio signals as contextual information for better queries? \n3. Is it possible to extend your idea to ASR-free queries, i.e., taking raw audio signal as input to the R-VOS model? \n4. Can you use entity/keywords extraction from speech instead of ASR? \n5. The approach relies on training data with the triplet set of video, text and audio, all synced. Is there enough data like this? How could you leverage the data with only aligned video & audio, or text and audio, etc?\n\nethical_concerns: No\n\njustification_for_ethical_concerns: No concerns.\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "164_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_164_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7067333333333333,
      "max_similarity": 0.7198,
      "avg_coverage": 0.4641,
      "max_coverage": 0.7273
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 590,
      "avg_human_length": 653.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 9,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "8cVjdhciaA",
        "similarity": 0.6931,
        "coverage": 0.7273,
        "human_length": 234,
        "human_text": "paper_topic_and_main_contributions: In this paper, GPT models are used to modify prompts for text-to-image generation. The authors show that, in the generation process, GPT modified prompts can reduce the number of edits by around 20%. A human study is conducted to compare the type of GPT generated edits and human edits, as well as the practicality of the usefulness of the generated prompts.\n\nreasons_to_accept: The paper is generally well written with clear motivation, solid experiments, and presented with concrete examples. It is interesting to see such analysis and how GPT models can contribute to the prompting process of text-to-image generation.\n\nreasons_to_reject: Some of the qualitative examples are a bit confusing without detailed analysis, please see the questions.\n\nquestions_for_the_authors: In Figure 7 in the appendix, it was quite surprising to see that Edit2 and the initial prompt are the same but ended up with both failure generations, though totally different, have you analyzed factors of such failure cases/prompts/edits? Or if any filtering has been done from the traces before performing in-context learning/prompting?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "ksL6z0NIr1",
        "similarity": 0.7198,
        "coverage": 0.2364,
        "human_length": 896,
        "human_text": "paper_topic_and_main_contributions: The paper investigates the benefits of incorporating GPT-k into the text2image pipeline by using GPT-k to edit the initial t2i prompt. They then investigate how these edits perform compared to human edits. They also investigate what common edit types are made by humans compared with GPT-k and how edit types and # of edits affect the results.\n\nreasons_to_accept: 1. The paper provides interesting and detailed experiments on the benefits and drawbacks of incorporating GPT-k models into the text-to-image pipeline. Some recent work was focusing on incorporating LLMs and diffusion models (see missing references) but this study provides more formal empirical investigation of this phenomenon.  2. The experiments are well-defined and executed. While additional models and settings are possible, we can already draw some preliminary conclusions from this study about the usefulness of GPT models to improve t2i generation.\n\nreasons_to_reject: 1. Authors create a dataset of t2i prompt edits by clustering a dataset of t2i prompts into 100K clusters using sentence embeddings. However, no evaluation or verification of edit clusters is provided. Thus, it is hard to judge if the clusters make sense or not or if they even refer to the same image. Taking the clusters with more than just a few instructions (authors select clusters of up to 20 instructions) seems like an unrealistic setting to me and potentially increasing the chance that some of the clusters will have unrelated prompts. It would be interesting to see how smaller clusters of e.g. under 5 instructions would affect the results.\n2. Each edit cluster has 1 to n edits. I find problematic how only the n-th edit of the trace was used for fine-tuning and few-shot prompts. First, as there was no guarantee that the edits were about the same image, but also the first and last edits will be very different and skipping multiple steps in the edit trace. This potentially teaches the model to hallucinate edits as it has no way to understand the intermediary steps. For example, steps in the middle might have have had different edit types completely altering the final prompt. There needs to be control for semantic/lexical similarity of first and last prompt. The experiment could also include models fine-tuned on a single edit step for a single or multiple edit types. Another way would be to vary n-th edit e.g. from 1 k=n to see the impact on i'-i_MS.\n3. The human evaluation is not very clear to me. First, gpt-3-curie was chosen even though babbage has better CLIP scores, RNE (Table 2), and closer distribution to human edits (Table 4). Second, the framing of the task as choosing between prompts is very vague. For example in Figure 6 it is very hard to choose what edit would be best. It would be more clear to frame it explicitly as an image selection task (which I suspect crowdworkers did anyway).\n4. In the effects of edit types experiment, the CLIP score reported is only between generated image and the last image in the edit cluster. Since each edit cluster can contain multiple types of edits, it would be more appropriate to compare to the first image on which the edit type was applied.\n\nquestions_for_the_authors: A: What are hyperparameters for clustering prompts into traces of edits and how were they chosen? I was surprised to see that you have clusters with such a huge number of edits, did you inspect them to see that the clusters make sense? How many clusters in absolute numbers do you have after selecting only those that have at most 20 edits?\nB: 131-135: How is the ordering for the traces determined?\nC: 150-152 this does not make sense to me. Before you mentioned you only focused on 95% of the traces. How come now it is the full 100K?\nD: 225-227 How exactly did you identify edit type using sequenceMatcher?\nE: 233: gpt-3 are also autoregressive but you say their distribution is more similar. so the claim \"GPT-2 models, due to their autoregressive training nature, have a tendency towards continual generation, resulting in a majority of edits being insert\" does not hold.\n\nmissing_references: Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, and Smaranda Muresan. 2023. I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. In Findings of the Association for Computational Linguistics: ACL 2023, pages 7370\u20137388, Toronto, Canada. Association for Computational Linguistics.\n\ntypos_grammar_style_and_presentation_improvements: 023-024 \"predicting spontaneous changes in the primary subject matters\"  not very clear what are primary subject matters 049-053 Figure 1 and description is very unclear. What is a trace and how did you define an edit? the division symbol is also confusing as it seems # of edits /trace is simply the size of a cluster of prompts, probably just keeping # edits is simpler 059-062 \"LLMs and T2I models are trained on different modalities\" that's not entirely true as many multimodal models are being released recently (including announced GPT-4) 094 adjusting -> adjustment 152 holdout -> heldout\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "XaFIfc96tf",
        "similarity": 0.7073,
        "coverage": 0.4286,
        "human_length": 830,
        "human_text": "paper_topic_and_main_contributions: The paper investigates the concept of \"prompt editing\" associated with text-to-image generation, in which the users apply successive edits to their prompts in order to generate their desired image. It does so by analyzing a dataset of user prompts scraped from the official Stable Diffusion Discord server and employing various GPT-based models to improve the prompt editing process. In their experiments the authors observe that while the GPT-based models seem to mostly insert modifiers, human users tend replace words and phrases, yielding a significantly different image in the end. Furthermore, it also seems that editing the prompt via GPT-based models results in images more similar to intermediate edits made by the users rather than the final image, suggesting that the GPT-based models may be more effectively used to reduce the number of edits necessary to arrive at the final image.\n\nreasons_to_accept: - An analysis of the prompt edits made by users at the Stable Diffusion Discord server - Exploration of the difference between the images generated by user-edited vs. GPT-generated prompts - Evaluation of various GPT-based models on this task, with the GPT-2 model family being finetuned and GPT-3 model family being prompted - Human evaluation of the prompts generated/edited by the GPT-based models, showing them to be better or comparable to user edited prompts - An ablation study on the edit types (insert/delete/swap/replace) on the generated output image\n\nreasons_to_reject: The paper is well written and hence there are only a few reasons why this paper might be rejected.\nPerhaps the strongest one is the fact that while the GPT-based models are a key part of the analysis, except for the prompt the authors used the paper doesn't contain any further details on how it was used as a metric. Furthermore, the OpenAI's APIs provide various parameters such as `temperature`, `top_p`, `presence_penalty` and `frequency_penalty`, which are not specified in the paper, further complicating potential future replication efforts. This should also be applicable to GPT-2 models that were publicly released and are not available only via an API (e.g. the HuggingFace Transformers provide APIs for specifying similar parameters).\nA similar issue can be found with the Stable Diffusion model that was used for image generation. It is not obvious what parameters, except for the prompts, were varied when the images were generated, and to what extent would changing these have an impact on the presented results.\nThere were also two relatively minor inconsistencies we found in the paper which unfortunately detract from the quality of the paper.  The first can be found on line 151 when they authors mention \"We split the 100k trace of edits into two halves,\" but the next part of the sentence says \"with 30k traces used for evaluations and the remaining 70k serving as holdout set\". When splitting something into two halves, it would be reasonable to expect a 50:50 split but that is not what we observe here. It is therefore unclear which of the potential interpretations the authors meant in this sentence.\nThe second can be found at the discussion of edit types which starts on line 217. When using SequenceMatcher, it is unclear how are the edits classified. If for instance one generated prompt both swaps the order of words, inserts a few new words towards the beginning and removes a few words from the end of the prompt, which category would such a change fall into? Would it be all of them or just one of them or neither?\n\nquestions_for_the_authors: **Question A**: Would you mind providing some more details on how you used the GPT-3 models in the evaluation, especially what parameters were provided to the API and also how expensive was it to execute the experiments described in the paper?\n**Question B**: Is there any specific reason why the `gpt-3.5-turbo` model wasn't used in the experiments as opposed to `text-davinci-003`, which is both more expensive and which the OpenAI's docs now consider legacy?\n**Question C**:  Would you be able to provide any intuition as to what extent are the GPT-3 results robust to changes of the prompt shown in Table 8?\n**Question D**: Similarly to the previous question, how did you arrive at choosing 16 shots for the in-context learning and to what extent would varying this number change the results?\n\ntypos_grammar_style_and_presentation_improvements: - line 051: \"issue\" -- it would be best to replace this word with say \"phenomenon\" or \"figure\" - line 060: \"LLMs and T2I models are trained on different modalities and architectures\" -- although the models do use different architectures, it is not clear what training on architectures would refer to - line 138: \"denotes\" -> \"denote\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "8cVjdhciaA",
        "length": 234,
        "human_text": "paper_topic_and_main_contributions: In this paper, GPT models are used to modify prompts for text-to-image generation. The authors show that, in the generation process, GPT modified prompts can reduce the number of edits by around 20%. A human study is conducted to compare the type of GPT generated edits and human edits, as well as the practicality of the usefulness of the generated prompts.\n\nreasons_to_accept: The paper is generally well written with clear motivation, solid experiments, and presented with concrete examples. It is interesting to see such analysis and how GPT models can contribute to the prompting process of text-to-image generation.\n\nreasons_to_reject: Some of the qualitative examples are a bit confusing without detailed analysis, please see the questions.\n\nquestions_for_the_authors: In Figure 7 in the appendix, it was quite surprising to see that Edit2 and the initial prompt are the same but ended up with both failure generations, though totally different, have you analyzed factors of such failure cases/prompts/edits? Or if any filtering has been done from the traces before performing in-context learning/prompting?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ksL6z0NIr1",
        "length": 896,
        "human_text": "paper_topic_and_main_contributions: The paper investigates the benefits of incorporating GPT-k into the text2image pipeline by using GPT-k to edit the initial t2i prompt. They then investigate how these edits perform compared to human edits. They also investigate what common edit types are made by humans compared with GPT-k and how edit types and # of edits affect the results.\n\nreasons_to_accept: 1. The paper provides interesting and detailed experiments on the benefits and drawbacks of incorporating GPT-k models into the text-to-image pipeline. Some recent work was focusing on incorporating LLMs and diffusion models (see missing references) but this study provides more formal empirical investigation of this phenomenon.  2. The experiments are well-defined and executed. While additional models and settings are possible, we can already draw some preliminary conclusions from this study about the usefulness of GPT models to improve t2i generation.\n\nreasons_to_reject: 1. Authors create a dataset of t2i prompt edits by clustering a dataset of t2i prompts into 100K clusters using sentence embeddings. However, no evaluation or verification of edit clusters is provided. Thus, it is hard to judge if the clusters make sense or not or if they even refer to the same image. Taking the clusters with more than just a few instructions (authors select clusters of up to 20 instructions) seems like an unrealistic setting to me and potentially increasing the chance that some of the clusters will have unrelated prompts. It would be interesting to see how smaller clusters of e.g. under 5 instructions would affect the results.\n2. Each edit cluster has 1 to n edits. I find problematic how only the n-th edit of the trace was used for fine-tuning and few-shot prompts. First, as there was no guarantee that the edits were about the same image, but also the first and last edits will be very different and skipping multiple steps in the edit trace. This potentially teaches the model to hallucinate edits as it has no way to understand the intermediary steps. For example, steps in the middle might have have had different edit types completely altering the final prompt. There needs to be control for semantic/lexical similarity of first and last prompt. The experiment could also include models fine-tuned on a single edit step for a single or multiple edit types. Another way would be to vary n-th edit e.g. from 1 k=n to see the impact on i'-i_MS.\n3. The human evaluation is not very clear to me. First, gpt-3-curie was chosen even though babbage has better CLIP scores, RNE (Table 2), and closer distribution to human edits (Table 4). Second, the framing of the task as choosing between prompts is very vague. For example in Figure 6 it is very hard to choose what edit would be best. It would be more clear to frame it explicitly as an image selection task (which I suspect crowdworkers did anyway).\n4. In the effects of edit types experiment, the CLIP score reported is only between generated image and the last image in the edit cluster. Since each edit cluster can contain multiple types of edits, it would be more appropriate to compare to the first image on which the edit type was applied.\n\nquestions_for_the_authors: A: What are hyperparameters for clustering prompts into traces of edits and how were they chosen? I was surprised to see that you have clusters with such a huge number of edits, did you inspect them to see that the clusters make sense? How many clusters in absolute numbers do you have after selecting only those that have at most 20 edits?\nB: 131-135: How is the ordering for the traces determined?\nC: 150-152 this does not make sense to me. Before you mentioned you only focused on 95% of the traces. How come now it is the full 100K?\nD: 225-227 How exactly did you identify edit type using sequenceMatcher?\nE: 233: gpt-3 are also autoregressive but you say their distribution is more similar. so the claim \"GPT-2 models, due to their autoregressive training nature, have a tendency towards continual generation, resulting in a majority of edits being insert\" does not hold.\n\nmissing_references: Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, and Smaranda Muresan. 2023. I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. In Findings of the Association for Computational Linguistics: ACL 2023, pages 7370\u20137388, Toronto, Canada. Association for Computational Linguistics.\n\ntypos_grammar_style_and_presentation_improvements: 023-024 \"predicting spontaneous changes in the primary subject matters\"  not very clear what are primary subject matters 049-053 Figure 1 and description is very unclear. What is a trace and how did you define an edit? the division symbol is also confusing as it seems # of edits /trace is simply the size of a cluster of prompts, probably just keeping # edits is simpler 059-062 \"LLMs and T2I models are trained on different modalities\" that's not entirely true as many multimodal models are being released recently (including announced GPT-4) 094 adjusting -> adjustment 152 holdout -> heldout\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "XaFIfc96tf",
        "length": 830,
        "human_text": "paper_topic_and_main_contributions: The paper investigates the concept of \"prompt editing\" associated with text-to-image generation, in which the users apply successive edits to their prompts in order to generate their desired image. It does so by analyzing a dataset of user prompts scraped from the official Stable Diffusion Discord server and employing various GPT-based models to improve the prompt editing process. In their experiments the authors observe that while the GPT-based models seem to mostly insert modifiers, human users tend replace words and phrases, yielding a significantly different image in the end. Furthermore, it also seems that editing the prompt via GPT-based models results in images more similar to intermediate edits made by the users rather than the final image, suggesting that the GPT-based models may be more effectively used to reduce the number of edits necessary to arrive at the final image.\n\nreasons_to_accept: - An analysis of the prompt edits made by users at the Stable Diffusion Discord server - Exploration of the difference between the images generated by user-edited vs. GPT-generated prompts - Evaluation of various GPT-based models on this task, with the GPT-2 model family being finetuned and GPT-3 model family being prompted - Human evaluation of the prompts generated/edited by the GPT-based models, showing them to be better or comparable to user edited prompts - An ablation study on the edit types (insert/delete/swap/replace) on the generated output image\n\nreasons_to_reject: The paper is well written and hence there are only a few reasons why this paper might be rejected.\nPerhaps the strongest one is the fact that while the GPT-based models are a key part of the analysis, except for the prompt the authors used the paper doesn't contain any further details on how it was used as a metric. Furthermore, the OpenAI's APIs provide various parameters such as `temperature`, `top_p`, `presence_penalty` and `frequency_penalty`, which are not specified in the paper, further complicating potential future replication efforts. This should also be applicable to GPT-2 models that were publicly released and are not available only via an API (e.g. the HuggingFace Transformers provide APIs for specifying similar parameters).\nA similar issue can be found with the Stable Diffusion model that was used for image generation. It is not obvious what parameters, except for the prompts, were varied when the images were generated, and to what extent would changing these have an impact on the presented results.\nThere were also two relatively minor inconsistencies we found in the paper which unfortunately detract from the quality of the paper.  The first can be found on line 151 when they authors mention \"We split the 100k trace of edits into two halves,\" but the next part of the sentence says \"with 30k traces used for evaluations and the remaining 70k serving as holdout set\". When splitting something into two halves, it would be reasonable to expect a 50:50 split but that is not what we observe here. It is therefore unclear which of the potential interpretations the authors meant in this sentence.\nThe second can be found at the discussion of edit types which starts on line 217. When using SequenceMatcher, it is unclear how are the edits classified. If for instance one generated prompt both swaps the order of words, inserts a few new words towards the beginning and removes a few words from the end of the prompt, which category would such a change fall into? Would it be all of them or just one of them or neither?\n\nquestions_for_the_authors: **Question A**: Would you mind providing some more details on how you used the GPT-3 models in the evaluation, especially what parameters were provided to the API and also how expensive was it to execute the experiments described in the paper?\n**Question B**: Is there any specific reason why the `gpt-3.5-turbo` model wasn't used in the experiments as opposed to `text-davinci-003`, which is both more expensive and which the OpenAI's docs now consider legacy?\n**Question C**:  Would you be able to provide any intuition as to what extent are the GPT-3 results robust to changes of the prompt shown in Table 8?\n**Question D**: Similarly to the previous question, how did you arrive at choosing 16 shots for the in-context learning and to what extent would varying this number change the results?\n\ntypos_grammar_style_and_presentation_improvements: - line 051: \"issue\" -- it would be best to replace this word with say \"phenomenon\" or \"figure\" - line 060: \"LLMs and T2I models are trained on different modalities and architectures\" -- although the models do use different architectures, it is not clear what training on architectures would refer to - line 138: \"denotes\" -> \"denote\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "164_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_164_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7068666666666666,
      "max_similarity": 0.7207,
      "avg_coverage": 0.47013333333333335,
      "max_coverage": 0.7273
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 560,
      "avg_human_length": 653.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 9,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "8cVjdhciaA",
        "similarity": 0.6948,
        "coverage": 0.7273,
        "human_length": 234,
        "human_text": "paper_topic_and_main_contributions: In this paper, GPT models are used to modify prompts for text-to-image generation. The authors show that, in the generation process, GPT modified prompts can reduce the number of edits by around 20%. A human study is conducted to compare the type of GPT generated edits and human edits, as well as the practicality of the usefulness of the generated prompts.\n\nreasons_to_accept: The paper is generally well written with clear motivation, solid experiments, and presented with concrete examples. It is interesting to see such analysis and how GPT models can contribute to the prompting process of text-to-image generation.\n\nreasons_to_reject: Some of the qualitative examples are a bit confusing without detailed analysis, please see the questions.\n\nquestions_for_the_authors: In Figure 7 in the appendix, it was quite surprising to see that Edit2 and the initial prompt are the same but ended up with both failure generations, though totally different, have you analyzed factors of such failure cases/prompts/edits? Or if any filtering has been done from the traces before performing in-context learning/prompting?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "ksL6z0NIr1",
        "similarity": 0.7207,
        "coverage": 0.2545,
        "human_length": 896,
        "human_text": "paper_topic_and_main_contributions: The paper investigates the benefits of incorporating GPT-k into the text2image pipeline by using GPT-k to edit the initial t2i prompt. They then investigate how these edits perform compared to human edits. They also investigate what common edit types are made by humans compared with GPT-k and how edit types and # of edits affect the results.\n\nreasons_to_accept: 1. The paper provides interesting and detailed experiments on the benefits and drawbacks of incorporating GPT-k models into the text-to-image pipeline. Some recent work was focusing on incorporating LLMs and diffusion models (see missing references) but this study provides more formal empirical investigation of this phenomenon.  2. The experiments are well-defined and executed. While additional models and settings are possible, we can already draw some preliminary conclusions from this study about the usefulness of GPT models to improve t2i generation.\n\nreasons_to_reject: 1. Authors create a dataset of t2i prompt edits by clustering a dataset of t2i prompts into 100K clusters using sentence embeddings. However, no evaluation or verification of edit clusters is provided. Thus, it is hard to judge if the clusters make sense or not or if they even refer to the same image. Taking the clusters with more than just a few instructions (authors select clusters of up to 20 instructions) seems like an unrealistic setting to me and potentially increasing the chance that some of the clusters will have unrelated prompts. It would be interesting to see how smaller clusters of e.g. under 5 instructions would affect the results.\n2. Each edit cluster has 1 to n edits. I find problematic how only the n-th edit of the trace was used for fine-tuning and few-shot prompts. First, as there was no guarantee that the edits were about the same image, but also the first and last edits will be very different and skipping multiple steps in the edit trace. This potentially teaches the model to hallucinate edits as it has no way to understand the intermediary steps. For example, steps in the middle might have have had different edit types completely altering the final prompt. There needs to be control for semantic/lexical similarity of first and last prompt. The experiment could also include models fine-tuned on a single edit step for a single or multiple edit types. Another way would be to vary n-th edit e.g. from 1 k=n to see the impact on i'-i_MS.\n3. The human evaluation is not very clear to me. First, gpt-3-curie was chosen even though babbage has better CLIP scores, RNE (Table 2), and closer distribution to human edits (Table 4). Second, the framing of the task as choosing between prompts is very vague. For example in Figure 6 it is very hard to choose what edit would be best. It would be more clear to frame it explicitly as an image selection task (which I suspect crowdworkers did anyway).\n4. In the effects of edit types experiment, the CLIP score reported is only between generated image and the last image in the edit cluster. Since each edit cluster can contain multiple types of edits, it would be more appropriate to compare to the first image on which the edit type was applied.\n\nquestions_for_the_authors: A: What are hyperparameters for clustering prompts into traces of edits and how were they chosen? I was surprised to see that you have clusters with such a huge number of edits, did you inspect them to see that the clusters make sense? How many clusters in absolute numbers do you have after selecting only those that have at most 20 edits?\nB: 131-135: How is the ordering for the traces determined?\nC: 150-152 this does not make sense to me. Before you mentioned you only focused on 95% of the traces. How come now it is the full 100K?\nD: 225-227 How exactly did you identify edit type using sequenceMatcher?\nE: 233: gpt-3 are also autoregressive but you say their distribution is more similar. so the claim \"GPT-2 models, due to their autoregressive training nature, have a tendency towards continual generation, resulting in a majority of edits being insert\" does not hold.\n\nmissing_references: Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, and Smaranda Muresan. 2023. I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. In Findings of the Association for Computational Linguistics: ACL 2023, pages 7370\u20137388, Toronto, Canada. Association for Computational Linguistics.\n\ntypos_grammar_style_and_presentation_improvements: 023-024 \"predicting spontaneous changes in the primary subject matters\"  not very clear what are primary subject matters 049-053 Figure 1 and description is very unclear. What is a trace and how did you define an edit? the division symbol is also confusing as it seems # of edits /trace is simply the size of a cluster of prompts, probably just keeping # edits is simpler 059-062 \"LLMs and T2I models are trained on different modalities\" that's not entirely true as many multimodal models are being released recently (including announced GPT-4) 094 adjusting -> adjustment 152 holdout -> heldout\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "XaFIfc96tf",
        "similarity": 0.7051,
        "coverage": 0.4286,
        "human_length": 830,
        "human_text": "paper_topic_and_main_contributions: The paper investigates the concept of \"prompt editing\" associated with text-to-image generation, in which the users apply successive edits to their prompts in order to generate their desired image. It does so by analyzing a dataset of user prompts scraped from the official Stable Diffusion Discord server and employing various GPT-based models to improve the prompt editing process. In their experiments the authors observe that while the GPT-based models seem to mostly insert modifiers, human users tend replace words and phrases, yielding a significantly different image in the end. Furthermore, it also seems that editing the prompt via GPT-based models results in images more similar to intermediate edits made by the users rather than the final image, suggesting that the GPT-based models may be more effectively used to reduce the number of edits necessary to arrive at the final image.\n\nreasons_to_accept: - An analysis of the prompt edits made by users at the Stable Diffusion Discord server - Exploration of the difference between the images generated by user-edited vs. GPT-generated prompts - Evaluation of various GPT-based models on this task, with the GPT-2 model family being finetuned and GPT-3 model family being prompted - Human evaluation of the prompts generated/edited by the GPT-based models, showing them to be better or comparable to user edited prompts - An ablation study on the edit types (insert/delete/swap/replace) on the generated output image\n\nreasons_to_reject: The paper is well written and hence there are only a few reasons why this paper might be rejected.\nPerhaps the strongest one is the fact that while the GPT-based models are a key part of the analysis, except for the prompt the authors used the paper doesn't contain any further details on how it was used as a metric. Furthermore, the OpenAI's APIs provide various parameters such as `temperature`, `top_p`, `presence_penalty` and `frequency_penalty`, which are not specified in the paper, further complicating potential future replication efforts. This should also be applicable to GPT-2 models that were publicly released and are not available only via an API (e.g. the HuggingFace Transformers provide APIs for specifying similar parameters).\nA similar issue can be found with the Stable Diffusion model that was used for image generation. It is not obvious what parameters, except for the prompts, were varied when the images were generated, and to what extent would changing these have an impact on the presented results.\nThere were also two relatively minor inconsistencies we found in the paper which unfortunately detract from the quality of the paper.  The first can be found on line 151 when they authors mention \"We split the 100k trace of edits into two halves,\" but the next part of the sentence says \"with 30k traces used for evaluations and the remaining 70k serving as holdout set\". When splitting something into two halves, it would be reasonable to expect a 50:50 split but that is not what we observe here. It is therefore unclear which of the potential interpretations the authors meant in this sentence.\nThe second can be found at the discussion of edit types which starts on line 217. When using SequenceMatcher, it is unclear how are the edits classified. If for instance one generated prompt both swaps the order of words, inserts a few new words towards the beginning and removes a few words from the end of the prompt, which category would such a change fall into? Would it be all of them or just one of them or neither?\n\nquestions_for_the_authors: **Question A**: Would you mind providing some more details on how you used the GPT-3 models in the evaluation, especially what parameters were provided to the API and also how expensive was it to execute the experiments described in the paper?\n**Question B**: Is there any specific reason why the `gpt-3.5-turbo` model wasn't used in the experiments as opposed to `text-davinci-003`, which is both more expensive and which the OpenAI's docs now consider legacy?\n**Question C**:  Would you be able to provide any intuition as to what extent are the GPT-3 results robust to changes of the prompt shown in Table 8?\n**Question D**: Similarly to the previous question, how did you arrive at choosing 16 shots for the in-context learning and to what extent would varying this number change the results?\n\ntypos_grammar_style_and_presentation_improvements: - line 051: \"issue\" -- it would be best to replace this word with say \"phenomenon\" or \"figure\" - line 060: \"LLMs and T2I models are trained on different modalities and architectures\" -- although the models do use different architectures, it is not clear what training on architectures would refer to - line 138: \"denotes\" -> \"denote\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "8cVjdhciaA",
        "length": 234,
        "human_text": "paper_topic_and_main_contributions: In this paper, GPT models are used to modify prompts for text-to-image generation. The authors show that, in the generation process, GPT modified prompts can reduce the number of edits by around 20%. A human study is conducted to compare the type of GPT generated edits and human edits, as well as the practicality of the usefulness of the generated prompts.\n\nreasons_to_accept: The paper is generally well written with clear motivation, solid experiments, and presented with concrete examples. It is interesting to see such analysis and how GPT models can contribute to the prompting process of text-to-image generation.\n\nreasons_to_reject: Some of the qualitative examples are a bit confusing without detailed analysis, please see the questions.\n\nquestions_for_the_authors: In Figure 7 in the appendix, it was quite surprising to see that Edit2 and the initial prompt are the same but ended up with both failure generations, though totally different, have you analyzed factors of such failure cases/prompts/edits? Or if any filtering has been done from the traces before performing in-context learning/prompting?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ksL6z0NIr1",
        "length": 896,
        "human_text": "paper_topic_and_main_contributions: The paper investigates the benefits of incorporating GPT-k into the text2image pipeline by using GPT-k to edit the initial t2i prompt. They then investigate how these edits perform compared to human edits. They also investigate what common edit types are made by humans compared with GPT-k and how edit types and # of edits affect the results.\n\nreasons_to_accept: 1. The paper provides interesting and detailed experiments on the benefits and drawbacks of incorporating GPT-k models into the text-to-image pipeline. Some recent work was focusing on incorporating LLMs and diffusion models (see missing references) but this study provides more formal empirical investigation of this phenomenon.  2. The experiments are well-defined and executed. While additional models and settings are possible, we can already draw some preliminary conclusions from this study about the usefulness of GPT models to improve t2i generation.\n\nreasons_to_reject: 1. Authors create a dataset of t2i prompt edits by clustering a dataset of t2i prompts into 100K clusters using sentence embeddings. However, no evaluation or verification of edit clusters is provided. Thus, it is hard to judge if the clusters make sense or not or if they even refer to the same image. Taking the clusters with more than just a few instructions (authors select clusters of up to 20 instructions) seems like an unrealistic setting to me and potentially increasing the chance that some of the clusters will have unrelated prompts. It would be interesting to see how smaller clusters of e.g. under 5 instructions would affect the results.\n2. Each edit cluster has 1 to n edits. I find problematic how only the n-th edit of the trace was used for fine-tuning and few-shot prompts. First, as there was no guarantee that the edits were about the same image, but also the first and last edits will be very different and skipping multiple steps in the edit trace. This potentially teaches the model to hallucinate edits as it has no way to understand the intermediary steps. For example, steps in the middle might have have had different edit types completely altering the final prompt. There needs to be control for semantic/lexical similarity of first and last prompt. The experiment could also include models fine-tuned on a single edit step for a single or multiple edit types. Another way would be to vary n-th edit e.g. from 1 k=n to see the impact on i'-i_MS.\n3. The human evaluation is not very clear to me. First, gpt-3-curie was chosen even though babbage has better CLIP scores, RNE (Table 2), and closer distribution to human edits (Table 4). Second, the framing of the task as choosing between prompts is very vague. For example in Figure 6 it is very hard to choose what edit would be best. It would be more clear to frame it explicitly as an image selection task (which I suspect crowdworkers did anyway).\n4. In the effects of edit types experiment, the CLIP score reported is only between generated image and the last image in the edit cluster. Since each edit cluster can contain multiple types of edits, it would be more appropriate to compare to the first image on which the edit type was applied.\n\nquestions_for_the_authors: A: What are hyperparameters for clustering prompts into traces of edits and how were they chosen? I was surprised to see that you have clusters with such a huge number of edits, did you inspect them to see that the clusters make sense? How many clusters in absolute numbers do you have after selecting only those that have at most 20 edits?\nB: 131-135: How is the ordering for the traces determined?\nC: 150-152 this does not make sense to me. Before you mentioned you only focused on 95% of the traces. How come now it is the full 100K?\nD: 225-227 How exactly did you identify edit type using sequenceMatcher?\nE: 233: gpt-3 are also autoregressive but you say their distribution is more similar. so the claim \"GPT-2 models, due to their autoregressive training nature, have a tendency towards continual generation, resulting in a majority of edits being insert\" does not hold.\n\nmissing_references: Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, and Smaranda Muresan. 2023. I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. In Findings of the Association for Computational Linguistics: ACL 2023, pages 7370\u20137388, Toronto, Canada. Association for Computational Linguistics.\n\ntypos_grammar_style_and_presentation_improvements: 023-024 \"predicting spontaneous changes in the primary subject matters\"  not very clear what are primary subject matters 049-053 Figure 1 and description is very unclear. What is a trace and how did you define an edit? the division symbol is also confusing as it seems # of edits /trace is simply the size of a cluster of prompts, probably just keeping # edits is simpler 059-062 \"LLMs and T2I models are trained on different modalities\" that's not entirely true as many multimodal models are being released recently (including announced GPT-4) 094 adjusting -> adjustment 152 holdout -> heldout\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "XaFIfc96tf",
        "length": 830,
        "human_text": "paper_topic_and_main_contributions: The paper investigates the concept of \"prompt editing\" associated with text-to-image generation, in which the users apply successive edits to their prompts in order to generate their desired image. It does so by analyzing a dataset of user prompts scraped from the official Stable Diffusion Discord server and employing various GPT-based models to improve the prompt editing process. In their experiments the authors observe that while the GPT-based models seem to mostly insert modifiers, human users tend replace words and phrases, yielding a significantly different image in the end. Furthermore, it also seems that editing the prompt via GPT-based models results in images more similar to intermediate edits made by the users rather than the final image, suggesting that the GPT-based models may be more effectively used to reduce the number of edits necessary to arrive at the final image.\n\nreasons_to_accept: - An analysis of the prompt edits made by users at the Stable Diffusion Discord server - Exploration of the difference between the images generated by user-edited vs. GPT-generated prompts - Evaluation of various GPT-based models on this task, with the GPT-2 model family being finetuned and GPT-3 model family being prompted - Human evaluation of the prompts generated/edited by the GPT-based models, showing them to be better or comparable to user edited prompts - An ablation study on the edit types (insert/delete/swap/replace) on the generated output image\n\nreasons_to_reject: The paper is well written and hence there are only a few reasons why this paper might be rejected.\nPerhaps the strongest one is the fact that while the GPT-based models are a key part of the analysis, except for the prompt the authors used the paper doesn't contain any further details on how it was used as a metric. Furthermore, the OpenAI's APIs provide various parameters such as `temperature`, `top_p`, `presence_penalty` and `frequency_penalty`, which are not specified in the paper, further complicating potential future replication efforts. This should also be applicable to GPT-2 models that were publicly released and are not available only via an API (e.g. the HuggingFace Transformers provide APIs for specifying similar parameters).\nA similar issue can be found with the Stable Diffusion model that was used for image generation. It is not obvious what parameters, except for the prompts, were varied when the images were generated, and to what extent would changing these have an impact on the presented results.\nThere were also two relatively minor inconsistencies we found in the paper which unfortunately detract from the quality of the paper.  The first can be found on line 151 when they authors mention \"We split the 100k trace of edits into two halves,\" but the next part of the sentence says \"with 30k traces used for evaluations and the remaining 70k serving as holdout set\". When splitting something into two halves, it would be reasonable to expect a 50:50 split but that is not what we observe here. It is therefore unclear which of the potential interpretations the authors meant in this sentence.\nThe second can be found at the discussion of edit types which starts on line 217. When using SequenceMatcher, it is unclear how are the edits classified. If for instance one generated prompt both swaps the order of words, inserts a few new words towards the beginning and removes a few words from the end of the prompt, which category would such a change fall into? Would it be all of them or just one of them or neither?\n\nquestions_for_the_authors: **Question A**: Would you mind providing some more details on how you used the GPT-3 models in the evaluation, especially what parameters were provided to the API and also how expensive was it to execute the experiments described in the paper?\n**Question B**: Is there any specific reason why the `gpt-3.5-turbo` model wasn't used in the experiments as opposed to `text-davinci-003`, which is both more expensive and which the OpenAI's docs now consider legacy?\n**Question C**:  Would you be able to provide any intuition as to what extent are the GPT-3 results robust to changes of the prompt shown in Table 8?\n**Question D**: Similarly to the previous question, how did you arrive at choosing 16 shots for the in-context learning and to what extent would varying this number change the results?\n\ntypos_grammar_style_and_presentation_improvements: - line 051: \"issue\" -- it would be best to replace this word with say \"phenomenon\" or \"figure\" - line 060: \"LLMs and T2I models are trained on different modalities and architectures\" -- although the models do use different architectures, it is not clear what training on architectures would refer to - line 138: \"denotes\" -> \"denote\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "96_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_96_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7148333333333333,
      "max_similarity": 0.7233,
      "avg_coverage": 0.40149999999999997,
      "max_coverage": 0.4783
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 349,
      "avg_human_length": 439.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 6,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "Rt0IRRyYM0",
        "similarity": 0.7233,
        "coverage": 0.4783,
        "human_length": 305,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method for context-supervised pre-training for dense passage retrieval. The main idea is to use generated pseudo queries as context for pre-training. It can be combined with existing frameworks like coCondenser or CoT-MAE. The proposed method is evaluated on the MS-MARCO passage retrieval dataset and out-of-domain zero-shot BEIR benchmarks. Experimental results show that the proposed method is effective compared to baselines.\n\nreasons_to_accept: 1. The paper is well written and easy to follow. The idea of using pseudo queries as context for pre-training is simple and clear. \n2. Experiments on popular passage retrieval datasets show that the proposed method is effective and outperforms other competitive methods.\n\nreasons_to_reject: 1. The technical contribution is not much. Generated pseudo queries have been widely used in the context of passage retrieval, including doc2query style sparse retrieval, zero-shot and few-shot dense retrieval. I acknowledge that this is the first paper to combine pseudo queries with coCondenser / CoT-MAE framework, but it is quite straightforward, and the overall contribution is limited. \n2. The proposed method requires training a query generator with labeled data, while methods like coCondenser work with unlabeled data only. This will limit the applicability of the proposed method.\n\nquestions_for_the_authors: A. Why are some numbers on TREC DL for your own model implementations missing?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "6qmm0B8S7i",
        "similarity": 0.712,
        "coverage": 0.25,
        "human_length": 740,
        "human_text": "paper_topic_and_main_contributions: This paper proposes the use of a pair of query-passage for pretraining the dense retrieval, referred to as query-as-context pretraining, motivated by criticizing that a pair of passage-passage in the same document, which is used on the previous context-supervised pretraining, is weakly related or not relevant. The proposed query-as-context pretraining is applied to coCondenser and CoT-MAE, which shows improvements on the standard datasets.\n\nreasons_to_accept: While query generation for pretraining has been widely used in DSI, the paper extensively applies the query generation on two existing models (i.e., coCondenser and CoT-MAE) in the dense passage retrieval, which makes meaningful and novel contribution in the literature. Different from the existing doc2query that focuses on document expansion, this work applies the query generation for the contrastive learning. In the experiment results, although the proposed method does not achieve the SOTA performance without improving CoT-MAE-v2, the performances are consistently improved on the standard datasets.\n\nreasons_to_reject: - The proposed query generation and its resulting pretraining is quite similar to the works introduced in DSI (i.e., NCI and DSI-QG), which makes the technical part less novel. Comparing to the existing NCI (or DSI-QG), the novelty and value of this work needs be clearly presented.  - The authors criticize that the use of a passage-passage pair in the same document is not optimal for pretraining the dense retrieval, and propose the query-as-context as an alternative approach. However, rather than the proposed alternative view, the proposed query-passage and the existing passage-passage pairs can be integrated in a complementary manner, based on the multi-task learning. Not all passage-passage pairs are problematic in pretraining, as some of the pairs would be relevant. The authors\u2019 critic on the usefulness of the existing passage-passage pair is largely unvalidated. In particular, the experiments in Section 5.2, without mixing two types of pairs, the results of only passage-passage pair need to be presented. Experiments for the effect of combining two pretraining losses derived from passage-passage and passage-query pairs may be required.\n\nquestions_for_the_authors: 1) Query generation has been widely used in DSI as below. Comparing the case of DSI, what distinguishable effects are expected by applying the query generation for pretraining dense retrieval?  - Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Allen Sun, Weiwei Deng, Qi Zhang, and Mao Yang. 2022. A neural corpus indexer for document retrieval. CoRR, abs/2206.02743.\n- Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging the gap between indexing and retrieval for differentiable search index with query generation. CoRR, abs/2206.10128 2) The last version of CoT-MAE is CoT-MAE-v2 that uses multi-view representation and decoding. Similar improvements could be made over CoT-MAE-v2 too?  3) In Table 3, the use of single query is most dominant, and increasing number of queries does not so make substantial difference. Can you check the diversity across generated queries? How to control the diversity of the generated queries?  4) In Table 3, it would help us to check the effect of single query when providing the case of the zero query number.\n\nmissing_references: - Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Allen Sun, Weiwei Deng, Qi Zhang, and Mao Yang. 2022. A neural corpus indexer for document retrieval. CoRR, abs/2206.02743.\n- Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging the gap between indexing and retrieval for differentiable search index with query generation. CoRR, abs/2206.10128 - Xing Wu, Guangyuan Ma, Peng Wang, Meng Lin, Zijia Lin, Fuzheng Zhang, Songlin Hu, CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval, arXiv:2304.03158, 2023\n\ntypos_grammar_style_and_presentation_improvements: In Section 3.1, in Appendix, it would be helpful to present the generated query samples for some passages.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "ok7Anvt2CM",
        "similarity": 0.7092,
        "coverage": 0.4762,
        "human_length": 272,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a pre-training technique designed to address the problem of weakly correlated pairs of passages within the same document. The proposed method assumes that a query derived from a passage is likely to be more relevant to that passage, thereby forming a passage-query pair. \nThe experimental results demonstrate that the method yields substantial improvements, thereby showcasing its effectiveness and efficiency.\n\nreasons_to_accept: Strongness  1. They proposed a simple context-supervised pretraining technique by utilizing query-passage pairs. \n2. The results of the experiment show that the method delivers significant enhancements. \n3. They also examined the impact of the number of generated queries and the mixed context.\n\nreasons_to_reject: Weakness 1. The criteria for dividing passages need to be more accurate. Instead of simply dividing by token length, shouldn't the division be based on semantic units?\n\nquestions_for_the_authors: 1. How were the values of topp and topk determined for T5 fine-tuning? \n2. The performance could vary depending on how the T5 model is pre-trained. Additionally, what about exploring the use of other generation models besides T5?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "Rt0IRRyYM0",
        "length": 305,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method for context-supervised pre-training for dense passage retrieval. The main idea is to use generated pseudo queries as context for pre-training. It can be combined with existing frameworks like coCondenser or CoT-MAE. The proposed method is evaluated on the MS-MARCO passage retrieval dataset and out-of-domain zero-shot BEIR benchmarks. Experimental results show that the proposed method is effective compared to baselines.\n\nreasons_to_accept: 1. The paper is well written and easy to follow. The idea of using pseudo queries as context for pre-training is simple and clear. \n2. Experiments on popular passage retrieval datasets show that the proposed method is effective and outperforms other competitive methods.\n\nreasons_to_reject: 1. The technical contribution is not much. Generated pseudo queries have been widely used in the context of passage retrieval, including doc2query style sparse retrieval, zero-shot and few-shot dense retrieval. I acknowledge that this is the first paper to combine pseudo queries with coCondenser / CoT-MAE framework, but it is quite straightforward, and the overall contribution is limited. \n2. The proposed method requires training a query generator with labeled data, while methods like coCondenser work with unlabeled data only. This will limit the applicability of the proposed method.\n\nquestions_for_the_authors: A. Why are some numbers on TREC DL for your own model implementations missing?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "6qmm0B8S7i",
        "length": 740,
        "human_text": "paper_topic_and_main_contributions: This paper proposes the use of a pair of query-passage for pretraining the dense retrieval, referred to as query-as-context pretraining, motivated by criticizing that a pair of passage-passage in the same document, which is used on the previous context-supervised pretraining, is weakly related or not relevant. The proposed query-as-context pretraining is applied to coCondenser and CoT-MAE, which shows improvements on the standard datasets.\n\nreasons_to_accept: While query generation for pretraining has been widely used in DSI, the paper extensively applies the query generation on two existing models (i.e., coCondenser and CoT-MAE) in the dense passage retrieval, which makes meaningful and novel contribution in the literature. Different from the existing doc2query that focuses on document expansion, this work applies the query generation for the contrastive learning. In the experiment results, although the proposed method does not achieve the SOTA performance without improving CoT-MAE-v2, the performances are consistently improved on the standard datasets.\n\nreasons_to_reject: - The proposed query generation and its resulting pretraining is quite similar to the works introduced in DSI (i.e., NCI and DSI-QG), which makes the technical part less novel. Comparing to the existing NCI (or DSI-QG), the novelty and value of this work needs be clearly presented.  - The authors criticize that the use of a passage-passage pair in the same document is not optimal for pretraining the dense retrieval, and propose the query-as-context as an alternative approach. However, rather than the proposed alternative view, the proposed query-passage and the existing passage-passage pairs can be integrated in a complementary manner, based on the multi-task learning. Not all passage-passage pairs are problematic in pretraining, as some of the pairs would be relevant. The authors\u2019 critic on the usefulness of the existing passage-passage pair is largely unvalidated. In particular, the experiments in Section 5.2, without mixing two types of pairs, the results of only passage-passage pair need to be presented. Experiments for the effect of combining two pretraining losses derived from passage-passage and passage-query pairs may be required.\n\nquestions_for_the_authors: 1) Query generation has been widely used in DSI as below. Comparing the case of DSI, what distinguishable effects are expected by applying the query generation for pretraining dense retrieval?  - Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Allen Sun, Weiwei Deng, Qi Zhang, and Mao Yang. 2022. A neural corpus indexer for document retrieval. CoRR, abs/2206.02743.\n- Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging the gap between indexing and retrieval for differentiable search index with query generation. CoRR, abs/2206.10128 2) The last version of CoT-MAE is CoT-MAE-v2 that uses multi-view representation and decoding. Similar improvements could be made over CoT-MAE-v2 too?  3) In Table 3, the use of single query is most dominant, and increasing number of queries does not so make substantial difference. Can you check the diversity across generated queries? How to control the diversity of the generated queries?  4) In Table 3, it would help us to check the effect of single query when providing the case of the zero query number.\n\nmissing_references: - Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Allen Sun, Weiwei Deng, Qi Zhang, and Mao Yang. 2022. A neural corpus indexer for document retrieval. CoRR, abs/2206.02743.\n- Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging the gap between indexing and retrieval for differentiable search index with query generation. CoRR, abs/2206.10128 - Xing Wu, Guangyuan Ma, Peng Wang, Meng Lin, Zijia Lin, Fuzheng Zhang, Songlin Hu, CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval, arXiv:2304.03158, 2023\n\ntypos_grammar_style_and_presentation_improvements: In Section 3.1, in Appendix, it would be helpful to present the generated query samples for some passages.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ok7Anvt2CM",
        "length": 272,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a pre-training technique designed to address the problem of weakly correlated pairs of passages within the same document. The proposed method assumes that a query derived from a passage is likely to be more relevant to that passage, thereby forming a passage-query pair. \nThe experimental results demonstrate that the method yields substantial improvements, thereby showcasing its effectiveness and efficiency.\n\nreasons_to_accept: Strongness  1. They proposed a simple context-supervised pretraining technique by utilizing query-passage pairs. \n2. The results of the experiment show that the method delivers significant enhancements. \n3. They also examined the impact of the number of generated queries and the mixed context.\n\nreasons_to_reject: Weakness 1. The criteria for dividing passages need to be more accurate. Instead of simply dividing by token length, shouldn't the division be based on semantic units?\n\nquestions_for_the_authors: 1. How were the values of topp and topk determined for T5 fine-tuning? \n2. The performance could vary depending on how the T5 model is pre-trained. Additionally, what about exploring the use of other generation models besides T5?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "96_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_96_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7074333333333334,
      "max_similarity": 0.7121,
      "avg_coverage": 0.40903333333333336,
      "max_coverage": 0.5238
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 419,
      "avg_human_length": 439.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 6,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "Rt0IRRyYM0",
        "similarity": 0.7121,
        "coverage": 0.4783,
        "human_length": 305,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method for context-supervised pre-training for dense passage retrieval. The main idea is to use generated pseudo queries as context for pre-training. It can be combined with existing frameworks like coCondenser or CoT-MAE. The proposed method is evaluated on the MS-MARCO passage retrieval dataset and out-of-domain zero-shot BEIR benchmarks. Experimental results show that the proposed method is effective compared to baselines.\n\nreasons_to_accept: 1. The paper is well written and easy to follow. The idea of using pseudo queries as context for pre-training is simple and clear. \n2. Experiments on popular passage retrieval datasets show that the proposed method is effective and outperforms other competitive methods.\n\nreasons_to_reject: 1. The technical contribution is not much. Generated pseudo queries have been widely used in the context of passage retrieval, including doc2query style sparse retrieval, zero-shot and few-shot dense retrieval. I acknowledge that this is the first paper to combine pseudo queries with coCondenser / CoT-MAE framework, but it is quite straightforward, and the overall contribution is limited. \n2. The proposed method requires training a query generator with labeled data, while methods like coCondenser work with unlabeled data only. This will limit the applicability of the proposed method.\n\nquestions_for_the_authors: A. Why are some numbers on TREC DL for your own model implementations missing?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "6qmm0B8S7i",
        "similarity": 0.705,
        "coverage": 0.225,
        "human_length": 740,
        "human_text": "paper_topic_and_main_contributions: This paper proposes the use of a pair of query-passage for pretraining the dense retrieval, referred to as query-as-context pretraining, motivated by criticizing that a pair of passage-passage in the same document, which is used on the previous context-supervised pretraining, is weakly related or not relevant. The proposed query-as-context pretraining is applied to coCondenser and CoT-MAE, which shows improvements on the standard datasets.\n\nreasons_to_accept: While query generation for pretraining has been widely used in DSI, the paper extensively applies the query generation on two existing models (i.e., coCondenser and CoT-MAE) in the dense passage retrieval, which makes meaningful and novel contribution in the literature. Different from the existing doc2query that focuses on document expansion, this work applies the query generation for the contrastive learning. In the experiment results, although the proposed method does not achieve the SOTA performance without improving CoT-MAE-v2, the performances are consistently improved on the standard datasets.\n\nreasons_to_reject: - The proposed query generation and its resulting pretraining is quite similar to the works introduced in DSI (i.e., NCI and DSI-QG), which makes the technical part less novel. Comparing to the existing NCI (or DSI-QG), the novelty and value of this work needs be clearly presented.  - The authors criticize that the use of a passage-passage pair in the same document is not optimal for pretraining the dense retrieval, and propose the query-as-context as an alternative approach. However, rather than the proposed alternative view, the proposed query-passage and the existing passage-passage pairs can be integrated in a complementary manner, based on the multi-task learning. Not all passage-passage pairs are problematic in pretraining, as some of the pairs would be relevant. The authors\u2019 critic on the usefulness of the existing passage-passage pair is largely unvalidated. In particular, the experiments in Section 5.2, without mixing two types of pairs, the results of only passage-passage pair need to be presented. Experiments for the effect of combining two pretraining losses derived from passage-passage and passage-query pairs may be required.\n\nquestions_for_the_authors: 1) Query generation has been widely used in DSI as below. Comparing the case of DSI, what distinguishable effects are expected by applying the query generation for pretraining dense retrieval?  - Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Allen Sun, Weiwei Deng, Qi Zhang, and Mao Yang. 2022. A neural corpus indexer for document retrieval. CoRR, abs/2206.02743.\n- Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging the gap between indexing and retrieval for differentiable search index with query generation. CoRR, abs/2206.10128 2) The last version of CoT-MAE is CoT-MAE-v2 that uses multi-view representation and decoding. Similar improvements could be made over CoT-MAE-v2 too?  3) In Table 3, the use of single query is most dominant, and increasing number of queries does not so make substantial difference. Can you check the diversity across generated queries? How to control the diversity of the generated queries?  4) In Table 3, it would help us to check the effect of single query when providing the case of the zero query number.\n\nmissing_references: - Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Allen Sun, Weiwei Deng, Qi Zhang, and Mao Yang. 2022. A neural corpus indexer for document retrieval. CoRR, abs/2206.02743.\n- Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging the gap between indexing and retrieval for differentiable search index with query generation. CoRR, abs/2206.10128 - Xing Wu, Guangyuan Ma, Peng Wang, Meng Lin, Zijia Lin, Fuzheng Zhang, Songlin Hu, CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval, arXiv:2304.03158, 2023\n\ntypos_grammar_style_and_presentation_improvements: In Section 3.1, in Appendix, it would be helpful to present the generated query samples for some passages.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "ok7Anvt2CM",
        "similarity": 0.7052,
        "coverage": 0.5238,
        "human_length": 272,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a pre-training technique designed to address the problem of weakly correlated pairs of passages within the same document. The proposed method assumes that a query derived from a passage is likely to be more relevant to that passage, thereby forming a passage-query pair. \nThe experimental results demonstrate that the method yields substantial improvements, thereby showcasing its effectiveness and efficiency.\n\nreasons_to_accept: Strongness  1. They proposed a simple context-supervised pretraining technique by utilizing query-passage pairs. \n2. The results of the experiment show that the method delivers significant enhancements. \n3. They also examined the impact of the number of generated queries and the mixed context.\n\nreasons_to_reject: Weakness 1. The criteria for dividing passages need to be more accurate. Instead of simply dividing by token length, shouldn't the division be based on semantic units?\n\nquestions_for_the_authors: 1. How were the values of topp and topk determined for T5 fine-tuning? \n2. The performance could vary depending on how the T5 model is pre-trained. Additionally, what about exploring the use of other generation models besides T5?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "Rt0IRRyYM0",
        "length": 305,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method for context-supervised pre-training for dense passage retrieval. The main idea is to use generated pseudo queries as context for pre-training. It can be combined with existing frameworks like coCondenser or CoT-MAE. The proposed method is evaluated on the MS-MARCO passage retrieval dataset and out-of-domain zero-shot BEIR benchmarks. Experimental results show that the proposed method is effective compared to baselines.\n\nreasons_to_accept: 1. The paper is well written and easy to follow. The idea of using pseudo queries as context for pre-training is simple and clear. \n2. Experiments on popular passage retrieval datasets show that the proposed method is effective and outperforms other competitive methods.\n\nreasons_to_reject: 1. The technical contribution is not much. Generated pseudo queries have been widely used in the context of passage retrieval, including doc2query style sparse retrieval, zero-shot and few-shot dense retrieval. I acknowledge that this is the first paper to combine pseudo queries with coCondenser / CoT-MAE framework, but it is quite straightforward, and the overall contribution is limited. \n2. The proposed method requires training a query generator with labeled data, while methods like coCondenser work with unlabeled data only. This will limit the applicability of the proposed method.\n\nquestions_for_the_authors: A. Why are some numbers on TREC DL for your own model implementations missing?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "6qmm0B8S7i",
        "length": 740,
        "human_text": "paper_topic_and_main_contributions: This paper proposes the use of a pair of query-passage for pretraining the dense retrieval, referred to as query-as-context pretraining, motivated by criticizing that a pair of passage-passage in the same document, which is used on the previous context-supervised pretraining, is weakly related or not relevant. The proposed query-as-context pretraining is applied to coCondenser and CoT-MAE, which shows improvements on the standard datasets.\n\nreasons_to_accept: While query generation for pretraining has been widely used in DSI, the paper extensively applies the query generation on two existing models (i.e., coCondenser and CoT-MAE) in the dense passage retrieval, which makes meaningful and novel contribution in the literature. Different from the existing doc2query that focuses on document expansion, this work applies the query generation for the contrastive learning. In the experiment results, although the proposed method does not achieve the SOTA performance without improving CoT-MAE-v2, the performances are consistently improved on the standard datasets.\n\nreasons_to_reject: - The proposed query generation and its resulting pretraining is quite similar to the works introduced in DSI (i.e., NCI and DSI-QG), which makes the technical part less novel. Comparing to the existing NCI (or DSI-QG), the novelty and value of this work needs be clearly presented.  - The authors criticize that the use of a passage-passage pair in the same document is not optimal for pretraining the dense retrieval, and propose the query-as-context as an alternative approach. However, rather than the proposed alternative view, the proposed query-passage and the existing passage-passage pairs can be integrated in a complementary manner, based on the multi-task learning. Not all passage-passage pairs are problematic in pretraining, as some of the pairs would be relevant. The authors\u2019 critic on the usefulness of the existing passage-passage pair is largely unvalidated. In particular, the experiments in Section 5.2, without mixing two types of pairs, the results of only passage-passage pair need to be presented. Experiments for the effect of combining two pretraining losses derived from passage-passage and passage-query pairs may be required.\n\nquestions_for_the_authors: 1) Query generation has been widely used in DSI as below. Comparing the case of DSI, what distinguishable effects are expected by applying the query generation for pretraining dense retrieval?  - Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Allen Sun, Weiwei Deng, Qi Zhang, and Mao Yang. 2022. A neural corpus indexer for document retrieval. CoRR, abs/2206.02743.\n- Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging the gap between indexing and retrieval for differentiable search index with query generation. CoRR, abs/2206.10128 2) The last version of CoT-MAE is CoT-MAE-v2 that uses multi-view representation and decoding. Similar improvements could be made over CoT-MAE-v2 too?  3) In Table 3, the use of single query is most dominant, and increasing number of queries does not so make substantial difference. Can you check the diversity across generated queries? How to control the diversity of the generated queries?  4) In Table 3, it would help us to check the effect of single query when providing the case of the zero query number.\n\nmissing_references: - Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Allen Sun, Weiwei Deng, Qi Zhang, and Mao Yang. 2022. A neural corpus indexer for document retrieval. CoRR, abs/2206.02743.\n- Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging the gap between indexing and retrieval for differentiable search index with query generation. CoRR, abs/2206.10128 - Xing Wu, Guangyuan Ma, Peng Wang, Meng Lin, Zijia Lin, Fuzheng Zhang, Songlin Hu, CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval, arXiv:2304.03158, 2023\n\ntypos_grammar_style_and_presentation_improvements: In Section 3.1, in Appendix, it would be helpful to present the generated query samples for some passages.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ok7Anvt2CM",
        "length": 272,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a pre-training technique designed to address the problem of weakly correlated pairs of passages within the same document. The proposed method assumes that a query derived from a passage is likely to be more relevant to that passage, thereby forming a passage-query pair. \nThe experimental results demonstrate that the method yields substantial improvements, thereby showcasing its effectiveness and efficiency.\n\nreasons_to_accept: Strongness  1. They proposed a simple context-supervised pretraining technique by utilizing query-passage pairs. \n2. The results of the experiment show that the method delivers significant enhancements. \n3. They also examined the impact of the number of generated queries and the mixed context.\n\nreasons_to_reject: Weakness 1. The criteria for dividing passages need to be more accurate. Instead of simply dividing by token length, shouldn't the division be based on semantic units?\n\nquestions_for_the_authors: 1. How were the values of topp and topk determined for T5 fine-tuning? \n2. The performance could vary depending on how the T5 model is pre-trained. Additionally, what about exploring the use of other generation models besides T5?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "47_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_47_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7081,
      "max_similarity": 0.7181,
      "avg_coverage": 0.6721,
      "max_coverage": 0.7647
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 750,
      "avg_human_length": 334.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 11,
      "suggestions_count": 14
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "E9tXyCblLg",
        "similarity": 0.704,
        "coverage": 0.6429,
        "human_length": 292,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a method for detecting semantic or usage differences and a method for extracting their typical instances in context based on the variance of contextualised word vectors. The approach should be effective in corpus pairs whose sizes are considerably different and also in infrequent words. The authors propose two methods: one for detecting words that have semantic differences in two corpora and one for extracting the representative instances.\n\nreasons_to_accept: The approach might be useful to compare diachronic corpora and detect periods of creation for some document collections. It could also be applied to detect texts developed by non-native speakers and even discriminate between texts developed by speakers with different mother tongues.\n\nreasons_to_reject: It goes without saying that vectors rely on specific texts, and that some degree of similarity between collections of texts can be measured. The authors stated a few limitations on the suggested approach themselves: the assumption that the form of a word is constant as its meaning(s) change. \nAlthough two-dimensional vectors are specified, it is unclear if this is always the case. Words that are formally identical but have different meanings ought to be connected to various vectors.\n\nmissing_references: https://aclanthology.org/P19-1321/ https://paperswithcode.com/paper/learn-interpretable-word-embeddings\n\nethical_concerns: Yes\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "thl9fnAVLq",
        "similarity": 0.7022,
        "coverage": 0.7647,
        "human_length": 312,
        "human_text": "paper_topic_and_main_contributions: The authors present a new method for checking whether the same words occurring in two different corpora have the same or different meanings. The method is relatively simple because it uses pre-trained contextual word vectors and is based on counting the norm of the mean vectors. The intuition is that if a word has more meanings (more boundary meanings), the mean vector is shorter. A suitable coefficient for comparing norms in two corpora was defined and called \"coverage\".  An additional task that was solved was to identify a representative occurrence of a word for a meaning that does not occur in the other corpus.  The solution is based on counting the cosine similarity between a vector representing a particular occurrence of a word and a vector representing the difference between the weighted mean norms. Experiments were conducted on two pairs of corpora: English 1800s/2000s (COHA) and native and non-native speakers of English (ICNALE).  SMEval-2020 Task 1 data (from COHA) was used for quantitative evaluation.  The results obtained were very good, at the level obtained by much more computationally demanding methods.\n\nreasons_to_accept: The proposed method is relatively simple, but very effective. The authors presented the mathematical basis of the proposed measures. An extended qualitative analysis was conducted with some interesting observations about the English language itself.\n\nreasons_to_reject: I do not see any\n\nquestions_for_the_authors: A: In a sketchy algorithm given in section 2.2 you write that the xs-xt should be counted. But the definition of representativeness  contains weights here.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "ye6gVGuSqN",
        "similarity": 0.7181,
        "coverage": 0.6087,
        "human_length": 400,
        "human_text": "paper_topic_and_main_contributions: This paper proposes methods for measuring semantic differences in words between two corpora, contributing to computationally-aided linguistic analysis.  Using a pre-trained large language model, e.g., BERT, new methods are used to examine the coverage of the meanings of words, through the norm of the mean word vectors across meanings. The proposed methods are easy to implement and without pre-training any language models, and do not require alignments between words or corpora for comparison. The proposed methods excel in semantic difference detection tasks compared to previous works, reveal potential semantic shift and difference in POS using diachronic datasets. Furthermore, the differences between native and non-native English usages are revealed using the proposed methods, in terms of different aspects, which can be beneficial for further investigation in second language acquisition.  The paper also discusses the limitations of the proposed methods, specifically the presumption of using von Mises-Fisher distribution and also other limitations for applications.\n\nreasons_to_accept: 1. The proposed methods present a straightforward and effective approach for detecting semantic differences and the mathematical background is illustrated in certain detail. \n2. Use-cases are given with both quantitative and qualitative analyses. \n3. The codes are provided and the work is reproducible. \n4. The work can be beneficial in research in other disciplines, such as second language acquisition.\n\nreasons_to_reject: 1. This work has only done experiments and reported results in English corpora, it would be very interesting to see some analysis in different languages or across languages, especially in the non-native English users with their mother tongue languages, and explore further potential in using variance metrics in second language acquisition research. \n2. As mentioned in the limitation section, that the usage of a large language model to obtain word vectors implicitly assumes that it models the target language as well. The methods rely on a monolingual language model trained on certain time period of data can be limited in detecting semantic shifts across a wider period of time. The current approach can be improved and this topic can be explored further.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "E9tXyCblLg",
        "length": 292,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a method for detecting semantic or usage differences and a method for extracting their typical instances in context based on the variance of contextualised word vectors. The approach should be effective in corpus pairs whose sizes are considerably different and also in infrequent words. The authors propose two methods: one for detecting words that have semantic differences in two corpora and one for extracting the representative instances.\n\nreasons_to_accept: The approach might be useful to compare diachronic corpora and detect periods of creation for some document collections. It could also be applied to detect texts developed by non-native speakers and even discriminate between texts developed by speakers with different mother tongues.\n\nreasons_to_reject: It goes without saying that vectors rely on specific texts, and that some degree of similarity between collections of texts can be measured. The authors stated a few limitations on the suggested approach themselves: the assumption that the form of a word is constant as its meaning(s) change. \nAlthough two-dimensional vectors are specified, it is unclear if this is always the case. Words that are formally identical but have different meanings ought to be connected to various vectors.\n\nmissing_references: https://aclanthology.org/P19-1321/ https://paperswithcode.com/paper/learn-interpretable-word-embeddings\n\nethical_concerns: Yes\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "thl9fnAVLq",
        "length": 312,
        "human_text": "paper_topic_and_main_contributions: The authors present a new method for checking whether the same words occurring in two different corpora have the same or different meanings. The method is relatively simple because it uses pre-trained contextual word vectors and is based on counting the norm of the mean vectors. The intuition is that if a word has more meanings (more boundary meanings), the mean vector is shorter. A suitable coefficient for comparing norms in two corpora was defined and called \"coverage\".  An additional task that was solved was to identify a representative occurrence of a word for a meaning that does not occur in the other corpus.  The solution is based on counting the cosine similarity between a vector representing a particular occurrence of a word and a vector representing the difference between the weighted mean norms. Experiments were conducted on two pairs of corpora: English 1800s/2000s (COHA) and native and non-native speakers of English (ICNALE).  SMEval-2020 Task 1 data (from COHA) was used for quantitative evaluation.  The results obtained were very good, at the level obtained by much more computationally demanding methods.\n\nreasons_to_accept: The proposed method is relatively simple, but very effective. The authors presented the mathematical basis of the proposed measures. An extended qualitative analysis was conducted with some interesting observations about the English language itself.\n\nreasons_to_reject: I do not see any\n\nquestions_for_the_authors: A: In a sketchy algorithm given in section 2.2 you write that the xs-xt should be counted. But the definition of representativeness  contains weights here.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ye6gVGuSqN",
        "length": 400,
        "human_text": "paper_topic_and_main_contributions: This paper proposes methods for measuring semantic differences in words between two corpora, contributing to computationally-aided linguistic analysis.  Using a pre-trained large language model, e.g., BERT, new methods are used to examine the coverage of the meanings of words, through the norm of the mean word vectors across meanings. The proposed methods are easy to implement and without pre-training any language models, and do not require alignments between words or corpora for comparison. The proposed methods excel in semantic difference detection tasks compared to previous works, reveal potential semantic shift and difference in POS using diachronic datasets. Furthermore, the differences between native and non-native English usages are revealed using the proposed methods, in terms of different aspects, which can be beneficial for further investigation in second language acquisition.  The paper also discusses the limitations of the proposed methods, specifically the presumption of using von Mises-Fisher distribution and also other limitations for applications.\n\nreasons_to_accept: 1. The proposed methods present a straightforward and effective approach for detecting semantic differences and the mathematical background is illustrated in certain detail. \n2. Use-cases are given with both quantitative and qualitative analyses. \n3. The codes are provided and the work is reproducible. \n4. The work can be beneficial in research in other disciplines, such as second language acquisition.\n\nreasons_to_reject: 1. This work has only done experiments and reported results in English corpora, it would be very interesting to see some analysis in different languages or across languages, especially in the non-native English users with their mother tongue languages, and explore further potential in using variance metrics in second language acquisition research. \n2. As mentioned in the limitation section, that the usage of a large language model to obtain word vectors implicitly assumes that it models the target language as well. The methods rely on a monolingual language model trained on certain time period of data can be limited in detecting semantic shifts across a wider period of time. The current approach can be improved and this topic can be explored further.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "47_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_47_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7078333333333333,
      "max_similarity": 0.7177,
      "avg_coverage": 0.6721,
      "max_coverage": 0.7647
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 773,
      "avg_human_length": 334.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 11,
      "suggestions_count": 14
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "E9tXyCblLg",
        "similarity": 0.7035,
        "coverage": 0.6429,
        "human_length": 292,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a method for detecting semantic or usage differences and a method for extracting their typical instances in context based on the variance of contextualised word vectors. The approach should be effective in corpus pairs whose sizes are considerably different and also in infrequent words. The authors propose two methods: one for detecting words that have semantic differences in two corpora and one for extracting the representative instances.\n\nreasons_to_accept: The approach might be useful to compare diachronic corpora and detect periods of creation for some document collections. It could also be applied to detect texts developed by non-native speakers and even discriminate between texts developed by speakers with different mother tongues.\n\nreasons_to_reject: It goes without saying that vectors rely on specific texts, and that some degree of similarity between collections of texts can be measured. The authors stated a few limitations on the suggested approach themselves: the assumption that the form of a word is constant as its meaning(s) change. \nAlthough two-dimensional vectors are specified, it is unclear if this is always the case. Words that are formally identical but have different meanings ought to be connected to various vectors.\n\nmissing_references: https://aclanthology.org/P19-1321/ https://paperswithcode.com/paper/learn-interpretable-word-embeddings\n\nethical_concerns: Yes\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "thl9fnAVLq",
        "similarity": 0.7023,
        "coverage": 0.7647,
        "human_length": 312,
        "human_text": "paper_topic_and_main_contributions: The authors present a new method for checking whether the same words occurring in two different corpora have the same or different meanings. The method is relatively simple because it uses pre-trained contextual word vectors and is based on counting the norm of the mean vectors. The intuition is that if a word has more meanings (more boundary meanings), the mean vector is shorter. A suitable coefficient for comparing norms in two corpora was defined and called \"coverage\".  An additional task that was solved was to identify a representative occurrence of a word for a meaning that does not occur in the other corpus.  The solution is based on counting the cosine similarity between a vector representing a particular occurrence of a word and a vector representing the difference between the weighted mean norms. Experiments were conducted on two pairs of corpora: English 1800s/2000s (COHA) and native and non-native speakers of English (ICNALE).  SMEval-2020 Task 1 data (from COHA) was used for quantitative evaluation.  The results obtained were very good, at the level obtained by much more computationally demanding methods.\n\nreasons_to_accept: The proposed method is relatively simple, but very effective. The authors presented the mathematical basis of the proposed measures. An extended qualitative analysis was conducted with some interesting observations about the English language itself.\n\nreasons_to_reject: I do not see any\n\nquestions_for_the_authors: A: In a sketchy algorithm given in section 2.2 you write that the xs-xt should be counted. But the definition of representativeness  contains weights here.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "ye6gVGuSqN",
        "similarity": 0.7177,
        "coverage": 0.6087,
        "human_length": 400,
        "human_text": "paper_topic_and_main_contributions: This paper proposes methods for measuring semantic differences in words between two corpora, contributing to computationally-aided linguistic analysis.  Using a pre-trained large language model, e.g., BERT, new methods are used to examine the coverage of the meanings of words, through the norm of the mean word vectors across meanings. The proposed methods are easy to implement and without pre-training any language models, and do not require alignments between words or corpora for comparison. The proposed methods excel in semantic difference detection tasks compared to previous works, reveal potential semantic shift and difference in POS using diachronic datasets. Furthermore, the differences between native and non-native English usages are revealed using the proposed methods, in terms of different aspects, which can be beneficial for further investigation in second language acquisition.  The paper also discusses the limitations of the proposed methods, specifically the presumption of using von Mises-Fisher distribution and also other limitations for applications.\n\nreasons_to_accept: 1. The proposed methods present a straightforward and effective approach for detecting semantic differences and the mathematical background is illustrated in certain detail. \n2. Use-cases are given with both quantitative and qualitative analyses. \n3. The codes are provided and the work is reproducible. \n4. The work can be beneficial in research in other disciplines, such as second language acquisition.\n\nreasons_to_reject: 1. This work has only done experiments and reported results in English corpora, it would be very interesting to see some analysis in different languages or across languages, especially in the non-native English users with their mother tongue languages, and explore further potential in using variance metrics in second language acquisition research. \n2. As mentioned in the limitation section, that the usage of a large language model to obtain word vectors implicitly assumes that it models the target language as well. The methods rely on a monolingual language model trained on certain time period of data can be limited in detecting semantic shifts across a wider period of time. The current approach can be improved and this topic can be explored further.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "E9tXyCblLg",
        "length": 292,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a method for detecting semantic or usage differences and a method for extracting their typical instances in context based on the variance of contextualised word vectors. The approach should be effective in corpus pairs whose sizes are considerably different and also in infrequent words. The authors propose two methods: one for detecting words that have semantic differences in two corpora and one for extracting the representative instances.\n\nreasons_to_accept: The approach might be useful to compare diachronic corpora and detect periods of creation for some document collections. It could also be applied to detect texts developed by non-native speakers and even discriminate between texts developed by speakers with different mother tongues.\n\nreasons_to_reject: It goes without saying that vectors rely on specific texts, and that some degree of similarity between collections of texts can be measured. The authors stated a few limitations on the suggested approach themselves: the assumption that the form of a word is constant as its meaning(s) change. \nAlthough two-dimensional vectors are specified, it is unclear if this is always the case. Words that are formally identical but have different meanings ought to be connected to various vectors.\n\nmissing_references: https://aclanthology.org/P19-1321/ https://paperswithcode.com/paper/learn-interpretable-word-embeddings\n\nethical_concerns: Yes\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "thl9fnAVLq",
        "length": 312,
        "human_text": "paper_topic_and_main_contributions: The authors present a new method for checking whether the same words occurring in two different corpora have the same or different meanings. The method is relatively simple because it uses pre-trained contextual word vectors and is based on counting the norm of the mean vectors. The intuition is that if a word has more meanings (more boundary meanings), the mean vector is shorter. A suitable coefficient for comparing norms in two corpora was defined and called \"coverage\".  An additional task that was solved was to identify a representative occurrence of a word for a meaning that does not occur in the other corpus.  The solution is based on counting the cosine similarity between a vector representing a particular occurrence of a word and a vector representing the difference between the weighted mean norms. Experiments were conducted on two pairs of corpora: English 1800s/2000s (COHA) and native and non-native speakers of English (ICNALE).  SMEval-2020 Task 1 data (from COHA) was used for quantitative evaluation.  The results obtained were very good, at the level obtained by much more computationally demanding methods.\n\nreasons_to_accept: The proposed method is relatively simple, but very effective. The authors presented the mathematical basis of the proposed measures. An extended qualitative analysis was conducted with some interesting observations about the English language itself.\n\nreasons_to_reject: I do not see any\n\nquestions_for_the_authors: A: In a sketchy algorithm given in section 2.2 you write that the xs-xt should be counted. But the definition of representativeness  contains weights here.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ye6gVGuSqN",
        "length": 400,
        "human_text": "paper_topic_and_main_contributions: This paper proposes methods for measuring semantic differences in words between two corpora, contributing to computationally-aided linguistic analysis.  Using a pre-trained large language model, e.g., BERT, new methods are used to examine the coverage of the meanings of words, through the norm of the mean word vectors across meanings. The proposed methods are easy to implement and without pre-training any language models, and do not require alignments between words or corpora for comparison. The proposed methods excel in semantic difference detection tasks compared to previous works, reveal potential semantic shift and difference in POS using diachronic datasets. Furthermore, the differences between native and non-native English usages are revealed using the proposed methods, in terms of different aspects, which can be beneficial for further investigation in second language acquisition.  The paper also discusses the limitations of the proposed methods, specifically the presumption of using von Mises-Fisher distribution and also other limitations for applications.\n\nreasons_to_accept: 1. The proposed methods present a straightforward and effective approach for detecting semantic differences and the mathematical background is illustrated in certain detail. \n2. Use-cases are given with both quantitative and qualitative analyses. \n3. The codes are provided and the work is reproducible. \n4. The work can be beneficial in research in other disciplines, such as second language acquisition.\n\nreasons_to_reject: 1. This work has only done experiments and reported results in English corpora, it would be very interesting to see some analysis in different languages or across languages, especially in the non-native English users with their mother tongue languages, and explore further potential in using variance metrics in second language acquisition research. \n2. As mentioned in the limitation section, that the usage of a large language model to obtain word vectors implicitly assumes that it models the target language as well. The methods rely on a monolingual language model trained on certain time period of data can be limited in detecting semantic shifts across a wider period of time. The current approach can be improved and this topic can be explored further.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "129_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_129_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.751825,
      "max_similarity": 0.7693,
      "avg_coverage": 0.4625,
      "max_coverage": 0.5455
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 548,
      "avg_human_length": 429.75
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "qmu1nK9e8C",
        "similarity": 0.7375,
        "coverage": 0.5455,
        "human_length": 380,
        "human_text": "paper_topic_and_main_contributions: This paper tackles an important issue in crowdsourced data, which is the handling of disagreements. They suggest that representing annotators and data instances as embeddings helps models learn better without additional computing resources.\n\nreasons_to_accept: The idea of representing annotators and annotations with embeddings is interesting and could potentially be used in other tasks as well.\n\nreasons_to_reject: It's not entirely clear to me what is being done here. Perhaps more organization and a diagram of the experimental setup would help.  If my understanding is correct, I'm also not convinced that the way the annotator embeddings were created is appropriate. Unless we are dealing with a very dense matrix, embeddings based on what the annotator has labeled most likely are more influenced by what the annotator has labeled and not necessarily by the annotator's preferences. For example, if an annotator is only given negative sentences to label in a sentiment analysis task, it doesn't make sense to say that that annotator has a tendency to label things as negative. ( You mention this issue in the limitations section and argue that it isn't the main focus of the paper, but I argue that how you deal with missing information is crucial in your work if you're proposing a new method)\n\nquestions_for_the_authors: A. How dependent is your method on what instances the annotators were assigned to label? If we were to recreate the dataset but switch up who annotates what, would you still get similar embeddings and results?\nB. Besides quality, what requirements did you have for your dataset?\nC. What are you trying to tell us through the TSNE plots?\n\ntypos_grammar_style_and_presentation_improvements: Make sure all the figures are referred to in the paper. For example, for Figure 3, you refer to Figure 3a, but never mention Figure 3b. In that case, do you really need that plot?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 1: Poor: I cannot identify the contributions of this paper, or I believe the claims are not sufficiently backed up by evidence. I would fight to have it rejected.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "CTjNKKxOT0",
        "similarity": 0.7598,
        "coverage": 0.4583,
        "human_length": 408,
        "human_text": "paper_topic_and_main_contributions: The authors discuss about improving new methods for modeling annotators. They have introduced those methods by adding in an extra set of layers for embedding the annotators and their annotations. The authors have field tested their methods with publicly available datasets in the domain of modeling annotations. The authors have also evaluated the methods across different methods/metrics which is useful for understanding the limits of their work.\n\nreasons_to_accept: 1. The paper introduces an important direction in the domain of modeling annotators. \n2. The authors have extensively stress tested their methods across different datasets and different methods (more in the long appendix).\n\nreasons_to_reject: The reasons to reject are in the questions and improvements. Open to hear feedback from the authors on these aspects.\n\nquestions_for_the_authors: A The method Dawid & Skyene is not considered as a baseline. Any specific reason as to why? \nB The datasets SBIC/Toxic Ratings (Kumar et al.) is also not included for experiments. They consist of large amounts of data points with annotations and annotator information that can be essential. \nC The datasets as a whole other than GOE is relatively small. Does this impact the generalization of your work?\n\nmissing_references: A baseline -> https://www.jstor.org/stable/2346806 Dataset -> https://kumarde.com/papers/designing.pdf\n\ntypos_grammar_style_and_presentation_improvements: This paper is 22 pages long with all the appendix and extra results but the main 8 pages are missing key elements that is crucial for an EMNLP submission. The language aspect of it, the authors have done a good job of having lot of tables and numbers on the readers but they should think about how to include the empirical results on how their methods perform better (or not) than other baselines. The #s on tables is one thing, but a challenge in this domain is that numbers are not the full story, and individual data points do tell a different interesting story.  For sharing/uploading code for papers https://anonymous.4open.science/\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "wyjsvIcsM8",
        "similarity": 0.7407,
        "coverage": 0.5385,
        "human_length": 373,
        "human_text": "paper_topic_and_main_contributions: The paper reports on experiments focusing on the integration of annotator characteristics in modeling some subjective NLP tasks through creating annotator and annotation embeddings which are incorporated into the model. The evaluation on six NLP datasets revealed that the inclusion thereof leads to significant performance gains when learning from annotation disagreements.\n\nreasons_to_accept: - the paper is clearly written and provides sufficient level of detail to understand the carried-out research - a novel method of integration of annotator idiosyncracies into the modelling task is presented - experiments are thorough in terms of the number of models and datasets explored, which make the findings relatively generic - a very thorough interpretation of the results and findings is provided\n\nreasons_to_reject: - the paper is somewhat wrongly balanced, i.e. the annex is longer than the main body of the paper and includes some relevant results, e.g., the results of the experiments on the \"few annotators\" datasets, which would better fit to be placed in the main body of the article\n\nquestions_for_the_authors: - why did you put the results and the findings for the \"few annotators\" datasets in the annex?\n- in 42-44 you mention \"under-represented\" groups whose opinions may not agree with the majority. I am very puzzled by this wording since a given annotator can be in one case in the majority, and in another case in the minority. So, what does this concept of \"under-represented\" group mean here? Wouldn't  it make sense to speak more of subjective tasks and opinions that might emerge from many factors?\n\ntypos_grammar_style_and_presentation_improvements: - the datasets are referenced sometimes with full names, sometimes with the acronyms introduced earlier. Therefore, it would be better to use a consistent naming convention - Table 4 and Table 9 are partially redundant - maybe enumerating the main contributions in the introduction would improve the presentation\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "oanWLPgRmU",
        "similarity": 0.7693,
        "coverage": 0.3077,
        "human_length": 558,
        "human_text": "paper_topic_and_main_contributions: While the general practice is to force annotators\u2019 agreement through aggregation strategies (e.g., majority voting), many tasks are legitimately subjective, so the disagreement among annotators can be due to their different points of view rather than, e.g. noise or bad guidelines.  This paper proposes a method to leverage and study annotators\u2019 disagreement by embedding annotators and their annotations.  Experiments are performed on a large set of diverse datasets, and the results are analyzed from a wide variety of points of view.\n\nreasons_to_accept: The paper addresses an interesting and, in my opinion, very important problem clearly.  It has two main focuses: on the one hand, it shows that performance improves when adding annotators and annotations embedding to a simple transformer model (interestingly, the method increases the number of parameters by 1% only).  On the other hand, the paper performs a deep analysis of what the embeddings convey, their contributions to the performance, and many other interesting aspects.  Many different datasets are explored (different tasks, number of annotators, number of instances, etc). References to previous work are clear and frame the problem well.\n\nreasons_to_reject: In my opinion, the paper's contribution comes more from the performed analysis than from the increase in performance per se.  This is because the paper assumes a closed set of annotators for which classification will be performed. This is far from most practical scenarios, where models are trained once and then used by different target users.\n\nquestions_for_the_authors: Question A: How are unknown annotators (e.g., last paragraph of section 8) modeled? Is a zeroed embedding vector produced? Is there an embedding for all unknown annotators? \n  Question B: In section 3, the problem is framed as that of maximizing the annotator-specific correct label. Am I correct that this is how the models are evaluated in Section 6?  Question C: While I understand the paper's focus is on the analyses rather than on improving performance per se (a focus that I appreciate), I think the experiment considering unknown annotators should be given more relevance. This is because the paper still emphasizes \u201cpractical\u201d aspects, e.g., performance improvement and model efficiency. While it is nice to show that the performance improves when knowing the specific annotator\u2019s ID, in the majority of practical use cases, the set of annotators of the training set and those of the model target \u201cusers\u201d are disjoint; however, it might be possible to collect the user\u2019s opinion on a subset of instances, similarly to a cold start scenario in a recommender system. I would give more relevance to this experiment, for which results are currently relegated to the Appendix. I would also be interested in knowing the relationship between the number of available annotations for unknown annotators and the performance.\n\ntypos_grammar_style_and_presentation_improvements: I think that adding a picture depicting the architecture and how the embedding and the matrices described in section 4 fit in it would make the understanding of the sections much more intuitive.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "qmu1nK9e8C",
        "length": 380,
        "human_text": "paper_topic_and_main_contributions: This paper tackles an important issue in crowdsourced data, which is the handling of disagreements. They suggest that representing annotators and data instances as embeddings helps models learn better without additional computing resources.\n\nreasons_to_accept: The idea of representing annotators and annotations with embeddings is interesting and could potentially be used in other tasks as well.\n\nreasons_to_reject: It's not entirely clear to me what is being done here. Perhaps more organization and a diagram of the experimental setup would help.  If my understanding is correct, I'm also not convinced that the way the annotator embeddings were created is appropriate. Unless we are dealing with a very dense matrix, embeddings based on what the annotator has labeled most likely are more influenced by what the annotator has labeled and not necessarily by the annotator's preferences. For example, if an annotator is only given negative sentences to label in a sentiment analysis task, it doesn't make sense to say that that annotator has a tendency to label things as negative. ( You mention this issue in the limitations section and argue that it isn't the main focus of the paper, but I argue that how you deal with missing information is crucial in your work if you're proposing a new method)\n\nquestions_for_the_authors: A. How dependent is your method on what instances the annotators were assigned to label? If we were to recreate the dataset but switch up who annotates what, would you still get similar embeddings and results?\nB. Besides quality, what requirements did you have for your dataset?\nC. What are you trying to tell us through the TSNE plots?\n\ntypos_grammar_style_and_presentation_improvements: Make sure all the figures are referred to in the paper. For example, for Figure 3, you refer to Figure 3a, but never mention Figure 3b. In that case, do you really need that plot?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 1: Poor: I cannot identify the contributions of this paper, or I believe the claims are not sufficiently backed up by evidence. I would fight to have it rejected.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "CTjNKKxOT0",
        "length": 408,
        "human_text": "paper_topic_and_main_contributions: The authors discuss about improving new methods for modeling annotators. They have introduced those methods by adding in an extra set of layers for embedding the annotators and their annotations. The authors have field tested their methods with publicly available datasets in the domain of modeling annotations. The authors have also evaluated the methods across different methods/metrics which is useful for understanding the limits of their work.\n\nreasons_to_accept: 1. The paper introduces an important direction in the domain of modeling annotators. \n2. The authors have extensively stress tested their methods across different datasets and different methods (more in the long appendix).\n\nreasons_to_reject: The reasons to reject are in the questions and improvements. Open to hear feedback from the authors on these aspects.\n\nquestions_for_the_authors: A The method Dawid & Skyene is not considered as a baseline. Any specific reason as to why? \nB The datasets SBIC/Toxic Ratings (Kumar et al.) is also not included for experiments. They consist of large amounts of data points with annotations and annotator information that can be essential. \nC The datasets as a whole other than GOE is relatively small. Does this impact the generalization of your work?\n\nmissing_references: A baseline -> https://www.jstor.org/stable/2346806 Dataset -> https://kumarde.com/papers/designing.pdf\n\ntypos_grammar_style_and_presentation_improvements: This paper is 22 pages long with all the appendix and extra results but the main 8 pages are missing key elements that is crucial for an EMNLP submission. The language aspect of it, the authors have done a good job of having lot of tables and numbers on the readers but they should think about how to include the empirical results on how their methods perform better (or not) than other baselines. The #s on tables is one thing, but a challenge in this domain is that numbers are not the full story, and individual data points do tell a different interesting story.  For sharing/uploading code for papers https://anonymous.4open.science/\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "wyjsvIcsM8",
        "length": 373,
        "human_text": "paper_topic_and_main_contributions: The paper reports on experiments focusing on the integration of annotator characteristics in modeling some subjective NLP tasks through creating annotator and annotation embeddings which are incorporated into the model. The evaluation on six NLP datasets revealed that the inclusion thereof leads to significant performance gains when learning from annotation disagreements.\n\nreasons_to_accept: - the paper is clearly written and provides sufficient level of detail to understand the carried-out research - a novel method of integration of annotator idiosyncracies into the modelling task is presented - experiments are thorough in terms of the number of models and datasets explored, which make the findings relatively generic - a very thorough interpretation of the results and findings is provided\n\nreasons_to_reject: - the paper is somewhat wrongly balanced, i.e. the annex is longer than the main body of the paper and includes some relevant results, e.g., the results of the experiments on the \"few annotators\" datasets, which would better fit to be placed in the main body of the article\n\nquestions_for_the_authors: - why did you put the results and the findings for the \"few annotators\" datasets in the annex?\n- in 42-44 you mention \"under-represented\" groups whose opinions may not agree with the majority. I am very puzzled by this wording since a given annotator can be in one case in the majority, and in another case in the minority. So, what does this concept of \"under-represented\" group mean here? Wouldn't  it make sense to speak more of subjective tasks and opinions that might emerge from many factors?\n\ntypos_grammar_style_and_presentation_improvements: - the datasets are referenced sometimes with full names, sometimes with the acronyms introduced earlier. Therefore, it would be better to use a consistent naming convention - Table 4 and Table 9 are partially redundant - maybe enumerating the main contributions in the introduction would improve the presentation\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "oanWLPgRmU",
        "length": 558,
        "human_text": "paper_topic_and_main_contributions: While the general practice is to force annotators\u2019 agreement through aggregation strategies (e.g., majority voting), many tasks are legitimately subjective, so the disagreement among annotators can be due to their different points of view rather than, e.g. noise or bad guidelines.  This paper proposes a method to leverage and study annotators\u2019 disagreement by embedding annotators and their annotations.  Experiments are performed on a large set of diverse datasets, and the results are analyzed from a wide variety of points of view.\n\nreasons_to_accept: The paper addresses an interesting and, in my opinion, very important problem clearly.  It has two main focuses: on the one hand, it shows that performance improves when adding annotators and annotations embedding to a simple transformer model (interestingly, the method increases the number of parameters by 1% only).  On the other hand, the paper performs a deep analysis of what the embeddings convey, their contributions to the performance, and many other interesting aspects.  Many different datasets are explored (different tasks, number of annotators, number of instances, etc). References to previous work are clear and frame the problem well.\n\nreasons_to_reject: In my opinion, the paper's contribution comes more from the performed analysis than from the increase in performance per se.  This is because the paper assumes a closed set of annotators for which classification will be performed. This is far from most practical scenarios, where models are trained once and then used by different target users.\n\nquestions_for_the_authors: Question A: How are unknown annotators (e.g., last paragraph of section 8) modeled? Is a zeroed embedding vector produced? Is there an embedding for all unknown annotators? \n  Question B: In section 3, the problem is framed as that of maximizing the annotator-specific correct label. Am I correct that this is how the models are evaluated in Section 6?  Question C: While I understand the paper's focus is on the analyses rather than on improving performance per se (a focus that I appreciate), I think the experiment considering unknown annotators should be given more relevance. This is because the paper still emphasizes \u201cpractical\u201d aspects, e.g., performance improvement and model efficiency. While it is nice to show that the performance improves when knowing the specific annotator\u2019s ID, in the majority of practical use cases, the set of annotators of the training set and those of the model target \u201cusers\u201d are disjoint; however, it might be possible to collect the user\u2019s opinion on a subset of instances, similarly to a cold start scenario in a recommender system. I would give more relevance to this experiment, for which results are currently relegated to the Appendix. I would also be interested in knowing the relationship between the number of available annotations for unknown annotators and the performance.\n\ntypos_grammar_style_and_presentation_improvements: I think that adding a picture depicting the architecture and how the embedding and the matrices described in section 4 fit in it would make the understanding of the sections much more intuitive.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "129_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_129_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.75005,
      "max_similarity": 0.7649,
      "avg_coverage": 0.473075,
      "max_coverage": 0.5909
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 531,
      "avg_human_length": 429.75
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "qmu1nK9e8C",
        "similarity": 0.7399,
        "coverage": 0.5909,
        "human_length": 380,
        "human_text": "paper_topic_and_main_contributions: This paper tackles an important issue in crowdsourced data, which is the handling of disagreements. They suggest that representing annotators and data instances as embeddings helps models learn better without additional computing resources.\n\nreasons_to_accept: The idea of representing annotators and annotations with embeddings is interesting and could potentially be used in other tasks as well.\n\nreasons_to_reject: It's not entirely clear to me what is being done here. Perhaps more organization and a diagram of the experimental setup would help.  If my understanding is correct, I'm also not convinced that the way the annotator embeddings were created is appropriate. Unless we are dealing with a very dense matrix, embeddings based on what the annotator has labeled most likely are more influenced by what the annotator has labeled and not necessarily by the annotator's preferences. For example, if an annotator is only given negative sentences to label in a sentiment analysis task, it doesn't make sense to say that that annotator has a tendency to label things as negative. ( You mention this issue in the limitations section and argue that it isn't the main focus of the paper, but I argue that how you deal with missing information is crucial in your work if you're proposing a new method)\n\nquestions_for_the_authors: A. How dependent is your method on what instances the annotators were assigned to label? If we were to recreate the dataset but switch up who annotates what, would you still get similar embeddings and results?\nB. Besides quality, what requirements did you have for your dataset?\nC. What are you trying to tell us through the TSNE plots?\n\ntypos_grammar_style_and_presentation_improvements: Make sure all the figures are referred to in the paper. For example, for Figure 3, you refer to Figure 3a, but never mention Figure 3b. In that case, do you really need that plot?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 1: Poor: I cannot identify the contributions of this paper, or I believe the claims are not sufficiently backed up by evidence. I would fight to have it rejected.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "CTjNKKxOT0",
        "similarity": 0.7567,
        "coverage": 0.4167,
        "human_length": 408,
        "human_text": "paper_topic_and_main_contributions: The authors discuss about improving new methods for modeling annotators. They have introduced those methods by adding in an extra set of layers for embedding the annotators and their annotations. The authors have field tested their methods with publicly available datasets in the domain of modeling annotations. The authors have also evaluated the methods across different methods/metrics which is useful for understanding the limits of their work.\n\nreasons_to_accept: 1. The paper introduces an important direction in the domain of modeling annotators. \n2. The authors have extensively stress tested their methods across different datasets and different methods (more in the long appendix).\n\nreasons_to_reject: The reasons to reject are in the questions and improvements. Open to hear feedback from the authors on these aspects.\n\nquestions_for_the_authors: A The method Dawid & Skyene is not considered as a baseline. Any specific reason as to why? \nB The datasets SBIC/Toxic Ratings (Kumar et al.) is also not included for experiments. They consist of large amounts of data points with annotations and annotator information that can be essential. \nC The datasets as a whole other than GOE is relatively small. Does this impact the generalization of your work?\n\nmissing_references: A baseline -> https://www.jstor.org/stable/2346806 Dataset -> https://kumarde.com/papers/designing.pdf\n\ntypos_grammar_style_and_presentation_improvements: This paper is 22 pages long with all the appendix and extra results but the main 8 pages are missing key elements that is crucial for an EMNLP submission. The language aspect of it, the authors have done a good job of having lot of tables and numbers on the readers but they should think about how to include the empirical results on how their methods perform better (or not) than other baselines. The #s on tables is one thing, but a challenge in this domain is that numbers are not the full story, and individual data points do tell a different interesting story.  For sharing/uploading code for papers https://anonymous.4open.science/\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "wyjsvIcsM8",
        "similarity": 0.7387,
        "coverage": 0.5385,
        "human_length": 373,
        "human_text": "paper_topic_and_main_contributions: The paper reports on experiments focusing on the integration of annotator characteristics in modeling some subjective NLP tasks through creating annotator and annotation embeddings which are incorporated into the model. The evaluation on six NLP datasets revealed that the inclusion thereof leads to significant performance gains when learning from annotation disagreements.\n\nreasons_to_accept: - the paper is clearly written and provides sufficient level of detail to understand the carried-out research - a novel method of integration of annotator idiosyncracies into the modelling task is presented - experiments are thorough in terms of the number of models and datasets explored, which make the findings relatively generic - a very thorough interpretation of the results and findings is provided\n\nreasons_to_reject: - the paper is somewhat wrongly balanced, i.e. the annex is longer than the main body of the paper and includes some relevant results, e.g., the results of the experiments on the \"few annotators\" datasets, which would better fit to be placed in the main body of the article\n\nquestions_for_the_authors: - why did you put the results and the findings for the \"few annotators\" datasets in the annex?\n- in 42-44 you mention \"under-represented\" groups whose opinions may not agree with the majority. I am very puzzled by this wording since a given annotator can be in one case in the majority, and in another case in the minority. So, what does this concept of \"under-represented\" group mean here? Wouldn't  it make sense to speak more of subjective tasks and opinions that might emerge from many factors?\n\ntypos_grammar_style_and_presentation_improvements: - the datasets are referenced sometimes with full names, sometimes with the acronyms introduced earlier. Therefore, it would be better to use a consistent naming convention - Table 4 and Table 9 are partially redundant - maybe enumerating the main contributions in the introduction would improve the presentation\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "oanWLPgRmU",
        "similarity": 0.7649,
        "coverage": 0.3462,
        "human_length": 558,
        "human_text": "paper_topic_and_main_contributions: While the general practice is to force annotators\u2019 agreement through aggregation strategies (e.g., majority voting), many tasks are legitimately subjective, so the disagreement among annotators can be due to their different points of view rather than, e.g. noise or bad guidelines.  This paper proposes a method to leverage and study annotators\u2019 disagreement by embedding annotators and their annotations.  Experiments are performed on a large set of diverse datasets, and the results are analyzed from a wide variety of points of view.\n\nreasons_to_accept: The paper addresses an interesting and, in my opinion, very important problem clearly.  It has two main focuses: on the one hand, it shows that performance improves when adding annotators and annotations embedding to a simple transformer model (interestingly, the method increases the number of parameters by 1% only).  On the other hand, the paper performs a deep analysis of what the embeddings convey, their contributions to the performance, and many other interesting aspects.  Many different datasets are explored (different tasks, number of annotators, number of instances, etc). References to previous work are clear and frame the problem well.\n\nreasons_to_reject: In my opinion, the paper's contribution comes more from the performed analysis than from the increase in performance per se.  This is because the paper assumes a closed set of annotators for which classification will be performed. This is far from most practical scenarios, where models are trained once and then used by different target users.\n\nquestions_for_the_authors: Question A: How are unknown annotators (e.g., last paragraph of section 8) modeled? Is a zeroed embedding vector produced? Is there an embedding for all unknown annotators? \n  Question B: In section 3, the problem is framed as that of maximizing the annotator-specific correct label. Am I correct that this is how the models are evaluated in Section 6?  Question C: While I understand the paper's focus is on the analyses rather than on improving performance per se (a focus that I appreciate), I think the experiment considering unknown annotators should be given more relevance. This is because the paper still emphasizes \u201cpractical\u201d aspects, e.g., performance improvement and model efficiency. While it is nice to show that the performance improves when knowing the specific annotator\u2019s ID, in the majority of practical use cases, the set of annotators of the training set and those of the model target \u201cusers\u201d are disjoint; however, it might be possible to collect the user\u2019s opinion on a subset of instances, similarly to a cold start scenario in a recommender system. I would give more relevance to this experiment, for which results are currently relegated to the Appendix. I would also be interested in knowing the relationship between the number of available annotations for unknown annotators and the performance.\n\ntypos_grammar_style_and_presentation_improvements: I think that adding a picture depicting the architecture and how the embedding and the matrices described in section 4 fit in it would make the understanding of the sections much more intuitive.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "qmu1nK9e8C",
        "length": 380,
        "human_text": "paper_topic_and_main_contributions: This paper tackles an important issue in crowdsourced data, which is the handling of disagreements. They suggest that representing annotators and data instances as embeddings helps models learn better without additional computing resources.\n\nreasons_to_accept: The idea of representing annotators and annotations with embeddings is interesting and could potentially be used in other tasks as well.\n\nreasons_to_reject: It's not entirely clear to me what is being done here. Perhaps more organization and a diagram of the experimental setup would help.  If my understanding is correct, I'm also not convinced that the way the annotator embeddings were created is appropriate. Unless we are dealing with a very dense matrix, embeddings based on what the annotator has labeled most likely are more influenced by what the annotator has labeled and not necessarily by the annotator's preferences. For example, if an annotator is only given negative sentences to label in a sentiment analysis task, it doesn't make sense to say that that annotator has a tendency to label things as negative. ( You mention this issue in the limitations section and argue that it isn't the main focus of the paper, but I argue that how you deal with missing information is crucial in your work if you're proposing a new method)\n\nquestions_for_the_authors: A. How dependent is your method on what instances the annotators were assigned to label? If we were to recreate the dataset but switch up who annotates what, would you still get similar embeddings and results?\nB. Besides quality, what requirements did you have for your dataset?\nC. What are you trying to tell us through the TSNE plots?\n\ntypos_grammar_style_and_presentation_improvements: Make sure all the figures are referred to in the paper. For example, for Figure 3, you refer to Figure 3a, but never mention Figure 3b. In that case, do you really need that plot?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 1: Poor: I cannot identify the contributions of this paper, or I believe the claims are not sufficiently backed up by evidence. I would fight to have it rejected.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "CTjNKKxOT0",
        "length": 408,
        "human_text": "paper_topic_and_main_contributions: The authors discuss about improving new methods for modeling annotators. They have introduced those methods by adding in an extra set of layers for embedding the annotators and their annotations. The authors have field tested their methods with publicly available datasets in the domain of modeling annotations. The authors have also evaluated the methods across different methods/metrics which is useful for understanding the limits of their work.\n\nreasons_to_accept: 1. The paper introduces an important direction in the domain of modeling annotators. \n2. The authors have extensively stress tested their methods across different datasets and different methods (more in the long appendix).\n\nreasons_to_reject: The reasons to reject are in the questions and improvements. Open to hear feedback from the authors on these aspects.\n\nquestions_for_the_authors: A The method Dawid & Skyene is not considered as a baseline. Any specific reason as to why? \nB The datasets SBIC/Toxic Ratings (Kumar et al.) is also not included for experiments. They consist of large amounts of data points with annotations and annotator information that can be essential. \nC The datasets as a whole other than GOE is relatively small. Does this impact the generalization of your work?\n\nmissing_references: A baseline -> https://www.jstor.org/stable/2346806 Dataset -> https://kumarde.com/papers/designing.pdf\n\ntypos_grammar_style_and_presentation_improvements: This paper is 22 pages long with all the appendix and extra results but the main 8 pages are missing key elements that is crucial for an EMNLP submission. The language aspect of it, the authors have done a good job of having lot of tables and numbers on the readers but they should think about how to include the empirical results on how their methods perform better (or not) than other baselines. The #s on tables is one thing, but a challenge in this domain is that numbers are not the full story, and individual data points do tell a different interesting story.  For sharing/uploading code for papers https://anonymous.4open.science/\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "wyjsvIcsM8",
        "length": 373,
        "human_text": "paper_topic_and_main_contributions: The paper reports on experiments focusing on the integration of annotator characteristics in modeling some subjective NLP tasks through creating annotator and annotation embeddings which are incorporated into the model. The evaluation on six NLP datasets revealed that the inclusion thereof leads to significant performance gains when learning from annotation disagreements.\n\nreasons_to_accept: - the paper is clearly written and provides sufficient level of detail to understand the carried-out research - a novel method of integration of annotator idiosyncracies into the modelling task is presented - experiments are thorough in terms of the number of models and datasets explored, which make the findings relatively generic - a very thorough interpretation of the results and findings is provided\n\nreasons_to_reject: - the paper is somewhat wrongly balanced, i.e. the annex is longer than the main body of the paper and includes some relevant results, e.g., the results of the experiments on the \"few annotators\" datasets, which would better fit to be placed in the main body of the article\n\nquestions_for_the_authors: - why did you put the results and the findings for the \"few annotators\" datasets in the annex?\n- in 42-44 you mention \"under-represented\" groups whose opinions may not agree with the majority. I am very puzzled by this wording since a given annotator can be in one case in the majority, and in another case in the minority. So, what does this concept of \"under-represented\" group mean here? Wouldn't  it make sense to speak more of subjective tasks and opinions that might emerge from many factors?\n\ntypos_grammar_style_and_presentation_improvements: - the datasets are referenced sometimes with full names, sometimes with the acronyms introduced earlier. Therefore, it would be better to use a consistent naming convention - Table 4 and Table 9 are partially redundant - maybe enumerating the main contributions in the introduction would improve the presentation\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "oanWLPgRmU",
        "length": 558,
        "human_text": "paper_topic_and_main_contributions: While the general practice is to force annotators\u2019 agreement through aggregation strategies (e.g., majority voting), many tasks are legitimately subjective, so the disagreement among annotators can be due to their different points of view rather than, e.g. noise or bad guidelines.  This paper proposes a method to leverage and study annotators\u2019 disagreement by embedding annotators and their annotations.  Experiments are performed on a large set of diverse datasets, and the results are analyzed from a wide variety of points of view.\n\nreasons_to_accept: The paper addresses an interesting and, in my opinion, very important problem clearly.  It has two main focuses: on the one hand, it shows that performance improves when adding annotators and annotations embedding to a simple transformer model (interestingly, the method increases the number of parameters by 1% only).  On the other hand, the paper performs a deep analysis of what the embeddings convey, their contributions to the performance, and many other interesting aspects.  Many different datasets are explored (different tasks, number of annotators, number of instances, etc). References to previous work are clear and frame the problem well.\n\nreasons_to_reject: In my opinion, the paper's contribution comes more from the performed analysis than from the increase in performance per se.  This is because the paper assumes a closed set of annotators for which classification will be performed. This is far from most practical scenarios, where models are trained once and then used by different target users.\n\nquestions_for_the_authors: Question A: How are unknown annotators (e.g., last paragraph of section 8) modeled? Is a zeroed embedding vector produced? Is there an embedding for all unknown annotators? \n  Question B: In section 3, the problem is framed as that of maximizing the annotator-specific correct label. Am I correct that this is how the models are evaluated in Section 6?  Question C: While I understand the paper's focus is on the analyses rather than on improving performance per se (a focus that I appreciate), I think the experiment considering unknown annotators should be given more relevance. This is because the paper still emphasizes \u201cpractical\u201d aspects, e.g., performance improvement and model efficiency. While it is nice to show that the performance improves when knowing the specific annotator\u2019s ID, in the majority of practical use cases, the set of annotators of the training set and those of the model target \u201cusers\u201d are disjoint; however, it might be possible to collect the user\u2019s opinion on a subset of instances, similarly to a cold start scenario in a recommender system. I would give more relevance to this experiment, for which results are currently relegated to the Appendix. I would also be interested in knowing the relationship between the number of available annotations for unknown annotators and the performance.\n\ntypos_grammar_style_and_presentation_improvements: I think that adding a picture depicting the architecture and how the embedding and the matrices described in section 4 fit in it would make the understanding of the sections much more intuitive.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "137_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_137_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7091,
      "max_similarity": 0.7233,
      "avg_coverage": 0.7652333333333333,
      "max_coverage": 1.0
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 457,
      "avg_human_length": 328.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 6,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "vhwXgW3KBC",
        "similarity": 0.7233,
        "coverage": 0.6,
        "human_length": 339,
        "human_text": "paper_topic_and_main_contributions: - What is this paper about? This paper aims to provide a new multilingual open-domain dialogue evaluation benchmark including datasets, evaluation methods, and current metrics analysis.\n- what contributions does it make? This paper collects 9 languages versions of multiple dialogue datasets with machine translation, analyzes current reference-free metric performance on the proposed datasets, and proposes new metrics.\n\nreasons_to_accept: - This paper constructs xDial-Eval which contains 12 turn-level and 6 dialogue-level open-source datasets. The original English version datasets are translated into nine different languages with commercial machine translation models.\n- This paper assesses current discriminative and generative reference-free metrics on the proposed multilingual benchmark. The most recent LLMs are also evaluated in this paper.\n- This paper introduces an ensemble metric for the proposed multilingual benchmark by combining generative and discrimination models.\n\nreasons_to_reject: - One of the concerns about this work is that the impact of the machine translation models on multiple parts. The datasets are translated and the synthetic datasets for fine-tuning are translated. If some sampled instances from the benchmark can be evaluated by human, it could be better.\n- Regarding the proposed metric which combines LLMs and FineD, one concern is the computational limitations of the open-source LLMs and the training of the BERT-style model, especially for 9 languages of 18 datasets.\n\nquestions_for_the_authors: 1. How long will it cost to run the whole evaluation of the proposed benchmark with the Ensemble metric such as LLama-7B on a typical GPU?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "c7HpWPCgLR",
        "similarity": 0.7113,
        "coverage": 0.6957,
        "human_length": 434,
        "human_text": "paper_topic_and_main_contributions: This work proposes xDial-Eval, an evaluation benchmark for multilingual open domain dialogue by machine translating 18 English open domain dialogue datasets into 8 languages. To demonstrate the usability of the dataset, several metrics BERT-based/generation-based LLMs are evaluated on the benchmark. The work also suggests ensembling metrics to be better than commercial LLM systems  The contribution is primarily a resource contribution followed by NLP engineering experiment.\n\nreasons_to_accept: Good choice of languages for the dataset.  Good number and types of LLMs used for comparison.  Open domain multilingual conversations have received less interest and this work is a contribution towards that direction\n\nreasons_to_reject: The proposed dataset is an aggregation of different dialogue datasets focussing on different properties of dialogue - yet the emphasis is more on \u201ccoherence\u201d and excludes other properties of dialgoue evaluation.\nFurther, the proposed dataset, despite being an evaluation benchmark for multilingual dialogue, uses automatic translation APIs and quality evaluation is done only on automatic metrics.\n\nquestions_for_the_authors: A. How are the human annotations for evaluation obtained? What are the scores from the metrics compared against to compute the correlation? \n(After the rebuttal, please also write this clearly in the experimentation section)\n\ntypos_grammar_style_and_presentation_improvements: A. It is disputed whether LLMs can be used for translation quality evaluation as zero-shot , further a lot of segment level evaluation metrics are not yet equipped to do quality evaluation(https://aclanthology.org/2023.acl-long.730/) . As it is a dataset, it is essential to have some human evaluation to assess the quality.   B. 025 to 031 and 574 to 582, correlation scale ranges from -1 to 1, please check if the improvement can be quantified in absolute percentages C. Please mention the languages in the abstract D. I am slightly skeptic about the results produced by fine-tuning. If similar translation methods are used for training and evaluation, it is likely that the model is getting biased towards synthetic data. I wonder if this is causing artificial gain (See https://arxiv.org/pdf/2201.13405.pdf, https://aclanthology.org/2020.emnlp-main.618/) A quick check on the DuRecDial2.0 dataset may be helpful https://aclanthology.org/2021.emnlp-main.356/\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "mi82DXOt1Q",
        "similarity": 0.6927,
        "coverage": 1.0,
        "human_length": 212,
        "human_text": "paper_topic_and_main_contributions: This paper establishes a benchmark for evaluating multi-lingual, multi-turn dialogues, creating a dataset that covers various languages to address the current lack of evaluation benchmarks for language. The study also establishes strong self-supervised and multilingual baselines.\n\nreasons_to_accept: This paper is well organized and clearly written.\n\nreasons_to_reject: This paper presents a benchmark for evaluating multilingual dialogues and creates a multi-round dialogue dataset in up to ten different languages, providing an excellent tool for assessing the multilingual ability of large models. However, in my opinion, the practicality and effectiveness of such evaluations in the field of multilingualism are not as useful and effective as other evaluation tools that examine the overall capabilities of large models. Therefore, the actual relevance of evaluating the multilingual ability of large models needs to be carefully considered, especially given that the multilingual ability of large language models is generally not poor nowadays.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "vhwXgW3KBC",
        "length": 339,
        "human_text": "paper_topic_and_main_contributions: - What is this paper about? This paper aims to provide a new multilingual open-domain dialogue evaluation benchmark including datasets, evaluation methods, and current metrics analysis.\n- what contributions does it make? This paper collects 9 languages versions of multiple dialogue datasets with machine translation, analyzes current reference-free metric performance on the proposed datasets, and proposes new metrics.\n\nreasons_to_accept: - This paper constructs xDial-Eval which contains 12 turn-level and 6 dialogue-level open-source datasets. The original English version datasets are translated into nine different languages with commercial machine translation models.\n- This paper assesses current discriminative and generative reference-free metrics on the proposed multilingual benchmark. The most recent LLMs are also evaluated in this paper.\n- This paper introduces an ensemble metric for the proposed multilingual benchmark by combining generative and discrimination models.\n\nreasons_to_reject: - One of the concerns about this work is that the impact of the machine translation models on multiple parts. The datasets are translated and the synthetic datasets for fine-tuning are translated. If some sampled instances from the benchmark can be evaluated by human, it could be better.\n- Regarding the proposed metric which combines LLMs and FineD, one concern is the computational limitations of the open-source LLMs and the training of the BERT-style model, especially for 9 languages of 18 datasets.\n\nquestions_for_the_authors: 1. How long will it cost to run the whole evaluation of the proposed benchmark with the Ensemble metric such as LLama-7B on a typical GPU?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "c7HpWPCgLR",
        "length": 434,
        "human_text": "paper_topic_and_main_contributions: This work proposes xDial-Eval, an evaluation benchmark for multilingual open domain dialogue by machine translating 18 English open domain dialogue datasets into 8 languages. To demonstrate the usability of the dataset, several metrics BERT-based/generation-based LLMs are evaluated on the benchmark. The work also suggests ensembling metrics to be better than commercial LLM systems  The contribution is primarily a resource contribution followed by NLP engineering experiment.\n\nreasons_to_accept: Good choice of languages for the dataset.  Good number and types of LLMs used for comparison.  Open domain multilingual conversations have received less interest and this work is a contribution towards that direction\n\nreasons_to_reject: The proposed dataset is an aggregation of different dialogue datasets focussing on different properties of dialogue - yet the emphasis is more on \u201ccoherence\u201d and excludes other properties of dialgoue evaluation.\nFurther, the proposed dataset, despite being an evaluation benchmark for multilingual dialogue, uses automatic translation APIs and quality evaluation is done only on automatic metrics.\n\nquestions_for_the_authors: A. How are the human annotations for evaluation obtained? What are the scores from the metrics compared against to compute the correlation? \n(After the rebuttal, please also write this clearly in the experimentation section)\n\ntypos_grammar_style_and_presentation_improvements: A. It is disputed whether LLMs can be used for translation quality evaluation as zero-shot , further a lot of segment level evaluation metrics are not yet equipped to do quality evaluation(https://aclanthology.org/2023.acl-long.730/) . As it is a dataset, it is essential to have some human evaluation to assess the quality.   B. 025 to 031 and 574 to 582, correlation scale ranges from -1 to 1, please check if the improvement can be quantified in absolute percentages C. Please mention the languages in the abstract D. I am slightly skeptic about the results produced by fine-tuning. If similar translation methods are used for training and evaluation, it is likely that the model is getting biased towards synthetic data. I wonder if this is causing artificial gain (See https://arxiv.org/pdf/2201.13405.pdf, https://aclanthology.org/2020.emnlp-main.618/) A quick check on the DuRecDial2.0 dataset may be helpful https://aclanthology.org/2021.emnlp-main.356/\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "mi82DXOt1Q",
        "length": 212,
        "human_text": "paper_topic_and_main_contributions: This paper establishes a benchmark for evaluating multi-lingual, multi-turn dialogues, creating a dataset that covers various languages to address the current lack of evaluation benchmarks for language. The study also establishes strong self-supervised and multilingual baselines.\n\nreasons_to_accept: This paper is well organized and clearly written.\n\nreasons_to_reject: This paper presents a benchmark for evaluating multilingual dialogues and creates a multi-round dialogue dataset in up to ten different languages, providing an excellent tool for assessing the multilingual ability of large models. However, in my opinion, the practicality and effectiveness of such evaluations in the field of multilingualism are not as useful and effective as other evaluation tools that examine the overall capabilities of large models. Therefore, the actual relevance of evaluating the multilingual ability of large models needs to be carefully considered, especially given that the multilingual ability of large language models is generally not poor nowadays.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "137_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_137_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7401333333333334,
      "max_similarity": 0.753,
      "avg_coverage": 0.6143333333333333,
      "max_coverage": 0.7778
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 343,
      "avg_human_length": 328.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 5,
      "suggestions_count": 1
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "vhwXgW3KBC",
        "similarity": 0.753,
        "coverage": 0.5,
        "human_length": 339,
        "human_text": "paper_topic_and_main_contributions: - What is this paper about? This paper aims to provide a new multilingual open-domain dialogue evaluation benchmark including datasets, evaluation methods, and current metrics analysis.\n- what contributions does it make? This paper collects 9 languages versions of multiple dialogue datasets with machine translation, analyzes current reference-free metric performance on the proposed datasets, and proposes new metrics.\n\nreasons_to_accept: - This paper constructs xDial-Eval which contains 12 turn-level and 6 dialogue-level open-source datasets. The original English version datasets are translated into nine different languages with commercial machine translation models.\n- This paper assesses current discriminative and generative reference-free metrics on the proposed multilingual benchmark. The most recent LLMs are also evaluated in this paper.\n- This paper introduces an ensemble metric for the proposed multilingual benchmark by combining generative and discrimination models.\n\nreasons_to_reject: - One of the concerns about this work is that the impact of the machine translation models on multiple parts. The datasets are translated and the synthetic datasets for fine-tuning are translated. If some sampled instances from the benchmark can be evaluated by human, it could be better.\n- Regarding the proposed metric which combines LLMs and FineD, one concern is the computational limitations of the open-source LLMs and the training of the BERT-style model, especially for 9 languages of 18 datasets.\n\nquestions_for_the_authors: 1. How long will it cost to run the whole evaluation of the proposed benchmark with the Ensemble metric such as LLama-7B on a typical GPU?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "c7HpWPCgLR",
        "similarity": 0.7407,
        "coverage": 0.5652,
        "human_length": 434,
        "human_text": "paper_topic_and_main_contributions: This work proposes xDial-Eval, an evaluation benchmark for multilingual open domain dialogue by machine translating 18 English open domain dialogue datasets into 8 languages. To demonstrate the usability of the dataset, several metrics BERT-based/generation-based LLMs are evaluated on the benchmark. The work also suggests ensembling metrics to be better than commercial LLM systems  The contribution is primarily a resource contribution followed by NLP engineering experiment.\n\nreasons_to_accept: Good choice of languages for the dataset.  Good number and types of LLMs used for comparison.  Open domain multilingual conversations have received less interest and this work is a contribution towards that direction\n\nreasons_to_reject: The proposed dataset is an aggregation of different dialogue datasets focussing on different properties of dialogue - yet the emphasis is more on \u201ccoherence\u201d and excludes other properties of dialgoue evaluation.\nFurther, the proposed dataset, despite being an evaluation benchmark for multilingual dialogue, uses automatic translation APIs and quality evaluation is done only on automatic metrics.\n\nquestions_for_the_authors: A. How are the human annotations for evaluation obtained? What are the scores from the metrics compared against to compute the correlation? \n(After the rebuttal, please also write this clearly in the experimentation section)\n\ntypos_grammar_style_and_presentation_improvements: A. It is disputed whether LLMs can be used for translation quality evaluation as zero-shot , further a lot of segment level evaluation metrics are not yet equipped to do quality evaluation(https://aclanthology.org/2023.acl-long.730/) . As it is a dataset, it is essential to have some human evaluation to assess the quality.   B. 025 to 031 and 574 to 582, correlation scale ranges from -1 to 1, please check if the improvement can be quantified in absolute percentages C. Please mention the languages in the abstract D. I am slightly skeptic about the results produced by fine-tuning. If similar translation methods are used for training and evaluation, it is likely that the model is getting biased towards synthetic data. I wonder if this is causing artificial gain (See https://arxiv.org/pdf/2201.13405.pdf, https://aclanthology.org/2020.emnlp-main.618/) A quick check on the DuRecDial2.0 dataset may be helpful https://aclanthology.org/2021.emnlp-main.356/\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "mi82DXOt1Q",
        "similarity": 0.7267,
        "coverage": 0.7778,
        "human_length": 212,
        "human_text": "paper_topic_and_main_contributions: This paper establishes a benchmark for evaluating multi-lingual, multi-turn dialogues, creating a dataset that covers various languages to address the current lack of evaluation benchmarks for language. The study also establishes strong self-supervised and multilingual baselines.\n\nreasons_to_accept: This paper is well organized and clearly written.\n\nreasons_to_reject: This paper presents a benchmark for evaluating multilingual dialogues and creates a multi-round dialogue dataset in up to ten different languages, providing an excellent tool for assessing the multilingual ability of large models. However, in my opinion, the practicality and effectiveness of such evaluations in the field of multilingualism are not as useful and effective as other evaluation tools that examine the overall capabilities of large models. Therefore, the actual relevance of evaluating the multilingual ability of large models needs to be carefully considered, especially given that the multilingual ability of large language models is generally not poor nowadays.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "vhwXgW3KBC",
        "length": 339,
        "human_text": "paper_topic_and_main_contributions: - What is this paper about? This paper aims to provide a new multilingual open-domain dialogue evaluation benchmark including datasets, evaluation methods, and current metrics analysis.\n- what contributions does it make? This paper collects 9 languages versions of multiple dialogue datasets with machine translation, analyzes current reference-free metric performance on the proposed datasets, and proposes new metrics.\n\nreasons_to_accept: - This paper constructs xDial-Eval which contains 12 turn-level and 6 dialogue-level open-source datasets. The original English version datasets are translated into nine different languages with commercial machine translation models.\n- This paper assesses current discriminative and generative reference-free metrics on the proposed multilingual benchmark. The most recent LLMs are also evaluated in this paper.\n- This paper introduces an ensemble metric for the proposed multilingual benchmark by combining generative and discrimination models.\n\nreasons_to_reject: - One of the concerns about this work is that the impact of the machine translation models on multiple parts. The datasets are translated and the synthetic datasets for fine-tuning are translated. If some sampled instances from the benchmark can be evaluated by human, it could be better.\n- Regarding the proposed metric which combines LLMs and FineD, one concern is the computational limitations of the open-source LLMs and the training of the BERT-style model, especially for 9 languages of 18 datasets.\n\nquestions_for_the_authors: 1. How long will it cost to run the whole evaluation of the proposed benchmark with the Ensemble metric such as LLama-7B on a typical GPU?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "c7HpWPCgLR",
        "length": 434,
        "human_text": "paper_topic_and_main_contributions: This work proposes xDial-Eval, an evaluation benchmark for multilingual open domain dialogue by machine translating 18 English open domain dialogue datasets into 8 languages. To demonstrate the usability of the dataset, several metrics BERT-based/generation-based LLMs are evaluated on the benchmark. The work also suggests ensembling metrics to be better than commercial LLM systems  The contribution is primarily a resource contribution followed by NLP engineering experiment.\n\nreasons_to_accept: Good choice of languages for the dataset.  Good number and types of LLMs used for comparison.  Open domain multilingual conversations have received less interest and this work is a contribution towards that direction\n\nreasons_to_reject: The proposed dataset is an aggregation of different dialogue datasets focussing on different properties of dialogue - yet the emphasis is more on \u201ccoherence\u201d and excludes other properties of dialgoue evaluation.\nFurther, the proposed dataset, despite being an evaluation benchmark for multilingual dialogue, uses automatic translation APIs and quality evaluation is done only on automatic metrics.\n\nquestions_for_the_authors: A. How are the human annotations for evaluation obtained? What are the scores from the metrics compared against to compute the correlation? \n(After the rebuttal, please also write this clearly in the experimentation section)\n\ntypos_grammar_style_and_presentation_improvements: A. It is disputed whether LLMs can be used for translation quality evaluation as zero-shot , further a lot of segment level evaluation metrics are not yet equipped to do quality evaluation(https://aclanthology.org/2023.acl-long.730/) . As it is a dataset, it is essential to have some human evaluation to assess the quality.   B. 025 to 031 and 574 to 582, correlation scale ranges from -1 to 1, please check if the improvement can be quantified in absolute percentages C. Please mention the languages in the abstract D. I am slightly skeptic about the results produced by fine-tuning. If similar translation methods are used for training and evaluation, it is likely that the model is getting biased towards synthetic data. I wonder if this is causing artificial gain (See https://arxiv.org/pdf/2201.13405.pdf, https://aclanthology.org/2020.emnlp-main.618/) A quick check on the DuRecDial2.0 dataset may be helpful https://aclanthology.org/2021.emnlp-main.356/\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "mi82DXOt1Q",
        "length": 212,
        "human_text": "paper_topic_and_main_contributions: This paper establishes a benchmark for evaluating multi-lingual, multi-turn dialogues, creating a dataset that covers various languages to address the current lack of evaluation benchmarks for language. The study also establishes strong self-supervised and multilingual baselines.\n\nreasons_to_accept: This paper is well organized and clearly written.\n\nreasons_to_reject: This paper presents a benchmark for evaluating multilingual dialogues and creates a multi-round dialogue dataset in up to ten different languages, providing an excellent tool for assessing the multilingual ability of large models. However, in my opinion, the practicality and effectiveness of such evaluations in the field of multilingualism are not as useful and effective as other evaluation tools that examine the overall capabilities of large models. Therefore, the actual relevance of evaluating the multilingual ability of large models needs to be carefully considered, especially given that the multilingual ability of large language models is generally not poor nowadays.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "27_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_27_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7101999999999999,
      "max_similarity": 0.7127,
      "avg_coverage": 0.4293,
      "max_coverage": 0.5417
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 435,
      "avg_human_length": 393.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 6,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "VO4x9DK0Pr",
        "similarity": 0.7127,
        "coverage": 0.2917,
        "human_length": 499,
        "human_text": "paper_topic_and_main_contributions: This paper combines language model and explicit planning for multiple-step logical reasoning. For logical reasoning, planning offers two benefits: lowering the number of reasoning steps and interpretability. There is also a challenge of planning in models: model exploitation, which causes the model to be overconfident about some wrong reasoning paths. This paper presents a framework for logical reasoning that combines language models and planning. The framework is based on a base model and two improvements. The base model consists of models for deduction using beam search; the first improvement introduces planning by modifying scores in the base model; the second improvement adds a contrastive loss and KL-divergence to the original loss function. The method applies to both small models like T5 and LLMs like GPT-3.5. Experiments and analysis shows that planning does help deduction.\n\nreasons_to_accept: 1. The idea of combining LM and planning is novel and meaningful. \n2. The idea is supported by detailed description of the method and extensive experiments and analysis. \n3. The method applies to both small models and large models, which is a practical advantage (although the cost may be higher than other methods).\n\nreasons_to_reject: 1. This paper is not well-organized. Much important information is presented in the appendix, like figures of models (fig. 6) and the results of analysis (fig. 8, 9). There also seems to be two sections for related work (sec 3.4 and 5). \n2. The baselines methods are limited. For small models, the method is based on tuning, so comparing to prompted GPT-3.5 is not convincing. Instead, the paper may include some tuning baselines for deductive reasoning like RuleTaker [1] and Neural Unification [2].\n\nquestions_for_the_authors: A. In analysis-II, what is the data that supports the claim that \u201cwithout \u2126, System-B 602 performs worse than System-A\u201d?\nB. Figure 1:Can you explain why each deduction step (e.g., x2x3->x5) is valid? It seems none of these are actually deductions.  C. Compared to baseline methods, how much more computation does this method cost?\n\nmissing_references: [1] Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI'20). Article 537, 3882\u20133890.\n[2] Gabriele Picco, Thanh Lam Hoang, Marco Luca Sbodio, and Vanessa Lopez. 2021. Neural Unification for Logic Reasoning over Natural Language. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3939\u20133950, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\ntypos_grammar_style_and_presentation_improvements: 1. Analysis-IV: Since this analysis does not produce results, it could be abbreviated as a footnote or moved to appendix. \n2. Some sentences may be informal and redundant, like line 266-267 and line 292-293.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "kggjBSDaju",
        "similarity": 0.711,
        "coverage": 0.4545,
        "human_length": 274,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a method that perform multi-step logical reasoning by employing explicit planning into its inference procedure. The system significantly outperforms other competing methods on EntailmentBank and QASC. The authors also propose a training strategy that safeguards the planning process from being led astray by spurious features. Extensive empirical studies demonstrate that explicit planning plays an important role in the system's performance.\n\nreasons_to_accept: 1. The method makes an interesting connection between logical reasoning and planning / contrastive learning 2. The empirical results are pretty strong - small models can match the performance of GPT-3\n\nreasons_to_reject: 1. The writing and presentation of the paper can be improved.  I didn't have any idea of the method until I really read through the method section.  The paper would benefit from first giving a high-level overview of the method at the beginning of the paper than just saying they do planning. \n2. The method is quite complex, making people question whether this can really be deployed in the real-world. However, I still think there is value in studying this.  Not too much of a concern for me.\n\nquestions_for_the_authors: 1. the authors should consider improving figure 1.  The figure basically looks the same as the selection-inference paper. Personally, I didn't get anything out from figure 1.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "Nx2W4ShVJk",
        "similarity": 0.7069,
        "coverage": 0.5417,
        "human_length": 408,
        "human_text": "paper_topic_and_main_contributions: The paper presents a method for performing multi-step logical reasoning via planning. The method uses language models to construct and evaluate reasoning paths. The models are trained using a planning strategy that looks at future predictions and integrates the results into the score of past predictions to teach them to look ahead. Contrastive training is used to learn to score good and bad reasoning paths. The proposed method outperforms the baselines and achieves similar results to larger models.\n\nreasons_to_accept: The work presented is of high quality. The paper is easy to read and the ideas are explained in a clear way. All the necessary information to understand the method is provided.\nThe proposed method is particularly interesting and promising for tackling challenging planning and reasoning tasks, in particular the fact that the algorithm is model-agnostic and can be integrated with other LMs. The authors provide very extensive experiments demonstrating the quality of their method. The various ablation studies also clearly identify the proposed systems A and B as the reasons for the improved performance, highlighting precisely the contributions of each system.\n\nreasons_to_reject: The paper contains many references to the appendix as it contains important results that should be in the main paper, including a part of the conclusion. The back and forth while reading can be detrimental to comprehension. Fortunately, the information needed to understand the method and assess the claim is in the main content.\nWhile the experiments are extensive, there is little comparison with existing methods. It would be great to include a comparison with the state-of-the-art for each dataset beyond the single GPT-3.5. Further comparison with other planning methods, such as those mentioned in the related work, might be interesting (as far as I understand, the only comparison is with the work of [Creswell et al., 2023]).\n\nquestions_for_the_authors: Question A: Have you compared your method with the state-of-the-art of each dataset? with other planning strategies?\nQuestion B: How did you choose the buffer size B for beam search? Does the number of ongoing paths has an impact on the performance?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "VO4x9DK0Pr",
        "length": 499,
        "human_text": "paper_topic_and_main_contributions: This paper combines language model and explicit planning for multiple-step logical reasoning. For logical reasoning, planning offers two benefits: lowering the number of reasoning steps and interpretability. There is also a challenge of planning in models: model exploitation, which causes the model to be overconfident about some wrong reasoning paths. This paper presents a framework for logical reasoning that combines language models and planning. The framework is based on a base model and two improvements. The base model consists of models for deduction using beam search; the first improvement introduces planning by modifying scores in the base model; the second improvement adds a contrastive loss and KL-divergence to the original loss function. The method applies to both small models like T5 and LLMs like GPT-3.5. Experiments and analysis shows that planning does help deduction.\n\nreasons_to_accept: 1. The idea of combining LM and planning is novel and meaningful. \n2. The idea is supported by detailed description of the method and extensive experiments and analysis. \n3. The method applies to both small models and large models, which is a practical advantage (although the cost may be higher than other methods).\n\nreasons_to_reject: 1. This paper is not well-organized. Much important information is presented in the appendix, like figures of models (fig. 6) and the results of analysis (fig. 8, 9). There also seems to be two sections for related work (sec 3.4 and 5). \n2. The baselines methods are limited. For small models, the method is based on tuning, so comparing to prompted GPT-3.5 is not convincing. Instead, the paper may include some tuning baselines for deductive reasoning like RuleTaker [1] and Neural Unification [2].\n\nquestions_for_the_authors: A. In analysis-II, what is the data that supports the claim that \u201cwithout \u2126, System-B 602 performs worse than System-A\u201d?\nB. Figure 1:Can you explain why each deduction step (e.g., x2x3->x5) is valid? It seems none of these are actually deductions.  C. Compared to baseline methods, how much more computation does this method cost?\n\nmissing_references: [1] Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI'20). Article 537, 3882\u20133890.\n[2] Gabriele Picco, Thanh Lam Hoang, Marco Luca Sbodio, and Vanessa Lopez. 2021. Neural Unification for Logic Reasoning over Natural Language. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3939\u20133950, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\ntypos_grammar_style_and_presentation_improvements: 1. Analysis-IV: Since this analysis does not produce results, it could be abbreviated as a footnote or moved to appendix. \n2. Some sentences may be informal and redundant, like line 266-267 and line 292-293.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "kggjBSDaju",
        "length": 274,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a method that perform multi-step logical reasoning by employing explicit planning into its inference procedure. The system significantly outperforms other competing methods on EntailmentBank and QASC. The authors also propose a training strategy that safeguards the planning process from being led astray by spurious features. Extensive empirical studies demonstrate that explicit planning plays an important role in the system's performance.\n\nreasons_to_accept: 1. The method makes an interesting connection between logical reasoning and planning / contrastive learning 2. The empirical results are pretty strong - small models can match the performance of GPT-3\n\nreasons_to_reject: 1. The writing and presentation of the paper can be improved.  I didn't have any idea of the method until I really read through the method section.  The paper would benefit from first giving a high-level overview of the method at the beginning of the paper than just saying they do planning. \n2. The method is quite complex, making people question whether this can really be deployed in the real-world. However, I still think there is value in studying this.  Not too much of a concern for me.\n\nquestions_for_the_authors: 1. the authors should consider improving figure 1.  The figure basically looks the same as the selection-inference paper. Personally, I didn't get anything out from figure 1.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "Nx2W4ShVJk",
        "length": 408,
        "human_text": "paper_topic_and_main_contributions: The paper presents a method for performing multi-step logical reasoning via planning. The method uses language models to construct and evaluate reasoning paths. The models are trained using a planning strategy that looks at future predictions and integrates the results into the score of past predictions to teach them to look ahead. Contrastive training is used to learn to score good and bad reasoning paths. The proposed method outperforms the baselines and achieves similar results to larger models.\n\nreasons_to_accept: The work presented is of high quality. The paper is easy to read and the ideas are explained in a clear way. All the necessary information to understand the method is provided.\nThe proposed method is particularly interesting and promising for tackling challenging planning and reasoning tasks, in particular the fact that the algorithm is model-agnostic and can be integrated with other LMs. The authors provide very extensive experiments demonstrating the quality of their method. The various ablation studies also clearly identify the proposed systems A and B as the reasons for the improved performance, highlighting precisely the contributions of each system.\n\nreasons_to_reject: The paper contains many references to the appendix as it contains important results that should be in the main paper, including a part of the conclusion. The back and forth while reading can be detrimental to comprehension. Fortunately, the information needed to understand the method and assess the claim is in the main content.\nWhile the experiments are extensive, there is little comparison with existing methods. It would be great to include a comparison with the state-of-the-art for each dataset beyond the single GPT-3.5. Further comparison with other planning methods, such as those mentioned in the related work, might be interesting (as far as I understand, the only comparison is with the work of [Creswell et al., 2023]).\n\nquestions_for_the_authors: Question A: Have you compared your method with the state-of-the-art of each dataset? with other planning strategies?\nQuestion B: How did you choose the buffer size B for beam search? Does the number of ongoing paths has an impact on the performance?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "27_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_27_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7266333333333334,
      "max_similarity": 0.7303,
      "avg_coverage": 0.3737333333333333,
      "max_coverage": 0.4545
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 439,
      "avg_human_length": 393.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 5,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "VO4x9DK0Pr",
        "similarity": 0.7303,
        "coverage": 0.25,
        "human_length": 499,
        "human_text": "paper_topic_and_main_contributions: This paper combines language model and explicit planning for multiple-step logical reasoning. For logical reasoning, planning offers two benefits: lowering the number of reasoning steps and interpretability. There is also a challenge of planning in models: model exploitation, which causes the model to be overconfident about some wrong reasoning paths. This paper presents a framework for logical reasoning that combines language models and planning. The framework is based on a base model and two improvements. The base model consists of models for deduction using beam search; the first improvement introduces planning by modifying scores in the base model; the second improvement adds a contrastive loss and KL-divergence to the original loss function. The method applies to both small models like T5 and LLMs like GPT-3.5. Experiments and analysis shows that planning does help deduction.\n\nreasons_to_accept: 1. The idea of combining LM and planning is novel and meaningful. \n2. The idea is supported by detailed description of the method and extensive experiments and analysis. \n3. The method applies to both small models and large models, which is a practical advantage (although the cost may be higher than other methods).\n\nreasons_to_reject: 1. This paper is not well-organized. Much important information is presented in the appendix, like figures of models (fig. 6) and the results of analysis (fig. 8, 9). There also seems to be two sections for related work (sec 3.4 and 5). \n2. The baselines methods are limited. For small models, the method is based on tuning, so comparing to prompted GPT-3.5 is not convincing. Instead, the paper may include some tuning baselines for deductive reasoning like RuleTaker [1] and Neural Unification [2].\n\nquestions_for_the_authors: A. In analysis-II, what is the data that supports the claim that \u201cwithout \u2126, System-B 602 performs worse than System-A\u201d?\nB. Figure 1:Can you explain why each deduction step (e.g., x2x3->x5) is valid? It seems none of these are actually deductions.  C. Compared to baseline methods, how much more computation does this method cost?\n\nmissing_references: [1] Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI'20). Article 537, 3882\u20133890.\n[2] Gabriele Picco, Thanh Lam Hoang, Marco Luca Sbodio, and Vanessa Lopez. 2021. Neural Unification for Logic Reasoning over Natural Language. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3939\u20133950, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\ntypos_grammar_style_and_presentation_improvements: 1. Analysis-IV: Since this analysis does not produce results, it could be abbreviated as a footnote or moved to appendix. \n2. Some sentences may be informal and redundant, like line 266-267 and line 292-293.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "kggjBSDaju",
        "similarity": 0.7295,
        "coverage": 0.4545,
        "human_length": 274,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a method that perform multi-step logical reasoning by employing explicit planning into its inference procedure. The system significantly outperforms other competing methods on EntailmentBank and QASC. The authors also propose a training strategy that safeguards the planning process from being led astray by spurious features. Extensive empirical studies demonstrate that explicit planning plays an important role in the system's performance.\n\nreasons_to_accept: 1. The method makes an interesting connection between logical reasoning and planning / contrastive learning 2. The empirical results are pretty strong - small models can match the performance of GPT-3\n\nreasons_to_reject: 1. The writing and presentation of the paper can be improved.  I didn't have any idea of the method until I really read through the method section.  The paper would benefit from first giving a high-level overview of the method at the beginning of the paper than just saying they do planning. \n2. The method is quite complex, making people question whether this can really be deployed in the real-world. However, I still think there is value in studying this.  Not too much of a concern for me.\n\nquestions_for_the_authors: 1. the authors should consider improving figure 1.  The figure basically looks the same as the selection-inference paper. Personally, I didn't get anything out from figure 1.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "Nx2W4ShVJk",
        "similarity": 0.7201,
        "coverage": 0.4167,
        "human_length": 408,
        "human_text": "paper_topic_and_main_contributions: The paper presents a method for performing multi-step logical reasoning via planning. The method uses language models to construct and evaluate reasoning paths. The models are trained using a planning strategy that looks at future predictions and integrates the results into the score of past predictions to teach them to look ahead. Contrastive training is used to learn to score good and bad reasoning paths. The proposed method outperforms the baselines and achieves similar results to larger models.\n\nreasons_to_accept: The work presented is of high quality. The paper is easy to read and the ideas are explained in a clear way. All the necessary information to understand the method is provided.\nThe proposed method is particularly interesting and promising for tackling challenging planning and reasoning tasks, in particular the fact that the algorithm is model-agnostic and can be integrated with other LMs. The authors provide very extensive experiments demonstrating the quality of their method. The various ablation studies also clearly identify the proposed systems A and B as the reasons for the improved performance, highlighting precisely the contributions of each system.\n\nreasons_to_reject: The paper contains many references to the appendix as it contains important results that should be in the main paper, including a part of the conclusion. The back and forth while reading can be detrimental to comprehension. Fortunately, the information needed to understand the method and assess the claim is in the main content.\nWhile the experiments are extensive, there is little comparison with existing methods. It would be great to include a comparison with the state-of-the-art for each dataset beyond the single GPT-3.5. Further comparison with other planning methods, such as those mentioned in the related work, might be interesting (as far as I understand, the only comparison is with the work of [Creswell et al., 2023]).\n\nquestions_for_the_authors: Question A: Have you compared your method with the state-of-the-art of each dataset? with other planning strategies?\nQuestion B: How did you choose the buffer size B for beam search? Does the number of ongoing paths has an impact on the performance?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "VO4x9DK0Pr",
        "length": 499,
        "human_text": "paper_topic_and_main_contributions: This paper combines language model and explicit planning for multiple-step logical reasoning. For logical reasoning, planning offers two benefits: lowering the number of reasoning steps and interpretability. There is also a challenge of planning in models: model exploitation, which causes the model to be overconfident about some wrong reasoning paths. This paper presents a framework for logical reasoning that combines language models and planning. The framework is based on a base model and two improvements. The base model consists of models for deduction using beam search; the first improvement introduces planning by modifying scores in the base model; the second improvement adds a contrastive loss and KL-divergence to the original loss function. The method applies to both small models like T5 and LLMs like GPT-3.5. Experiments and analysis shows that planning does help deduction.\n\nreasons_to_accept: 1. The idea of combining LM and planning is novel and meaningful. \n2. The idea is supported by detailed description of the method and extensive experiments and analysis. \n3. The method applies to both small models and large models, which is a practical advantage (although the cost may be higher than other methods).\n\nreasons_to_reject: 1. This paper is not well-organized. Much important information is presented in the appendix, like figures of models (fig. 6) and the results of analysis (fig. 8, 9). There also seems to be two sections for related work (sec 3.4 and 5). \n2. The baselines methods are limited. For small models, the method is based on tuning, so comparing to prompted GPT-3.5 is not convincing. Instead, the paper may include some tuning baselines for deductive reasoning like RuleTaker [1] and Neural Unification [2].\n\nquestions_for_the_authors: A. In analysis-II, what is the data that supports the claim that \u201cwithout \u2126, System-B 602 performs worse than System-A\u201d?\nB. Figure 1:Can you explain why each deduction step (e.g., x2x3->x5) is valid? It seems none of these are actually deductions.  C. Compared to baseline methods, how much more computation does this method cost?\n\nmissing_references: [1] Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI'20). Article 537, 3882\u20133890.\n[2] Gabriele Picco, Thanh Lam Hoang, Marco Luca Sbodio, and Vanessa Lopez. 2021. Neural Unification for Logic Reasoning over Natural Language. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3939\u20133950, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\ntypos_grammar_style_and_presentation_improvements: 1. Analysis-IV: Since this analysis does not produce results, it could be abbreviated as a footnote or moved to appendix. \n2. Some sentences may be informal and redundant, like line 266-267 and line 292-293.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "kggjBSDaju",
        "length": 274,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a method that perform multi-step logical reasoning by employing explicit planning into its inference procedure. The system significantly outperforms other competing methods on EntailmentBank and QASC. The authors also propose a training strategy that safeguards the planning process from being led astray by spurious features. Extensive empirical studies demonstrate that explicit planning plays an important role in the system's performance.\n\nreasons_to_accept: 1. The method makes an interesting connection between logical reasoning and planning / contrastive learning 2. The empirical results are pretty strong - small models can match the performance of GPT-3\n\nreasons_to_reject: 1. The writing and presentation of the paper can be improved.  I didn't have any idea of the method until I really read through the method section.  The paper would benefit from first giving a high-level overview of the method at the beginning of the paper than just saying they do planning. \n2. The method is quite complex, making people question whether this can really be deployed in the real-world. However, I still think there is value in studying this.  Not too much of a concern for me.\n\nquestions_for_the_authors: 1. the authors should consider improving figure 1.  The figure basically looks the same as the selection-inference paper. Personally, I didn't get anything out from figure 1.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "Nx2W4ShVJk",
        "length": 408,
        "human_text": "paper_topic_and_main_contributions: The paper presents a method for performing multi-step logical reasoning via planning. The method uses language models to construct and evaluate reasoning paths. The models are trained using a planning strategy that looks at future predictions and integrates the results into the score of past predictions to teach them to look ahead. Contrastive training is used to learn to score good and bad reasoning paths. The proposed method outperforms the baselines and achieves similar results to larger models.\n\nreasons_to_accept: The work presented is of high quality. The paper is easy to read and the ideas are explained in a clear way. All the necessary information to understand the method is provided.\nThe proposed method is particularly interesting and promising for tackling challenging planning and reasoning tasks, in particular the fact that the algorithm is model-agnostic and can be integrated with other LMs. The authors provide very extensive experiments demonstrating the quality of their method. The various ablation studies also clearly identify the proposed systems A and B as the reasons for the improved performance, highlighting precisely the contributions of each system.\n\nreasons_to_reject: The paper contains many references to the appendix as it contains important results that should be in the main paper, including a part of the conclusion. The back and forth while reading can be detrimental to comprehension. Fortunately, the information needed to understand the method and assess the claim is in the main content.\nWhile the experiments are extensive, there is little comparison with existing methods. It would be great to include a comparison with the state-of-the-art for each dataset beyond the single GPT-3.5. Further comparison with other planning methods, such as those mentioned in the related work, might be interesting (as far as I understand, the only comparison is with the work of [Creswell et al., 2023]).\n\nquestions_for_the_authors: Question A: Have you compared your method with the state-of-the-art of each dataset? with other planning strategies?\nQuestion B: How did you choose the buffer size B for beam search? Does the number of ongoing paths has an impact on the performance?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "62_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_62_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7311666666666666,
      "max_similarity": 0.7381,
      "avg_coverage": 0.09756666666666665,
      "max_coverage": 0.1538
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 38,
      "avg_human_length": 375.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": false,
      "has_weaknesses": false,
      "has_suggestions": false
    },
    "content_count_metrics": {
      "strengths_count": 0,
      "weaknesses_count": 0,
      "suggestions_count": 0
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "7O79QSFAVH",
        "similarity": 0.7271,
        "coverage": 0.0556,
        "human_length": 419,
        "human_text": "paper_topic_and_main_contributions: This paper proposes context aware OOD intent detection (caro) framework to model multi-turn contexts in OOD intent detection tasks. Moreover, Caro introduces a two-stage self-training scheme to mine OOD samples from unlabeled data. The main contribution of this paper is to show the better performance of Caro compared with other models on variants of STAR dataset.\n\nreasons_to_accept: We can see Caro (a context ware OOD intent detection ) works well on the new evaluation dataset created from STAR dataset.\n\nreasons_to_reject: We cannot confirm whether the issue set here has been solved with the evaluation dataset used in the experiment.\n\nquestions_for_the_authors: Question 1: This paper try to solve OOD intent detection in the case where  \"the testing distribution is subject to change and out-of-domain (OOD) intentions that are not seen in the training process may emerge in testing\". How is the situation in which the testing distribution has changed replicated in the evaluation data created manually?\nQuestion 2: In Section 3.2, it seems arbitrary that 30% should be added. In order to make Du, the labels of OOD and IND are used, but since it is originally an unlabeled dataset, it seems unnatural that the ratio of IND and OOD is known. If you want to combine IND and OOD to make an unlabeled dataset, I think it might be better to make a few patterns with the ratio changed from 0% to 100% to see its robustness.\nQuestion 3: In this paper, experimental results are shown only for STAR data, but Chen and Yu (2021) also showed the results on FLOW and ROSTD. With only one data set, it is difficult to know whether the method works for a particular data set or works generally. I think it\u2019s better to show the results on other datasets too.\nQuestion 4: Although it seems that there is no table showing the number of IND/OOD data, it is better to show the statistics.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "o8r4KkSIhM",
        "similarity": 0.7381,
        "coverage": 0.0833,
        "human_length": 453,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a novel context-aware OOD intent detection (Caro) framework for multi-turn dialogue contexts, by tackling two main challenges: (1) How to alleviate the long distance obstacle and learn robust representations? ( 2) How to utilize unlabeled data? For the first challenge, by following the information bottleneck principle, Caro extracts robust representations from these contexts and removes irrelevant information using a multi-view information bottleneck loss. For the second one, Caro introduces a two-stage  self-training scheme to mine OOD samples. Specificalluy, the first stage builds a preliminary OOD detector with OOD samples synthesized from IND data, while the second stage refine it based on the real-world OOD samples selected from the unlabeled data. The author have conducted extensive experiments and analysis, showing that Caro is effective in OOD intent detection for multi-turn dialogue.\n\nreasons_to_accept: 1.The proposed method is clear and simple; It extracts robust intent representations from multi-turn dialogue contexts by combining the features from diverse views following the information bottleneck principle, which is novel and intuitive. \n2.The analysis and experiments are somewhat comprehensive. The performance on multi-turn OOD detection tasks is quite impressive, with a significant improvement in F1-OOD score of over 29% compared to the previous top-performing method. \n3.The task is interesting and the paper is well written.\n\nreasons_to_reject: 1.There is a lack of theoretical analysis of the feasibility of the information bottleneck principle in OOD intent detection from multi-turn dialogue contexts, especially why it is more suitable for the dialogues with longer turns. \n2.The proposed method has not been verified on other datasets with different distributions/domains, and it only focused on STAR with two versions (small/large), thus the universality needs to be further verified.\n\nquestions_for_the_authors: 1. Which feature is used to generate the pesudo OOD samples in the first stage, the aggregated features or the single-view features? \n2. What is the difference between the Adaptive Reception Field and the Attetion mechanism or convolution kernel? \n3. Is the performance of the OOD intent detection positively correlated with the turns of the dialogue? Is the longer the round of the dialogue, the better the OOD detection? Can you give a fine-grained analysis?\n\nethical_concerns: Yes\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "ak9bQxbSHe",
        "similarity": 0.7283,
        "coverage": 0.1538,
        "human_length": 253,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a novel framework called Caro for addressing the challenging and relatively unexplored problem of Out-of-Domain (OOD) intent detection in the context of multi-turn dialogues. Traditional OOD intent detection approaches often focus on single-turn interactions, but real-world dialogue systems often involve multi-turn conversations. Caro aims to accurately detect OOD intents while considering the complexities of multi-turn contexts.\n\nreasons_to_accept: 1) The paper introduces a novel framework called \"Caro\" that addresses a challenging and under-explored problem in the field of intent detection. \n2) By constructing diverse views of input data and optimizing an unsupervised multi-view loss, Caro retains predictive information relevant to intent detection while discarding irrelevant information. \n3)Caro introduces a two-stage self-training process to mine OOD samples from unlabeled data. This addresses the challenge of refining OOD detection without access to labeled OOD samples during training.\n\nreasons_to_reject: I'm not very familiar with this area anymore. please getting the opinions of other reviewers.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "7O79QSFAVH",
        "length": 419,
        "human_text": "paper_topic_and_main_contributions: This paper proposes context aware OOD intent detection (caro) framework to model multi-turn contexts in OOD intent detection tasks. Moreover, Caro introduces a two-stage self-training scheme to mine OOD samples from unlabeled data. The main contribution of this paper is to show the better performance of Caro compared with other models on variants of STAR dataset.\n\nreasons_to_accept: We can see Caro (a context ware OOD intent detection ) works well on the new evaluation dataset created from STAR dataset.\n\nreasons_to_reject: We cannot confirm whether the issue set here has been solved with the evaluation dataset used in the experiment.\n\nquestions_for_the_authors: Question 1: This paper try to solve OOD intent detection in the case where  \"the testing distribution is subject to change and out-of-domain (OOD) intentions that are not seen in the training process may emerge in testing\". How is the situation in which the testing distribution has changed replicated in the evaluation data created manually?\nQuestion 2: In Section 3.2, it seems arbitrary that 30% should be added. In order to make Du, the labels of OOD and IND are used, but since it is originally an unlabeled dataset, it seems unnatural that the ratio of IND and OOD is known. If you want to combine IND and OOD to make an unlabeled dataset, I think it might be better to make a few patterns with the ratio changed from 0% to 100% to see its robustness.\nQuestion 3: In this paper, experimental results are shown only for STAR data, but Chen and Yu (2021) also showed the results on FLOW and ROSTD. With only one data set, it is difficult to know whether the method works for a particular data set or works generally. I think it\u2019s better to show the results on other datasets too.\nQuestion 4: Although it seems that there is no table showing the number of IND/OOD data, it is better to show the statistics.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "o8r4KkSIhM",
        "length": 453,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a novel context-aware OOD intent detection (Caro) framework for multi-turn dialogue contexts, by tackling two main challenges: (1) How to alleviate the long distance obstacle and learn robust representations? ( 2) How to utilize unlabeled data? For the first challenge, by following the information bottleneck principle, Caro extracts robust representations from these contexts and removes irrelevant information using a multi-view information bottleneck loss. For the second one, Caro introduces a two-stage  self-training scheme to mine OOD samples. Specificalluy, the first stage builds a preliminary OOD detector with OOD samples synthesized from IND data, while the second stage refine it based on the real-world OOD samples selected from the unlabeled data. The author have conducted extensive experiments and analysis, showing that Caro is effective in OOD intent detection for multi-turn dialogue.\n\nreasons_to_accept: 1.The proposed method is clear and simple; It extracts robust intent representations from multi-turn dialogue contexts by combining the features from diverse views following the information bottleneck principle, which is novel and intuitive. \n2.The analysis and experiments are somewhat comprehensive. The performance on multi-turn OOD detection tasks is quite impressive, with a significant improvement in F1-OOD score of over 29% compared to the previous top-performing method. \n3.The task is interesting and the paper is well written.\n\nreasons_to_reject: 1.There is a lack of theoretical analysis of the feasibility of the information bottleneck principle in OOD intent detection from multi-turn dialogue contexts, especially why it is more suitable for the dialogues with longer turns. \n2.The proposed method has not been verified on other datasets with different distributions/domains, and it only focused on STAR with two versions (small/large), thus the universality needs to be further verified.\n\nquestions_for_the_authors: 1. Which feature is used to generate the pesudo OOD samples in the first stage, the aggregated features or the single-view features? \n2. What is the difference between the Adaptive Reception Field and the Attetion mechanism or convolution kernel? \n3. Is the performance of the OOD intent detection positively correlated with the turns of the dialogue? Is the longer the round of the dialogue, the better the OOD detection? Can you give a fine-grained analysis?\n\nethical_concerns: Yes\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ak9bQxbSHe",
        "length": 253,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a novel framework called Caro for addressing the challenging and relatively unexplored problem of Out-of-Domain (OOD) intent detection in the context of multi-turn dialogues. Traditional OOD intent detection approaches often focus on single-turn interactions, but real-world dialogue systems often involve multi-turn conversations. Caro aims to accurately detect OOD intents while considering the complexities of multi-turn contexts.\n\nreasons_to_accept: 1) The paper introduces a novel framework called \"Caro\" that addresses a challenging and under-explored problem in the field of intent detection. \n2) By constructing diverse views of input data and optimizing an unsupervised multi-view loss, Caro retains predictive information relevant to intent detection while discarding irrelevant information. \n3)Caro introduces a two-stage self-training process to mine OOD samples from unlabeled data. This addresses the challenge of refining OOD detection without access to labeled OOD samples during training.\n\nreasons_to_reject: I'm not very familiar with this area anymore. please getting the opinions of other reviewers.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "62_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_62_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7091333333333334,
      "max_similarity": 0.7147,
      "avg_coverage": 0.20656666666666668,
      "max_coverage": 0.2308
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 83,
      "avg_human_length": 375.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 2,
      "weaknesses_count": 1,
      "suggestions_count": 1
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "7O79QSFAVH",
        "similarity": 0.7147,
        "coverage": 0.2222,
        "human_length": 419,
        "human_text": "paper_topic_and_main_contributions: This paper proposes context aware OOD intent detection (caro) framework to model multi-turn contexts in OOD intent detection tasks. Moreover, Caro introduces a two-stage self-training scheme to mine OOD samples from unlabeled data. The main contribution of this paper is to show the better performance of Caro compared with other models on variants of STAR dataset.\n\nreasons_to_accept: We can see Caro (a context ware OOD intent detection ) works well on the new evaluation dataset created from STAR dataset.\n\nreasons_to_reject: We cannot confirm whether the issue set here has been solved with the evaluation dataset used in the experiment.\n\nquestions_for_the_authors: Question 1: This paper try to solve OOD intent detection in the case where  \"the testing distribution is subject to change and out-of-domain (OOD) intentions that are not seen in the training process may emerge in testing\". How is the situation in which the testing distribution has changed replicated in the evaluation data created manually?\nQuestion 2: In Section 3.2, it seems arbitrary that 30% should be added. In order to make Du, the labels of OOD and IND are used, but since it is originally an unlabeled dataset, it seems unnatural that the ratio of IND and OOD is known. If you want to combine IND and OOD to make an unlabeled dataset, I think it might be better to make a few patterns with the ratio changed from 0% to 100% to see its robustness.\nQuestion 3: In this paper, experimental results are shown only for STAR data, but Chen and Yu (2021) also showed the results on FLOW and ROSTD. With only one data set, it is difficult to know whether the method works for a particular data set or works generally. I think it\u2019s better to show the results on other datasets too.\nQuestion 4: Although it seems that there is no table showing the number of IND/OOD data, it is better to show the statistics.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "o8r4KkSIhM",
        "similarity": 0.7105,
        "coverage": 0.1667,
        "human_length": 453,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a novel context-aware OOD intent detection (Caro) framework for multi-turn dialogue contexts, by tackling two main challenges: (1) How to alleviate the long distance obstacle and learn robust representations? ( 2) How to utilize unlabeled data? For the first challenge, by following the information bottleneck principle, Caro extracts robust representations from these contexts and removes irrelevant information using a multi-view information bottleneck loss. For the second one, Caro introduces a two-stage  self-training scheme to mine OOD samples. Specificalluy, the first stage builds a preliminary OOD detector with OOD samples synthesized from IND data, while the second stage refine it based on the real-world OOD samples selected from the unlabeled data. The author have conducted extensive experiments and analysis, showing that Caro is effective in OOD intent detection for multi-turn dialogue.\n\nreasons_to_accept: 1.The proposed method is clear and simple; It extracts robust intent representations from multi-turn dialogue contexts by combining the features from diverse views following the information bottleneck principle, which is novel and intuitive. \n2.The analysis and experiments are somewhat comprehensive. The performance on multi-turn OOD detection tasks is quite impressive, with a significant improvement in F1-OOD score of over 29% compared to the previous top-performing method. \n3.The task is interesting and the paper is well written.\n\nreasons_to_reject: 1.There is a lack of theoretical analysis of the feasibility of the information bottleneck principle in OOD intent detection from multi-turn dialogue contexts, especially why it is more suitable for the dialogues with longer turns. \n2.The proposed method has not been verified on other datasets with different distributions/domains, and it only focused on STAR with two versions (small/large), thus the universality needs to be further verified.\n\nquestions_for_the_authors: 1. Which feature is used to generate the pesudo OOD samples in the first stage, the aggregated features or the single-view features? \n2. What is the difference between the Adaptive Reception Field and the Attetion mechanism or convolution kernel? \n3. Is the performance of the OOD intent detection positively correlated with the turns of the dialogue? Is the longer the round of the dialogue, the better the OOD detection? Can you give a fine-grained analysis?\n\nethical_concerns: Yes\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "ak9bQxbSHe",
        "similarity": 0.7022,
        "coverage": 0.2308,
        "human_length": 253,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a novel framework called Caro for addressing the challenging and relatively unexplored problem of Out-of-Domain (OOD) intent detection in the context of multi-turn dialogues. Traditional OOD intent detection approaches often focus on single-turn interactions, but real-world dialogue systems often involve multi-turn conversations. Caro aims to accurately detect OOD intents while considering the complexities of multi-turn contexts.\n\nreasons_to_accept: 1) The paper introduces a novel framework called \"Caro\" that addresses a challenging and under-explored problem in the field of intent detection. \n2) By constructing diverse views of input data and optimizing an unsupervised multi-view loss, Caro retains predictive information relevant to intent detection while discarding irrelevant information. \n3)Caro introduces a two-stage self-training process to mine OOD samples from unlabeled data. This addresses the challenge of refining OOD detection without access to labeled OOD samples during training.\n\nreasons_to_reject: I'm not very familiar with this area anymore. please getting the opinions of other reviewers.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "7O79QSFAVH",
        "length": 419,
        "human_text": "paper_topic_and_main_contributions: This paper proposes context aware OOD intent detection (caro) framework to model multi-turn contexts in OOD intent detection tasks. Moreover, Caro introduces a two-stage self-training scheme to mine OOD samples from unlabeled data. The main contribution of this paper is to show the better performance of Caro compared with other models on variants of STAR dataset.\n\nreasons_to_accept: We can see Caro (a context ware OOD intent detection ) works well on the new evaluation dataset created from STAR dataset.\n\nreasons_to_reject: We cannot confirm whether the issue set here has been solved with the evaluation dataset used in the experiment.\n\nquestions_for_the_authors: Question 1: This paper try to solve OOD intent detection in the case where  \"the testing distribution is subject to change and out-of-domain (OOD) intentions that are not seen in the training process may emerge in testing\". How is the situation in which the testing distribution has changed replicated in the evaluation data created manually?\nQuestion 2: In Section 3.2, it seems arbitrary that 30% should be added. In order to make Du, the labels of OOD and IND are used, but since it is originally an unlabeled dataset, it seems unnatural that the ratio of IND and OOD is known. If you want to combine IND and OOD to make an unlabeled dataset, I think it might be better to make a few patterns with the ratio changed from 0% to 100% to see its robustness.\nQuestion 3: In this paper, experimental results are shown only for STAR data, but Chen and Yu (2021) also showed the results on FLOW and ROSTD. With only one data set, it is difficult to know whether the method works for a particular data set or works generally. I think it\u2019s better to show the results on other datasets too.\nQuestion 4: Although it seems that there is no table showing the number of IND/OOD data, it is better to show the statistics.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "o8r4KkSIhM",
        "length": 453,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a novel context-aware OOD intent detection (Caro) framework for multi-turn dialogue contexts, by tackling two main challenges: (1) How to alleviate the long distance obstacle and learn robust representations? ( 2) How to utilize unlabeled data? For the first challenge, by following the information bottleneck principle, Caro extracts robust representations from these contexts and removes irrelevant information using a multi-view information bottleneck loss. For the second one, Caro introduces a two-stage  self-training scheme to mine OOD samples. Specificalluy, the first stage builds a preliminary OOD detector with OOD samples synthesized from IND data, while the second stage refine it based on the real-world OOD samples selected from the unlabeled data. The author have conducted extensive experiments and analysis, showing that Caro is effective in OOD intent detection for multi-turn dialogue.\n\nreasons_to_accept: 1.The proposed method is clear and simple; It extracts robust intent representations from multi-turn dialogue contexts by combining the features from diverse views following the information bottleneck principle, which is novel and intuitive. \n2.The analysis and experiments are somewhat comprehensive. The performance on multi-turn OOD detection tasks is quite impressive, with a significant improvement in F1-OOD score of over 29% compared to the previous top-performing method. \n3.The task is interesting and the paper is well written.\n\nreasons_to_reject: 1.There is a lack of theoretical analysis of the feasibility of the information bottleneck principle in OOD intent detection from multi-turn dialogue contexts, especially why it is more suitable for the dialogues with longer turns. \n2.The proposed method has not been verified on other datasets with different distributions/domains, and it only focused on STAR with two versions (small/large), thus the universality needs to be further verified.\n\nquestions_for_the_authors: 1. Which feature is used to generate the pesudo OOD samples in the first stage, the aggregated features or the single-view features? \n2. What is the difference between the Adaptive Reception Field and the Attetion mechanism or convolution kernel? \n3. Is the performance of the OOD intent detection positively correlated with the turns of the dialogue? Is the longer the round of the dialogue, the better the OOD detection? Can you give a fine-grained analysis?\n\nethical_concerns: Yes\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ak9bQxbSHe",
        "length": 253,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a novel framework called Caro for addressing the challenging and relatively unexplored problem of Out-of-Domain (OOD) intent detection in the context of multi-turn dialogues. Traditional OOD intent detection approaches often focus on single-turn interactions, but real-world dialogue systems often involve multi-turn conversations. Caro aims to accurately detect OOD intents while considering the complexities of multi-turn contexts.\n\nreasons_to_accept: 1) The paper introduces a novel framework called \"Caro\" that addresses a challenging and under-explored problem in the field of intent detection. \n2) By constructing diverse views of input data and optimizing an unsupervised multi-view loss, Caro retains predictive information relevant to intent detection while discarding irrelevant information. \n3)Caro introduces a two-stage self-training process to mine OOD samples from unlabeled data. This addresses the challenge of refining OOD detection without access to labeled OOD samples during training.\n\nreasons_to_reject: I'm not very familiar with this area anymore. please getting the opinions of other reviewers.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "138_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_138_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6928333333333333,
      "max_similarity": 0.7159,
      "avg_coverage": 0.5273,
      "max_coverage": 0.7692
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 568,
      "avg_human_length": 516.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 11,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "E7iy0aj33A",
        "similarity": 0.7159,
        "coverage": 0.2742,
        "human_length": 915,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel approach for multilingual audio-visual speech recognition tasks by developing a single model on a multilingual dataset. The primary problem being addressed in the paper is the limited success of current audio-visual speech recognition models in multilingual datasets, as they are mostly focused on monolingual data, particularly English. The paper aims to design a single model capable of effectively distinguishing the language and properly recognizing the input speech in multiple languages.\nThe main contributions of the paper are as follows: 1. Propose a single model framework for multilingual audio-visual speech recognition, inspired by the intuitive human language understanding process. The model is designed to determine the language of the input speech and recognize the speech correctly.\n2. Introduce an audio-visual guiding linguistic representation extractor by fine-tuning a largely pre-trained audio-visual representation model using prompts to provide language information. This allows the model to extract comprehensive linguistic information from the audio and video inputs.\n3. Address the issue of imbalanced language distribution in multilingual datasets by employing a weighted objective function that balances the distribution of language data while updating the loss during training. This aims to improve the performance of speech recognition in low-resource languages and prevent the model from being biased toward dominant languages.\n4. Validate the effectiveness of the proposed model by conducting experiments on MuAViC, a multilingual audio-visual corpus containing 9 languages. The results show that the proposed model outperforms the previous multilingual model with an average 11% WER reduction in a clean environment and an average 13% WER reduction in a noisy environment.\n\nreasons_to_accept: 1. Novelty: The approach of fine-tuning a largely pre-trained audio-visual representation model with language prompts is new and innovative, enabling the model to effectively recognize and transcribe speech in multiple languages.\n2. Robustness: The proposed model demonstrates performance improvements in low-resource languages and exhibits strong results in both clean and noisy environments. This has significant practical implications in real-world applications where speech recognition systems often face challenges in noisy conditions.\n3. Addressing Language Imbalance: The paper proposes an objective function weight to balance the distribution of language data, mitigating potential biases towards dominant languages while improving the recognition and transcription of low-resource languages.\n4. Comprehensive Experiments: The experiments are conducted on a recent and challenging dataset (MuAViC), consisting of 1200 hours of audio-visual speech data in 9 languages, providing a comprehensive evaluation of the model.\n5. Potential Impact: The proposed single model approach can reduce the need for language-specific models and improve the performance of speech recognition in low-resource languages. This would benefit the NLP community in developing more efficient and robust multilingual speech recognition systems.\n\nreasons_to_reject: 1. One weakness is the lack of an in-depth ablation study on the proposed approach. Some questions remain unanswered, such as the effectiveness of prompt fine-tuning in various settings and the sensitivity of the model to the choice of hyperparameters. \n2. The paper does not provide results for a setting where the language is unknown during inference. Dealing with an unknown language is a crucial aspect in real-world scenarios. \n3. The authors mention the imbalance in the dataset, but the exact distribution is not provided. It is important to understand how each language is uniformly represented in experiments and the impact on the overall performance. \n4. Although the proposed approach performs better than existing models, it may still be interesting to investigate how this method can benefit from other recent advancements such as self-supervised learning or transformers in multimodal learning. \n5. The paper primarily focuses on the MuAViC dataset. It would be beneficial to consider other datasets to explore the generalization of the proposed method across diverse data sources.\n\nquestions_for_the_authors: A. It is not clear in the methodology section how the trainable continuous embeddings (prompts) are generated and optimized during the fine-tuning process. Can such prompts be learned automatically, or do they require manual annotation? Please provide more details on this process to help readers better understand how these prompts contribute to the overall model.\nB. In the experiments, the proposed model is compared primarily with the work of Anwar et al. (2023). It would be beneficial to include comparisons with other relevant methods and show how the proposed approach performs in comparison to a broader range of existing methods in the field of multilingual AVSR. Are there other methods that could be included for comparison?\nC. The paper focuses on a dataset containing nine languages which may be considered heavily biased towards Indo-European languages or the languages with a relatively large amount of available data. How would the proposed method perform for languages with few data samples or for languages from very different linguistic families (e.g., tonal languages, agglutinative languages)? Are there plans to extend or validate the model on more diverse sets of languages in the future?\nD. The model has been tested in both clean and noisy environments. Although it shows improvement over the previous multilingual model, the performance seems to be lower in noisy environments. What are the possible reasons for the reduced performance, and could this be improved upon with further modifications to the model architecture or training strategy?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "kH1PQ9xgo2",
        "similarity": 0.6979,
        "coverage": 0.5385,
        "human_length": 363,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a single multilingual AVASR model for handling audio+visual input in multiple languages.  The model hypothesizes detected language in addition to transcript for spoken input.  It utilizes an encoder decoder architecture where the encoder is a pre-trained AV transformer model, and decoder is a multi-lingual transformer architecture.  A classifier is used for predicting the language of input speech.  Proposed model is trained/evaluated on the recently released MuAViC dataset.\n\nreasons_to_accept: - Formulation and empirical study of a single simple multi-lingual AVASR model\n\nreasons_to_reject: - In the results presented in Table 1, the baseline numbers from Anwar et al don\u2019t match those reported in Table 1.  Not sure which paper to look at for source of the reported Anwar et al. numbers.  Also, for a fair comparison of numbers in the Avg. column, especially to compare w/ Multilingual performance from Anwar et al, En results should not be included in Table 1.\n- The model includes a prompt, but the value of this component is not clear or empirically determined.  Are the prompts sometimes there and other times not?  What is the performance if the prompts are not included in the model training/test?\n- Paper lacks clarity / errors in places.  E.g.      + 013-014: \u201c \u2026 both label and nuance\u201d. Not clear what this means      + 014-016: \u201c \u2026 predict correct speech with correct label\u201d \u2026 is it predicting speech, or words? \n     + 054-057: \u201c \u2026 fine-tuning followed by pre-training\u201d or the other way around? \n     + 219: Does Eq (12) need a sum over all samples?  Otherwise \\gamma has no impact on the optimization. \n     + Table 1 caption mentions boldfaced or underlined letters, but there are no such letters/numbers in the table.\n\nquestions_for_the_authors: Please identify which results from Anwar et al. are cited in the paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "mN83MWpa1z",
        "similarity": 0.6647,
        "coverage": 0.7692,
        "human_length": 270,
        "human_text": "paper_topic_and_main_contributions: The paper deals with multilingual audio-visual speech recognition. The authors describe a method that extracts a set of audio-visual features and augments them with  embeddings which have been trained to optimize a language classification loss. Experiments are evaluated on a dataset of multilingual video recordings of TED speeches.\n\nreasons_to_accept: The paper describes experiments in the area of multi-lingual multimodal speech recognition and a publishes a dataset that can be used by others for that purpose.\n\nreasons_to_reject: Generally, I don't agree with the conclusions that the authors derive from their experimental results. For instance, the authors claim an average 11% WER reduction due to the proposed method compared to their previous model. However, this compares results on two different test sets - the proposed model has been evaluated on English data, too, while the previous model has no result for English test data. If you exclude the English test data from the comparison (apple to apple) the previous model achieves an Avg WER of 39.51% on clean data and the proposed model achieves an Avg. WER of 39.45% which is not really an improvement. Furthermore, I believe the proposed approach misses a comparison to a multi-task training baseline which includes a language classification loss.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "E7iy0aj33A",
        "length": 915,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel approach for multilingual audio-visual speech recognition tasks by developing a single model on a multilingual dataset. The primary problem being addressed in the paper is the limited success of current audio-visual speech recognition models in multilingual datasets, as they are mostly focused on monolingual data, particularly English. The paper aims to design a single model capable of effectively distinguishing the language and properly recognizing the input speech in multiple languages.\nThe main contributions of the paper are as follows: 1. Propose a single model framework for multilingual audio-visual speech recognition, inspired by the intuitive human language understanding process. The model is designed to determine the language of the input speech and recognize the speech correctly.\n2. Introduce an audio-visual guiding linguistic representation extractor by fine-tuning a largely pre-trained audio-visual representation model using prompts to provide language information. This allows the model to extract comprehensive linguistic information from the audio and video inputs.\n3. Address the issue of imbalanced language distribution in multilingual datasets by employing a weighted objective function that balances the distribution of language data while updating the loss during training. This aims to improve the performance of speech recognition in low-resource languages and prevent the model from being biased toward dominant languages.\n4. Validate the effectiveness of the proposed model by conducting experiments on MuAViC, a multilingual audio-visual corpus containing 9 languages. The results show that the proposed model outperforms the previous multilingual model with an average 11% WER reduction in a clean environment and an average 13% WER reduction in a noisy environment.\n\nreasons_to_accept: 1. Novelty: The approach of fine-tuning a largely pre-trained audio-visual representation model with language prompts is new and innovative, enabling the model to effectively recognize and transcribe speech in multiple languages.\n2. Robustness: The proposed model demonstrates performance improvements in low-resource languages and exhibits strong results in both clean and noisy environments. This has significant practical implications in real-world applications where speech recognition systems often face challenges in noisy conditions.\n3. Addressing Language Imbalance: The paper proposes an objective function weight to balance the distribution of language data, mitigating potential biases towards dominant languages while improving the recognition and transcription of low-resource languages.\n4. Comprehensive Experiments: The experiments are conducted on a recent and challenging dataset (MuAViC), consisting of 1200 hours of audio-visual speech data in 9 languages, providing a comprehensive evaluation of the model.\n5. Potential Impact: The proposed single model approach can reduce the need for language-specific models and improve the performance of speech recognition in low-resource languages. This would benefit the NLP community in developing more efficient and robust multilingual speech recognition systems.\n\nreasons_to_reject: 1. One weakness is the lack of an in-depth ablation study on the proposed approach. Some questions remain unanswered, such as the effectiveness of prompt fine-tuning in various settings and the sensitivity of the model to the choice of hyperparameters. \n2. The paper does not provide results for a setting where the language is unknown during inference. Dealing with an unknown language is a crucial aspect in real-world scenarios. \n3. The authors mention the imbalance in the dataset, but the exact distribution is not provided. It is important to understand how each language is uniformly represented in experiments and the impact on the overall performance. \n4. Although the proposed approach performs better than existing models, it may still be interesting to investigate how this method can benefit from other recent advancements such as self-supervised learning or transformers in multimodal learning. \n5. The paper primarily focuses on the MuAViC dataset. It would be beneficial to consider other datasets to explore the generalization of the proposed method across diverse data sources.\n\nquestions_for_the_authors: A. It is not clear in the methodology section how the trainable continuous embeddings (prompts) are generated and optimized during the fine-tuning process. Can such prompts be learned automatically, or do they require manual annotation? Please provide more details on this process to help readers better understand how these prompts contribute to the overall model.\nB. In the experiments, the proposed model is compared primarily with the work of Anwar et al. (2023). It would be beneficial to include comparisons with other relevant methods and show how the proposed approach performs in comparison to a broader range of existing methods in the field of multilingual AVSR. Are there other methods that could be included for comparison?\nC. The paper focuses on a dataset containing nine languages which may be considered heavily biased towards Indo-European languages or the languages with a relatively large amount of available data. How would the proposed method perform for languages with few data samples or for languages from very different linguistic families (e.g., tonal languages, agglutinative languages)? Are there plans to extend or validate the model on more diverse sets of languages in the future?\nD. The model has been tested in both clean and noisy environments. Although it shows improvement over the previous multilingual model, the performance seems to be lower in noisy environments. What are the possible reasons for the reduced performance, and could this be improved upon with further modifications to the model architecture or training strategy?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "kH1PQ9xgo2",
        "length": 363,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a single multilingual AVASR model for handling audio+visual input in multiple languages.  The model hypothesizes detected language in addition to transcript for spoken input.  It utilizes an encoder decoder architecture where the encoder is a pre-trained AV transformer model, and decoder is a multi-lingual transformer architecture.  A classifier is used for predicting the language of input speech.  Proposed model is trained/evaluated on the recently released MuAViC dataset.\n\nreasons_to_accept: - Formulation and empirical study of a single simple multi-lingual AVASR model\n\nreasons_to_reject: - In the results presented in Table 1, the baseline numbers from Anwar et al don\u2019t match those reported in Table 1.  Not sure which paper to look at for source of the reported Anwar et al. numbers.  Also, for a fair comparison of numbers in the Avg. column, especially to compare w/ Multilingual performance from Anwar et al, En results should not be included in Table 1.\n- The model includes a prompt, but the value of this component is not clear or empirically determined.  Are the prompts sometimes there and other times not?  What is the performance if the prompts are not included in the model training/test?\n- Paper lacks clarity / errors in places.  E.g.      + 013-014: \u201c \u2026 both label and nuance\u201d. Not clear what this means      + 014-016: \u201c \u2026 predict correct speech with correct label\u201d \u2026 is it predicting speech, or words? \n     + 054-057: \u201c \u2026 fine-tuning followed by pre-training\u201d or the other way around? \n     + 219: Does Eq (12) need a sum over all samples?  Otherwise \\gamma has no impact on the optimization. \n     + Table 1 caption mentions boldfaced or underlined letters, but there are no such letters/numbers in the table.\n\nquestions_for_the_authors: Please identify which results from Anwar et al. are cited in the paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "mN83MWpa1z",
        "length": 270,
        "human_text": "paper_topic_and_main_contributions: The paper deals with multilingual audio-visual speech recognition. The authors describe a method that extracts a set of audio-visual features and augments them with  embeddings which have been trained to optimize a language classification loss. Experiments are evaluated on a dataset of multilingual video recordings of TED speeches.\n\nreasons_to_accept: The paper describes experiments in the area of multi-lingual multimodal speech recognition and a publishes a dataset that can be used by others for that purpose.\n\nreasons_to_reject: Generally, I don't agree with the conclusions that the authors derive from their experimental results. For instance, the authors claim an average 11% WER reduction due to the proposed method compared to their previous model. However, this compares results on two different test sets - the proposed model has been evaluated on English data, too, while the previous model has no result for English test data. If you exclude the English test data from the comparison (apple to apple) the previous model achieves an Avg WER of 39.51% on clean data and the proposed model achieves an Avg. WER of 39.45% which is not really an improvement. Furthermore, I believe the proposed approach misses a comparison to a multi-task training baseline which includes a language classification loss.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "138_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_138_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6919,
      "max_similarity": 0.7172,
      "avg_coverage": 0.5070333333333333,
      "max_coverage": 0.6923
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 579,
      "avg_human_length": 516.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 10,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "E7iy0aj33A",
        "similarity": 0.7172,
        "coverage": 0.2903,
        "human_length": 915,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel approach for multilingual audio-visual speech recognition tasks by developing a single model on a multilingual dataset. The primary problem being addressed in the paper is the limited success of current audio-visual speech recognition models in multilingual datasets, as they are mostly focused on monolingual data, particularly English. The paper aims to design a single model capable of effectively distinguishing the language and properly recognizing the input speech in multiple languages.\nThe main contributions of the paper are as follows: 1. Propose a single model framework for multilingual audio-visual speech recognition, inspired by the intuitive human language understanding process. The model is designed to determine the language of the input speech and recognize the speech correctly.\n2. Introduce an audio-visual guiding linguistic representation extractor by fine-tuning a largely pre-trained audio-visual representation model using prompts to provide language information. This allows the model to extract comprehensive linguistic information from the audio and video inputs.\n3. Address the issue of imbalanced language distribution in multilingual datasets by employing a weighted objective function that balances the distribution of language data while updating the loss during training. This aims to improve the performance of speech recognition in low-resource languages and prevent the model from being biased toward dominant languages.\n4. Validate the effectiveness of the proposed model by conducting experiments on MuAViC, a multilingual audio-visual corpus containing 9 languages. The results show that the proposed model outperforms the previous multilingual model with an average 11% WER reduction in a clean environment and an average 13% WER reduction in a noisy environment.\n\nreasons_to_accept: 1. Novelty: The approach of fine-tuning a largely pre-trained audio-visual representation model with language prompts is new and innovative, enabling the model to effectively recognize and transcribe speech in multiple languages.\n2. Robustness: The proposed model demonstrates performance improvements in low-resource languages and exhibits strong results in both clean and noisy environments. This has significant practical implications in real-world applications where speech recognition systems often face challenges in noisy conditions.\n3. Addressing Language Imbalance: The paper proposes an objective function weight to balance the distribution of language data, mitigating potential biases towards dominant languages while improving the recognition and transcription of low-resource languages.\n4. Comprehensive Experiments: The experiments are conducted on a recent and challenging dataset (MuAViC), consisting of 1200 hours of audio-visual speech data in 9 languages, providing a comprehensive evaluation of the model.\n5. Potential Impact: The proposed single model approach can reduce the need for language-specific models and improve the performance of speech recognition in low-resource languages. This would benefit the NLP community in developing more efficient and robust multilingual speech recognition systems.\n\nreasons_to_reject: 1. One weakness is the lack of an in-depth ablation study on the proposed approach. Some questions remain unanswered, such as the effectiveness of prompt fine-tuning in various settings and the sensitivity of the model to the choice of hyperparameters. \n2. The paper does not provide results for a setting where the language is unknown during inference. Dealing with an unknown language is a crucial aspect in real-world scenarios. \n3. The authors mention the imbalance in the dataset, but the exact distribution is not provided. It is important to understand how each language is uniformly represented in experiments and the impact on the overall performance. \n4. Although the proposed approach performs better than existing models, it may still be interesting to investigate how this method can benefit from other recent advancements such as self-supervised learning or transformers in multimodal learning. \n5. The paper primarily focuses on the MuAViC dataset. It would be beneficial to consider other datasets to explore the generalization of the proposed method across diverse data sources.\n\nquestions_for_the_authors: A. It is not clear in the methodology section how the trainable continuous embeddings (prompts) are generated and optimized during the fine-tuning process. Can such prompts be learned automatically, or do they require manual annotation? Please provide more details on this process to help readers better understand how these prompts contribute to the overall model.\nB. In the experiments, the proposed model is compared primarily with the work of Anwar et al. (2023). It would be beneficial to include comparisons with other relevant methods and show how the proposed approach performs in comparison to a broader range of existing methods in the field of multilingual AVSR. Are there other methods that could be included for comparison?\nC. The paper focuses on a dataset containing nine languages which may be considered heavily biased towards Indo-European languages or the languages with a relatively large amount of available data. How would the proposed method perform for languages with few data samples or for languages from very different linguistic families (e.g., tonal languages, agglutinative languages)? Are there plans to extend or validate the model on more diverse sets of languages in the future?\nD. The model has been tested in both clean and noisy environments. Although it shows improvement over the previous multilingual model, the performance seems to be lower in noisy environments. What are the possible reasons for the reduced performance, and could this be improved upon with further modifications to the model architecture or training strategy?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "kH1PQ9xgo2",
        "similarity": 0.6964,
        "coverage": 0.5385,
        "human_length": 363,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a single multilingual AVASR model for handling audio+visual input in multiple languages.  The model hypothesizes detected language in addition to transcript for spoken input.  It utilizes an encoder decoder architecture where the encoder is a pre-trained AV transformer model, and decoder is a multi-lingual transformer architecture.  A classifier is used for predicting the language of input speech.  Proposed model is trained/evaluated on the recently released MuAViC dataset.\n\nreasons_to_accept: - Formulation and empirical study of a single simple multi-lingual AVASR model\n\nreasons_to_reject: - In the results presented in Table 1, the baseline numbers from Anwar et al don\u2019t match those reported in Table 1.  Not sure which paper to look at for source of the reported Anwar et al. numbers.  Also, for a fair comparison of numbers in the Avg. column, especially to compare w/ Multilingual performance from Anwar et al, En results should not be included in Table 1.\n- The model includes a prompt, but the value of this component is not clear or empirically determined.  Are the prompts sometimes there and other times not?  What is the performance if the prompts are not included in the model training/test?\n- Paper lacks clarity / errors in places.  E.g.      + 013-014: \u201c \u2026 both label and nuance\u201d. Not clear what this means      + 014-016: \u201c \u2026 predict correct speech with correct label\u201d \u2026 is it predicting speech, or words? \n     + 054-057: \u201c \u2026 fine-tuning followed by pre-training\u201d or the other way around? \n     + 219: Does Eq (12) need a sum over all samples?  Otherwise \\gamma has no impact on the optimization. \n     + Table 1 caption mentions boldfaced or underlined letters, but there are no such letters/numbers in the table.\n\nquestions_for_the_authors: Please identify which results from Anwar et al. are cited in the paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "mN83MWpa1z",
        "similarity": 0.6621,
        "coverage": 0.6923,
        "human_length": 270,
        "human_text": "paper_topic_and_main_contributions: The paper deals with multilingual audio-visual speech recognition. The authors describe a method that extracts a set of audio-visual features and augments them with  embeddings which have been trained to optimize a language classification loss. Experiments are evaluated on a dataset of multilingual video recordings of TED speeches.\n\nreasons_to_accept: The paper describes experiments in the area of multi-lingual multimodal speech recognition and a publishes a dataset that can be used by others for that purpose.\n\nreasons_to_reject: Generally, I don't agree with the conclusions that the authors derive from their experimental results. For instance, the authors claim an average 11% WER reduction due to the proposed method compared to their previous model. However, this compares results on two different test sets - the proposed model has been evaluated on English data, too, while the previous model has no result for English test data. If you exclude the English test data from the comparison (apple to apple) the previous model achieves an Avg WER of 39.51% on clean data and the proposed model achieves an Avg. WER of 39.45% which is not really an improvement. Furthermore, I believe the proposed approach misses a comparison to a multi-task training baseline which includes a language classification loss.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "E7iy0aj33A",
        "length": 915,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel approach for multilingual audio-visual speech recognition tasks by developing a single model on a multilingual dataset. The primary problem being addressed in the paper is the limited success of current audio-visual speech recognition models in multilingual datasets, as they are mostly focused on monolingual data, particularly English. The paper aims to design a single model capable of effectively distinguishing the language and properly recognizing the input speech in multiple languages.\nThe main contributions of the paper are as follows: 1. Propose a single model framework for multilingual audio-visual speech recognition, inspired by the intuitive human language understanding process. The model is designed to determine the language of the input speech and recognize the speech correctly.\n2. Introduce an audio-visual guiding linguistic representation extractor by fine-tuning a largely pre-trained audio-visual representation model using prompts to provide language information. This allows the model to extract comprehensive linguistic information from the audio and video inputs.\n3. Address the issue of imbalanced language distribution in multilingual datasets by employing a weighted objective function that balances the distribution of language data while updating the loss during training. This aims to improve the performance of speech recognition in low-resource languages and prevent the model from being biased toward dominant languages.\n4. Validate the effectiveness of the proposed model by conducting experiments on MuAViC, a multilingual audio-visual corpus containing 9 languages. The results show that the proposed model outperforms the previous multilingual model with an average 11% WER reduction in a clean environment and an average 13% WER reduction in a noisy environment.\n\nreasons_to_accept: 1. Novelty: The approach of fine-tuning a largely pre-trained audio-visual representation model with language prompts is new and innovative, enabling the model to effectively recognize and transcribe speech in multiple languages.\n2. Robustness: The proposed model demonstrates performance improvements in low-resource languages and exhibits strong results in both clean and noisy environments. This has significant practical implications in real-world applications where speech recognition systems often face challenges in noisy conditions.\n3. Addressing Language Imbalance: The paper proposes an objective function weight to balance the distribution of language data, mitigating potential biases towards dominant languages while improving the recognition and transcription of low-resource languages.\n4. Comprehensive Experiments: The experiments are conducted on a recent and challenging dataset (MuAViC), consisting of 1200 hours of audio-visual speech data in 9 languages, providing a comprehensive evaluation of the model.\n5. Potential Impact: The proposed single model approach can reduce the need for language-specific models and improve the performance of speech recognition in low-resource languages. This would benefit the NLP community in developing more efficient and robust multilingual speech recognition systems.\n\nreasons_to_reject: 1. One weakness is the lack of an in-depth ablation study on the proposed approach. Some questions remain unanswered, such as the effectiveness of prompt fine-tuning in various settings and the sensitivity of the model to the choice of hyperparameters. \n2. The paper does not provide results for a setting where the language is unknown during inference. Dealing with an unknown language is a crucial aspect in real-world scenarios. \n3. The authors mention the imbalance in the dataset, but the exact distribution is not provided. It is important to understand how each language is uniformly represented in experiments and the impact on the overall performance. \n4. Although the proposed approach performs better than existing models, it may still be interesting to investigate how this method can benefit from other recent advancements such as self-supervised learning or transformers in multimodal learning. \n5. The paper primarily focuses on the MuAViC dataset. It would be beneficial to consider other datasets to explore the generalization of the proposed method across diverse data sources.\n\nquestions_for_the_authors: A. It is not clear in the methodology section how the trainable continuous embeddings (prompts) are generated and optimized during the fine-tuning process. Can such prompts be learned automatically, or do they require manual annotation? Please provide more details on this process to help readers better understand how these prompts contribute to the overall model.\nB. In the experiments, the proposed model is compared primarily with the work of Anwar et al. (2023). It would be beneficial to include comparisons with other relevant methods and show how the proposed approach performs in comparison to a broader range of existing methods in the field of multilingual AVSR. Are there other methods that could be included for comparison?\nC. The paper focuses on a dataset containing nine languages which may be considered heavily biased towards Indo-European languages or the languages with a relatively large amount of available data. How would the proposed method perform for languages with few data samples or for languages from very different linguistic families (e.g., tonal languages, agglutinative languages)? Are there plans to extend or validate the model on more diverse sets of languages in the future?\nD. The model has been tested in both clean and noisy environments. Although it shows improvement over the previous multilingual model, the performance seems to be lower in noisy environments. What are the possible reasons for the reduced performance, and could this be improved upon with further modifications to the model architecture or training strategy?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "kH1PQ9xgo2",
        "length": 363,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a single multilingual AVASR model for handling audio+visual input in multiple languages.  The model hypothesizes detected language in addition to transcript for spoken input.  It utilizes an encoder decoder architecture where the encoder is a pre-trained AV transformer model, and decoder is a multi-lingual transformer architecture.  A classifier is used for predicting the language of input speech.  Proposed model is trained/evaluated on the recently released MuAViC dataset.\n\nreasons_to_accept: - Formulation and empirical study of a single simple multi-lingual AVASR model\n\nreasons_to_reject: - In the results presented in Table 1, the baseline numbers from Anwar et al don\u2019t match those reported in Table 1.  Not sure which paper to look at for source of the reported Anwar et al. numbers.  Also, for a fair comparison of numbers in the Avg. column, especially to compare w/ Multilingual performance from Anwar et al, En results should not be included in Table 1.\n- The model includes a prompt, but the value of this component is not clear or empirically determined.  Are the prompts sometimes there and other times not?  What is the performance if the prompts are not included in the model training/test?\n- Paper lacks clarity / errors in places.  E.g.      + 013-014: \u201c \u2026 both label and nuance\u201d. Not clear what this means      + 014-016: \u201c \u2026 predict correct speech with correct label\u201d \u2026 is it predicting speech, or words? \n     + 054-057: \u201c \u2026 fine-tuning followed by pre-training\u201d or the other way around? \n     + 219: Does Eq (12) need a sum over all samples?  Otherwise \\gamma has no impact on the optimization. \n     + Table 1 caption mentions boldfaced or underlined letters, but there are no such letters/numbers in the table.\n\nquestions_for_the_authors: Please identify which results from Anwar et al. are cited in the paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "mN83MWpa1z",
        "length": 270,
        "human_text": "paper_topic_and_main_contributions: The paper deals with multilingual audio-visual speech recognition. The authors describe a method that extracts a set of audio-visual features and augments them with  embeddings which have been trained to optimize a language classification loss. Experiments are evaluated on a dataset of multilingual video recordings of TED speeches.\n\nreasons_to_accept: The paper describes experiments in the area of multi-lingual multimodal speech recognition and a publishes a dataset that can be used by others for that purpose.\n\nreasons_to_reject: Generally, I don't agree with the conclusions that the authors derive from their experimental results. For instance, the authors claim an average 11% WER reduction due to the proposed method compared to their previous model. However, this compares results on two different test sets - the proposed model has been evaluated on English data, too, while the previous model has no result for English test data. If you exclude the English test data from the comparison (apple to apple) the previous model achieves an Avg WER of 39.51% on clean data and the proposed model achieves an Avg. WER of 39.45% which is not really an improvement. Furthermore, I believe the proposed approach misses a comparison to a multi-task training baseline which includes a language classification loss.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "13_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_13_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7288,
      "max_similarity": 0.7372,
      "avg_coverage": 0.5730000000000001,
      "max_coverage": 0.8333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 488,
      "avg_human_length": 344.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "FvDY2GU1EB",
        "similarity": 0.7222,
        "coverage": 0.8333,
        "human_length": 253,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the task of \"meme captioning\" which involves understanding memes and their visual metaphors while interpreting the text associated with the meme. The authors release a new dataset called MemeCap. The authors conduct extensive experiments using state-of-the-art Vision and Language (VL) models to evaluate their performance on the meme captioning task.\n\nreasons_to_accept: This paper introduces an innovative dataset and conducts exhaustive experiments to meticulously evaluate the performance of cutting-edge models, rendering it immensely valuable for future research endeavors.\n\nreasons_to_reject: After reading this paper, it reminded me of one of ACL23's best paper: 'Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest.' The MemeCap dataset shares a similar concept with this influential ACL paper. However, a notable concern is that this paper fails to cite the aforementioned ACL paper, which is considered unacceptable in scholarly practices. Furthermore, the passage does not delve into the distinctions between these two works, leaving an important aspect unaddressed.\n\nmissing_references: *Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest* ACL23 best paper\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "9or1vL6qm2",
        "similarity": 0.727,
        "coverage": 0.6154,
        "human_length": 220,
        "human_text": "paper_topic_and_main_contributions: The paper proposed a new vision-language dataset of meme pictures and corresponding captions. The scale of the dataset is 6.3K. For each sample, a meme picture, literal image captions, visual metaphors and meme captions are provided. In addition, the authors tested a series of SOTA vision-language and language models on the dataset, showing the lack of meme understanding ability of SOTA models.\n\nreasons_to_accept: An novel meme caption dataset to help improve the humorous understanding ability of downstream large models.\n\nreasons_to_reject: There's only the MiniGPT4 model be tested under fine-tuning setting. The other two models, Flamingo and Llama, are not tested under fine-tuning setting.\n\nquestions_for_the_authors: A: Why not provide full-training setting on all models? \nB: It is interesting to see that Llama, a pure language model, can achieve similar or even better performance than VL models, shown in Table 2. Is there any further explorations on this part?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "Qwc59hPK6e",
        "similarity": 0.7372,
        "coverage": 0.2703,
        "human_length": 560,
        "human_text": "paper_topic_and_main_contributions: This work proposes a task of meme captioning and release a new dataset, MEMECAP. The dataset contains memes along with the title, the meme captions, the literal image captions, and the visual metaphors. It utilizes recent Visual-and-Language models to this dataset to analyze their performances on the task of meme captioning.\n\nreasons_to_accept: 1. An interesting dataset with meme, title, literal image caption, visual metaphors, and meme caption.  2. Both automatic evaluation and human evaluations on the task of meme captioning are provided.\n\nreasons_to_reject: 1. As a dataset collected by crowdsourcing, there is almost no quality analysis on the collected data (only in the part of human evaluation, using 30 memes, in Section 5.2, the results of \"human\").  2. There are about 6.3K memes in the datasets, but the authors only utilize 28 memes for the dataset analysis in Section 3.4, and 30 memes for the baseline evaluations in Section 5.2. The numbers of memes used in these evaluations are too small. The evaluations are not convincing enough.  3. The dataset is somewhat similar to (and somewhat different from) the dataset proposed in \"Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest\", which gives an explanation (can be regarded as a caption) to a cartoon to understand the humor (potential meaning, similar to metaphor) of the cartoon (similar to meme). Although there are some differences, e.g., one is metaphor and the other is humor, it makes this work less exciting to me.  Other minor comments: 4. In the experiments, this paper uses MiniGPT4 based on LLaMa-13B (L288), but also uses LLaMA-7B (L302). Because the authors want to compare the settings of \"without accessing to image, LLaMa\" and \"with accessing to image, MiniGPT4\", it's better to utilize the same model size.\n\nquestions_for_the_authors: 1. The performance of \"MiniGPT4 fine-tuned\" is worse than \"MiniGPT4 zero-shot\". In L437~439, the authors think that it is because \"the frozen language and vision model may not have enough information about memes\". This claim is not exactly rational. This claim is more proper if the observation is that \"MiniGPT4 zero-shot\" is bad and \"MiniGPT4 fine-tuned\" is slightly better than \"MiniGPT4 zero-shot\".  2. The most important contribution of this work is the dataset with both meme caption and visual metaphor, while there are already existing datasets for visual metaphor. I am still not so clear that whether the task of meme captioning is strongly required, it seems that there is a large overlap on the task of meme captioning and visual metaphor.  3 (Minor, not important). The results/values with \"+\" in Table 3 (i.e., removing part of input can improve the performance) are not discussed.\n\nmissing_references: 1. Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest, ACL 2023.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "FvDY2GU1EB",
        "length": 253,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the task of \"meme captioning\" which involves understanding memes and their visual metaphors while interpreting the text associated with the meme. The authors release a new dataset called MemeCap. The authors conduct extensive experiments using state-of-the-art Vision and Language (VL) models to evaluate their performance on the meme captioning task.\n\nreasons_to_accept: This paper introduces an innovative dataset and conducts exhaustive experiments to meticulously evaluate the performance of cutting-edge models, rendering it immensely valuable for future research endeavors.\n\nreasons_to_reject: After reading this paper, it reminded me of one of ACL23's best paper: 'Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest.' The MemeCap dataset shares a similar concept with this influential ACL paper. However, a notable concern is that this paper fails to cite the aforementioned ACL paper, which is considered unacceptable in scholarly practices. Furthermore, the passage does not delve into the distinctions between these two works, leaving an important aspect unaddressed.\n\nmissing_references: *Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest* ACL23 best paper\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "9or1vL6qm2",
        "length": 220,
        "human_text": "paper_topic_and_main_contributions: The paper proposed a new vision-language dataset of meme pictures and corresponding captions. The scale of the dataset is 6.3K. For each sample, a meme picture, literal image captions, visual metaphors and meme captions are provided. In addition, the authors tested a series of SOTA vision-language and language models on the dataset, showing the lack of meme understanding ability of SOTA models.\n\nreasons_to_accept: An novel meme caption dataset to help improve the humorous understanding ability of downstream large models.\n\nreasons_to_reject: There's only the MiniGPT4 model be tested under fine-tuning setting. The other two models, Flamingo and Llama, are not tested under fine-tuning setting.\n\nquestions_for_the_authors: A: Why not provide full-training setting on all models? \nB: It is interesting to see that Llama, a pure language model, can achieve similar or even better performance than VL models, shown in Table 2. Is there any further explorations on this part?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "Qwc59hPK6e",
        "length": 560,
        "human_text": "paper_topic_and_main_contributions: This work proposes a task of meme captioning and release a new dataset, MEMECAP. The dataset contains memes along with the title, the meme captions, the literal image captions, and the visual metaphors. It utilizes recent Visual-and-Language models to this dataset to analyze their performances on the task of meme captioning.\n\nreasons_to_accept: 1. An interesting dataset with meme, title, literal image caption, visual metaphors, and meme caption.  2. Both automatic evaluation and human evaluations on the task of meme captioning are provided.\n\nreasons_to_reject: 1. As a dataset collected by crowdsourcing, there is almost no quality analysis on the collected data (only in the part of human evaluation, using 30 memes, in Section 5.2, the results of \"human\").  2. There are about 6.3K memes in the datasets, but the authors only utilize 28 memes for the dataset analysis in Section 3.4, and 30 memes for the baseline evaluations in Section 5.2. The numbers of memes used in these evaluations are too small. The evaluations are not convincing enough.  3. The dataset is somewhat similar to (and somewhat different from) the dataset proposed in \"Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest\", which gives an explanation (can be regarded as a caption) to a cartoon to understand the humor (potential meaning, similar to metaphor) of the cartoon (similar to meme). Although there are some differences, e.g., one is metaphor and the other is humor, it makes this work less exciting to me.  Other minor comments: 4. In the experiments, this paper uses MiniGPT4 based on LLaMa-13B (L288), but also uses LLaMA-7B (L302). Because the authors want to compare the settings of \"without accessing to image, LLaMa\" and \"with accessing to image, MiniGPT4\", it's better to utilize the same model size.\n\nquestions_for_the_authors: 1. The performance of \"MiniGPT4 fine-tuned\" is worse than \"MiniGPT4 zero-shot\". In L437~439, the authors think that it is because \"the frozen language and vision model may not have enough information about memes\". This claim is not exactly rational. This claim is more proper if the observation is that \"MiniGPT4 zero-shot\" is bad and \"MiniGPT4 fine-tuned\" is slightly better than \"MiniGPT4 zero-shot\".  2. The most important contribution of this work is the dataset with both meme caption and visual metaphor, while there are already existing datasets for visual metaphor. I am still not so clear that whether the task of meme captioning is strongly required, it seems that there is a large overlap on the task of meme captioning and visual metaphor.  3 (Minor, not important). The results/values with \"+\" in Table 3 (i.e., removing part of input can improve the performance) are not discussed.\n\nmissing_references: 1. Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest, ACL 2023.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "13_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_13_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7275999999999999,
      "max_similarity": 0.7392,
      "avg_coverage": 0.5730000000000001,
      "max_coverage": 0.8333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 469,
      "avg_human_length": 344.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 7,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "FvDY2GU1EB",
        "similarity": 0.7183,
        "coverage": 0.8333,
        "human_length": 253,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the task of \"meme captioning\" which involves understanding memes and their visual metaphors while interpreting the text associated with the meme. The authors release a new dataset called MemeCap. The authors conduct extensive experiments using state-of-the-art Vision and Language (VL) models to evaluate their performance on the meme captioning task.\n\nreasons_to_accept: This paper introduces an innovative dataset and conducts exhaustive experiments to meticulously evaluate the performance of cutting-edge models, rendering it immensely valuable for future research endeavors.\n\nreasons_to_reject: After reading this paper, it reminded me of one of ACL23's best paper: 'Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest.' The MemeCap dataset shares a similar concept with this influential ACL paper. However, a notable concern is that this paper fails to cite the aforementioned ACL paper, which is considered unacceptable in scholarly practices. Furthermore, the passage does not delve into the distinctions between these two works, leaving an important aspect unaddressed.\n\nmissing_references: *Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest* ACL23 best paper\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "9or1vL6qm2",
        "similarity": 0.7253,
        "coverage": 0.6154,
        "human_length": 220,
        "human_text": "paper_topic_and_main_contributions: The paper proposed a new vision-language dataset of meme pictures and corresponding captions. The scale of the dataset is 6.3K. For each sample, a meme picture, literal image captions, visual metaphors and meme captions are provided. In addition, the authors tested a series of SOTA vision-language and language models on the dataset, showing the lack of meme understanding ability of SOTA models.\n\nreasons_to_accept: An novel meme caption dataset to help improve the humorous understanding ability of downstream large models.\n\nreasons_to_reject: There's only the MiniGPT4 model be tested under fine-tuning setting. The other two models, Flamingo and Llama, are not tested under fine-tuning setting.\n\nquestions_for_the_authors: A: Why not provide full-training setting on all models? \nB: It is interesting to see that Llama, a pure language model, can achieve similar or even better performance than VL models, shown in Table 2. Is there any further explorations on this part?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "Qwc59hPK6e",
        "similarity": 0.7392,
        "coverage": 0.2703,
        "human_length": 560,
        "human_text": "paper_topic_and_main_contributions: This work proposes a task of meme captioning and release a new dataset, MEMECAP. The dataset contains memes along with the title, the meme captions, the literal image captions, and the visual metaphors. It utilizes recent Visual-and-Language models to this dataset to analyze their performances on the task of meme captioning.\n\nreasons_to_accept: 1. An interesting dataset with meme, title, literal image caption, visual metaphors, and meme caption.  2. Both automatic evaluation and human evaluations on the task of meme captioning are provided.\n\nreasons_to_reject: 1. As a dataset collected by crowdsourcing, there is almost no quality analysis on the collected data (only in the part of human evaluation, using 30 memes, in Section 5.2, the results of \"human\").  2. There are about 6.3K memes in the datasets, but the authors only utilize 28 memes for the dataset analysis in Section 3.4, and 30 memes for the baseline evaluations in Section 5.2. The numbers of memes used in these evaluations are too small. The evaluations are not convincing enough.  3. The dataset is somewhat similar to (and somewhat different from) the dataset proposed in \"Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest\", which gives an explanation (can be regarded as a caption) to a cartoon to understand the humor (potential meaning, similar to metaphor) of the cartoon (similar to meme). Although there are some differences, e.g., one is metaphor and the other is humor, it makes this work less exciting to me.  Other minor comments: 4. In the experiments, this paper uses MiniGPT4 based on LLaMa-13B (L288), but also uses LLaMA-7B (L302). Because the authors want to compare the settings of \"without accessing to image, LLaMa\" and \"with accessing to image, MiniGPT4\", it's better to utilize the same model size.\n\nquestions_for_the_authors: 1. The performance of \"MiniGPT4 fine-tuned\" is worse than \"MiniGPT4 zero-shot\". In L437~439, the authors think that it is because \"the frozen language and vision model may not have enough information about memes\". This claim is not exactly rational. This claim is more proper if the observation is that \"MiniGPT4 zero-shot\" is bad and \"MiniGPT4 fine-tuned\" is slightly better than \"MiniGPT4 zero-shot\".  2. The most important contribution of this work is the dataset with both meme caption and visual metaphor, while there are already existing datasets for visual metaphor. I am still not so clear that whether the task of meme captioning is strongly required, it seems that there is a large overlap on the task of meme captioning and visual metaphor.  3 (Minor, not important). The results/values with \"+\" in Table 3 (i.e., removing part of input can improve the performance) are not discussed.\n\nmissing_references: 1. Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest, ACL 2023.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "FvDY2GU1EB",
        "length": 253,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the task of \"meme captioning\" which involves understanding memes and their visual metaphors while interpreting the text associated with the meme. The authors release a new dataset called MemeCap. The authors conduct extensive experiments using state-of-the-art Vision and Language (VL) models to evaluate their performance on the meme captioning task.\n\nreasons_to_accept: This paper introduces an innovative dataset and conducts exhaustive experiments to meticulously evaluate the performance of cutting-edge models, rendering it immensely valuable for future research endeavors.\n\nreasons_to_reject: After reading this paper, it reminded me of one of ACL23's best paper: 'Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest.' The MemeCap dataset shares a similar concept with this influential ACL paper. However, a notable concern is that this paper fails to cite the aforementioned ACL paper, which is considered unacceptable in scholarly practices. Furthermore, the passage does not delve into the distinctions between these two works, leaving an important aspect unaddressed.\n\nmissing_references: *Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest* ACL23 best paper\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "9or1vL6qm2",
        "length": 220,
        "human_text": "paper_topic_and_main_contributions: The paper proposed a new vision-language dataset of meme pictures and corresponding captions. The scale of the dataset is 6.3K. For each sample, a meme picture, literal image captions, visual metaphors and meme captions are provided. In addition, the authors tested a series of SOTA vision-language and language models on the dataset, showing the lack of meme understanding ability of SOTA models.\n\nreasons_to_accept: An novel meme caption dataset to help improve the humorous understanding ability of downstream large models.\n\nreasons_to_reject: There's only the MiniGPT4 model be tested under fine-tuning setting. The other two models, Flamingo and Llama, are not tested under fine-tuning setting.\n\nquestions_for_the_authors: A: Why not provide full-training setting on all models? \nB: It is interesting to see that Llama, a pure language model, can achieve similar or even better performance than VL models, shown in Table 2. Is there any further explorations on this part?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "Qwc59hPK6e",
        "length": 560,
        "human_text": "paper_topic_and_main_contributions: This work proposes a task of meme captioning and release a new dataset, MEMECAP. The dataset contains memes along with the title, the meme captions, the literal image captions, and the visual metaphors. It utilizes recent Visual-and-Language models to this dataset to analyze their performances on the task of meme captioning.\n\nreasons_to_accept: 1. An interesting dataset with meme, title, literal image caption, visual metaphors, and meme caption.  2. Both automatic evaluation and human evaluations on the task of meme captioning are provided.\n\nreasons_to_reject: 1. As a dataset collected by crowdsourcing, there is almost no quality analysis on the collected data (only in the part of human evaluation, using 30 memes, in Section 5.2, the results of \"human\").  2. There are about 6.3K memes in the datasets, but the authors only utilize 28 memes for the dataset analysis in Section 3.4, and 30 memes for the baseline evaluations in Section 5.2. The numbers of memes used in these evaluations are too small. The evaluations are not convincing enough.  3. The dataset is somewhat similar to (and somewhat different from) the dataset proposed in \"Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest\", which gives an explanation (can be regarded as a caption) to a cartoon to understand the humor (potential meaning, similar to metaphor) of the cartoon (similar to meme). Although there are some differences, e.g., one is metaphor and the other is humor, it makes this work less exciting to me.  Other minor comments: 4. In the experiments, this paper uses MiniGPT4 based on LLaMa-13B (L288), but also uses LLaMA-7B (L302). Because the authors want to compare the settings of \"without accessing to image, LLaMa\" and \"with accessing to image, MiniGPT4\", it's better to utilize the same model size.\n\nquestions_for_the_authors: 1. The performance of \"MiniGPT4 fine-tuned\" is worse than \"MiniGPT4 zero-shot\". In L437~439, the authors think that it is because \"the frozen language and vision model may not have enough information about memes\". This claim is not exactly rational. This claim is more proper if the observation is that \"MiniGPT4 zero-shot\" is bad and \"MiniGPT4 fine-tuned\" is slightly better than \"MiniGPT4 zero-shot\".  2. The most important contribution of this work is the dataset with both meme caption and visual metaphor, while there are already existing datasets for visual metaphor. I am still not so clear that whether the task of meme captioning is strongly required, it seems that there is a large overlap on the task of meme captioning and visual metaphor.  3 (Minor, not important). The results/values with \"+\" in Table 3 (i.e., removing part of input can improve the performance) are not discussed.\n\nmissing_references: 1. Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest, ACL 2023.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "87_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_87_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6933333333333334,
      "max_similarity": 0.7066,
      "avg_coverage": 0.514,
      "max_coverage": 0.6087
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 375,
      "avg_human_length": 399.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 6,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "FTKjkahon6",
        "similarity": 0.7066,
        "coverage": 0.6087,
        "human_length": 341,
        "human_text": "paper_topic_and_main_contributions: This work proposes a framework named proto-lm to address the interpretability in large language models. For each category, several sentences are selected as prototypes. They estimate a similarity score for each input token with those tokens selected as prototypes and use the score as the attribution weights of tokens.\n\nreasons_to_accept: This paper studies a prototypical network approach to attribute the importance of words.\n\nreasons_to_reject: The approach this work proposes actually is more similar to the concept bottleneck models(https://arxiv.org/abs/2007.04612) . However, the approach proposed in the concept bottleneck is more elegant and neat. For example, in this paper, there is no clear explanation about how these prototype sentences are selected.  Prototypical Networks are initially proposed to address few-shot learning problems. So the problem of prototype selection is simple enough, which could be done by selecting samples from new classes.  In this work, these prototypes are selected automatically or by humans? Does it require different prototypes for different datasets? How about the completeness of the chosen prototypes? These concerns reflect the flexibility of the model but are not addressed in the experiments. The experiments are not strong as there are no other interpretable models as baselines to compare.  Meanwhile,  I suggest some comparison with other interpretable models could be conducted, e.g. concept bottleneck,  integrated-gradient(https://arxiv.org/abs/1703.01365). Experiments based on human annotation are hard to reproduce. I suggest the authors could try some faithfulness metrics proposed in this paper(https://arxiv.org/pdf/2204.05514.pdf). And clearly explain how to select prototypes.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "iRxOPpnJB6",
        "similarity": 0.6843,
        "coverage": 0.5,
        "human_length": 445,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a new approach to enhance the interpretability of language models by incorporating prototypical networks. The authors suggest using a pre-trained language model as a backbone and then learning a set of prototypes associated with the classification classes for a given task. These prototypes are then leveraged during inference to produce interpretable predictions. Importantly, this framework retains competitive performance compared to the original language model, making it possible to achieve explainable decisions with minimal computational overhead and without compromising efficacy.\n\nreasons_to_accept: The authors extensively evaluate the performance of the proposed methodology through a diverse set of experiments. They assess the effectiveness of the approach along with the quality of the explanations it generates. These explanations are further evaluated regarding their usefulness in identifying misclassifications and their alignment with human reasoning.\nFurthermore, the authors conduct a comprehensive ablation study, which delves into various aspects of tuning hyper-parameters and practical implementation of the framework. This study offers an intuitive understanding of how the method operates internally, shedding light on its inner workings and potential areas of improvement.\n\nreasons_to_reject: The authors primarily focus their experiments on sentiment classification and natural language inference (NLI) tasks. This emphasis prompts an important question concerning the method's generalizability and applicability across a broader spectrum of potential use cases. While the results for sentiment classification and NLI tasks are likely valuable, the effectiveness of the proposed approach in other domains and tasks remains to be explored. Therefore, further investigation into its adaptability to diverse applications is warranted to fully understand its scope and potential impact in various fields.\n\nquestions_for_the_authors: Question A: You mentioned the necessity of tuning the hyper-parameters such as $\\lambda$, $N$, and $K$ for each task. If we were to consider limited access to examples from a new task, have you observed the robustness of the hyper-parameters derived from the most relevant scenario? Is there a significant impact on performance if you were to employ the same set of hyper-parameters for all NLI tasks, for instance?\nQuestion B: In the main text, your focus seems to be on encoder/encoder-decoder architectures. I'm curious about the method's performance with decoder-only language models. Have you conducted any experiments to explore this aspect?\n\ntypos_grammar_style_and_presentation_improvements: Line 126: which **is** the Line 156: $Wc$ -> $w_c$ (since you are referring to the column-vector of $W_h$) Line 291: On the other **hand** Lines 473 / 478: \\citet Line 750: thsi -> this\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "mB9XuxV9nL",
        "similarity": 0.6891,
        "coverage": 0.4333,
        "human_length": 411,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a prototype-based method that adds a layer of interpretability to language models, and show its viability in a multi-class classification setting. The authors add an addition layer (dubbed the prototypical layer) on top of a pretriained LLM that learns representative examples of a particular class. They alter the standard cross-entropy loss function to include terms that make tighten a particular prototype group while placing it away from a group it doesn't belong to but is close in the embedded space. The modified loss function is comparable in performance with the original LLM, but can point to examples in the representative space that explain the model's choices.\n\nreasons_to_accept: 1. The authors extend current work  jointly learning prototypes representative of a class and making the final prediction of the model go through a calculation of similarities with representative protypical spaces. \n2. Experiments varying the size and uniqueness of prototypical space and its effect on model accuracy. \n3. Experiments to judge faithfulness with Comprehensiveness and Sufficiency scores, and a human study for simulatability.\n\nreasons_to_reject: 1. The framework proposed in the method is very similar in spirit to the proposed in Friedrich et. al. 2021 [Interactively Providing Explanations for Transformer Language Models], who use a modified loss function, although using two different networks for word and sentence level interpretability. The similarities in the modification of the loss function [Equation (1-2), https://arxiv.org/pdf/2110.02058.pdf] might be a bit too similar for the work to meet the criteria of novelty.\n\nquestions_for_the_authors: A. While choosing the size of the prototypical space, class imbalance, which is a major artifact of many NLP datasets, is never discussed. Could you add a discussion of how that interferes with choosing the size of the prototypical space ?\n\nmissing_references: The paper seems to miss two major related work -  1. Friedrich et al 2021: Interactively Providing Explanations for Transformer Language Models. The version I am referring to is https://arxiv.org/pdf/2110.02058.pdf.  2. Luo et. al. 2023, ACL: Prototype-Based Interpretability for Legal Citation Prediction. Although I can understand if the authors missed this one.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "FTKjkahon6",
        "length": 341,
        "human_text": "paper_topic_and_main_contributions: This work proposes a framework named proto-lm to address the interpretability in large language models. For each category, several sentences are selected as prototypes. They estimate a similarity score for each input token with those tokens selected as prototypes and use the score as the attribution weights of tokens.\n\nreasons_to_accept: This paper studies a prototypical network approach to attribute the importance of words.\n\nreasons_to_reject: The approach this work proposes actually is more similar to the concept bottleneck models(https://arxiv.org/abs/2007.04612) . However, the approach proposed in the concept bottleneck is more elegant and neat. For example, in this paper, there is no clear explanation about how these prototype sentences are selected.  Prototypical Networks are initially proposed to address few-shot learning problems. So the problem of prototype selection is simple enough, which could be done by selecting samples from new classes.  In this work, these prototypes are selected automatically or by humans? Does it require different prototypes for different datasets? How about the completeness of the chosen prototypes? These concerns reflect the flexibility of the model but are not addressed in the experiments. The experiments are not strong as there are no other interpretable models as baselines to compare.  Meanwhile,  I suggest some comparison with other interpretable models could be conducted, e.g. concept bottleneck,  integrated-gradient(https://arxiv.org/abs/1703.01365). Experiments based on human annotation are hard to reproduce. I suggest the authors could try some faithfulness metrics proposed in this paper(https://arxiv.org/pdf/2204.05514.pdf). And clearly explain how to select prototypes.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "iRxOPpnJB6",
        "length": 445,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a new approach to enhance the interpretability of language models by incorporating prototypical networks. The authors suggest using a pre-trained language model as a backbone and then learning a set of prototypes associated with the classification classes for a given task. These prototypes are then leveraged during inference to produce interpretable predictions. Importantly, this framework retains competitive performance compared to the original language model, making it possible to achieve explainable decisions with minimal computational overhead and without compromising efficacy.\n\nreasons_to_accept: The authors extensively evaluate the performance of the proposed methodology through a diverse set of experiments. They assess the effectiveness of the approach along with the quality of the explanations it generates. These explanations are further evaluated regarding their usefulness in identifying misclassifications and their alignment with human reasoning.\nFurthermore, the authors conduct a comprehensive ablation study, which delves into various aspects of tuning hyper-parameters and practical implementation of the framework. This study offers an intuitive understanding of how the method operates internally, shedding light on its inner workings and potential areas of improvement.\n\nreasons_to_reject: The authors primarily focus their experiments on sentiment classification and natural language inference (NLI) tasks. This emphasis prompts an important question concerning the method's generalizability and applicability across a broader spectrum of potential use cases. While the results for sentiment classification and NLI tasks are likely valuable, the effectiveness of the proposed approach in other domains and tasks remains to be explored. Therefore, further investigation into its adaptability to diverse applications is warranted to fully understand its scope and potential impact in various fields.\n\nquestions_for_the_authors: Question A: You mentioned the necessity of tuning the hyper-parameters such as $\\lambda$, $N$, and $K$ for each task. If we were to consider limited access to examples from a new task, have you observed the robustness of the hyper-parameters derived from the most relevant scenario? Is there a significant impact on performance if you were to employ the same set of hyper-parameters for all NLI tasks, for instance?\nQuestion B: In the main text, your focus seems to be on encoder/encoder-decoder architectures. I'm curious about the method's performance with decoder-only language models. Have you conducted any experiments to explore this aspect?\n\ntypos_grammar_style_and_presentation_improvements: Line 126: which **is** the Line 156: $Wc$ -> $w_c$ (since you are referring to the column-vector of $W_h$) Line 291: On the other **hand** Lines 473 / 478: \\citet Line 750: thsi -> this\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "mB9XuxV9nL",
        "length": 411,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a prototype-based method that adds a layer of interpretability to language models, and show its viability in a multi-class classification setting. The authors add an addition layer (dubbed the prototypical layer) on top of a pretriained LLM that learns representative examples of a particular class. They alter the standard cross-entropy loss function to include terms that make tighten a particular prototype group while placing it away from a group it doesn't belong to but is close in the embedded space. The modified loss function is comparable in performance with the original LLM, but can point to examples in the representative space that explain the model's choices.\n\nreasons_to_accept: 1. The authors extend current work  jointly learning prototypes representative of a class and making the final prediction of the model go through a calculation of similarities with representative protypical spaces. \n2. Experiments varying the size and uniqueness of prototypical space and its effect on model accuracy. \n3. Experiments to judge faithfulness with Comprehensiveness and Sufficiency scores, and a human study for simulatability.\n\nreasons_to_reject: 1. The framework proposed in the method is very similar in spirit to the proposed in Friedrich et. al. 2021 [Interactively Providing Explanations for Transformer Language Models], who use a modified loss function, although using two different networks for word and sentence level interpretability. The similarities in the modification of the loss function [Equation (1-2), https://arxiv.org/pdf/2110.02058.pdf] might be a bit too similar for the work to meet the criteria of novelty.\n\nquestions_for_the_authors: A. While choosing the size of the prototypical space, class imbalance, which is a major artifact of many NLP datasets, is never discussed. Could you add a discussion of how that interferes with choosing the size of the prototypical space ?\n\nmissing_references: The paper seems to miss two major related work -  1. Friedrich et al 2021: Interactively Providing Explanations for Transformer Language Models. The version I am referring to is https://arxiv.org/pdf/2110.02058.pdf.  2. Luo et. al. 2023, ACL: Prototype-Based Interpretability for Legal Citation Prediction. Although I can understand if the authors missed this one.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "87_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_87_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6946666666666667,
      "max_similarity": 0.7068,
      "avg_coverage": 0.514,
      "max_coverage": 0.6087
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 348,
      "avg_human_length": 399.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 6,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "FTKjkahon6",
        "similarity": 0.7068,
        "coverage": 0.6087,
        "human_length": 341,
        "human_text": "paper_topic_and_main_contributions: This work proposes a framework named proto-lm to address the interpretability in large language models. For each category, several sentences are selected as prototypes. They estimate a similarity score for each input token with those tokens selected as prototypes and use the score as the attribution weights of tokens.\n\nreasons_to_accept: This paper studies a prototypical network approach to attribute the importance of words.\n\nreasons_to_reject: The approach this work proposes actually is more similar to the concept bottleneck models(https://arxiv.org/abs/2007.04612) . However, the approach proposed in the concept bottleneck is more elegant and neat. For example, in this paper, there is no clear explanation about how these prototype sentences are selected.  Prototypical Networks are initially proposed to address few-shot learning problems. So the problem of prototype selection is simple enough, which could be done by selecting samples from new classes.  In this work, these prototypes are selected automatically or by humans? Does it require different prototypes for different datasets? How about the completeness of the chosen prototypes? These concerns reflect the flexibility of the model but are not addressed in the experiments. The experiments are not strong as there are no other interpretable models as baselines to compare.  Meanwhile,  I suggest some comparison with other interpretable models could be conducted, e.g. concept bottleneck,  integrated-gradient(https://arxiv.org/abs/1703.01365). Experiments based on human annotation are hard to reproduce. I suggest the authors could try some faithfulness metrics proposed in this paper(https://arxiv.org/pdf/2204.05514.pdf). And clearly explain how to select prototypes.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "iRxOPpnJB6",
        "similarity": 0.6866,
        "coverage": 0.5,
        "human_length": 445,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a new approach to enhance the interpretability of language models by incorporating prototypical networks. The authors suggest using a pre-trained language model as a backbone and then learning a set of prototypes associated with the classification classes for a given task. These prototypes are then leveraged during inference to produce interpretable predictions. Importantly, this framework retains competitive performance compared to the original language model, making it possible to achieve explainable decisions with minimal computational overhead and without compromising efficacy.\n\nreasons_to_accept: The authors extensively evaluate the performance of the proposed methodology through a diverse set of experiments. They assess the effectiveness of the approach along with the quality of the explanations it generates. These explanations are further evaluated regarding their usefulness in identifying misclassifications and their alignment with human reasoning.\nFurthermore, the authors conduct a comprehensive ablation study, which delves into various aspects of tuning hyper-parameters and practical implementation of the framework. This study offers an intuitive understanding of how the method operates internally, shedding light on its inner workings and potential areas of improvement.\n\nreasons_to_reject: The authors primarily focus their experiments on sentiment classification and natural language inference (NLI) tasks. This emphasis prompts an important question concerning the method's generalizability and applicability across a broader spectrum of potential use cases. While the results for sentiment classification and NLI tasks are likely valuable, the effectiveness of the proposed approach in other domains and tasks remains to be explored. Therefore, further investigation into its adaptability to diverse applications is warranted to fully understand its scope and potential impact in various fields.\n\nquestions_for_the_authors: Question A: You mentioned the necessity of tuning the hyper-parameters such as $\\lambda$, $N$, and $K$ for each task. If we were to consider limited access to examples from a new task, have you observed the robustness of the hyper-parameters derived from the most relevant scenario? Is there a significant impact on performance if you were to employ the same set of hyper-parameters for all NLI tasks, for instance?\nQuestion B: In the main text, your focus seems to be on encoder/encoder-decoder architectures. I'm curious about the method's performance with decoder-only language models. Have you conducted any experiments to explore this aspect?\n\ntypos_grammar_style_and_presentation_improvements: Line 126: which **is** the Line 156: $Wc$ -> $w_c$ (since you are referring to the column-vector of $W_h$) Line 291: On the other **hand** Lines 473 / 478: \\citet Line 750: thsi -> this\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "mB9XuxV9nL",
        "similarity": 0.6906,
        "coverage": 0.4333,
        "human_length": 411,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a prototype-based method that adds a layer of interpretability to language models, and show its viability in a multi-class classification setting. The authors add an addition layer (dubbed the prototypical layer) on top of a pretriained LLM that learns representative examples of a particular class. They alter the standard cross-entropy loss function to include terms that make tighten a particular prototype group while placing it away from a group it doesn't belong to but is close in the embedded space. The modified loss function is comparable in performance with the original LLM, but can point to examples in the representative space that explain the model's choices.\n\nreasons_to_accept: 1. The authors extend current work  jointly learning prototypes representative of a class and making the final prediction of the model go through a calculation of similarities with representative protypical spaces. \n2. Experiments varying the size and uniqueness of prototypical space and its effect on model accuracy. \n3. Experiments to judge faithfulness with Comprehensiveness and Sufficiency scores, and a human study for simulatability.\n\nreasons_to_reject: 1. The framework proposed in the method is very similar in spirit to the proposed in Friedrich et. al. 2021 [Interactively Providing Explanations for Transformer Language Models], who use a modified loss function, although using two different networks for word and sentence level interpretability. The similarities in the modification of the loss function [Equation (1-2), https://arxiv.org/pdf/2110.02058.pdf] might be a bit too similar for the work to meet the criteria of novelty.\n\nquestions_for_the_authors: A. While choosing the size of the prototypical space, class imbalance, which is a major artifact of many NLP datasets, is never discussed. Could you add a discussion of how that interferes with choosing the size of the prototypical space ?\n\nmissing_references: The paper seems to miss two major related work -  1. Friedrich et al 2021: Interactively Providing Explanations for Transformer Language Models. The version I am referring to is https://arxiv.org/pdf/2110.02058.pdf.  2. Luo et. al. 2023, ACL: Prototype-Based Interpretability for Legal Citation Prediction. Although I can understand if the authors missed this one.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "FTKjkahon6",
        "length": 341,
        "human_text": "paper_topic_and_main_contributions: This work proposes a framework named proto-lm to address the interpretability in large language models. For each category, several sentences are selected as prototypes. They estimate a similarity score for each input token with those tokens selected as prototypes and use the score as the attribution weights of tokens.\n\nreasons_to_accept: This paper studies a prototypical network approach to attribute the importance of words.\n\nreasons_to_reject: The approach this work proposes actually is more similar to the concept bottleneck models(https://arxiv.org/abs/2007.04612) . However, the approach proposed in the concept bottleneck is more elegant and neat. For example, in this paper, there is no clear explanation about how these prototype sentences are selected.  Prototypical Networks are initially proposed to address few-shot learning problems. So the problem of prototype selection is simple enough, which could be done by selecting samples from new classes.  In this work, these prototypes are selected automatically or by humans? Does it require different prototypes for different datasets? How about the completeness of the chosen prototypes? These concerns reflect the flexibility of the model but are not addressed in the experiments. The experiments are not strong as there are no other interpretable models as baselines to compare.  Meanwhile,  I suggest some comparison with other interpretable models could be conducted, e.g. concept bottleneck,  integrated-gradient(https://arxiv.org/abs/1703.01365). Experiments based on human annotation are hard to reproduce. I suggest the authors could try some faithfulness metrics proposed in this paper(https://arxiv.org/pdf/2204.05514.pdf). And clearly explain how to select prototypes.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "iRxOPpnJB6",
        "length": 445,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a new approach to enhance the interpretability of language models by incorporating prototypical networks. The authors suggest using a pre-trained language model as a backbone and then learning a set of prototypes associated with the classification classes for a given task. These prototypes are then leveraged during inference to produce interpretable predictions. Importantly, this framework retains competitive performance compared to the original language model, making it possible to achieve explainable decisions with minimal computational overhead and without compromising efficacy.\n\nreasons_to_accept: The authors extensively evaluate the performance of the proposed methodology through a diverse set of experiments. They assess the effectiveness of the approach along with the quality of the explanations it generates. These explanations are further evaluated regarding their usefulness in identifying misclassifications and their alignment with human reasoning.\nFurthermore, the authors conduct a comprehensive ablation study, which delves into various aspects of tuning hyper-parameters and practical implementation of the framework. This study offers an intuitive understanding of how the method operates internally, shedding light on its inner workings and potential areas of improvement.\n\nreasons_to_reject: The authors primarily focus their experiments on sentiment classification and natural language inference (NLI) tasks. This emphasis prompts an important question concerning the method's generalizability and applicability across a broader spectrum of potential use cases. While the results for sentiment classification and NLI tasks are likely valuable, the effectiveness of the proposed approach in other domains and tasks remains to be explored. Therefore, further investigation into its adaptability to diverse applications is warranted to fully understand its scope and potential impact in various fields.\n\nquestions_for_the_authors: Question A: You mentioned the necessity of tuning the hyper-parameters such as $\\lambda$, $N$, and $K$ for each task. If we were to consider limited access to examples from a new task, have you observed the robustness of the hyper-parameters derived from the most relevant scenario? Is there a significant impact on performance if you were to employ the same set of hyper-parameters for all NLI tasks, for instance?\nQuestion B: In the main text, your focus seems to be on encoder/encoder-decoder architectures. I'm curious about the method's performance with decoder-only language models. Have you conducted any experiments to explore this aspect?\n\ntypos_grammar_style_and_presentation_improvements: Line 126: which **is** the Line 156: $Wc$ -> $w_c$ (since you are referring to the column-vector of $W_h$) Line 291: On the other **hand** Lines 473 / 478: \\citet Line 750: thsi -> this\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "mB9XuxV9nL",
        "length": 411,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a prototype-based method that adds a layer of interpretability to language models, and show its viability in a multi-class classification setting. The authors add an addition layer (dubbed the prototypical layer) on top of a pretriained LLM that learns representative examples of a particular class. They alter the standard cross-entropy loss function to include terms that make tighten a particular prototype group while placing it away from a group it doesn't belong to but is close in the embedded space. The modified loss function is comparable in performance with the original LLM, but can point to examples in the representative space that explain the model's choices.\n\nreasons_to_accept: 1. The authors extend current work  jointly learning prototypes representative of a class and making the final prediction of the model go through a calculation of similarities with representative protypical spaces. \n2. Experiments varying the size and uniqueness of prototypical space and its effect on model accuracy. \n3. Experiments to judge faithfulness with Comprehensiveness and Sufficiency scores, and a human study for simulatability.\n\nreasons_to_reject: 1. The framework proposed in the method is very similar in spirit to the proposed in Friedrich et. al. 2021 [Interactively Providing Explanations for Transformer Language Models], who use a modified loss function, although using two different networks for word and sentence level interpretability. The similarities in the modification of the loss function [Equation (1-2), https://arxiv.org/pdf/2110.02058.pdf] might be a bit too similar for the work to meet the criteria of novelty.\n\nquestions_for_the_authors: A. While choosing the size of the prototypical space, class imbalance, which is a major artifact of many NLP datasets, is never discussed. Could you add a discussion of how that interferes with choosing the size of the prototypical space ?\n\nmissing_references: The paper seems to miss two major related work -  1. Friedrich et al 2021: Interactively Providing Explanations for Transformer Language Models. The version I am referring to is https://arxiv.org/pdf/2110.02058.pdf.  2. Luo et. al. 2023, ACL: Prototype-Based Interpretability for Legal Citation Prediction. Although I can understand if the authors missed this one.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "99_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_99_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7269,
      "max_similarity": 0.7517,
      "avg_coverage": 0.6617000000000001,
      "max_coverage": 0.875
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 867,
      "avg_human_length": 360.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 7,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "1AwSqgb2mG",
        "similarity": 0.7072,
        "coverage": 0.875,
        "human_length": 186,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a TE dataset built on natural claim and evidence pairs extracted from Wikipedia; especially annotations over sub-sentence units. The authors conduct sufficient NLP experiments to support claims about shortcomings of prior NLI datasets and the usefulness of the dataset.\n\nreasons_to_accept: - good presentation and well written - paper identifies use-cases of models trained on Textual entailment and finds that the underlying datasets are not suited for such use cases. Hence, they propose a new dataset for this.\n- I think such a dataset is a useful and interesting contribution to the community - good amount of relevant experiments conducted\n\nreasons_to_reject: - None really\n\nquestions_for_the_authors: - what is the average hourly for m-turkers if you pay 0.75$ per HIT?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "kdeWxPxVlE",
        "similarity": 0.7218,
        "coverage": 0.7143,
        "human_length": 181,
        "human_text": "paper_topic_and_main_contributions: This paper proposes WICE, a new fine-grained textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia. \nIt provides entailment judgments over subsentence units of the claim, and a minimal subset of evidence sentences that support each subclaim. \nExperiments show that decomposing complex claims into subclaims can be a valuable pre-processing step for both annotation and entailment prediction.\n\nreasons_to_accept: 1. Fine-grained textual entailment is more realistic and deserves further research. \n2. Paper is well-written and the experiments are solid.\n\nreasons_to_reject: 1. The data comes from Wikipedia, which makes it difficult to test the model's capabilities, as many models are pre-trained on the Wikipedia.\n\nmissing_references: [1] Logic-Regularized Reasoning for Interpretable Fact Verification. AAAI 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "Z4x5QhIhy6",
        "similarity": 0.7517,
        "coverage": 0.3958,
        "human_length": 713,
        "human_text": "paper_topic_and_main_contributions: This paper is about a new entailment dataset created from Wikipedia. \nTheir dataset has three key features compared to existing similar datasets; (1) its premises are long; (2) negative examples are natural ones, i.e. not synthetic ones; and (3) its annotation is fine-grained and allows to identify which parts of claims are not supported by evidence text.\nQuantitative analyses in Table 3 shows that their dataset is more challenging than previous datasets (FEVER and VitaminC).\nThe dataset allows to conduct three entailment-related tasks; (a) entailment classification; (b) evidence retrieval; and (c) non-supported token detection.\nThe authors demonstrated that these three tasks defined on their dataset were difficult by comparing the performances of several competitive models with those of humans.\nThe authors also proposed Claim-Split, which automatically splits a claim into sub-claims using GPT-3.5. \nTheir experiments showed that Claim-Split was effective for entailment classification.\n\nreasons_to_accept: The new entailment dataset will be useful for textual entailment recognition research.\nA series of the quantitative analyses and experiments are informative for RTE researchers.\n\nreasons_to_reject: The weakness of this paper is the lack of description and justification for some important aspects of the paper.\nFirst, the contributions of the paper are not clearly described. \nI guess they are (1) the new entailment dataset that has the above three features, (2) the Claim-Split method, and (3) the quantitative analyses and experiments.\nSecond, it should be justified why the authors did not directly use GPT-3.5 for the three tasks (entailment classification, evidence retrieval, and non-supported token detection). The authors used GPT-3.5 for Claim-Split. \nTheir problem settings therefore allow to use GPT-3.5. It should be a reasonable baseline given that we can use it relatively easily.\nThird, 50 samples might be too few to draw any conclusion about human performances for the tasks. \nUsing only 50 samples should have been justified too.\nFourth, I wonder why AUROC was used for the entailment classification results in Table 8 while F1 and accuracy were used for the entailment classification results in Tables 4 and 5. This needs explanation.\nLast, there are many descriptions that need more detailed explanations or justification. \nI understand that there may be many details that cannot be written in the paper. \nBut some of them seem important information or key research decisions. Below are some examples: p.1 - What does it mean for a token to be supported? A fact or a proposition can be supported. \nBut I don't really understand how a token is supported. Do you simply mean that a token is supported if the token is written in an evidence text?\n- What do you mean by \"ecologically valid\"? Do you mean naturally occurring negative examples?\np.3 - \"In total, only 8.6% of the claims included one of the two types of errors.\" \n... What are the two types?\n- \"we re-retrieve the cited web article(s)\" ... Why did you need to re-retrieve them?\n- \"Also, we filter claims that are decomposed into either only one or more than six subclaims.\" \n... Why did you need this filter? Why six?\np.4 - In Table 3, WiCE Subcl and WiCE Claim do not add up to 100%.\np.6 - \"p(SUPPORTED) + 0.5 \u00d7 p(PARTIALLY-SUPPORTED)\" ... How was this derived?\n- \"As there can be multiple gold sets of supporting sentences for each claim/subclaim in WICE,\" ... I could not understand this clause.\n- In Table 6, why was ANLI+WiCE worse than WiCE in the \"w/ evidence context\" setting, while it was better in the other setting?\np.7 - I could not understand footnote 7.\n\ntypos_grammar_style_and_presentation_improvements: p.2 \"Real world claims, such are those\" --> \"Real world claims, such as those\" p.6 \"Performance of models fine-tuned with evidence context on WICE is supported in the bottom half of the table.\" \n--> \"Performance of models fine-tuned with evidence context on WICE is reported in the bottom half of the table.\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "1AwSqgb2mG",
        "length": 186,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a TE dataset built on natural claim and evidence pairs extracted from Wikipedia; especially annotations over sub-sentence units. The authors conduct sufficient NLP experiments to support claims about shortcomings of prior NLI datasets and the usefulness of the dataset.\n\nreasons_to_accept: - good presentation and well written - paper identifies use-cases of models trained on Textual entailment and finds that the underlying datasets are not suited for such use cases. Hence, they propose a new dataset for this.\n- I think such a dataset is a useful and interesting contribution to the community - good amount of relevant experiments conducted\n\nreasons_to_reject: - None really\n\nquestions_for_the_authors: - what is the average hourly for m-turkers if you pay 0.75$ per HIT?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "kdeWxPxVlE",
        "length": 181,
        "human_text": "paper_topic_and_main_contributions: This paper proposes WICE, a new fine-grained textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia. \nIt provides entailment judgments over subsentence units of the claim, and a minimal subset of evidence sentences that support each subclaim. \nExperiments show that decomposing complex claims into subclaims can be a valuable pre-processing step for both annotation and entailment prediction.\n\nreasons_to_accept: 1. Fine-grained textual entailment is more realistic and deserves further research. \n2. Paper is well-written and the experiments are solid.\n\nreasons_to_reject: 1. The data comes from Wikipedia, which makes it difficult to test the model's capabilities, as many models are pre-trained on the Wikipedia.\n\nmissing_references: [1] Logic-Regularized Reasoning for Interpretable Fact Verification. AAAI 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "Z4x5QhIhy6",
        "length": 713,
        "human_text": "paper_topic_and_main_contributions: This paper is about a new entailment dataset created from Wikipedia. \nTheir dataset has three key features compared to existing similar datasets; (1) its premises are long; (2) negative examples are natural ones, i.e. not synthetic ones; and (3) its annotation is fine-grained and allows to identify which parts of claims are not supported by evidence text.\nQuantitative analyses in Table 3 shows that their dataset is more challenging than previous datasets (FEVER and VitaminC).\nThe dataset allows to conduct three entailment-related tasks; (a) entailment classification; (b) evidence retrieval; and (c) non-supported token detection.\nThe authors demonstrated that these three tasks defined on their dataset were difficult by comparing the performances of several competitive models with those of humans.\nThe authors also proposed Claim-Split, which automatically splits a claim into sub-claims using GPT-3.5. \nTheir experiments showed that Claim-Split was effective for entailment classification.\n\nreasons_to_accept: The new entailment dataset will be useful for textual entailment recognition research.\nA series of the quantitative analyses and experiments are informative for RTE researchers.\n\nreasons_to_reject: The weakness of this paper is the lack of description and justification for some important aspects of the paper.\nFirst, the contributions of the paper are not clearly described. \nI guess they are (1) the new entailment dataset that has the above three features, (2) the Claim-Split method, and (3) the quantitative analyses and experiments.\nSecond, it should be justified why the authors did not directly use GPT-3.5 for the three tasks (entailment classification, evidence retrieval, and non-supported token detection). The authors used GPT-3.5 for Claim-Split. \nTheir problem settings therefore allow to use GPT-3.5. It should be a reasonable baseline given that we can use it relatively easily.\nThird, 50 samples might be too few to draw any conclusion about human performances for the tasks. \nUsing only 50 samples should have been justified too.\nFourth, I wonder why AUROC was used for the entailment classification results in Table 8 while F1 and accuracy were used for the entailment classification results in Tables 4 and 5. This needs explanation.\nLast, there are many descriptions that need more detailed explanations or justification. \nI understand that there may be many details that cannot be written in the paper. \nBut some of them seem important information or key research decisions. Below are some examples: p.1 - What does it mean for a token to be supported? A fact or a proposition can be supported. \nBut I don't really understand how a token is supported. Do you simply mean that a token is supported if the token is written in an evidence text?\n- What do you mean by \"ecologically valid\"? Do you mean naturally occurring negative examples?\np.3 - \"In total, only 8.6% of the claims included one of the two types of errors.\" \n... What are the two types?\n- \"we re-retrieve the cited web article(s)\" ... Why did you need to re-retrieve them?\n- \"Also, we filter claims that are decomposed into either only one or more than six subclaims.\" \n... Why did you need this filter? Why six?\np.4 - In Table 3, WiCE Subcl and WiCE Claim do not add up to 100%.\np.6 - \"p(SUPPORTED) + 0.5 \u00d7 p(PARTIALLY-SUPPORTED)\" ... How was this derived?\n- \"As there can be multiple gold sets of supporting sentences for each claim/subclaim in WICE,\" ... I could not understand this clause.\n- In Table 6, why was ANLI+WiCE worse than WiCE in the \"w/ evidence context\" setting, while it was better in the other setting?\np.7 - I could not understand footnote 7.\n\ntypos_grammar_style_and_presentation_improvements: p.2 \"Real world claims, such are those\" --> \"Real world claims, such as those\" p.6 \"Performance of models fine-tuned with evidence context on WICE is supported in the bottom half of the table.\" \n--> \"Performance of models fine-tuned with evidence context on WICE is reported in the bottom half of the table.\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "99_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_99_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7269333333333333,
      "max_similarity": 0.7506,
      "avg_coverage": 0.6895000000000001,
      "max_coverage": 0.875
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 991,
      "avg_human_length": 360.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 7,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "1AwSqgb2mG",
        "similarity": 0.7094,
        "coverage": 0.875,
        "human_length": 186,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a TE dataset built on natural claim and evidence pairs extracted from Wikipedia; especially annotations over sub-sentence units. The authors conduct sufficient NLP experiments to support claims about shortcomings of prior NLI datasets and the usefulness of the dataset.\n\nreasons_to_accept: - good presentation and well written - paper identifies use-cases of models trained on Textual entailment and finds that the underlying datasets are not suited for such use cases. Hence, they propose a new dataset for this.\n- I think such a dataset is a useful and interesting contribution to the community - good amount of relevant experiments conducted\n\nreasons_to_reject: - None really\n\nquestions_for_the_authors: - what is the average hourly for m-turkers if you pay 0.75$ per HIT?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "kdeWxPxVlE",
        "similarity": 0.7208,
        "coverage": 0.7143,
        "human_length": 181,
        "human_text": "paper_topic_and_main_contributions: This paper proposes WICE, a new fine-grained textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia. \nIt provides entailment judgments over subsentence units of the claim, and a minimal subset of evidence sentences that support each subclaim. \nExperiments show that decomposing complex claims into subclaims can be a valuable pre-processing step for both annotation and entailment prediction.\n\nreasons_to_accept: 1. Fine-grained textual entailment is more realistic and deserves further research. \n2. Paper is well-written and the experiments are solid.\n\nreasons_to_reject: 1. The data comes from Wikipedia, which makes it difficult to test the model's capabilities, as many models are pre-trained on the Wikipedia.\n\nmissing_references: [1] Logic-Regularized Reasoning for Interpretable Fact Verification. AAAI 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "Z4x5QhIhy6",
        "similarity": 0.7506,
        "coverage": 0.4792,
        "human_length": 713,
        "human_text": "paper_topic_and_main_contributions: This paper is about a new entailment dataset created from Wikipedia. \nTheir dataset has three key features compared to existing similar datasets; (1) its premises are long; (2) negative examples are natural ones, i.e. not synthetic ones; and (3) its annotation is fine-grained and allows to identify which parts of claims are not supported by evidence text.\nQuantitative analyses in Table 3 shows that their dataset is more challenging than previous datasets (FEVER and VitaminC).\nThe dataset allows to conduct three entailment-related tasks; (a) entailment classification; (b) evidence retrieval; and (c) non-supported token detection.\nThe authors demonstrated that these three tasks defined on their dataset were difficult by comparing the performances of several competitive models with those of humans.\nThe authors also proposed Claim-Split, which automatically splits a claim into sub-claims using GPT-3.5. \nTheir experiments showed that Claim-Split was effective for entailment classification.\n\nreasons_to_accept: The new entailment dataset will be useful for textual entailment recognition research.\nA series of the quantitative analyses and experiments are informative for RTE researchers.\n\nreasons_to_reject: The weakness of this paper is the lack of description and justification for some important aspects of the paper.\nFirst, the contributions of the paper are not clearly described. \nI guess they are (1) the new entailment dataset that has the above three features, (2) the Claim-Split method, and (3) the quantitative analyses and experiments.\nSecond, it should be justified why the authors did not directly use GPT-3.5 for the three tasks (entailment classification, evidence retrieval, and non-supported token detection). The authors used GPT-3.5 for Claim-Split. \nTheir problem settings therefore allow to use GPT-3.5. It should be a reasonable baseline given that we can use it relatively easily.\nThird, 50 samples might be too few to draw any conclusion about human performances for the tasks. \nUsing only 50 samples should have been justified too.\nFourth, I wonder why AUROC was used for the entailment classification results in Table 8 while F1 and accuracy were used for the entailment classification results in Tables 4 and 5. This needs explanation.\nLast, there are many descriptions that need more detailed explanations or justification. \nI understand that there may be many details that cannot be written in the paper. \nBut some of them seem important information or key research decisions. Below are some examples: p.1 - What does it mean for a token to be supported? A fact or a proposition can be supported. \nBut I don't really understand how a token is supported. Do you simply mean that a token is supported if the token is written in an evidence text?\n- What do you mean by \"ecologically valid\"? Do you mean naturally occurring negative examples?\np.3 - \"In total, only 8.6% of the claims included one of the two types of errors.\" \n... What are the two types?\n- \"we re-retrieve the cited web article(s)\" ... Why did you need to re-retrieve them?\n- \"Also, we filter claims that are decomposed into either only one or more than six subclaims.\" \n... Why did you need this filter? Why six?\np.4 - In Table 3, WiCE Subcl and WiCE Claim do not add up to 100%.\np.6 - \"p(SUPPORTED) + 0.5 \u00d7 p(PARTIALLY-SUPPORTED)\" ... How was this derived?\n- \"As there can be multiple gold sets of supporting sentences for each claim/subclaim in WICE,\" ... I could not understand this clause.\n- In Table 6, why was ANLI+WiCE worse than WiCE in the \"w/ evidence context\" setting, while it was better in the other setting?\np.7 - I could not understand footnote 7.\n\ntypos_grammar_style_and_presentation_improvements: p.2 \"Real world claims, such are those\" --> \"Real world claims, such as those\" p.6 \"Performance of models fine-tuned with evidence context on WICE is supported in the bottom half of the table.\" \n--> \"Performance of models fine-tuned with evidence context on WICE is reported in the bottom half of the table.\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "1AwSqgb2mG",
        "length": 186,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a TE dataset built on natural claim and evidence pairs extracted from Wikipedia; especially annotations over sub-sentence units. The authors conduct sufficient NLP experiments to support claims about shortcomings of prior NLI datasets and the usefulness of the dataset.\n\nreasons_to_accept: - good presentation and well written - paper identifies use-cases of models trained on Textual entailment and finds that the underlying datasets are not suited for such use cases. Hence, they propose a new dataset for this.\n- I think such a dataset is a useful and interesting contribution to the community - good amount of relevant experiments conducted\n\nreasons_to_reject: - None really\n\nquestions_for_the_authors: - what is the average hourly for m-turkers if you pay 0.75$ per HIT?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "kdeWxPxVlE",
        "length": 181,
        "human_text": "paper_topic_and_main_contributions: This paper proposes WICE, a new fine-grained textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia. \nIt provides entailment judgments over subsentence units of the claim, and a minimal subset of evidence sentences that support each subclaim. \nExperiments show that decomposing complex claims into subclaims can be a valuable pre-processing step for both annotation and entailment prediction.\n\nreasons_to_accept: 1. Fine-grained textual entailment is more realistic and deserves further research. \n2. Paper is well-written and the experiments are solid.\n\nreasons_to_reject: 1. The data comes from Wikipedia, which makes it difficult to test the model's capabilities, as many models are pre-trained on the Wikipedia.\n\nmissing_references: [1] Logic-Regularized Reasoning for Interpretable Fact Verification. AAAI 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "Z4x5QhIhy6",
        "length": 713,
        "human_text": "paper_topic_and_main_contributions: This paper is about a new entailment dataset created from Wikipedia. \nTheir dataset has three key features compared to existing similar datasets; (1) its premises are long; (2) negative examples are natural ones, i.e. not synthetic ones; and (3) its annotation is fine-grained and allows to identify which parts of claims are not supported by evidence text.\nQuantitative analyses in Table 3 shows that their dataset is more challenging than previous datasets (FEVER and VitaminC).\nThe dataset allows to conduct three entailment-related tasks; (a) entailment classification; (b) evidence retrieval; and (c) non-supported token detection.\nThe authors demonstrated that these three tasks defined on their dataset were difficult by comparing the performances of several competitive models with those of humans.\nThe authors also proposed Claim-Split, which automatically splits a claim into sub-claims using GPT-3.5. \nTheir experiments showed that Claim-Split was effective for entailment classification.\n\nreasons_to_accept: The new entailment dataset will be useful for textual entailment recognition research.\nA series of the quantitative analyses and experiments are informative for RTE researchers.\n\nreasons_to_reject: The weakness of this paper is the lack of description and justification for some important aspects of the paper.\nFirst, the contributions of the paper are not clearly described. \nI guess they are (1) the new entailment dataset that has the above three features, (2) the Claim-Split method, and (3) the quantitative analyses and experiments.\nSecond, it should be justified why the authors did not directly use GPT-3.5 for the three tasks (entailment classification, evidence retrieval, and non-supported token detection). The authors used GPT-3.5 for Claim-Split. \nTheir problem settings therefore allow to use GPT-3.5. It should be a reasonable baseline given that we can use it relatively easily.\nThird, 50 samples might be too few to draw any conclusion about human performances for the tasks. \nUsing only 50 samples should have been justified too.\nFourth, I wonder why AUROC was used for the entailment classification results in Table 8 while F1 and accuracy were used for the entailment classification results in Tables 4 and 5. This needs explanation.\nLast, there are many descriptions that need more detailed explanations or justification. \nI understand that there may be many details that cannot be written in the paper. \nBut some of them seem important information or key research decisions. Below are some examples: p.1 - What does it mean for a token to be supported? A fact or a proposition can be supported. \nBut I don't really understand how a token is supported. Do you simply mean that a token is supported if the token is written in an evidence text?\n- What do you mean by \"ecologically valid\"? Do you mean naturally occurring negative examples?\np.3 - \"In total, only 8.6% of the claims included one of the two types of errors.\" \n... What are the two types?\n- \"we re-retrieve the cited web article(s)\" ... Why did you need to re-retrieve them?\n- \"Also, we filter claims that are decomposed into either only one or more than six subclaims.\" \n... Why did you need this filter? Why six?\np.4 - In Table 3, WiCE Subcl and WiCE Claim do not add up to 100%.\np.6 - \"p(SUPPORTED) + 0.5 \u00d7 p(PARTIALLY-SUPPORTED)\" ... How was this derived?\n- \"As there can be multiple gold sets of supporting sentences for each claim/subclaim in WICE,\" ... I could not understand this clause.\n- In Table 6, why was ANLI+WiCE worse than WiCE in the \"w/ evidence context\" setting, while it was better in the other setting?\np.7 - I could not understand footnote 7.\n\ntypos_grammar_style_and_presentation_improvements: p.2 \"Real world claims, such are those\" --> \"Real world claims, such as those\" p.6 \"Performance of models fine-tuned with evidence context on WICE is supported in the bottom half of the table.\" \n--> \"Performance of models fine-tuned with evidence context on WICE is reported in the bottom half of the table.\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "48_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_48_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7344333333333334,
      "max_similarity": 0.7623,
      "avg_coverage": 0.43293333333333334,
      "max_coverage": 0.5
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 455,
      "avg_human_length": 399.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 5,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "ZlFFZiAbte",
        "similarity": 0.7275,
        "coverage": 0.4762,
        "human_length": 390,
        "human_text": "paper_topic_and_main_contributions: - This paper explores the diagnosis and treatment of False Behavior in the Patches generation process within the Program Repair Model.\n- This paper proposes a highly compatible framework for diagnosing and treating False Behavior and accordingly introduces the Behavior Vector for diagnosis. Additionally, two methods for treatment are proposed: abortion and masked bypassing.\n- Experimental results on numerous datasets show that simply adding the modules proposed in this paper can effectively increase the number of correctly repaired programs, without changing the baseline.\n\nreasons_to_accept: This paper enhances the performance of the program repair model by analyzing its internal generation process from a novel perspective. This direction of improvement represents a promising path worth exploring.\n\nreasons_to_reject: - The writing and readability of this paper leave room for improvement. Some key points in the article lack detailed descriptions and explanations, making the content unclear and difficult for readers to comprehend. \n    - The paper fails to clearly explain the difference between the Attention MASK introduced in section 3.2. There is a lack of example illustrations. \n    - The meanings of each row and column in the results tables are not clearly explained, and the purposes of some experiments are not explicitly stated. Furthermore, it is unclear whether the parameters of the models remain consistent across different experimental results. It would be beneficial to clarify whether the observed improvements result from parameter variations or methodological enhancements. \n    - The distinctions between Balanced Accuracy and the F1 score are not clearly explained, nor are the respective functions of these two metrics.\n\nquestions_for_the_authors: - Is the \"Intended Tolerance for Search\" in Table 5 derived under the same parameter settings as the \"Intended Tolerance for Search\" in Table 3?\n\ntypos_grammar_style_and_presentation_improvements: In line 12 of section 2.1, \"wight\" should be corrected to \"weight\".\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "JjFp24TQD4",
        "similarity": 0.7623,
        "coverage": 0.3226,
        "human_length": 525,
        "human_text": "paper_topic_and_main_contributions: **Summary:** Current Automatic Program Repair (APR) studies mainly focus on improving these models to generate correct patches for the faulty program. However, investigating the false behavior of these models was not addressed in the previous works. To address this, the paper proposes a methodology for diagnosing and treating the false behaviors of the transformer-based program repair models. The paper mainly proposes a behavior vector together with a behavior discriminator to quantify the behavior of the models and employ them to improve the general APR models\u2019 performance.\n**Contributions:** -The paper introduces a novel perspective to improve program repair tasks by diagnosing and treating the false behaviors of transformer-based program repair models. \n-The paper proposes a behavior vector based on transformer-based models to diagnose and mitigate false behaviors of the model in generating the patches for the given buggy program. \n-The evaluation results for 55,562 instances of different datasets showed the effectiveness of the approach in identifying correct and incorrect patches (It correctly classified 86.9% of correct and 86.4% of incorrect patches).\n\nreasons_to_accept: - The paper proposed a novel perspective to improve program repair tasks by considering the false behavior of the models.\n- It provides extensive experimental results on three different models and four different datasets.\n- It shows the effectiveness of the approach in identifying correct and incorrect patches. Furthermore, it employs false behavior diagnoses to improve the APR models.  - The paper is well-written and easy to follow.\n\nreasons_to_reject: - The novelty of the behavior vector is questionable and it is not clear if it works better than using the model itself to classify the patches (e.g., using a specific token like [CLS] as the classification vector representation, see the \u201cQuestions For The Authors\u201d) .\n- The training details of the CodeGen model are not provided. E.g., the input size, learning rate, etc.\n\nquestions_for_the_authors: - It is not clear if the proposed behavior vector is the best option to provide representation for classifying the patches. Could you please justify why we can not use the model itself to classify the patches? Or adding a special token (e.g., [CLS] token) to the inputs and employing that as the behavior vector representation?\n- It is not clear how the CodeGen model was trained (fine-tuned). Could you please provide the training details, including the learning rate, input structure, and input size?\n- Do you think the output of BeDisc can be used as an implicit uncertainty estimation?  - Table 6 is a bit vague. What does T2 stand for (I know it refers to the treatments. However, it would be better to provide the details)? Why does the 7th pair cause the computational capacity issue?\n\ntypos_grammar_style_and_presentation_improvements: - At 2.1 add what is d_k (Eq 2)? I suggest to provide one or two sentences about that.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "MnlrLApm1G",
        "similarity": 0.7135,
        "coverage": 0.5,
        "human_length": 284,
        "human_text": "paper_topic_and_main_contributions: Automated program repair is an widely discussed topic and has potential to reduce costs associated with bug fixing in software development and maintenance procedure. This paper proposes some novel techniques to improve performance of such program repairs. \nAuthors proposed a method to diagnose and treat false behaviors of transformer-based program repair models with the help of a behavior vector, a behavior discriminator (BeDisc) that identifies false behaviors, and two methods for false behavior treatment. \nTheir novelty is in improving program repair tasks by analyzing the internal behaviors of transformer-based models by demonstrating internal behavior of a transformer-based program repair model.\n\nreasons_to_accept: Along with a good overall presentation of the paper it has some strengths. Treatment 1, Abortions has good potential false behavior diagnosis. Also Treatment 2, masked Bypassing eliminates suspicious target tokens responsible for false behaviour while producing patches. \nOverall improvements seem to be significant. Qualitative and quantitatively significant datasets are used.\n\nreasons_to_reject: One of the weakness is that as per table 2. still there are some false negatives. It is not clear how significant can such false negatives be for such systems. \nAnother weak point is overall the paper is not easy to read for general audience.\n\nquestions_for_the_authors: A. Can you please clarify impact of false negatives on the system?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "ZlFFZiAbte",
        "length": 390,
        "human_text": "paper_topic_and_main_contributions: - This paper explores the diagnosis and treatment of False Behavior in the Patches generation process within the Program Repair Model.\n- This paper proposes a highly compatible framework for diagnosing and treating False Behavior and accordingly introduces the Behavior Vector for diagnosis. Additionally, two methods for treatment are proposed: abortion and masked bypassing.\n- Experimental results on numerous datasets show that simply adding the modules proposed in this paper can effectively increase the number of correctly repaired programs, without changing the baseline.\n\nreasons_to_accept: This paper enhances the performance of the program repair model by analyzing its internal generation process from a novel perspective. This direction of improvement represents a promising path worth exploring.\n\nreasons_to_reject: - The writing and readability of this paper leave room for improvement. Some key points in the article lack detailed descriptions and explanations, making the content unclear and difficult for readers to comprehend. \n    - The paper fails to clearly explain the difference between the Attention MASK introduced in section 3.2. There is a lack of example illustrations. \n    - The meanings of each row and column in the results tables are not clearly explained, and the purposes of some experiments are not explicitly stated. Furthermore, it is unclear whether the parameters of the models remain consistent across different experimental results. It would be beneficial to clarify whether the observed improvements result from parameter variations or methodological enhancements. \n    - The distinctions between Balanced Accuracy and the F1 score are not clearly explained, nor are the respective functions of these two metrics.\n\nquestions_for_the_authors: - Is the \"Intended Tolerance for Search\" in Table 5 derived under the same parameter settings as the \"Intended Tolerance for Search\" in Table 3?\n\ntypos_grammar_style_and_presentation_improvements: In line 12 of section 2.1, \"wight\" should be corrected to \"weight\".\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "JjFp24TQD4",
        "length": 525,
        "human_text": "paper_topic_and_main_contributions: **Summary:** Current Automatic Program Repair (APR) studies mainly focus on improving these models to generate correct patches for the faulty program. However, investigating the false behavior of these models was not addressed in the previous works. To address this, the paper proposes a methodology for diagnosing and treating the false behaviors of the transformer-based program repair models. The paper mainly proposes a behavior vector together with a behavior discriminator to quantify the behavior of the models and employ them to improve the general APR models\u2019 performance.\n**Contributions:** -The paper introduces a novel perspective to improve program repair tasks by diagnosing and treating the false behaviors of transformer-based program repair models. \n-The paper proposes a behavior vector based on transformer-based models to diagnose and mitigate false behaviors of the model in generating the patches for the given buggy program. \n-The evaluation results for 55,562 instances of different datasets showed the effectiveness of the approach in identifying correct and incorrect patches (It correctly classified 86.9% of correct and 86.4% of incorrect patches).\n\nreasons_to_accept: - The paper proposed a novel perspective to improve program repair tasks by considering the false behavior of the models.\n- It provides extensive experimental results on three different models and four different datasets.\n- It shows the effectiveness of the approach in identifying correct and incorrect patches. Furthermore, it employs false behavior diagnoses to improve the APR models.  - The paper is well-written and easy to follow.\n\nreasons_to_reject: - The novelty of the behavior vector is questionable and it is not clear if it works better than using the model itself to classify the patches (e.g., using a specific token like [CLS] as the classification vector representation, see the \u201cQuestions For The Authors\u201d) .\n- The training details of the CodeGen model are not provided. E.g., the input size, learning rate, etc.\n\nquestions_for_the_authors: - It is not clear if the proposed behavior vector is the best option to provide representation for classifying the patches. Could you please justify why we can not use the model itself to classify the patches? Or adding a special token (e.g., [CLS] token) to the inputs and employing that as the behavior vector representation?\n- It is not clear how the CodeGen model was trained (fine-tuned). Could you please provide the training details, including the learning rate, input structure, and input size?\n- Do you think the output of BeDisc can be used as an implicit uncertainty estimation?  - Table 6 is a bit vague. What does T2 stand for (I know it refers to the treatments. However, it would be better to provide the details)? Why does the 7th pair cause the computational capacity issue?\n\ntypos_grammar_style_and_presentation_improvements: - At 2.1 add what is d_k (Eq 2)? I suggest to provide one or two sentences about that.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "MnlrLApm1G",
        "length": 284,
        "human_text": "paper_topic_and_main_contributions: Automated program repair is an widely discussed topic and has potential to reduce costs associated with bug fixing in software development and maintenance procedure. This paper proposes some novel techniques to improve performance of such program repairs. \nAuthors proposed a method to diagnose and treat false behaviors of transformer-based program repair models with the help of a behavior vector, a behavior discriminator (BeDisc) that identifies false behaviors, and two methods for false behavior treatment. \nTheir novelty is in improving program repair tasks by analyzing the internal behaviors of transformer-based models by demonstrating internal behavior of a transformer-based program repair model.\n\nreasons_to_accept: Along with a good overall presentation of the paper it has some strengths. Treatment 1, Abortions has good potential false behavior diagnosis. Also Treatment 2, masked Bypassing eliminates suspicious target tokens responsible for false behaviour while producing patches. \nOverall improvements seem to be significant. Qualitative and quantitatively significant datasets are used.\n\nreasons_to_reject: One of the weakness is that as per table 2. still there are some false negatives. It is not clear how significant can such false negatives be for such systems. \nAnother weak point is overall the paper is not easy to read for general audience.\n\nquestions_for_the_authors: A. Can you please clarify impact of false negatives on the system?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "48_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_48_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7397,
      "max_similarity": 0.7662,
      "avg_coverage": 0.45953333333333335,
      "max_coverage": 0.5238
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 431,
      "avg_human_length": 399.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 5,
      "suggestions_count": 4
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "ZlFFZiAbte",
        "similarity": 0.7331,
        "coverage": 0.5238,
        "human_length": 390,
        "human_text": "paper_topic_and_main_contributions: - This paper explores the diagnosis and treatment of False Behavior in the Patches generation process within the Program Repair Model.\n- This paper proposes a highly compatible framework for diagnosing and treating False Behavior and accordingly introduces the Behavior Vector for diagnosis. Additionally, two methods for treatment are proposed: abortion and masked bypassing.\n- Experimental results on numerous datasets show that simply adding the modules proposed in this paper can effectively increase the number of correctly repaired programs, without changing the baseline.\n\nreasons_to_accept: This paper enhances the performance of the program repair model by analyzing its internal generation process from a novel perspective. This direction of improvement represents a promising path worth exploring.\n\nreasons_to_reject: - The writing and readability of this paper leave room for improvement. Some key points in the article lack detailed descriptions and explanations, making the content unclear and difficult for readers to comprehend. \n    - The paper fails to clearly explain the difference between the Attention MASK introduced in section 3.2. There is a lack of example illustrations. \n    - The meanings of each row and column in the results tables are not clearly explained, and the purposes of some experiments are not explicitly stated. Furthermore, it is unclear whether the parameters of the models remain consistent across different experimental results. It would be beneficial to clarify whether the observed improvements result from parameter variations or methodological enhancements. \n    - The distinctions between Balanced Accuracy and the F1 score are not clearly explained, nor are the respective functions of these two metrics.\n\nquestions_for_the_authors: - Is the \"Intended Tolerance for Search\" in Table 5 derived under the same parameter settings as the \"Intended Tolerance for Search\" in Table 3?\n\ntypos_grammar_style_and_presentation_improvements: In line 12 of section 2.1, \"wight\" should be corrected to \"weight\".\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "JjFp24TQD4",
        "similarity": 0.7662,
        "coverage": 0.3548,
        "human_length": 525,
        "human_text": "paper_topic_and_main_contributions: **Summary:** Current Automatic Program Repair (APR) studies mainly focus on improving these models to generate correct patches for the faulty program. However, investigating the false behavior of these models was not addressed in the previous works. To address this, the paper proposes a methodology for diagnosing and treating the false behaviors of the transformer-based program repair models. The paper mainly proposes a behavior vector together with a behavior discriminator to quantify the behavior of the models and employ them to improve the general APR models\u2019 performance.\n**Contributions:** -The paper introduces a novel perspective to improve program repair tasks by diagnosing and treating the false behaviors of transformer-based program repair models. \n-The paper proposes a behavior vector based on transformer-based models to diagnose and mitigate false behaviors of the model in generating the patches for the given buggy program. \n-The evaluation results for 55,562 instances of different datasets showed the effectiveness of the approach in identifying correct and incorrect patches (It correctly classified 86.9% of correct and 86.4% of incorrect patches).\n\nreasons_to_accept: - The paper proposed a novel perspective to improve program repair tasks by considering the false behavior of the models.\n- It provides extensive experimental results on three different models and four different datasets.\n- It shows the effectiveness of the approach in identifying correct and incorrect patches. Furthermore, it employs false behavior diagnoses to improve the APR models.  - The paper is well-written and easy to follow.\n\nreasons_to_reject: - The novelty of the behavior vector is questionable and it is not clear if it works better than using the model itself to classify the patches (e.g., using a specific token like [CLS] as the classification vector representation, see the \u201cQuestions For The Authors\u201d) .\n- The training details of the CodeGen model are not provided. E.g., the input size, learning rate, etc.\n\nquestions_for_the_authors: - It is not clear if the proposed behavior vector is the best option to provide representation for classifying the patches. Could you please justify why we can not use the model itself to classify the patches? Or adding a special token (e.g., [CLS] token) to the inputs and employing that as the behavior vector representation?\n- It is not clear how the CodeGen model was trained (fine-tuned). Could you please provide the training details, including the learning rate, input structure, and input size?\n- Do you think the output of BeDisc can be used as an implicit uncertainty estimation?  - Table 6 is a bit vague. What does T2 stand for (I know it refers to the treatments. However, it would be better to provide the details)? Why does the 7th pair cause the computational capacity issue?\n\ntypos_grammar_style_and_presentation_improvements: - At 2.1 add what is d_k (Eq 2)? I suggest to provide one or two sentences about that.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "MnlrLApm1G",
        "similarity": 0.7198,
        "coverage": 0.5,
        "human_length": 284,
        "human_text": "paper_topic_and_main_contributions: Automated program repair is an widely discussed topic and has potential to reduce costs associated with bug fixing in software development and maintenance procedure. This paper proposes some novel techniques to improve performance of such program repairs. \nAuthors proposed a method to diagnose and treat false behaviors of transformer-based program repair models with the help of a behavior vector, a behavior discriminator (BeDisc) that identifies false behaviors, and two methods for false behavior treatment. \nTheir novelty is in improving program repair tasks by analyzing the internal behaviors of transformer-based models by demonstrating internal behavior of a transformer-based program repair model.\n\nreasons_to_accept: Along with a good overall presentation of the paper it has some strengths. Treatment 1, Abortions has good potential false behavior diagnosis. Also Treatment 2, masked Bypassing eliminates suspicious target tokens responsible for false behaviour while producing patches. \nOverall improvements seem to be significant. Qualitative and quantitatively significant datasets are used.\n\nreasons_to_reject: One of the weakness is that as per table 2. still there are some false negatives. It is not clear how significant can such false negatives be for such systems. \nAnother weak point is overall the paper is not easy to read for general audience.\n\nquestions_for_the_authors: A. Can you please clarify impact of false negatives on the system?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "ZlFFZiAbte",
        "length": 390,
        "human_text": "paper_topic_and_main_contributions: - This paper explores the diagnosis and treatment of False Behavior in the Patches generation process within the Program Repair Model.\n- This paper proposes a highly compatible framework for diagnosing and treating False Behavior and accordingly introduces the Behavior Vector for diagnosis. Additionally, two methods for treatment are proposed: abortion and masked bypassing.\n- Experimental results on numerous datasets show that simply adding the modules proposed in this paper can effectively increase the number of correctly repaired programs, without changing the baseline.\n\nreasons_to_accept: This paper enhances the performance of the program repair model by analyzing its internal generation process from a novel perspective. This direction of improvement represents a promising path worth exploring.\n\nreasons_to_reject: - The writing and readability of this paper leave room for improvement. Some key points in the article lack detailed descriptions and explanations, making the content unclear and difficult for readers to comprehend. \n    - The paper fails to clearly explain the difference between the Attention MASK introduced in section 3.2. There is a lack of example illustrations. \n    - The meanings of each row and column in the results tables are not clearly explained, and the purposes of some experiments are not explicitly stated. Furthermore, it is unclear whether the parameters of the models remain consistent across different experimental results. It would be beneficial to clarify whether the observed improvements result from parameter variations or methodological enhancements. \n    - The distinctions between Balanced Accuracy and the F1 score are not clearly explained, nor are the respective functions of these two metrics.\n\nquestions_for_the_authors: - Is the \"Intended Tolerance for Search\" in Table 5 derived under the same parameter settings as the \"Intended Tolerance for Search\" in Table 3?\n\ntypos_grammar_style_and_presentation_improvements: In line 12 of section 2.1, \"wight\" should be corrected to \"weight\".\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "JjFp24TQD4",
        "length": 525,
        "human_text": "paper_topic_and_main_contributions: **Summary:** Current Automatic Program Repair (APR) studies mainly focus on improving these models to generate correct patches for the faulty program. However, investigating the false behavior of these models was not addressed in the previous works. To address this, the paper proposes a methodology for diagnosing and treating the false behaviors of the transformer-based program repair models. The paper mainly proposes a behavior vector together with a behavior discriminator to quantify the behavior of the models and employ them to improve the general APR models\u2019 performance.\n**Contributions:** -The paper introduces a novel perspective to improve program repair tasks by diagnosing and treating the false behaviors of transformer-based program repair models. \n-The paper proposes a behavior vector based on transformer-based models to diagnose and mitigate false behaviors of the model in generating the patches for the given buggy program. \n-The evaluation results for 55,562 instances of different datasets showed the effectiveness of the approach in identifying correct and incorrect patches (It correctly classified 86.9% of correct and 86.4% of incorrect patches).\n\nreasons_to_accept: - The paper proposed a novel perspective to improve program repair tasks by considering the false behavior of the models.\n- It provides extensive experimental results on three different models and four different datasets.\n- It shows the effectiveness of the approach in identifying correct and incorrect patches. Furthermore, it employs false behavior diagnoses to improve the APR models.  - The paper is well-written and easy to follow.\n\nreasons_to_reject: - The novelty of the behavior vector is questionable and it is not clear if it works better than using the model itself to classify the patches (e.g., using a specific token like [CLS] as the classification vector representation, see the \u201cQuestions For The Authors\u201d) .\n- The training details of the CodeGen model are not provided. E.g., the input size, learning rate, etc.\n\nquestions_for_the_authors: - It is not clear if the proposed behavior vector is the best option to provide representation for classifying the patches. Could you please justify why we can not use the model itself to classify the patches? Or adding a special token (e.g., [CLS] token) to the inputs and employing that as the behavior vector representation?\n- It is not clear how the CodeGen model was trained (fine-tuned). Could you please provide the training details, including the learning rate, input structure, and input size?\n- Do you think the output of BeDisc can be used as an implicit uncertainty estimation?  - Table 6 is a bit vague. What does T2 stand for (I know it refers to the treatments. However, it would be better to provide the details)? Why does the 7th pair cause the computational capacity issue?\n\ntypos_grammar_style_and_presentation_improvements: - At 2.1 add what is d_k (Eq 2)? I suggest to provide one or two sentences about that.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "MnlrLApm1G",
        "length": 284,
        "human_text": "paper_topic_and_main_contributions: Automated program repair is an widely discussed topic and has potential to reduce costs associated with bug fixing in software development and maintenance procedure. This paper proposes some novel techniques to improve performance of such program repairs. \nAuthors proposed a method to diagnose and treat false behaviors of transformer-based program repair models with the help of a behavior vector, a behavior discriminator (BeDisc) that identifies false behaviors, and two methods for false behavior treatment. \nTheir novelty is in improving program repair tasks by analyzing the internal behaviors of transformer-based models by demonstrating internal behavior of a transformer-based program repair model.\n\nreasons_to_accept: Along with a good overall presentation of the paper it has some strengths. Treatment 1, Abortions has good potential false behavior diagnosis. Also Treatment 2, masked Bypassing eliminates suspicious target tokens responsible for false behaviour while producing patches. \nOverall improvements seem to be significant. Qualitative and quantitatively significant datasets are used.\n\nreasons_to_reject: One of the weakness is that as per table 2. still there are some false negatives. It is not clear how significant can such false negatives be for such systems. \nAnother weak point is overall the paper is not easy to read for general audience.\n\nquestions_for_the_authors: A. Can you please clarify impact of false negatives on the system?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "158_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_158_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7301666666666667,
      "max_similarity": 0.7466,
      "avg_coverage": 0.6162666666666666,
      "max_coverage": 0.7059
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 848,
      "avg_human_length": 379.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 9,
      "suggestions_count": 15
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "frrb5tLalt",
        "similarity": 0.7466,
        "coverage": 0.7059,
        "human_length": 477,
        "human_text": "paper_topic_and_main_contributions: This paper looks at the problem of compressing the model size of a multilingual language model when its intended downstream use is on a monolingual task. The method the authors present is called vocabulary trimming (VT), which involves deleting tokens from the vocabulary which do not feature in the target language.  The main contributions of this paper are the following: - Explanation of VT, a method for removing tokens from the multilingual vocabulary which are not relevant to the target language.  - Extensive empirical work showing that VT has a minimal impact on downstream monolingual tasks (question answering, question generation, sentiment analysis, natural language inference) - Additional empirical work exploring the impact of vocabulary size and when VT is carried out - Exploration of VT as a debiasing technique\n\nreasons_to_accept: 1) The paper presents extensive empirical work backing up their claims covering multiple high-resource languages. \n2) Their figures are particularly clear and helpful for explaining their method 3) Their method is well-situated in the current literature and they cover a wide range of related work\n\nreasons_to_reject: 1) From reading the introduction and section 4, I was unclear why VT was an improvement over simply using an existing monolingual model. It would be good to have a better justification/explanation in the introduction and/or a comparison with monolingual models in the analysis in section 4 2) On lines 103-4, the authors say it is a \"natural question\" to ask if VT impacts bias levels. As someone unfamiliar with this field, I found the switch to VT's impact on bias jarring. It would help if the authors could make it clearer how they made this jump e.g. comparison with existing literature\n\ntypos_grammar_style_and_presentation_improvements: - line 1: missing bracket after \"(LMs)\" - Figure 4: graphs are too small to interpret easily. Perhaps these results would be easier to understand in a table?\n- Table 1: having two metrics side by side was confusing and overloaded the table. Given they both say roughly the same thing, perhaps just pick one and put the full results for the other in the appendix - Tables 1, 2, 3: There is too much information here to parse easily. Given the main point here is that there is not much change over the baseline, a delta might be easier to understand than a raw result.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "4m6NZAgQLb",
        "similarity": 0.7369,
        "coverage": 0.6429,
        "human_length": 275,
        "human_text": "paper_topic_and_main_contributions: This paper introduces vocabulary trimming (VT) which aims to reduce a multilingual LM to a monolingual LM. The method involves trimming the vocabulary of a multilingual LM, using a target language's monolingual corpus, to include only tokens relevant to the target language. They consider applying VT before or after finetuning, and show the tradeoffs between both methods. Furthermore, they compare a trimmed multilingual model to a monolingual model trained from scratch and show that the trimmed model has less bias.\n\nreasons_to_accept: - Fairly efficient method because you can potentially extract several monolingual LMs from a single multilingual LM. \n-\n\nreasons_to_reject: - No analysis of tradeoff between extracting monolingual LM and potentially losing helpful cross-lingual transfer.  - Given that it is a long paper, it is missing important analysis such as effect of monolingual corpora domain and size, result on resourced languages and effect of % of target language composition in original multilingual corpora.  - Only evaluate on English - Did not evaluate on larger LMs.\n\nquestions_for_the_authors: - Why report only accuracy for NLI?\n- Please report effect of VT on low resourced languages.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "qbGBfTcjY7",
        "similarity": 0.707,
        "coverage": 0.5,
        "human_length": 387,
        "human_text": "paper_topic_and_main_contributions: Authors propose a simple vocabulary trimming technique that removes tokens from the vocabulary of multilingual LMs by identifying language-specific tokens and retaining those. Given a large enough text corpus (on which to perform token retention), theoretically there should be no drop in model performance.\n\nreasons_to_accept: - This method can significantly reduce the memory footprint of multilingual models, especially smaller multilingual models where a large portion of the parameters are tied to the vocabulary embedding lookup table.\n- While the capabilities are of this method are mostly tied to memory reduction\n\nreasons_to_reject: - This method essentially removes the capability of the model to do well in languages other than the intended language. While theoretically sound and would work well on academic datasets, real world text is rife with code-switching and text in multiple languages. This could limit the capabilities of this method.\n- Much of the benefits of this method could be potentially covered by just loading the vocabulary embeddings into CPU memory and pushing the required vocabulary embeddings per batch into GPU memory on-the-fly. Have the authors compared the inference latency increases using this method?\n\nquestions_for_the_authors: - It's unclear why pre-FT VT reduces finetuning time -- if a language can be modeled with a subset of vocabulary V, V_subset, then finetuning would theoretically have the same computational complexity since we only use a subset of the vocabulary anyways (assuming V_subset contains all the tokens we need for that particular language). Is it just the extra time to perform the embedding lookup?\n- Is there a performane drop for pre-FT VT, since you can't leverage translate-train-all or cross-lingual transfer from high resource languages?\n- Have the authors tried this method for ultra-low-resource languages and on datasets like MasakhaNER?\n- What is the size of the per-language text corpora used to perform vocabulary retention?\n- Do the authors see a marked increase in OOV after applying this technique?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "frrb5tLalt",
        "length": 477,
        "human_text": "paper_topic_and_main_contributions: This paper looks at the problem of compressing the model size of a multilingual language model when its intended downstream use is on a monolingual task. The method the authors present is called vocabulary trimming (VT), which involves deleting tokens from the vocabulary which do not feature in the target language.  The main contributions of this paper are the following: - Explanation of VT, a method for removing tokens from the multilingual vocabulary which are not relevant to the target language.  - Extensive empirical work showing that VT has a minimal impact on downstream monolingual tasks (question answering, question generation, sentiment analysis, natural language inference) - Additional empirical work exploring the impact of vocabulary size and when VT is carried out - Exploration of VT as a debiasing technique\n\nreasons_to_accept: 1) The paper presents extensive empirical work backing up their claims covering multiple high-resource languages. \n2) Their figures are particularly clear and helpful for explaining their method 3) Their method is well-situated in the current literature and they cover a wide range of related work\n\nreasons_to_reject: 1) From reading the introduction and section 4, I was unclear why VT was an improvement over simply using an existing monolingual model. It would be good to have a better justification/explanation in the introduction and/or a comparison with monolingual models in the analysis in section 4 2) On lines 103-4, the authors say it is a \"natural question\" to ask if VT impacts bias levels. As someone unfamiliar with this field, I found the switch to VT's impact on bias jarring. It would help if the authors could make it clearer how they made this jump e.g. comparison with existing literature\n\ntypos_grammar_style_and_presentation_improvements: - line 1: missing bracket after \"(LMs)\" - Figure 4: graphs are too small to interpret easily. Perhaps these results would be easier to understand in a table?\n- Table 1: having two metrics side by side was confusing and overloaded the table. Given they both say roughly the same thing, perhaps just pick one and put the full results for the other in the appendix - Tables 1, 2, 3: There is too much information here to parse easily. Given the main point here is that there is not much change over the baseline, a delta might be easier to understand than a raw result.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "4m6NZAgQLb",
        "length": 275,
        "human_text": "paper_topic_and_main_contributions: This paper introduces vocabulary trimming (VT) which aims to reduce a multilingual LM to a monolingual LM. The method involves trimming the vocabulary of a multilingual LM, using a target language's monolingual corpus, to include only tokens relevant to the target language. They consider applying VT before or after finetuning, and show the tradeoffs between both methods. Furthermore, they compare a trimmed multilingual model to a monolingual model trained from scratch and show that the trimmed model has less bias.\n\nreasons_to_accept: - Fairly efficient method because you can potentially extract several monolingual LMs from a single multilingual LM. \n-\n\nreasons_to_reject: - No analysis of tradeoff between extracting monolingual LM and potentially losing helpful cross-lingual transfer.  - Given that it is a long paper, it is missing important analysis such as effect of monolingual corpora domain and size, result on resourced languages and effect of % of target language composition in original multilingual corpora.  - Only evaluate on English - Did not evaluate on larger LMs.\n\nquestions_for_the_authors: - Why report only accuracy for NLI?\n- Please report effect of VT on low resourced languages.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "qbGBfTcjY7",
        "length": 387,
        "human_text": "paper_topic_and_main_contributions: Authors propose a simple vocabulary trimming technique that removes tokens from the vocabulary of multilingual LMs by identifying language-specific tokens and retaining those. Given a large enough text corpus (on which to perform token retention), theoretically there should be no drop in model performance.\n\nreasons_to_accept: - This method can significantly reduce the memory footprint of multilingual models, especially smaller multilingual models where a large portion of the parameters are tied to the vocabulary embedding lookup table.\n- While the capabilities are of this method are mostly tied to memory reduction\n\nreasons_to_reject: - This method essentially removes the capability of the model to do well in languages other than the intended language. While theoretically sound and would work well on academic datasets, real world text is rife with code-switching and text in multiple languages. This could limit the capabilities of this method.\n- Much of the benefits of this method could be potentially covered by just loading the vocabulary embeddings into CPU memory and pushing the required vocabulary embeddings per batch into GPU memory on-the-fly. Have the authors compared the inference latency increases using this method?\n\nquestions_for_the_authors: - It's unclear why pre-FT VT reduces finetuning time -- if a language can be modeled with a subset of vocabulary V, V_subset, then finetuning would theoretically have the same computational complexity since we only use a subset of the vocabulary anyways (assuming V_subset contains all the tokens we need for that particular language). Is it just the extra time to perform the embedding lookup?\n- Is there a performane drop for pre-FT VT, since you can't leverage translate-train-all or cross-lingual transfer from high resource languages?\n- Have the authors tried this method for ultra-low-resource languages and on datasets like MasakhaNER?\n- What is the size of the per-language text corpora used to perform vocabulary retention?\n- Do the authors see a marked increase in OOV after applying this technique?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "158_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_158_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7300333333333334,
      "max_similarity": 0.7473,
      "avg_coverage": 0.6978,
      "max_coverage": 0.8235
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 904,
      "avg_human_length": 379.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 8,
      "suggestions_count": 14
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "frrb5tLalt",
        "similarity": 0.7473,
        "coverage": 0.8235,
        "human_length": 477,
        "human_text": "paper_topic_and_main_contributions: This paper looks at the problem of compressing the model size of a multilingual language model when its intended downstream use is on a monolingual task. The method the authors present is called vocabulary trimming (VT), which involves deleting tokens from the vocabulary which do not feature in the target language.  The main contributions of this paper are the following: - Explanation of VT, a method for removing tokens from the multilingual vocabulary which are not relevant to the target language.  - Extensive empirical work showing that VT has a minimal impact on downstream monolingual tasks (question answering, question generation, sentiment analysis, natural language inference) - Additional empirical work exploring the impact of vocabulary size and when VT is carried out - Exploration of VT as a debiasing technique\n\nreasons_to_accept: 1) The paper presents extensive empirical work backing up their claims covering multiple high-resource languages. \n2) Their figures are particularly clear and helpful for explaining their method 3) Their method is well-situated in the current literature and they cover a wide range of related work\n\nreasons_to_reject: 1) From reading the introduction and section 4, I was unclear why VT was an improvement over simply using an existing monolingual model. It would be good to have a better justification/explanation in the introduction and/or a comparison with monolingual models in the analysis in section 4 2) On lines 103-4, the authors say it is a \"natural question\" to ask if VT impacts bias levels. As someone unfamiliar with this field, I found the switch to VT's impact on bias jarring. It would help if the authors could make it clearer how they made this jump e.g. comparison with existing literature\n\ntypos_grammar_style_and_presentation_improvements: - line 1: missing bracket after \"(LMs)\" - Figure 4: graphs are too small to interpret easily. Perhaps these results would be easier to understand in a table?\n- Table 1: having two metrics side by side was confusing and overloaded the table. Given they both say roughly the same thing, perhaps just pick one and put the full results for the other in the appendix - Tables 1, 2, 3: There is too much information here to parse easily. Given the main point here is that there is not much change over the baseline, a delta might be easier to understand than a raw result.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "4m6NZAgQLb",
        "similarity": 0.7333,
        "coverage": 0.7143,
        "human_length": 275,
        "human_text": "paper_topic_and_main_contributions: This paper introduces vocabulary trimming (VT) which aims to reduce a multilingual LM to a monolingual LM. The method involves trimming the vocabulary of a multilingual LM, using a target language's monolingual corpus, to include only tokens relevant to the target language. They consider applying VT before or after finetuning, and show the tradeoffs between both methods. Furthermore, they compare a trimmed multilingual model to a monolingual model trained from scratch and show that the trimmed model has less bias.\n\nreasons_to_accept: - Fairly efficient method because you can potentially extract several monolingual LMs from a single multilingual LM. \n-\n\nreasons_to_reject: - No analysis of tradeoff between extracting monolingual LM and potentially losing helpful cross-lingual transfer.  - Given that it is a long paper, it is missing important analysis such as effect of monolingual corpora domain and size, result on resourced languages and effect of % of target language composition in original multilingual corpora.  - Only evaluate on English - Did not evaluate on larger LMs.\n\nquestions_for_the_authors: - Why report only accuracy for NLI?\n- Please report effect of VT on low resourced languages.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "qbGBfTcjY7",
        "similarity": 0.7095,
        "coverage": 0.5556,
        "human_length": 387,
        "human_text": "paper_topic_and_main_contributions: Authors propose a simple vocabulary trimming technique that removes tokens from the vocabulary of multilingual LMs by identifying language-specific tokens and retaining those. Given a large enough text corpus (on which to perform token retention), theoretically there should be no drop in model performance.\n\nreasons_to_accept: - This method can significantly reduce the memory footprint of multilingual models, especially smaller multilingual models where a large portion of the parameters are tied to the vocabulary embedding lookup table.\n- While the capabilities are of this method are mostly tied to memory reduction\n\nreasons_to_reject: - This method essentially removes the capability of the model to do well in languages other than the intended language. While theoretically sound and would work well on academic datasets, real world text is rife with code-switching and text in multiple languages. This could limit the capabilities of this method.\n- Much of the benefits of this method could be potentially covered by just loading the vocabulary embeddings into CPU memory and pushing the required vocabulary embeddings per batch into GPU memory on-the-fly. Have the authors compared the inference latency increases using this method?\n\nquestions_for_the_authors: - It's unclear why pre-FT VT reduces finetuning time -- if a language can be modeled with a subset of vocabulary V, V_subset, then finetuning would theoretically have the same computational complexity since we only use a subset of the vocabulary anyways (assuming V_subset contains all the tokens we need for that particular language). Is it just the extra time to perform the embedding lookup?\n- Is there a performane drop for pre-FT VT, since you can't leverage translate-train-all or cross-lingual transfer from high resource languages?\n- Have the authors tried this method for ultra-low-resource languages and on datasets like MasakhaNER?\n- What is the size of the per-language text corpora used to perform vocabulary retention?\n- Do the authors see a marked increase in OOV after applying this technique?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "frrb5tLalt",
        "length": 477,
        "human_text": "paper_topic_and_main_contributions: This paper looks at the problem of compressing the model size of a multilingual language model when its intended downstream use is on a monolingual task. The method the authors present is called vocabulary trimming (VT), which involves deleting tokens from the vocabulary which do not feature in the target language.  The main contributions of this paper are the following: - Explanation of VT, a method for removing tokens from the multilingual vocabulary which are not relevant to the target language.  - Extensive empirical work showing that VT has a minimal impact on downstream monolingual tasks (question answering, question generation, sentiment analysis, natural language inference) - Additional empirical work exploring the impact of vocabulary size and when VT is carried out - Exploration of VT as a debiasing technique\n\nreasons_to_accept: 1) The paper presents extensive empirical work backing up their claims covering multiple high-resource languages. \n2) Their figures are particularly clear and helpful for explaining their method 3) Their method is well-situated in the current literature and they cover a wide range of related work\n\nreasons_to_reject: 1) From reading the introduction and section 4, I was unclear why VT was an improvement over simply using an existing monolingual model. It would be good to have a better justification/explanation in the introduction and/or a comparison with monolingual models in the analysis in section 4 2) On lines 103-4, the authors say it is a \"natural question\" to ask if VT impacts bias levels. As someone unfamiliar with this field, I found the switch to VT's impact on bias jarring. It would help if the authors could make it clearer how they made this jump e.g. comparison with existing literature\n\ntypos_grammar_style_and_presentation_improvements: - line 1: missing bracket after \"(LMs)\" - Figure 4: graphs are too small to interpret easily. Perhaps these results would be easier to understand in a table?\n- Table 1: having two metrics side by side was confusing and overloaded the table. Given they both say roughly the same thing, perhaps just pick one and put the full results for the other in the appendix - Tables 1, 2, 3: There is too much information here to parse easily. Given the main point here is that there is not much change over the baseline, a delta might be easier to understand than a raw result.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "4m6NZAgQLb",
        "length": 275,
        "human_text": "paper_topic_and_main_contributions: This paper introduces vocabulary trimming (VT) which aims to reduce a multilingual LM to a monolingual LM. The method involves trimming the vocabulary of a multilingual LM, using a target language's monolingual corpus, to include only tokens relevant to the target language. They consider applying VT before or after finetuning, and show the tradeoffs between both methods. Furthermore, they compare a trimmed multilingual model to a monolingual model trained from scratch and show that the trimmed model has less bias.\n\nreasons_to_accept: - Fairly efficient method because you can potentially extract several monolingual LMs from a single multilingual LM. \n-\n\nreasons_to_reject: - No analysis of tradeoff between extracting monolingual LM and potentially losing helpful cross-lingual transfer.  - Given that it is a long paper, it is missing important analysis such as effect of monolingual corpora domain and size, result on resourced languages and effect of % of target language composition in original multilingual corpora.  - Only evaluate on English - Did not evaluate on larger LMs.\n\nquestions_for_the_authors: - Why report only accuracy for NLI?\n- Please report effect of VT on low resourced languages.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "qbGBfTcjY7",
        "length": 387,
        "human_text": "paper_topic_and_main_contributions: Authors propose a simple vocabulary trimming technique that removes tokens from the vocabulary of multilingual LMs by identifying language-specific tokens and retaining those. Given a large enough text corpus (on which to perform token retention), theoretically there should be no drop in model performance.\n\nreasons_to_accept: - This method can significantly reduce the memory footprint of multilingual models, especially smaller multilingual models where a large portion of the parameters are tied to the vocabulary embedding lookup table.\n- While the capabilities are of this method are mostly tied to memory reduction\n\nreasons_to_reject: - This method essentially removes the capability of the model to do well in languages other than the intended language. While theoretically sound and would work well on academic datasets, real world text is rife with code-switching and text in multiple languages. This could limit the capabilities of this method.\n- Much of the benefits of this method could be potentially covered by just loading the vocabulary embeddings into CPU memory and pushing the required vocabulary embeddings per batch into GPU memory on-the-fly. Have the authors compared the inference latency increases using this method?\n\nquestions_for_the_authors: - It's unclear why pre-FT VT reduces finetuning time -- if a language can be modeled with a subset of vocabulary V, V_subset, then finetuning would theoretically have the same computational complexity since we only use a subset of the vocabulary anyways (assuming V_subset contains all the tokens we need for that particular language). Is it just the extra time to perform the embedding lookup?\n- Is there a performane drop for pre-FT VT, since you can't leverage translate-train-all or cross-lingual transfer from high resource languages?\n- Have the authors tried this method for ultra-low-resource languages and on datasets like MasakhaNER?\n- What is the size of the per-language text corpora used to perform vocabulary retention?\n- Do the authors see a marked increase in OOV after applying this technique?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "189_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_189_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.745225,
      "max_similarity": 0.7653,
      "avg_coverage": 0.474675,
      "max_coverage": 0.5556
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 613,
      "avg_human_length": 423.5
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 9,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "0GKSZYu6FS",
        "similarity": 0.7653,
        "coverage": 0.3793,
        "human_length": 427,
        "human_text": "paper_topic_and_main_contributions: This paper is about mental health analysis topic. The main contributions are: a publicly accessible Chinese Cognitive Distortion Dataset is constructed for the first time. The authors explore the association between cognitive distortions and various mental disorders on social media. The authors attempt to incorporate information about users\u2019 cognitive distortions into mental disorder detection models to illustrate the effectiveness of using cognitive distortion.\n\nreasons_to_accept: The C2D2 dataset is valuable for the mental health field. It is a publicly accessible resource within this domain that includes numerous texts recording individuals\u2019 thoughts in various scenes with cognitive distortion. This dataset provides a profound connection between cognitive distortions and mental disorders. Addressing these distortions could potentially enhance mental health interventions. The authors find that integrating cognitive distortions as an additional feature can augment the performance of existing mental disorder detection models. In addition, the overall structure is well-organised.\n\nreasons_to_reject: The motivation \"However, mental health issues in developing countries often receive limited attention\" described for not annotating the dataset in English is inaccurate. English has a broader applicability, and the authors could also consider incorporating English annotations. While developing countries are essential to consider, it is also worthwhile to focus on datasets in languages other than Chinese.\nFor the test datasets, do the authors consider post-level model tests? Because the C2D2 is also the post-level dataset. Additionally, some other datasets could be added, like SWMH, T-SID, etc. \nIn Sec 6.2, the authors should train the RoBERTa-based model because of the performance shown in Table 5. \nIn Fig.4, do all posts with depression or PTSD have cognitive distortions? Add Non-sd labels?\nThere is no human expert performance on this dataset.\nMany details need to be expressed clearly. What is the meaning of ppopularity in (4)?\nThe reasonable about \"extracting posts that contain cognitive distortions\" should be discussed.  Why not other methods?\nIf possible, it is suggested that the problem could be treated as a multi-label task when multiple cognitive distortions occur simultaneously.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "aExCY7CUuj",
        "similarity": 0.7411,
        "coverage": 0.4138,
        "human_length": 422,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors propose a cognitive distortion dataset in Chinese, consisting of 7,500 examples with paired scenes, thoughts, and cognitive distortion labels. For the dataset construction, the authors do not take the traditional approach of annotating online posts, but instead recruit volunteers and use interviews to further select qualified volunteers. The volunteers are instructed to compose thoughts based on the given scene, and also label the thoughts composed by other volunteers. The quality of the dataset is further evaluated by experts.  The authors conduct experiments using various baseline models including fully-supervised training, as well as few-shot and zero-shot using ChatGPT, demonstrating the challenge of this dataset. The author also conducts experiments and comprehensive analysis using the cognitive detection model on datasets from social media, showing the correlations between cognitive distortions and various mental health disorders.\n\nreasons_to_accept: 1. The proposed new dataset is very valuable, as currently, there is a lack of publically available datasets in the mental health domain. Such a dataset will greatly boost the research for NLP in assisting mental health treatment. \n2. The proposed dataset is the first one for Chinese. This is especially valuable since, in China, the scarcity of mental health professionals and resources is even more severe. There is an increasing demand for building automatic assistance for mental health support. \n3. The dataset shows good quality, verified by experts. \n4. The experiments and analysis demonstrate the great potential of the vast scope of future research that can be extended based on this dataset.\n\nreasons_to_reject: 1. No evaluation regarding the human expert performance on this dataset. \n2. The dataset construction process involves asking the volunteers to \"compose\" distorted thoughts but not taking the thoughts from potential patients with mental disorders from online posts. Such a method, theoretically as a kind of proxy to the thoughts of real patients,  is further verified by experts, as stated in the paper. However, I'm wondering what the gap is between the thoughts from real patients and the composed thoughts.\n\nquestions_for_the_authors: Do you pay the volunteers and the experts? If so, how do you pay them? This needs to be specified in the Ethics section.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "zmzlWjkI0i",
        "similarity": 0.7394,
        "coverage": 0.5556,
        "human_length": 398,
        "human_text": "paper_topic_and_main_contributions: This paper discusses the creation of the Chinese Cognitive Distortion Dataset, a dataset based on Chinese tweets which were then translated into English. This dataset can be used to detect cognitive distortions (present or not) and the type of cognitive distortion (e.g., emotional reasoning, overgeneralization, etc) in text. It also examines its use in previously collected datasets with social media data from users with mental illnesses.\n\nreasons_to_accept: Presents an interesting tool to be used to detect cognitive distortions and has an interesting method that could be replicated to re-create a cognitive bias dataset with another language (e.g., English, French, etc.,).\n\nreasons_to_reject: By collecting data from volunteers with prompts and then correcting the volunteer's answers (as discussed in section 3.4), the dataset may include the researcher's biases and be less generalizable or naturalistic. Unclear of potential biases in the demographics of the volunteers who generated the data. The biases and limitations of the dataset due to it being generated by volunteers and then corrected should be discussed. As well, since the authors use social media datasets it's unverifiable if the users are formally diagnosed, thus the findings discussed on 449 to 453 and 465 to 471 as well of the rest of the section should emphasize how these findings are based on self-disclosure and potential self-diagnosis.\n\nquestions_for_the_authors: 1) Will the C2D2 dataset be available in Chinese, English, or both? \n2) What are the volunteer demographics? \n3) Where/how were the volunteers recruited from?\n\nmissing_references: The findings discussed in section 6.2 are similar to research on cognitive bias types in psychology although its novel being found on social media. Could support findings with research from psychology, psychiatry and the medical field.\n\ntypos_grammar_style_and_presentation_improvements: Figure 2, in the notes missing spaces between the \"&\" in the first line  line 203, missing spaces between the \"&\".\nLine 349, extra space before \"Table\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "HPXYIRgHgT",
        "similarity": 0.7351,
        "coverage": 0.55,
        "human_length": 447,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a new dataset of examples of various cognitive distortions in the Chinese language. The authors then perform machine learning experiments to test whether cognitive distortions are possible to detect automatically from text, and also test their relation to mental disorders, based on some English datasets annotated for depression and PTSD.\nThe dataset is created through a complex and rigorous process: by first doing a selection for choosing the set of responders, then asking them to produce examples of cognitive distortions, which are then verified by experts for different criteria. The dataset is made public.\nIn the experiments, pretrained LLMs are used to detect cognitive distortions in the proposed dataset and in the English mental disorders dataset, using machine translation to make the datasets compatible (following two objectives: measuring the prevalence of cognitive distortions in mental health disorder data using the trained models, and trying to improve mental disorder detection using cognitive distortion information).\nAll the responders used for creating the dataset have agreed to participate in the study, and the ethical section contains a thorough discussion.\n\nreasons_to_accept: - An original topic and the introduction of a new dataset, created through a very thorough process - Interesting findings related to the specific cognitive distortions that occur in various mental health disorders (depression and PTSD)\n\nreasons_to_reject: - Some potential methodological problems or unclarities in the cross-lingual experiments. It would have been useful to see an analysis of the impact of translation on the dataset, since this might distort the annotations themselves - at least a reiteration of the cognitive distortion detection experiment but on the English version of the data\n\nquestions_for_the_authors: Will the English version of the dataset also be made publicly available? It might widen the scope of its usefulness in other studies.\n\nmissing_references: Not directly about cognitive distortions, but on the relation between the more general \"cognitive styles\" (including cognitive biases) and mental health disorders: Uban, A. S., Chulvi, B., & Rosso, P. (2021). An emotion and cognitive based analysis of mental health disorders from social media data. Future Generation Computer Systems, 124, 480-494.\n\ntypos_grammar_style_and_presentation_improvements: - line 72: will contributes -> contribute - title of Section 4.2: \"Related dataset\" -> \"Related datasets\"?\n- line 324: preferable not to start the paragraph with \"And\" - lines 365-367: duplicate sentence (at least in meaning)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "0GKSZYu6FS",
        "length": 427,
        "human_text": "paper_topic_and_main_contributions: This paper is about mental health analysis topic. The main contributions are: a publicly accessible Chinese Cognitive Distortion Dataset is constructed for the first time. The authors explore the association between cognitive distortions and various mental disorders on social media. The authors attempt to incorporate information about users\u2019 cognitive distortions into mental disorder detection models to illustrate the effectiveness of using cognitive distortion.\n\nreasons_to_accept: The C2D2 dataset is valuable for the mental health field. It is a publicly accessible resource within this domain that includes numerous texts recording individuals\u2019 thoughts in various scenes with cognitive distortion. This dataset provides a profound connection between cognitive distortions and mental disorders. Addressing these distortions could potentially enhance mental health interventions. The authors find that integrating cognitive distortions as an additional feature can augment the performance of existing mental disorder detection models. In addition, the overall structure is well-organised.\n\nreasons_to_reject: The motivation \"However, mental health issues in developing countries often receive limited attention\" described for not annotating the dataset in English is inaccurate. English has a broader applicability, and the authors could also consider incorporating English annotations. While developing countries are essential to consider, it is also worthwhile to focus on datasets in languages other than Chinese.\nFor the test datasets, do the authors consider post-level model tests? Because the C2D2 is also the post-level dataset. Additionally, some other datasets could be added, like SWMH, T-SID, etc. \nIn Sec 6.2, the authors should train the RoBERTa-based model because of the performance shown in Table 5. \nIn Fig.4, do all posts with depression or PTSD have cognitive distortions? Add Non-sd labels?\nThere is no human expert performance on this dataset.\nMany details need to be expressed clearly. What is the meaning of ppopularity in (4)?\nThe reasonable about \"extracting posts that contain cognitive distortions\" should be discussed.  Why not other methods?\nIf possible, it is suggested that the problem could be treated as a multi-label task when multiple cognitive distortions occur simultaneously.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "aExCY7CUuj",
        "length": 422,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors propose a cognitive distortion dataset in Chinese, consisting of 7,500 examples with paired scenes, thoughts, and cognitive distortion labels. For the dataset construction, the authors do not take the traditional approach of annotating online posts, but instead recruit volunteers and use interviews to further select qualified volunteers. The volunteers are instructed to compose thoughts based on the given scene, and also label the thoughts composed by other volunteers. The quality of the dataset is further evaluated by experts.  The authors conduct experiments using various baseline models including fully-supervised training, as well as few-shot and zero-shot using ChatGPT, demonstrating the challenge of this dataset. The author also conducts experiments and comprehensive analysis using the cognitive detection model on datasets from social media, showing the correlations between cognitive distortions and various mental health disorders.\n\nreasons_to_accept: 1. The proposed new dataset is very valuable, as currently, there is a lack of publically available datasets in the mental health domain. Such a dataset will greatly boost the research for NLP in assisting mental health treatment. \n2. The proposed dataset is the first one for Chinese. This is especially valuable since, in China, the scarcity of mental health professionals and resources is even more severe. There is an increasing demand for building automatic assistance for mental health support. \n3. The dataset shows good quality, verified by experts. \n4. The experiments and analysis demonstrate the great potential of the vast scope of future research that can be extended based on this dataset.\n\nreasons_to_reject: 1. No evaluation regarding the human expert performance on this dataset. \n2. The dataset construction process involves asking the volunteers to \"compose\" distorted thoughts but not taking the thoughts from potential patients with mental disorders from online posts. Such a method, theoretically as a kind of proxy to the thoughts of real patients,  is further verified by experts, as stated in the paper. However, I'm wondering what the gap is between the thoughts from real patients and the composed thoughts.\n\nquestions_for_the_authors: Do you pay the volunteers and the experts? If so, how do you pay them? This needs to be specified in the Ethics section.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "zmzlWjkI0i",
        "length": 398,
        "human_text": "paper_topic_and_main_contributions: This paper discusses the creation of the Chinese Cognitive Distortion Dataset, a dataset based on Chinese tweets which were then translated into English. This dataset can be used to detect cognitive distortions (present or not) and the type of cognitive distortion (e.g., emotional reasoning, overgeneralization, etc) in text. It also examines its use in previously collected datasets with social media data from users with mental illnesses.\n\nreasons_to_accept: Presents an interesting tool to be used to detect cognitive distortions and has an interesting method that could be replicated to re-create a cognitive bias dataset with another language (e.g., English, French, etc.,).\n\nreasons_to_reject: By collecting data from volunteers with prompts and then correcting the volunteer's answers (as discussed in section 3.4), the dataset may include the researcher's biases and be less generalizable or naturalistic. Unclear of potential biases in the demographics of the volunteers who generated the data. The biases and limitations of the dataset due to it being generated by volunteers and then corrected should be discussed. As well, since the authors use social media datasets it's unverifiable if the users are formally diagnosed, thus the findings discussed on 449 to 453 and 465 to 471 as well of the rest of the section should emphasize how these findings are based on self-disclosure and potential self-diagnosis.\n\nquestions_for_the_authors: 1) Will the C2D2 dataset be available in Chinese, English, or both? \n2) What are the volunteer demographics? \n3) Where/how were the volunteers recruited from?\n\nmissing_references: The findings discussed in section 6.2 are similar to research on cognitive bias types in psychology although its novel being found on social media. Could support findings with research from psychology, psychiatry and the medical field.\n\ntypos_grammar_style_and_presentation_improvements: Figure 2, in the notes missing spaces between the \"&\" in the first line  line 203, missing spaces between the \"&\".\nLine 349, extra space before \"Table\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "HPXYIRgHgT",
        "length": 447,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a new dataset of examples of various cognitive distortions in the Chinese language. The authors then perform machine learning experiments to test whether cognitive distortions are possible to detect automatically from text, and also test their relation to mental disorders, based on some English datasets annotated for depression and PTSD.\nThe dataset is created through a complex and rigorous process: by first doing a selection for choosing the set of responders, then asking them to produce examples of cognitive distortions, which are then verified by experts for different criteria. The dataset is made public.\nIn the experiments, pretrained LLMs are used to detect cognitive distortions in the proposed dataset and in the English mental disorders dataset, using machine translation to make the datasets compatible (following two objectives: measuring the prevalence of cognitive distortions in mental health disorder data using the trained models, and trying to improve mental disorder detection using cognitive distortion information).\nAll the responders used for creating the dataset have agreed to participate in the study, and the ethical section contains a thorough discussion.\n\nreasons_to_accept: - An original topic and the introduction of a new dataset, created through a very thorough process - Interesting findings related to the specific cognitive distortions that occur in various mental health disorders (depression and PTSD)\n\nreasons_to_reject: - Some potential methodological problems or unclarities in the cross-lingual experiments. It would have been useful to see an analysis of the impact of translation on the dataset, since this might distort the annotations themselves - at least a reiteration of the cognitive distortion detection experiment but on the English version of the data\n\nquestions_for_the_authors: Will the English version of the dataset also be made publicly available? It might widen the scope of its usefulness in other studies.\n\nmissing_references: Not directly about cognitive distortions, but on the relation between the more general \"cognitive styles\" (including cognitive biases) and mental health disorders: Uban, A. S., Chulvi, B., & Rosso, P. (2021). An emotion and cognitive based analysis of mental health disorders from social media data. Future Generation Computer Systems, 124, 480-494.\n\ntypos_grammar_style_and_presentation_improvements: - line 72: will contributes -> contribute - title of Section 4.2: \"Related dataset\" -> \"Related datasets\"?\n- line 324: preferable not to start the paragraph with \"And\" - lines 365-367: duplicate sentence (at least in meaning)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "189_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_189_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.7449,
      "max_similarity": 0.7623,
      "avg_coverage": 0.478025,
      "max_coverage": 0.55
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 651,
      "avg_human_length": 423.5
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 9,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "0GKSZYu6FS",
        "similarity": 0.7623,
        "coverage": 0.4483,
        "human_length": 427,
        "human_text": "paper_topic_and_main_contributions: This paper is about mental health analysis topic. The main contributions are: a publicly accessible Chinese Cognitive Distortion Dataset is constructed for the first time. The authors explore the association between cognitive distortions and various mental disorders on social media. The authors attempt to incorporate information about users\u2019 cognitive distortions into mental disorder detection models to illustrate the effectiveness of using cognitive distortion.\n\nreasons_to_accept: The C2D2 dataset is valuable for the mental health field. It is a publicly accessible resource within this domain that includes numerous texts recording individuals\u2019 thoughts in various scenes with cognitive distortion. This dataset provides a profound connection between cognitive distortions and mental disorders. Addressing these distortions could potentially enhance mental health interventions. The authors find that integrating cognitive distortions as an additional feature can augment the performance of existing mental disorder detection models. In addition, the overall structure is well-organised.\n\nreasons_to_reject: The motivation \"However, mental health issues in developing countries often receive limited attention\" described for not annotating the dataset in English is inaccurate. English has a broader applicability, and the authors could also consider incorporating English annotations. While developing countries are essential to consider, it is also worthwhile to focus on datasets in languages other than Chinese.\nFor the test datasets, do the authors consider post-level model tests? Because the C2D2 is also the post-level dataset. Additionally, some other datasets could be added, like SWMH, T-SID, etc. \nIn Sec 6.2, the authors should train the RoBERTa-based model because of the performance shown in Table 5. \nIn Fig.4, do all posts with depression or PTSD have cognitive distortions? Add Non-sd labels?\nThere is no human expert performance on this dataset.\nMany details need to be expressed clearly. What is the meaning of ppopularity in (4)?\nThe reasonable about \"extracting posts that contain cognitive distortions\" should be discussed.  Why not other methods?\nIf possible, it is suggested that the problem could be treated as a multi-label task when multiple cognitive distortions occur simultaneously.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "aExCY7CUuj",
        "similarity": 0.751,
        "coverage": 0.4138,
        "human_length": 422,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors propose a cognitive distortion dataset in Chinese, consisting of 7,500 examples with paired scenes, thoughts, and cognitive distortion labels. For the dataset construction, the authors do not take the traditional approach of annotating online posts, but instead recruit volunteers and use interviews to further select qualified volunteers. The volunteers are instructed to compose thoughts based on the given scene, and also label the thoughts composed by other volunteers. The quality of the dataset is further evaluated by experts.  The authors conduct experiments using various baseline models including fully-supervised training, as well as few-shot and zero-shot using ChatGPT, demonstrating the challenge of this dataset. The author also conducts experiments and comprehensive analysis using the cognitive detection model on datasets from social media, showing the correlations between cognitive distortions and various mental health disorders.\n\nreasons_to_accept: 1. The proposed new dataset is very valuable, as currently, there is a lack of publically available datasets in the mental health domain. Such a dataset will greatly boost the research for NLP in assisting mental health treatment. \n2. The proposed dataset is the first one for Chinese. This is especially valuable since, in China, the scarcity of mental health professionals and resources is even more severe. There is an increasing demand for building automatic assistance for mental health support. \n3. The dataset shows good quality, verified by experts. \n4. The experiments and analysis demonstrate the great potential of the vast scope of future research that can be extended based on this dataset.\n\nreasons_to_reject: 1. No evaluation regarding the human expert performance on this dataset. \n2. The dataset construction process involves asking the volunteers to \"compose\" distorted thoughts but not taking the thoughts from potential patients with mental disorders from online posts. Such a method, theoretically as a kind of proxy to the thoughts of real patients,  is further verified by experts, as stated in the paper. However, I'm wondering what the gap is between the thoughts from real patients and the composed thoughts.\n\nquestions_for_the_authors: Do you pay the volunteers and the experts? If so, how do you pay them? This needs to be specified in the Ethics section.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "zmzlWjkI0i",
        "similarity": 0.7336,
        "coverage": 0.5,
        "human_length": 398,
        "human_text": "paper_topic_and_main_contributions: This paper discusses the creation of the Chinese Cognitive Distortion Dataset, a dataset based on Chinese tweets which were then translated into English. This dataset can be used to detect cognitive distortions (present or not) and the type of cognitive distortion (e.g., emotional reasoning, overgeneralization, etc) in text. It also examines its use in previously collected datasets with social media data from users with mental illnesses.\n\nreasons_to_accept: Presents an interesting tool to be used to detect cognitive distortions and has an interesting method that could be replicated to re-create a cognitive bias dataset with another language (e.g., English, French, etc.,).\n\nreasons_to_reject: By collecting data from volunteers with prompts and then correcting the volunteer's answers (as discussed in section 3.4), the dataset may include the researcher's biases and be less generalizable or naturalistic. Unclear of potential biases in the demographics of the volunteers who generated the data. The biases and limitations of the dataset due to it being generated by volunteers and then corrected should be discussed. As well, since the authors use social media datasets it's unverifiable if the users are formally diagnosed, thus the findings discussed on 449 to 453 and 465 to 471 as well of the rest of the section should emphasize how these findings are based on self-disclosure and potential self-diagnosis.\n\nquestions_for_the_authors: 1) Will the C2D2 dataset be available in Chinese, English, or both? \n2) What are the volunteer demographics? \n3) Where/how were the volunteers recruited from?\n\nmissing_references: The findings discussed in section 6.2 are similar to research on cognitive bias types in psychology although its novel being found on social media. Could support findings with research from psychology, psychiatry and the medical field.\n\ntypos_grammar_style_and_presentation_improvements: Figure 2, in the notes missing spaces between the \"&\" in the first line  line 203, missing spaces between the \"&\".\nLine 349, extra space before \"Table\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "HPXYIRgHgT",
        "similarity": 0.7327,
        "coverage": 0.55,
        "human_length": 447,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a new dataset of examples of various cognitive distortions in the Chinese language. The authors then perform machine learning experiments to test whether cognitive distortions are possible to detect automatically from text, and also test their relation to mental disorders, based on some English datasets annotated for depression and PTSD.\nThe dataset is created through a complex and rigorous process: by first doing a selection for choosing the set of responders, then asking them to produce examples of cognitive distortions, which are then verified by experts for different criteria. The dataset is made public.\nIn the experiments, pretrained LLMs are used to detect cognitive distortions in the proposed dataset and in the English mental disorders dataset, using machine translation to make the datasets compatible (following two objectives: measuring the prevalence of cognitive distortions in mental health disorder data using the trained models, and trying to improve mental disorder detection using cognitive distortion information).\nAll the responders used for creating the dataset have agreed to participate in the study, and the ethical section contains a thorough discussion.\n\nreasons_to_accept: - An original topic and the introduction of a new dataset, created through a very thorough process - Interesting findings related to the specific cognitive distortions that occur in various mental health disorders (depression and PTSD)\n\nreasons_to_reject: - Some potential methodological problems or unclarities in the cross-lingual experiments. It would have been useful to see an analysis of the impact of translation on the dataset, since this might distort the annotations themselves - at least a reiteration of the cognitive distortion detection experiment but on the English version of the data\n\nquestions_for_the_authors: Will the English version of the dataset also be made publicly available? It might widen the scope of its usefulness in other studies.\n\nmissing_references: Not directly about cognitive distortions, but on the relation between the more general \"cognitive styles\" (including cognitive biases) and mental health disorders: Uban, A. S., Chulvi, B., & Rosso, P. (2021). An emotion and cognitive based analysis of mental health disorders from social media data. Future Generation Computer Systems, 124, 480-494.\n\ntypos_grammar_style_and_presentation_improvements: - line 72: will contributes -> contribute - title of Section 4.2: \"Related dataset\" -> \"Related datasets\"?\n- line 324: preferable not to start the paragraph with \"And\" - lines 365-367: duplicate sentence (at least in meaning)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "0GKSZYu6FS",
        "length": 427,
        "human_text": "paper_topic_and_main_contributions: This paper is about mental health analysis topic. The main contributions are: a publicly accessible Chinese Cognitive Distortion Dataset is constructed for the first time. The authors explore the association between cognitive distortions and various mental disorders on social media. The authors attempt to incorporate information about users\u2019 cognitive distortions into mental disorder detection models to illustrate the effectiveness of using cognitive distortion.\n\nreasons_to_accept: The C2D2 dataset is valuable for the mental health field. It is a publicly accessible resource within this domain that includes numerous texts recording individuals\u2019 thoughts in various scenes with cognitive distortion. This dataset provides a profound connection between cognitive distortions and mental disorders. Addressing these distortions could potentially enhance mental health interventions. The authors find that integrating cognitive distortions as an additional feature can augment the performance of existing mental disorder detection models. In addition, the overall structure is well-organised.\n\nreasons_to_reject: The motivation \"However, mental health issues in developing countries often receive limited attention\" described for not annotating the dataset in English is inaccurate. English has a broader applicability, and the authors could also consider incorporating English annotations. While developing countries are essential to consider, it is also worthwhile to focus on datasets in languages other than Chinese.\nFor the test datasets, do the authors consider post-level model tests? Because the C2D2 is also the post-level dataset. Additionally, some other datasets could be added, like SWMH, T-SID, etc. \nIn Sec 6.2, the authors should train the RoBERTa-based model because of the performance shown in Table 5. \nIn Fig.4, do all posts with depression or PTSD have cognitive distortions? Add Non-sd labels?\nThere is no human expert performance on this dataset.\nMany details need to be expressed clearly. What is the meaning of ppopularity in (4)?\nThe reasonable about \"extracting posts that contain cognitive distortions\" should be discussed.  Why not other methods?\nIf possible, it is suggested that the problem could be treated as a multi-label task when multiple cognitive distortions occur simultaneously.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "aExCY7CUuj",
        "length": 422,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors propose a cognitive distortion dataset in Chinese, consisting of 7,500 examples with paired scenes, thoughts, and cognitive distortion labels. For the dataset construction, the authors do not take the traditional approach of annotating online posts, but instead recruit volunteers and use interviews to further select qualified volunteers. The volunteers are instructed to compose thoughts based on the given scene, and also label the thoughts composed by other volunteers. The quality of the dataset is further evaluated by experts.  The authors conduct experiments using various baseline models including fully-supervised training, as well as few-shot and zero-shot using ChatGPT, demonstrating the challenge of this dataset. The author also conducts experiments and comprehensive analysis using the cognitive detection model on datasets from social media, showing the correlations between cognitive distortions and various mental health disorders.\n\nreasons_to_accept: 1. The proposed new dataset is very valuable, as currently, there is a lack of publically available datasets in the mental health domain. Such a dataset will greatly boost the research for NLP in assisting mental health treatment. \n2. The proposed dataset is the first one for Chinese. This is especially valuable since, in China, the scarcity of mental health professionals and resources is even more severe. There is an increasing demand for building automatic assistance for mental health support. \n3. The dataset shows good quality, verified by experts. \n4. The experiments and analysis demonstrate the great potential of the vast scope of future research that can be extended based on this dataset.\n\nreasons_to_reject: 1. No evaluation regarding the human expert performance on this dataset. \n2. The dataset construction process involves asking the volunteers to \"compose\" distorted thoughts but not taking the thoughts from potential patients with mental disorders from online posts. Such a method, theoretically as a kind of proxy to the thoughts of real patients,  is further verified by experts, as stated in the paper. However, I'm wondering what the gap is between the thoughts from real patients and the composed thoughts.\n\nquestions_for_the_authors: Do you pay the volunteers and the experts? If so, how do you pay them? This needs to be specified in the Ethics section.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "zmzlWjkI0i",
        "length": 398,
        "human_text": "paper_topic_and_main_contributions: This paper discusses the creation of the Chinese Cognitive Distortion Dataset, a dataset based on Chinese tweets which were then translated into English. This dataset can be used to detect cognitive distortions (present or not) and the type of cognitive distortion (e.g., emotional reasoning, overgeneralization, etc) in text. It also examines its use in previously collected datasets with social media data from users with mental illnesses.\n\nreasons_to_accept: Presents an interesting tool to be used to detect cognitive distortions and has an interesting method that could be replicated to re-create a cognitive bias dataset with another language (e.g., English, French, etc.,).\n\nreasons_to_reject: By collecting data from volunteers with prompts and then correcting the volunteer's answers (as discussed in section 3.4), the dataset may include the researcher's biases and be less generalizable or naturalistic. Unclear of potential biases in the demographics of the volunteers who generated the data. The biases and limitations of the dataset due to it being generated by volunteers and then corrected should be discussed. As well, since the authors use social media datasets it's unverifiable if the users are formally diagnosed, thus the findings discussed on 449 to 453 and 465 to 471 as well of the rest of the section should emphasize how these findings are based on self-disclosure and potential self-diagnosis.\n\nquestions_for_the_authors: 1) Will the C2D2 dataset be available in Chinese, English, or both? \n2) What are the volunteer demographics? \n3) Where/how were the volunteers recruited from?\n\nmissing_references: The findings discussed in section 6.2 are similar to research on cognitive bias types in psychology although its novel being found on social media. Could support findings with research from psychology, psychiatry and the medical field.\n\ntypos_grammar_style_and_presentation_improvements: Figure 2, in the notes missing spaces between the \"&\" in the first line  line 203, missing spaces between the \"&\".\nLine 349, extra space before \"Table\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "HPXYIRgHgT",
        "length": 447,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a new dataset of examples of various cognitive distortions in the Chinese language. The authors then perform machine learning experiments to test whether cognitive distortions are possible to detect automatically from text, and also test their relation to mental disorders, based on some English datasets annotated for depression and PTSD.\nThe dataset is created through a complex and rigorous process: by first doing a selection for choosing the set of responders, then asking them to produce examples of cognitive distortions, which are then verified by experts for different criteria. The dataset is made public.\nIn the experiments, pretrained LLMs are used to detect cognitive distortions in the proposed dataset and in the English mental disorders dataset, using machine translation to make the datasets compatible (following two objectives: measuring the prevalence of cognitive distortions in mental health disorder data using the trained models, and trying to improve mental disorder detection using cognitive distortion information).\nAll the responders used for creating the dataset have agreed to participate in the study, and the ethical section contains a thorough discussion.\n\nreasons_to_accept: - An original topic and the introduction of a new dataset, created through a very thorough process - Interesting findings related to the specific cognitive distortions that occur in various mental health disorders (depression and PTSD)\n\nreasons_to_reject: - Some potential methodological problems or unclarities in the cross-lingual experiments. It would have been useful to see an analysis of the impact of translation on the dataset, since this might distort the annotations themselves - at least a reiteration of the cognitive distortion detection experiment but on the English version of the data\n\nquestions_for_the_authors: Will the English version of the dataset also be made publicly available? It might widen the scope of its usefulness in other studies.\n\nmissing_references: Not directly about cognitive distortions, but on the relation between the more general \"cognitive styles\" (including cognitive biases) and mental health disorders: Uban, A. S., Chulvi, B., & Rosso, P. (2021). An emotion and cognitive based analysis of mental health disorders from social media data. Future Generation Computer Systems, 124, 480-494.\n\ntypos_grammar_style_and_presentation_improvements: - line 72: will contributes -> contribute - title of Section 4.2: \"Related dataset\" -> \"Related datasets\"?\n- line 324: preferable not to start the paragraph with \"And\" - lines 365-367: duplicate sentence (at least in meaning)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "197_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_197_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7048333333333333,
      "max_similarity": 0.7202,
      "avg_coverage": 0.5386000000000001,
      "max_coverage": 0.6471
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 462,
      "avg_human_length": 433.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 6,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "3VMSp9tz9w",
        "similarity": 0.7104,
        "coverage": 0.4848,
        "human_length": 609,
        "human_text": "paper_topic_and_main_contributions: The paper presents MProto, a noise-robust prototype network for distantly supervised Named Entity Recognition (DS-NER).\nThey use dictionary or knowledge base lookups for distantly labeling raw text with NER labels. Different to other prototypical networks they allow multiple prototypes per NER class (e.g. three \"types\" of ORG prototypes) to account for intra-class variance. \nThe prototype assignment is done via optimal transport (OT) algorithms.\nBeing aware that the distant labels introduce a high rate of label errors or incompleteness (due to missing entries or other), they propose a novel \"denoised optimal transport\" (DOT) algorithm to distinguish the O-assignes entitiey between falsely unlabeled entity tokens and true negative examples to mitigate the noise problem and avoid overfitting to the O-class.\nThey test their approach on two NER datasets: CoNLL03 and BC5CDR, replacing the real human labels with the distant ones from Dictionary of Knowledge Base for training. They compare their MProto approach with the fully-supervised BERT model (trained on the obviously better real human labels) as upper bound, as well as several other DS-NER models. Though naturally weaker than the baseline, MProto beats all the other DS-NER models in their experiments.\nThe authors present some ablation studies, e.g. only allowing one prototype per NER class, or removing the DOT handling of O-labeled tokens, the latter showing that this is indeed a very important feature of their approach. This is also supported by a comparison of the learned class prototypes and tokens that belong to the classes (in the real fully human annotated versions of the datasets). \nThey further notice that dictionary or knowledge base coverage is an important factor for model performance through artificially reducing coverage, which of course is a downside. However, they show that their MProto still has less performance drops compared to the other DS-NER models in comparison.\n\nreasons_to_accept: The paper very elegantly deals with two relevant problems - learning from unlabeled data via distant labels as well as mitigating label noise. It is an interesting read for both perspectives, could inspire similar approaches and overall is well written and presented. The argumentation is clear and the main idea comes across quickly. The experiments are well fitted and seem sound, as well as the ablation and secondary analyses which justify their main ideas and choices. I also appreciate that they present the drop when reducing the coverage. \nThe presentation (i.e. chose of sections, figures etc.) is very good and clear, also the figures in the appendix concerning token-prototype similarity are really interesting.\n\nreasons_to_reject: I do not find major reasons to reject. \nThe section about the (denoised) optimal transport algorithm for dealing with unlabeled entities might be a bit hard to follow or too intricate for a reader that is unfamiliar with the matter. Of course, this is not a bad thing but rather nice to learn something new. But it might help to put a more high-level or intuitive summary at the start of the section.\n\ntypos_grammar_style_and_presentation_improvements: One thing that (for me) was not that clear is if the approach is token or span based. Maybe this could be made more clear? \nAlso, naming the source of distant labels (dictionary, knowledge base) came too late in the paper, which left me a bit confused for too long. Maybe this could be moved more to the beginning?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "2hiXstsfUO",
        "similarity": 0.7202,
        "coverage": 0.4839,
        "human_length": 422,
        "human_text": "paper_topic_and_main_contributions: This work focuses on distantly supervised named entity recognition (DS-NER) task. Distant supervision eliminates the expensive human annotation process but at the cost of noisy annotation. This paper proposes a prototype-based network approach for the DS-NER. The proposed method, referred to as MProto, considers multiple prototypes for each entity class. The multiple prototypes for an entity type help MProto to characterize sub-clusters within each entity type. MProto assigns each input token to one of the prototypes. The main contributions of this work are the formulation of token-prototype assignments. In the proposed formulation, this work also takes care of the issue of incomplete labeling, where entities are labeled as Other due to incomplete information in the knowledge resources used for distant supervision. Towards this, the two primary contributions are - 1. Formulation of token-prototype assignment as an optimal transport problem 2. To take care of the noise of incomplete labeling, a proposal of denoised optimal transport Extensive sets of experiments and analyses are performed to establish the effectiveness of the MProto.\n\nreasons_to_accept: 1. Addresses a very challenging issue of noise and intra-class variance in the context of distantly supervised NER task. \n2. The authors attempted to reduce noise arising from false negative Other (O) tagged entities with the help of multiple prototype network and denoised optimal transport method. \n3. With suitable experiments, authors have established that their methods yield competitive performance against SOTA in general and domain-specific datasets.\n\nreasons_to_reject: Not very strong reasons to reject it. However, a few points 1. Limited to only two datasets with only a few numbers of entity types. Noise and corresponding challenges increase multitude with more entity types. This happens primarily due to complex entity boundaries 2. No error analysis was presented. Though authors have shown quantitative results with and without DOT, it would be interesting to see how DOT impacts assignments. Similarly for with and without multiple prototypes\n\nquestions_for_the_authors: A few questions  1. How MProto handles label dependency between two or more consecutive tokens? In other words, how MProto handles labeling of entity mention spanning over more than one token?\n2. How the complexity of the model increases with fine-grained DS-NER\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "ipzljWq7D9",
        "similarity": 0.6839,
        "coverage": 0.6471,
        "human_length": 269,
        "human_text": "paper_topic_and_main_contributions: This paper describes a method to perform distantly supervised named-entity recognition (DS-NER), called MProto, that represent more than one prototype for each NER class. In this way, variance inside a single class can be intercepted and represented. \nResults presented in the paper seem to demonstrate the effectiveness of the idea.\n\nreasons_to_accept: The paper is clear and well written. The procedure is easy to follow.\n\nreasons_to_reject: There are some inconsistencies between the data presented and the previous work. \nFor instance, the authors use BC5CDR with dictionary in (Shang et al. 2018) and compare their work with AutoNER, described right in (Shang et al. 2018). By looking at the original paper, F1 on BC5CDR is 84.80, but in this paper is 79.99 (MProto's F1 is 81.47). \nSimilarly, the paper is compared with (Zhou et al. 2022), but only in its worst configuration (Conf-MPU_BERT) and not with the best one (Conf-MPU_LBiLSTM), that performs better than MProto.\n\nquestions_for_the_authors: Why did not you compare your results with the NCBI-Disease dataset, very similar to BC5CDR?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "3VMSp9tz9w",
        "length": 609,
        "human_text": "paper_topic_and_main_contributions: The paper presents MProto, a noise-robust prototype network for distantly supervised Named Entity Recognition (DS-NER).\nThey use dictionary or knowledge base lookups for distantly labeling raw text with NER labels. Different to other prototypical networks they allow multiple prototypes per NER class (e.g. three \"types\" of ORG prototypes) to account for intra-class variance. \nThe prototype assignment is done via optimal transport (OT) algorithms.\nBeing aware that the distant labels introduce a high rate of label errors or incompleteness (due to missing entries or other), they propose a novel \"denoised optimal transport\" (DOT) algorithm to distinguish the O-assignes entitiey between falsely unlabeled entity tokens and true negative examples to mitigate the noise problem and avoid overfitting to the O-class.\nThey test their approach on two NER datasets: CoNLL03 and BC5CDR, replacing the real human labels with the distant ones from Dictionary of Knowledge Base for training. They compare their MProto approach with the fully-supervised BERT model (trained on the obviously better real human labels) as upper bound, as well as several other DS-NER models. Though naturally weaker than the baseline, MProto beats all the other DS-NER models in their experiments.\nThe authors present some ablation studies, e.g. only allowing one prototype per NER class, or removing the DOT handling of O-labeled tokens, the latter showing that this is indeed a very important feature of their approach. This is also supported by a comparison of the learned class prototypes and tokens that belong to the classes (in the real fully human annotated versions of the datasets). \nThey further notice that dictionary or knowledge base coverage is an important factor for model performance through artificially reducing coverage, which of course is a downside. However, they show that their MProto still has less performance drops compared to the other DS-NER models in comparison.\n\nreasons_to_accept: The paper very elegantly deals with two relevant problems - learning from unlabeled data via distant labels as well as mitigating label noise. It is an interesting read for both perspectives, could inspire similar approaches and overall is well written and presented. The argumentation is clear and the main idea comes across quickly. The experiments are well fitted and seem sound, as well as the ablation and secondary analyses which justify their main ideas and choices. I also appreciate that they present the drop when reducing the coverage. \nThe presentation (i.e. chose of sections, figures etc.) is very good and clear, also the figures in the appendix concerning token-prototype similarity are really interesting.\n\nreasons_to_reject: I do not find major reasons to reject. \nThe section about the (denoised) optimal transport algorithm for dealing with unlabeled entities might be a bit hard to follow or too intricate for a reader that is unfamiliar with the matter. Of course, this is not a bad thing but rather nice to learn something new. But it might help to put a more high-level or intuitive summary at the start of the section.\n\ntypos_grammar_style_and_presentation_improvements: One thing that (for me) was not that clear is if the approach is token or span based. Maybe this could be made more clear? \nAlso, naming the source of distant labels (dictionary, knowledge base) came too late in the paper, which left me a bit confused for too long. Maybe this could be moved more to the beginning?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "2hiXstsfUO",
        "length": 422,
        "human_text": "paper_topic_and_main_contributions: This work focuses on distantly supervised named entity recognition (DS-NER) task. Distant supervision eliminates the expensive human annotation process but at the cost of noisy annotation. This paper proposes a prototype-based network approach for the DS-NER. The proposed method, referred to as MProto, considers multiple prototypes for each entity class. The multiple prototypes for an entity type help MProto to characterize sub-clusters within each entity type. MProto assigns each input token to one of the prototypes. The main contributions of this work are the formulation of token-prototype assignments. In the proposed formulation, this work also takes care of the issue of incomplete labeling, where entities are labeled as Other due to incomplete information in the knowledge resources used for distant supervision. Towards this, the two primary contributions are - 1. Formulation of token-prototype assignment as an optimal transport problem 2. To take care of the noise of incomplete labeling, a proposal of denoised optimal transport Extensive sets of experiments and analyses are performed to establish the effectiveness of the MProto.\n\nreasons_to_accept: 1. Addresses a very challenging issue of noise and intra-class variance in the context of distantly supervised NER task. \n2. The authors attempted to reduce noise arising from false negative Other (O) tagged entities with the help of multiple prototype network and denoised optimal transport method. \n3. With suitable experiments, authors have established that their methods yield competitive performance against SOTA in general and domain-specific datasets.\n\nreasons_to_reject: Not very strong reasons to reject it. However, a few points 1. Limited to only two datasets with only a few numbers of entity types. Noise and corresponding challenges increase multitude with more entity types. This happens primarily due to complex entity boundaries 2. No error analysis was presented. Though authors have shown quantitative results with and without DOT, it would be interesting to see how DOT impacts assignments. Similarly for with and without multiple prototypes\n\nquestions_for_the_authors: A few questions  1. How MProto handles label dependency between two or more consecutive tokens? In other words, how MProto handles labeling of entity mention spanning over more than one token?\n2. How the complexity of the model increases with fine-grained DS-NER\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ipzljWq7D9",
        "length": 269,
        "human_text": "paper_topic_and_main_contributions: This paper describes a method to perform distantly supervised named-entity recognition (DS-NER), called MProto, that represent more than one prototype for each NER class. In this way, variance inside a single class can be intercepted and represented. \nResults presented in the paper seem to demonstrate the effectiveness of the idea.\n\nreasons_to_accept: The paper is clear and well written. The procedure is easy to follow.\n\nreasons_to_reject: There are some inconsistencies between the data presented and the previous work. \nFor instance, the authors use BC5CDR with dictionary in (Shang et al. 2018) and compare their work with AutoNER, described right in (Shang et al. 2018). By looking at the original paper, F1 on BC5CDR is 84.80, but in this paper is 79.99 (MProto's F1 is 81.47). \nSimilarly, the paper is compared with (Zhou et al. 2022), but only in its worst configuration (Conf-MPU_BERT) and not with the best one (Conf-MPU_LBiLSTM), that performs better than MProto.\n\nquestions_for_the_authors: Why did not you compare your results with the NCBI-Disease dataset, very similar to BC5CDR?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "197_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_197_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7079333333333334,
      "max_similarity": 0.724,
      "avg_coverage": 0.5689333333333334,
      "max_coverage": 0.7059
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 509,
      "avg_human_length": 433.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 6,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "3VMSp9tz9w",
        "similarity": 0.7105,
        "coverage": 0.4848,
        "human_length": 609,
        "human_text": "paper_topic_and_main_contributions: The paper presents MProto, a noise-robust prototype network for distantly supervised Named Entity Recognition (DS-NER).\nThey use dictionary or knowledge base lookups for distantly labeling raw text with NER labels. Different to other prototypical networks they allow multiple prototypes per NER class (e.g. three \"types\" of ORG prototypes) to account for intra-class variance. \nThe prototype assignment is done via optimal transport (OT) algorithms.\nBeing aware that the distant labels introduce a high rate of label errors or incompleteness (due to missing entries or other), they propose a novel \"denoised optimal transport\" (DOT) algorithm to distinguish the O-assignes entitiey between falsely unlabeled entity tokens and true negative examples to mitigate the noise problem and avoid overfitting to the O-class.\nThey test their approach on two NER datasets: CoNLL03 and BC5CDR, replacing the real human labels with the distant ones from Dictionary of Knowledge Base for training. They compare their MProto approach with the fully-supervised BERT model (trained on the obviously better real human labels) as upper bound, as well as several other DS-NER models. Though naturally weaker than the baseline, MProto beats all the other DS-NER models in their experiments.\nThe authors present some ablation studies, e.g. only allowing one prototype per NER class, or removing the DOT handling of O-labeled tokens, the latter showing that this is indeed a very important feature of their approach. This is also supported by a comparison of the learned class prototypes and tokens that belong to the classes (in the real fully human annotated versions of the datasets). \nThey further notice that dictionary or knowledge base coverage is an important factor for model performance through artificially reducing coverage, which of course is a downside. However, they show that their MProto still has less performance drops compared to the other DS-NER models in comparison.\n\nreasons_to_accept: The paper very elegantly deals with two relevant problems - learning from unlabeled data via distant labels as well as mitigating label noise. It is an interesting read for both perspectives, could inspire similar approaches and overall is well written and presented. The argumentation is clear and the main idea comes across quickly. The experiments are well fitted and seem sound, as well as the ablation and secondary analyses which justify their main ideas and choices. I also appreciate that they present the drop when reducing the coverage. \nThe presentation (i.e. chose of sections, figures etc.) is very good and clear, also the figures in the appendix concerning token-prototype similarity are really interesting.\n\nreasons_to_reject: I do not find major reasons to reject. \nThe section about the (denoised) optimal transport algorithm for dealing with unlabeled entities might be a bit hard to follow or too intricate for a reader that is unfamiliar with the matter. Of course, this is not a bad thing but rather nice to learn something new. But it might help to put a more high-level or intuitive summary at the start of the section.\n\ntypos_grammar_style_and_presentation_improvements: One thing that (for me) was not that clear is if the approach is token or span based. Maybe this could be made more clear? \nAlso, naming the source of distant labels (dictionary, knowledge base) came too late in the paper, which left me a bit confused for too long. Maybe this could be moved more to the beginning?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "2hiXstsfUO",
        "similarity": 0.724,
        "coverage": 0.5161,
        "human_length": 422,
        "human_text": "paper_topic_and_main_contributions: This work focuses on distantly supervised named entity recognition (DS-NER) task. Distant supervision eliminates the expensive human annotation process but at the cost of noisy annotation. This paper proposes a prototype-based network approach for the DS-NER. The proposed method, referred to as MProto, considers multiple prototypes for each entity class. The multiple prototypes for an entity type help MProto to characterize sub-clusters within each entity type. MProto assigns each input token to one of the prototypes. The main contributions of this work are the formulation of token-prototype assignments. In the proposed formulation, this work also takes care of the issue of incomplete labeling, where entities are labeled as Other due to incomplete information in the knowledge resources used for distant supervision. Towards this, the two primary contributions are - 1. Formulation of token-prototype assignment as an optimal transport problem 2. To take care of the noise of incomplete labeling, a proposal of denoised optimal transport Extensive sets of experiments and analyses are performed to establish the effectiveness of the MProto.\n\nreasons_to_accept: 1. Addresses a very challenging issue of noise and intra-class variance in the context of distantly supervised NER task. \n2. The authors attempted to reduce noise arising from false negative Other (O) tagged entities with the help of multiple prototype network and denoised optimal transport method. \n3. With suitable experiments, authors have established that their methods yield competitive performance against SOTA in general and domain-specific datasets.\n\nreasons_to_reject: Not very strong reasons to reject it. However, a few points 1. Limited to only two datasets with only a few numbers of entity types. Noise and corresponding challenges increase multitude with more entity types. This happens primarily due to complex entity boundaries 2. No error analysis was presented. Though authors have shown quantitative results with and without DOT, it would be interesting to see how DOT impacts assignments. Similarly for with and without multiple prototypes\n\nquestions_for_the_authors: A few questions  1. How MProto handles label dependency between two or more consecutive tokens? In other words, how MProto handles labeling of entity mention spanning over more than one token?\n2. How the complexity of the model increases with fine-grained DS-NER\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "ipzljWq7D9",
        "similarity": 0.6893,
        "coverage": 0.7059,
        "human_length": 269,
        "human_text": "paper_topic_and_main_contributions: This paper describes a method to perform distantly supervised named-entity recognition (DS-NER), called MProto, that represent more than one prototype for each NER class. In this way, variance inside a single class can be intercepted and represented. \nResults presented in the paper seem to demonstrate the effectiveness of the idea.\n\nreasons_to_accept: The paper is clear and well written. The procedure is easy to follow.\n\nreasons_to_reject: There are some inconsistencies between the data presented and the previous work. \nFor instance, the authors use BC5CDR with dictionary in (Shang et al. 2018) and compare their work with AutoNER, described right in (Shang et al. 2018). By looking at the original paper, F1 on BC5CDR is 84.80, but in this paper is 79.99 (MProto's F1 is 81.47). \nSimilarly, the paper is compared with (Zhou et al. 2022), but only in its worst configuration (Conf-MPU_BERT) and not with the best one (Conf-MPU_LBiLSTM), that performs better than MProto.\n\nquestions_for_the_authors: Why did not you compare your results with the NCBI-Disease dataset, very similar to BC5CDR?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "3VMSp9tz9w",
        "length": 609,
        "human_text": "paper_topic_and_main_contributions: The paper presents MProto, a noise-robust prototype network for distantly supervised Named Entity Recognition (DS-NER).\nThey use dictionary or knowledge base lookups for distantly labeling raw text with NER labels. Different to other prototypical networks they allow multiple prototypes per NER class (e.g. three \"types\" of ORG prototypes) to account for intra-class variance. \nThe prototype assignment is done via optimal transport (OT) algorithms.\nBeing aware that the distant labels introduce a high rate of label errors or incompleteness (due to missing entries or other), they propose a novel \"denoised optimal transport\" (DOT) algorithm to distinguish the O-assignes entitiey between falsely unlabeled entity tokens and true negative examples to mitigate the noise problem and avoid overfitting to the O-class.\nThey test their approach on two NER datasets: CoNLL03 and BC5CDR, replacing the real human labels with the distant ones from Dictionary of Knowledge Base for training. They compare their MProto approach with the fully-supervised BERT model (trained on the obviously better real human labels) as upper bound, as well as several other DS-NER models. Though naturally weaker than the baseline, MProto beats all the other DS-NER models in their experiments.\nThe authors present some ablation studies, e.g. only allowing one prototype per NER class, or removing the DOT handling of O-labeled tokens, the latter showing that this is indeed a very important feature of their approach. This is also supported by a comparison of the learned class prototypes and tokens that belong to the classes (in the real fully human annotated versions of the datasets). \nThey further notice that dictionary or knowledge base coverage is an important factor for model performance through artificially reducing coverage, which of course is a downside. However, they show that their MProto still has less performance drops compared to the other DS-NER models in comparison.\n\nreasons_to_accept: The paper very elegantly deals with two relevant problems - learning from unlabeled data via distant labels as well as mitigating label noise. It is an interesting read for both perspectives, could inspire similar approaches and overall is well written and presented. The argumentation is clear and the main idea comes across quickly. The experiments are well fitted and seem sound, as well as the ablation and secondary analyses which justify their main ideas and choices. I also appreciate that they present the drop when reducing the coverage. \nThe presentation (i.e. chose of sections, figures etc.) is very good and clear, also the figures in the appendix concerning token-prototype similarity are really interesting.\n\nreasons_to_reject: I do not find major reasons to reject. \nThe section about the (denoised) optimal transport algorithm for dealing with unlabeled entities might be a bit hard to follow or too intricate for a reader that is unfamiliar with the matter. Of course, this is not a bad thing but rather nice to learn something new. But it might help to put a more high-level or intuitive summary at the start of the section.\n\ntypos_grammar_style_and_presentation_improvements: One thing that (for me) was not that clear is if the approach is token or span based. Maybe this could be made more clear? \nAlso, naming the source of distant labels (dictionary, knowledge base) came too late in the paper, which left me a bit confused for too long. Maybe this could be moved more to the beginning?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "2hiXstsfUO",
        "length": 422,
        "human_text": "paper_topic_and_main_contributions: This work focuses on distantly supervised named entity recognition (DS-NER) task. Distant supervision eliminates the expensive human annotation process but at the cost of noisy annotation. This paper proposes a prototype-based network approach for the DS-NER. The proposed method, referred to as MProto, considers multiple prototypes for each entity class. The multiple prototypes for an entity type help MProto to characterize sub-clusters within each entity type. MProto assigns each input token to one of the prototypes. The main contributions of this work are the formulation of token-prototype assignments. In the proposed formulation, this work also takes care of the issue of incomplete labeling, where entities are labeled as Other due to incomplete information in the knowledge resources used for distant supervision. Towards this, the two primary contributions are - 1. Formulation of token-prototype assignment as an optimal transport problem 2. To take care of the noise of incomplete labeling, a proposal of denoised optimal transport Extensive sets of experiments and analyses are performed to establish the effectiveness of the MProto.\n\nreasons_to_accept: 1. Addresses a very challenging issue of noise and intra-class variance in the context of distantly supervised NER task. \n2. The authors attempted to reduce noise arising from false negative Other (O) tagged entities with the help of multiple prototype network and denoised optimal transport method. \n3. With suitable experiments, authors have established that their methods yield competitive performance against SOTA in general and domain-specific datasets.\n\nreasons_to_reject: Not very strong reasons to reject it. However, a few points 1. Limited to only two datasets with only a few numbers of entity types. Noise and corresponding challenges increase multitude with more entity types. This happens primarily due to complex entity boundaries 2. No error analysis was presented. Though authors have shown quantitative results with and without DOT, it would be interesting to see how DOT impacts assignments. Similarly for with and without multiple prototypes\n\nquestions_for_the_authors: A few questions  1. How MProto handles label dependency between two or more consecutive tokens? In other words, how MProto handles labeling of entity mention spanning over more than one token?\n2. How the complexity of the model increases with fine-grained DS-NER\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ipzljWq7D9",
        "length": 269,
        "human_text": "paper_topic_and_main_contributions: This paper describes a method to perform distantly supervised named-entity recognition (DS-NER), called MProto, that represent more than one prototype for each NER class. In this way, variance inside a single class can be intercepted and represented. \nResults presented in the paper seem to demonstrate the effectiveness of the idea.\n\nreasons_to_accept: The paper is clear and well written. The procedure is easy to follow.\n\nreasons_to_reject: There are some inconsistencies between the data presented and the previous work. \nFor instance, the authors use BC5CDR with dictionary in (Shang et al. 2018) and compare their work with AutoNER, described right in (Shang et al. 2018). By looking at the original paper, F1 on BC5CDR is 84.80, but in this paper is 79.99 (MProto's F1 is 81.47). \nSimilarly, the paper is compared with (Zhou et al. 2022), but only in its worst configuration (Conf-MPU_BERT) and not with the best one (Conf-MPU_LBiLSTM), that performs better than MProto.\n\nquestions_for_the_authors: Why did not you compare your results with the NCBI-Disease dataset, very similar to BC5CDR?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "28_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_28_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6842333333333334,
      "max_similarity": 0.6972,
      "avg_coverage": 0.6317666666666667,
      "max_coverage": 0.8
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 500,
      "avg_human_length": 223.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 6,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "iIWaKiaSpp",
        "similarity": 0.6693,
        "coverage": 0.7143,
        "human_length": 198,
        "human_text": "paper_topic_and_main_contributions: The paper focuses on extractive QA. Specifically, the paper proposes an approach to\n\nreasons_to_accept: The idea of improving QA with three-option human feedback is nice, which shed lights on iteratively improving deployed QA systems. I am not an expert on reinforce learning, but the methodology is clear and able to follow. From the experimental results, the improvements are significant. The paper also provide comprehensive discussions on variants of experimental settings.\n\nreasons_to_reject: The motivation of improving extractive QA with human feedback is not clearly explained. By reading the introduction, I don't have a clear sense of why we need human feedback for QA.\n\nquestions_for_the_authors: A. What is the advantage of improving using human feedback compared to using human labels to continuously train the model? It is easier and cheaper to collect? Or other advantages?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "jKrf1qJ0fZ",
        "similarity": 0.6972,
        "coverage": 0.381,
        "human_length": 271,
        "human_text": "paper_topic_and_main_contributions: The authors propose a method to continually improve extractive QA via human feedback. The method does have high practical value and may be applied in building other NLP application systems. Extensive and carefully designed experiments demonstrate that an extractive QA system can be improved via interaction with human users over time and further broaden our understanding about continual learning with user feedback.\n\nreasons_to_accept: 1. The authors propose a method to continually improve extractive QA via human feedback. The proposed method does have high practical value and may be applied in building other NLP application systems.  2. Extensive and carefully designed experiments demonstrate that an extractive QA system can be improved via interaction with human users over time.\n\nreasons_to_reject: 1. Uncertainties introduced by human interactions are likely to affect the results and need to be carefully controlled with a well-defined guideline.\n\nquestions_for_the_authors: 1. What's the possible performance upper bound of the proposed method? How many rounds of interactions are optimal in general?\n\ntypos_grammar_style_and_presentation_improvements: Typos or grammar errors: 1. line 345, \"We are focused on ...\" 2. line 369, \"the systems sees\" 3. line 481, \"does not an explain ...\" 4. line 555, \"the that\" 5. line 620, \"out approach\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "hREKcOTq0t",
        "similarity": 0.6862,
        "coverage": 0.8,
        "human_length": 200,
        "human_text": "paper_topic_and_main_contributions: The paper is about a reinforcement learning for extractive QA. authors trained a model in different rounds using human feedback to improve the model. They also tested different version of the model with different initialization and on different dataset to ensure the performances on different domain. Authors relies on crowdworkers for the feedback. Performances show the effectiveness of the approach.\n\nreasons_to_accept: + well written paper + novel approach to improve extractive QA + relevant results and a new interesting research for qa systems + good experiments and ablations\n\nreasons_to_reject: + miss a proper evaluation of the quality of the crowdworkers to avoid misleading errors + no clear how ensure feedback quality\n\nquestions_for_the_authors: 1 Can you better clarify how you can ensure feedback quality? \n2 Why you did not perform an evaluation of the annotators job?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "iIWaKiaSpp",
        "length": 198,
        "human_text": "paper_topic_and_main_contributions: The paper focuses on extractive QA. Specifically, the paper proposes an approach to\n\nreasons_to_accept: The idea of improving QA with three-option human feedback is nice, which shed lights on iteratively improving deployed QA systems. I am not an expert on reinforce learning, but the methodology is clear and able to follow. From the experimental results, the improvements are significant. The paper also provide comprehensive discussions on variants of experimental settings.\n\nreasons_to_reject: The motivation of improving extractive QA with human feedback is not clearly explained. By reading the introduction, I don't have a clear sense of why we need human feedback for QA.\n\nquestions_for_the_authors: A. What is the advantage of improving using human feedback compared to using human labels to continuously train the model? It is easier and cheaper to collect? Or other advantages?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "jKrf1qJ0fZ",
        "length": 271,
        "human_text": "paper_topic_and_main_contributions: The authors propose a method to continually improve extractive QA via human feedback. The method does have high practical value and may be applied in building other NLP application systems. Extensive and carefully designed experiments demonstrate that an extractive QA system can be improved via interaction with human users over time and further broaden our understanding about continual learning with user feedback.\n\nreasons_to_accept: 1. The authors propose a method to continually improve extractive QA via human feedback. The proposed method does have high practical value and may be applied in building other NLP application systems.  2. Extensive and carefully designed experiments demonstrate that an extractive QA system can be improved via interaction with human users over time.\n\nreasons_to_reject: 1. Uncertainties introduced by human interactions are likely to affect the results and need to be carefully controlled with a well-defined guideline.\n\nquestions_for_the_authors: 1. What's the possible performance upper bound of the proposed method? How many rounds of interactions are optimal in general?\n\ntypos_grammar_style_and_presentation_improvements: Typos or grammar errors: 1. line 345, \"We are focused on ...\" 2. line 369, \"the systems sees\" 3. line 481, \"does not an explain ...\" 4. line 555, \"the that\" 5. line 620, \"out approach\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "hREKcOTq0t",
        "length": 200,
        "human_text": "paper_topic_and_main_contributions: The paper is about a reinforcement learning for extractive QA. authors trained a model in different rounds using human feedback to improve the model. They also tested different version of the model with different initialization and on different dataset to ensure the performances on different domain. Authors relies on crowdworkers for the feedback. Performances show the effectiveness of the approach.\n\nreasons_to_accept: + well written paper + novel approach to improve extractive QA + relevant results and a new interesting research for qa systems + good experiments and ablations\n\nreasons_to_reject: + miss a proper evaluation of the quality of the crowdworkers to avoid misleading errors + no clear how ensure feedback quality\n\nquestions_for_the_authors: 1 Can you better clarify how you can ensure feedback quality? \n2 Why you did not perform an evaluation of the annotators job?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "28_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_28_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6837666666666666,
      "max_similarity": 0.7045,
      "avg_coverage": 0.6079666666666667,
      "max_coverage": 0.8
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 425,
      "avg_human_length": 223.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 6,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "iIWaKiaSpp",
        "similarity": 0.6701,
        "coverage": 0.6429,
        "human_length": 198,
        "human_text": "paper_topic_and_main_contributions: The paper focuses on extractive QA. Specifically, the paper proposes an approach to\n\nreasons_to_accept: The idea of improving QA with three-option human feedback is nice, which shed lights on iteratively improving deployed QA systems. I am not an expert on reinforce learning, but the methodology is clear and able to follow. From the experimental results, the improvements are significant. The paper also provide comprehensive discussions on variants of experimental settings.\n\nreasons_to_reject: The motivation of improving extractive QA with human feedback is not clearly explained. By reading the introduction, I don't have a clear sense of why we need human feedback for QA.\n\nquestions_for_the_authors: A. What is the advantage of improving using human feedback compared to using human labels to continuously train the model? It is easier and cheaper to collect? Or other advantages?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "jKrf1qJ0fZ",
        "similarity": 0.7045,
        "coverage": 0.381,
        "human_length": 271,
        "human_text": "paper_topic_and_main_contributions: The authors propose a method to continually improve extractive QA via human feedback. The method does have high practical value and may be applied in building other NLP application systems. Extensive and carefully designed experiments demonstrate that an extractive QA system can be improved via interaction with human users over time and further broaden our understanding about continual learning with user feedback.\n\nreasons_to_accept: 1. The authors propose a method to continually improve extractive QA via human feedback. The proposed method does have high practical value and may be applied in building other NLP application systems.  2. Extensive and carefully designed experiments demonstrate that an extractive QA system can be improved via interaction with human users over time.\n\nreasons_to_reject: 1. Uncertainties introduced by human interactions are likely to affect the results and need to be carefully controlled with a well-defined guideline.\n\nquestions_for_the_authors: 1. What's the possible performance upper bound of the proposed method? How many rounds of interactions are optimal in general?\n\ntypos_grammar_style_and_presentation_improvements: Typos or grammar errors: 1. line 345, \"We are focused on ...\" 2. line 369, \"the systems sees\" 3. line 481, \"does not an explain ...\" 4. line 555, \"the that\" 5. line 620, \"out approach\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "hREKcOTq0t",
        "similarity": 0.6767,
        "coverage": 0.8,
        "human_length": 200,
        "human_text": "paper_topic_and_main_contributions: The paper is about a reinforcement learning for extractive QA. authors trained a model in different rounds using human feedback to improve the model. They also tested different version of the model with different initialization and on different dataset to ensure the performances on different domain. Authors relies on crowdworkers for the feedback. Performances show the effectiveness of the approach.\n\nreasons_to_accept: + well written paper + novel approach to improve extractive QA + relevant results and a new interesting research for qa systems + good experiments and ablations\n\nreasons_to_reject: + miss a proper evaluation of the quality of the crowdworkers to avoid misleading errors + no clear how ensure feedback quality\n\nquestions_for_the_authors: 1 Can you better clarify how you can ensure feedback quality? \n2 Why you did not perform an evaluation of the annotators job?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "iIWaKiaSpp",
        "length": 198,
        "human_text": "paper_topic_and_main_contributions: The paper focuses on extractive QA. Specifically, the paper proposes an approach to\n\nreasons_to_accept: The idea of improving QA with three-option human feedback is nice, which shed lights on iteratively improving deployed QA systems. I am not an expert on reinforce learning, but the methodology is clear and able to follow. From the experimental results, the improvements are significant. The paper also provide comprehensive discussions on variants of experimental settings.\n\nreasons_to_reject: The motivation of improving extractive QA with human feedback is not clearly explained. By reading the introduction, I don't have a clear sense of why we need human feedback for QA.\n\nquestions_for_the_authors: A. What is the advantage of improving using human feedback compared to using human labels to continuously train the model? It is easier and cheaper to collect? Or other advantages?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "jKrf1qJ0fZ",
        "length": 271,
        "human_text": "paper_topic_and_main_contributions: The authors propose a method to continually improve extractive QA via human feedback. The method does have high practical value and may be applied in building other NLP application systems. Extensive and carefully designed experiments demonstrate that an extractive QA system can be improved via interaction with human users over time and further broaden our understanding about continual learning with user feedback.\n\nreasons_to_accept: 1. The authors propose a method to continually improve extractive QA via human feedback. The proposed method does have high practical value and may be applied in building other NLP application systems.  2. Extensive and carefully designed experiments demonstrate that an extractive QA system can be improved via interaction with human users over time.\n\nreasons_to_reject: 1. Uncertainties introduced by human interactions are likely to affect the results and need to be carefully controlled with a well-defined guideline.\n\nquestions_for_the_authors: 1. What's the possible performance upper bound of the proposed method? How many rounds of interactions are optimal in general?\n\ntypos_grammar_style_and_presentation_improvements: Typos or grammar errors: 1. line 345, \"We are focused on ...\" 2. line 369, \"the systems sees\" 3. line 481, \"does not an explain ...\" 4. line 555, \"the that\" 5. line 620, \"out approach\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "hREKcOTq0t",
        "length": 200,
        "human_text": "paper_topic_and_main_contributions: The paper is about a reinforcement learning for extractive QA. authors trained a model in different rounds using human feedback to improve the model. They also tested different version of the model with different initialization and on different dataset to ensure the performances on different domain. Authors relies on crowdworkers for the feedback. Performances show the effectiveness of the approach.\n\nreasons_to_accept: + well written paper + novel approach to improve extractive QA + relevant results and a new interesting research for qa systems + good experiments and ablations\n\nreasons_to_reject: + miss a proper evaluation of the quality of the crowdworkers to avoid misleading errors + no clear how ensure feedback quality\n\nquestions_for_the_authors: 1 Can you better clarify how you can ensure feedback quality? \n2 Why you did not perform an evaluation of the annotators job?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "134_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_134_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7396333333333334,
      "max_similarity": 0.7627,
      "avg_coverage": 0.5022333333333333,
      "max_coverage": 0.6667
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 471,
      "avg_human_length": 336.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 9,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "frWd8cEKmp",
        "similarity": 0.7191,
        "coverage": 0.6667,
        "human_length": 212,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an annotation tool and protocal to annotate program semantics with natural language.\n\nreasons_to_accept: The authors contributed a dataset and an annotation protocol.\n\nreasons_to_reject: - The motivation is unclear to me, at least from a practical engineering perspective (e.g., to build an actually useable system). As the authors pointed out in L120, it was actually unclear how such a system would scale to more general general applications that have external libraries, which are very common in real-life applications.  - There might be computational linguistic/cognitive science motivations, though, and I am not qualified to judge. I am a bit worried that, by restricting our attention to a closed environment, we are missing the opportunity of collecting data on how language can be used to describe program semantics in a wider range of context.\n\nquestions_for_the_authors: A. The annotation in Figure 2 looks like pseudo code instead of natural language. Have you found any linguistically interesting annotations that do not look like templates?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Y0fJXCe7y6",
        "similarity": 0.7371,
        "coverage": 0.4,
        "human_length": 424,
        "human_text": "paper_topic_and_main_contributions: This paper's main contribution is a dataset containing fragments of standalone Python programs annotated with natural language preconditions and postconditions in a manner analogous to Hoare triples. The authors also contribute their annotation tool.\nThis resource is intended to support the development of programming assistants with improved understanding of code semantics. From the structure of the data, it appears the authors envision a system with the ability to infer an informal \"mental model\" of the behavior of a piece of code, and interrogate that model at various levels of granularity to provide improved consistency and/or explainability.\n\nreasons_to_accept: As the authors note in their motivation, scraping open-source repositories for comments leads to a few issues with the resulting data: comments don't cover all semantic levels of detail, and open-source projects often have complex dependencies that make it difficult to execute them in order to obtain ground-truth semantics. The proposed annotation scheme addresses the former, and the latter problem is addressed (at least in the authors' HTL dataset) by choosing to annotate standalone programs. In principle, addressing these issues should make the resulting resource useful to the community.\n\nreasons_to_reject: I don't see any issues in the proposed annotation scheme fundamental enough to merit rejection. However, the dataset (from what I can tell) appears to cover only 20 programs of the source dataset's ~400 (Schuster et al., 2021). Based on the histogram in Figure 3, the dataset as a whole contains 209 annotated triples. This restricted scale limits the usefulness of the primary contribution.\n\nquestions_for_the_authors: A: In your annotation tool, is it possible to select previous annotations (by span) and revise them?\n\ntypos_grammar_style_and_presentation_improvements: There are two orphans (single trailing lines across a page boundary): one on line 131 and one on line 270.\nPlease include some data statistics - at least the number of annotated triples + programs. The distribution of annotated code span lengths would be nice to see.\nIt's a little annoying to have to download the files individually from the Zenodo interface - it would be good to also provide them as a single archive!\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "NDeU2Vh4gE",
        "similarity": 0.7627,
        "coverage": 0.44,
        "human_length": 373,
        "human_text": "paper_topic_and_main_contributions: This paper presents a new dataset of **natural-language annotations** of program semantics (in the forms of pre- and post-conditions).\n\nreasons_to_accept: + A new bi-modal dataset that fills in the gap between informal annotations (e.g., code comments) and formal annotations (e.g., programmatic predicates)\n\nreasons_to_reject: - Lack of clear motivation for building the dataset (HTL): What are the downstream impacts of this dataset?  - No validation of annotation quality: How reliable are the annotations?\n- No in-depth descriptions of the annotated dataset: What are the distributions of these annotations? Are there any patterns and categories? Any artifacts or shortcuts?\n\nquestions_for_the_authors: A. Why 'self-contained' and 'reason locally' should be desiderata if they fundamentally limit what types of programs to annotate? As the authors claimed, \"Consequently our derived HTL dataset is subject to the same limitations and as such its instances are not representative of many real-world programs.\". If that is the case, what are the downstream impacts of this dataset?\nB. How can HTL augment existing large-scale bi-modal datasets scraped from GitHub? The paper conjectures that a language model can be fine-tuned with HTL but does not provide even preliminary results of how such fine-tuning can lead to a better LM.\nC. What are the main benefits of using natural language to annotate pre- and post-conditions if many can be easily translated to programmatic predicates? For example, in Figure 2, \" 'f' is a string\" can be easily translated to `type(f) == str`.\nD. Could you provide more in-depth qualitative and quantitative descriptions of HTL dataset?\n\ntypos_grammar_style_and_presentation_improvements: - Section 2, Dataset construction: The narrative can be straight to the point. Explain why you chose \"Programming Puzzles\" first and then exclude other alternatives.\n- Figure 3: Using a pie chart would be clearer.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "frWd8cEKmp",
        "length": 212,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an annotation tool and protocal to annotate program semantics with natural language.\n\nreasons_to_accept: The authors contributed a dataset and an annotation protocol.\n\nreasons_to_reject: - The motivation is unclear to me, at least from a practical engineering perspective (e.g., to build an actually useable system). As the authors pointed out in L120, it was actually unclear how such a system would scale to more general general applications that have external libraries, which are very common in real-life applications.  - There might be computational linguistic/cognitive science motivations, though, and I am not qualified to judge. I am a bit worried that, by restricting our attention to a closed environment, we are missing the opportunity of collecting data on how language can be used to describe program semantics in a wider range of context.\n\nquestions_for_the_authors: A. The annotation in Figure 2 looks like pseudo code instead of natural language. Have you found any linguistically interesting annotations that do not look like templates?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      },
      {
        "reviewer_id": "Y0fJXCe7y6",
        "length": 424,
        "human_text": "paper_topic_and_main_contributions: This paper's main contribution is a dataset containing fragments of standalone Python programs annotated with natural language preconditions and postconditions in a manner analogous to Hoare triples. The authors also contribute their annotation tool.\nThis resource is intended to support the development of programming assistants with improved understanding of code semantics. From the structure of the data, it appears the authors envision a system with the ability to infer an informal \"mental model\" of the behavior of a piece of code, and interrogate that model at various levels of granularity to provide improved consistency and/or explainability.\n\nreasons_to_accept: As the authors note in their motivation, scraping open-source repositories for comments leads to a few issues with the resulting data: comments don't cover all semantic levels of detail, and open-source projects often have complex dependencies that make it difficult to execute them in order to obtain ground-truth semantics. The proposed annotation scheme addresses the former, and the latter problem is addressed (at least in the authors' HTL dataset) by choosing to annotate standalone programs. In principle, addressing these issues should make the resulting resource useful to the community.\n\nreasons_to_reject: I don't see any issues in the proposed annotation scheme fundamental enough to merit rejection. However, the dataset (from what I can tell) appears to cover only 20 programs of the source dataset's ~400 (Schuster et al., 2021). Based on the histogram in Figure 3, the dataset as a whole contains 209 annotated triples. This restricted scale limits the usefulness of the primary contribution.\n\nquestions_for_the_authors: A: In your annotation tool, is it possible to select previous annotations (by span) and revise them?\n\ntypos_grammar_style_and_presentation_improvements: There are two orphans (single trailing lines across a page boundary): one on line 131 and one on line 270.\nPlease include some data statistics - at least the number of annotated triples + programs. The distribution of annotated code span lengths would be nice to see.\nIt's a little annoying to have to download the files individually from the Zenodo interface - it would be good to also provide them as a single archive!\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "NDeU2Vh4gE",
        "length": 373,
        "human_text": "paper_topic_and_main_contributions: This paper presents a new dataset of **natural-language annotations** of program semantics (in the forms of pre- and post-conditions).\n\nreasons_to_accept: + A new bi-modal dataset that fills in the gap between informal annotations (e.g., code comments) and formal annotations (e.g., programmatic predicates)\n\nreasons_to_reject: - Lack of clear motivation for building the dataset (HTL): What are the downstream impacts of this dataset?  - No validation of annotation quality: How reliable are the annotations?\n- No in-depth descriptions of the annotated dataset: What are the distributions of these annotations? Are there any patterns and categories? Any artifacts or shortcuts?\n\nquestions_for_the_authors: A. Why 'self-contained' and 'reason locally' should be desiderata if they fundamentally limit what types of programs to annotate? As the authors claimed, \"Consequently our derived HTL dataset is subject to the same limitations and as such its instances are not representative of many real-world programs.\". If that is the case, what are the downstream impacts of this dataset?\nB. How can HTL augment existing large-scale bi-modal datasets scraped from GitHub? The paper conjectures that a language model can be fine-tuned with HTL but does not provide even preliminary results of how such fine-tuning can lead to a better LM.\nC. What are the main benefits of using natural language to annotate pre- and post-conditions if many can be easily translated to programmatic predicates? For example, in Figure 2, \" 'f' is a string\" can be easily translated to `type(f) == str`.\nD. Could you provide more in-depth qualitative and quantitative descriptions of HTL dataset?\n\ntypos_grammar_style_and_presentation_improvements: - Section 2, Dataset construction: The narrative can be straight to the point. Explain why you chose \"Programming Puzzles\" first and then exclude other alternatives.\n- Figure 3: Using a pie chart would be clearer.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "134_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_134_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7404999999999999,
      "max_similarity": 0.7603,
      "avg_coverage": 0.5022333333333333,
      "max_coverage": 0.6667
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 417,
      "avg_human_length": 336.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 9,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "frWd8cEKmp",
        "similarity": 0.7239,
        "coverage": 0.6667,
        "human_length": 212,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an annotation tool and protocal to annotate program semantics with natural language.\n\nreasons_to_accept: The authors contributed a dataset and an annotation protocol.\n\nreasons_to_reject: - The motivation is unclear to me, at least from a practical engineering perspective (e.g., to build an actually useable system). As the authors pointed out in L120, it was actually unclear how such a system would scale to more general general applications that have external libraries, which are very common in real-life applications.  - There might be computational linguistic/cognitive science motivations, though, and I am not qualified to judge. I am a bit worried that, by restricting our attention to a closed environment, we are missing the opportunity of collecting data on how language can be used to describe program semantics in a wider range of context.\n\nquestions_for_the_authors: A. The annotation in Figure 2 looks like pseudo code instead of natural language. Have you found any linguistically interesting annotations that do not look like templates?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Y0fJXCe7y6",
        "similarity": 0.7373,
        "coverage": 0.4,
        "human_length": 424,
        "human_text": "paper_topic_and_main_contributions: This paper's main contribution is a dataset containing fragments of standalone Python programs annotated with natural language preconditions and postconditions in a manner analogous to Hoare triples. The authors also contribute their annotation tool.\nThis resource is intended to support the development of programming assistants with improved understanding of code semantics. From the structure of the data, it appears the authors envision a system with the ability to infer an informal \"mental model\" of the behavior of a piece of code, and interrogate that model at various levels of granularity to provide improved consistency and/or explainability.\n\nreasons_to_accept: As the authors note in their motivation, scraping open-source repositories for comments leads to a few issues with the resulting data: comments don't cover all semantic levels of detail, and open-source projects often have complex dependencies that make it difficult to execute them in order to obtain ground-truth semantics. The proposed annotation scheme addresses the former, and the latter problem is addressed (at least in the authors' HTL dataset) by choosing to annotate standalone programs. In principle, addressing these issues should make the resulting resource useful to the community.\n\nreasons_to_reject: I don't see any issues in the proposed annotation scheme fundamental enough to merit rejection. However, the dataset (from what I can tell) appears to cover only 20 programs of the source dataset's ~400 (Schuster et al., 2021). Based on the histogram in Figure 3, the dataset as a whole contains 209 annotated triples. This restricted scale limits the usefulness of the primary contribution.\n\nquestions_for_the_authors: A: In your annotation tool, is it possible to select previous annotations (by span) and revise them?\n\ntypos_grammar_style_and_presentation_improvements: There are two orphans (single trailing lines across a page boundary): one on line 131 and one on line 270.\nPlease include some data statistics - at least the number of annotated triples + programs. The distribution of annotated code span lengths would be nice to see.\nIt's a little annoying to have to download the files individually from the Zenodo interface - it would be good to also provide them as a single archive!\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "NDeU2Vh4gE",
        "similarity": 0.7603,
        "coverage": 0.44,
        "human_length": 373,
        "human_text": "paper_topic_and_main_contributions: This paper presents a new dataset of **natural-language annotations** of program semantics (in the forms of pre- and post-conditions).\n\nreasons_to_accept: + A new bi-modal dataset that fills in the gap between informal annotations (e.g., code comments) and formal annotations (e.g., programmatic predicates)\n\nreasons_to_reject: - Lack of clear motivation for building the dataset (HTL): What are the downstream impacts of this dataset?  - No validation of annotation quality: How reliable are the annotations?\n- No in-depth descriptions of the annotated dataset: What are the distributions of these annotations? Are there any patterns and categories? Any artifacts or shortcuts?\n\nquestions_for_the_authors: A. Why 'self-contained' and 'reason locally' should be desiderata if they fundamentally limit what types of programs to annotate? As the authors claimed, \"Consequently our derived HTL dataset is subject to the same limitations and as such its instances are not representative of many real-world programs.\". If that is the case, what are the downstream impacts of this dataset?\nB. How can HTL augment existing large-scale bi-modal datasets scraped from GitHub? The paper conjectures that a language model can be fine-tuned with HTL but does not provide even preliminary results of how such fine-tuning can lead to a better LM.\nC. What are the main benefits of using natural language to annotate pre- and post-conditions if many can be easily translated to programmatic predicates? For example, in Figure 2, \" 'f' is a string\" can be easily translated to `type(f) == str`.\nD. Could you provide more in-depth qualitative and quantitative descriptions of HTL dataset?\n\ntypos_grammar_style_and_presentation_improvements: - Section 2, Dataset construction: The narrative can be straight to the point. Explain why you chose \"Programming Puzzles\" first and then exclude other alternatives.\n- Figure 3: Using a pie chart would be clearer.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "frWd8cEKmp",
        "length": 212,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an annotation tool and protocal to annotate program semantics with natural language.\n\nreasons_to_accept: The authors contributed a dataset and an annotation protocol.\n\nreasons_to_reject: - The motivation is unclear to me, at least from a practical engineering perspective (e.g., to build an actually useable system). As the authors pointed out in L120, it was actually unclear how such a system would scale to more general general applications that have external libraries, which are very common in real-life applications.  - There might be computational linguistic/cognitive science motivations, though, and I am not qualified to judge. I am a bit worried that, by restricting our attention to a closed environment, we are missing the opportunity of collecting data on how language can be used to describe program semantics in a wider range of context.\n\nquestions_for_the_authors: A. The annotation in Figure 2 looks like pseudo code instead of natural language. Have you found any linguistically interesting annotations that do not look like templates?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      },
      {
        "reviewer_id": "Y0fJXCe7y6",
        "length": 424,
        "human_text": "paper_topic_and_main_contributions: This paper's main contribution is a dataset containing fragments of standalone Python programs annotated with natural language preconditions and postconditions in a manner analogous to Hoare triples. The authors also contribute their annotation tool.\nThis resource is intended to support the development of programming assistants with improved understanding of code semantics. From the structure of the data, it appears the authors envision a system with the ability to infer an informal \"mental model\" of the behavior of a piece of code, and interrogate that model at various levels of granularity to provide improved consistency and/or explainability.\n\nreasons_to_accept: As the authors note in their motivation, scraping open-source repositories for comments leads to a few issues with the resulting data: comments don't cover all semantic levels of detail, and open-source projects often have complex dependencies that make it difficult to execute them in order to obtain ground-truth semantics. The proposed annotation scheme addresses the former, and the latter problem is addressed (at least in the authors' HTL dataset) by choosing to annotate standalone programs. In principle, addressing these issues should make the resulting resource useful to the community.\n\nreasons_to_reject: I don't see any issues in the proposed annotation scheme fundamental enough to merit rejection. However, the dataset (from what I can tell) appears to cover only 20 programs of the source dataset's ~400 (Schuster et al., 2021). Based on the histogram in Figure 3, the dataset as a whole contains 209 annotated triples. This restricted scale limits the usefulness of the primary contribution.\n\nquestions_for_the_authors: A: In your annotation tool, is it possible to select previous annotations (by span) and revise them?\n\ntypos_grammar_style_and_presentation_improvements: There are two orphans (single trailing lines across a page boundary): one on line 131 and one on line 270.\nPlease include some data statistics - at least the number of annotated triples + programs. The distribution of annotated code span lengths would be nice to see.\nIt's a little annoying to have to download the files individually from the Zenodo interface - it would be good to also provide them as a single archive!\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "NDeU2Vh4gE",
        "length": 373,
        "human_text": "paper_topic_and_main_contributions: This paper presents a new dataset of **natural-language annotations** of program semantics (in the forms of pre- and post-conditions).\n\nreasons_to_accept: + A new bi-modal dataset that fills in the gap between informal annotations (e.g., code comments) and formal annotations (e.g., programmatic predicates)\n\nreasons_to_reject: - Lack of clear motivation for building the dataset (HTL): What are the downstream impacts of this dataset?  - No validation of annotation quality: How reliable are the annotations?\n- No in-depth descriptions of the annotated dataset: What are the distributions of these annotations? Are there any patterns and categories? Any artifacts or shortcuts?\n\nquestions_for_the_authors: A. Why 'self-contained' and 'reason locally' should be desiderata if they fundamentally limit what types of programs to annotate? As the authors claimed, \"Consequently our derived HTL dataset is subject to the same limitations and as such its instances are not representative of many real-world programs.\". If that is the case, what are the downstream impacts of this dataset?\nB. How can HTL augment existing large-scale bi-modal datasets scraped from GitHub? The paper conjectures that a language model can be fine-tuned with HTL but does not provide even preliminary results of how such fine-tuning can lead to a better LM.\nC. What are the main benefits of using natural language to annotate pre- and post-conditions if many can be easily translated to programmatic predicates? For example, in Figure 2, \" 'f' is a string\" can be easily translated to `type(f) == str`.\nD. Could you provide more in-depth qualitative and quantitative descriptions of HTL dataset?\n\ntypos_grammar_style_and_presentation_improvements: - Section 2, Dataset construction: The narrative can be straight to the point. Explain why you chose \"Programming Puzzles\" first and then exclude other alternatives.\n- Figure 3: Using a pie chart would be clearer.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "154_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_154_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7394999999999999,
      "max_similarity": 0.7514,
      "avg_coverage": 0.5022000000000001,
      "max_coverage": 0.5238
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 512,
      "avg_human_length": 416.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "zOZSw6ikVP",
        "similarity": 0.7514,
        "coverage": 0.4828,
        "human_length": 481,
        "human_text": "paper_topic_and_main_contributions: This paper investigates the added toxicity (introduced by MT systems) in a large-scale multilingual MT setting. The authors evaluate and analyze added toxicity when translating a large test set, translating from English into 164 languages.  They find that low-resource languages are more likely have added toxicity. \nThey use source contribution and robustness to interpret what causes toxicity, and find that toxic target words lean to have lower source attention weights and are less robust.\n\nreasons_to_accept: - The authors conducted experiments on a large set of language pairs.  - The authors have done a lot of evaluations and analyses.  - They provide three suggestions to reduce toxicity, including curating training data, mitigating hallucinations, and improving translation robustness.\n\nreasons_to_reject: - The findings are not convincing enough, the statistical numbers are not conclusive. Such as 40.1%, 40.7% in line 391; 45.6%, 54.8% in lines 392-393; 45.7%,54.3% in lines 401-402, etc.   - The authors also point out that the results vary across languages.  - Some experiments lack some motivations such as using translation robustness to interpret added toxicity.  - the organization can be further improved, such as section 3, the methodology and experimental settings are mixed toghether.  - some parts are not clear to me, such as: added toxicity varies from 0 to 5% in line 12 but the authors mentioned that they remove languages with over 5% toxicity in line 303.  - some assumptions are not strong enough, such as the relation between hallucination and low source contribution. Using attention weights to present contribution is also debatable.\n\nquestions_for_the_authors: - line 303 why you remove languages with over 5% toxicity?\n- line 418: what's the motivation of using robustness of translations?\n- figure 3, the right figure, do you have any explainations for the space with Gini 80-90%, source contribution 70-80%, outliers?  How many examples in the cyan box?\n\ntypos_grammar_style_and_presentation_improvements: Title: Maybe \"Added toxicity\" is better than toxicity as you only investigate added toxicity.   - section 2: change the section name to background, there is no need to put dffinitions in the title. You can just give a term (in bold) and its definition in the following text. It would be clearer to readers instead of letting readers searching for terms.  - line 259: change to Experiments?\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: this paper works on toxity in MT.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "0Dubo9Gcqc",
        "similarity": 0.7454,
        "coverage": 0.5238,
        "human_length": 450,
        "human_text": "paper_topic_and_main_contributions: This paper studies the error of \"added toxicity\" in Machine Translation both quantitatively through automatic detection and qualitatively through human evaluation.\n\nreasons_to_accept: The authors use a measurement for input attribution to explore the cause and source of added toxicity.\n\nreasons_to_reject: This paper lacks relevant citations and sufficient details to support the claims made: - Key citations on human evaluation are missing despite criticism and discussion of human evaluation in Lines 41-46 and an attempted human evaluation in Section 6.\n- Experimental decisions are not explained or justified; see Questions E and F.  - The results are presented in an imprecise way: no statistics are given for the claim in Line 341; terms used when reporting results are not explained (see Questions A, B, and C); The top graph in Figure 2 is trying to show because the axis labels are confusing (see Question J)\n\nquestions_for_the_authors: -QA: What is the \"nonce (non-sense)\" axis? \n-QB: What does it mean for a noun to be the \"most toxic?\" Does that mean the noun in the source text frequently contributes to a toxic token? \n-QC: Do you have any insight into why the parental nouns are the \"most toxic?\" \n-QD: Where in Section 4 do you explain how the percentages are calculated as mentioned in Line 148? \n-QE: How did you choose your thresholds for high-, mid-, and low-toxicity? \n-QF: Why did you decide to exclude languages with more than 5% detected toxicity as opposed to another threshold? \n-QG: Did you filter the datasets for toxicity that already exists in the source language before translating them? \n-QH: Which model output the examples in Figure 1? \n-QI: Can you provide more context for the HolisticBias dataset? What was it designed for? \n-QJ: Figure 2 questions: Why do the y-ticks on the graph in Figure 2 seem like log scale? What does the x-axis label (Index of languages ordered by toxicity) mean? Since you have already sorted the languages by toxicity, we would expect the fraction of toxic-labeled examples to decrease as languages get \"less\" toxic, so what interesting phenomenon is this graph depicting?\n\nmissing_references: -Multidimensional Quality Metrics, Lommel et al. 2013 https://aclanthology.org/2013.tc-1.6.pdf -Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation, Freitag et al. 2021 https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00437/108866\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "LkQlB34W81",
        "similarity": 0.7217,
        "coverage": 0.5,
        "human_length": 317,
        "human_text": "paper_topic_and_main_contributions: This paper investigates the concept of \"added toxicity\" to translations (i.e., hallucinated or mistranslated sentences that add toxicity from source to target). They translate the HolisticBias dataset, which uses templates to point out toxic translations of a model. Then, measure the layer-wise attention weight contributions of each translated token to possible toxic translations. The analysis consists of 164 languages on 13 axes of toxicity. The paper investigates the robustness of translations with the automatic Gini impurity measure. Last, the paper conducts a human evaluation of the HolisticBias translations for 8 languages.\n\nreasons_to_accept: - A large and detailed empirical study of \"added toxicity\" in translations of a comprehensive set of languages. Strong fit for EMNLP.  - A well-written, easy-to-read paper (even for researchers not in the field), with good takeaways in the conclusion section.\n- Strong analysis of the human evaluation part of the automatic translations.\n\nreasons_to_reject: - Not any strong reasons to reject, only that the research has been conducted with 1 large model (results of one other model put in the appendix). Initially given the title, I expected a study with multiple models from a small # of params to large and whether the findings would hold at scale.\n\nquestions_for_the_authors: - L326--331: I might be missing something: How do you read the bottom of Figure 2? Isn't nonce sharing an almost 10% part of the overall distribution of toxic translations? What about body type (magenta), this also seems to take a large share, but not mentioned in the text.\n\ntypos_grammar_style_and_presentation_improvements: - The abbreviation AT Level in Table 1 is \"Added Toxicity\" right? Wasn't mentioned in the text as far as I know.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "zOZSw6ikVP",
        "length": 481,
        "human_text": "paper_topic_and_main_contributions: This paper investigates the added toxicity (introduced by MT systems) in a large-scale multilingual MT setting. The authors evaluate and analyze added toxicity when translating a large test set, translating from English into 164 languages.  They find that low-resource languages are more likely have added toxicity. \nThey use source contribution and robustness to interpret what causes toxicity, and find that toxic target words lean to have lower source attention weights and are less robust.\n\nreasons_to_accept: - The authors conducted experiments on a large set of language pairs.  - The authors have done a lot of evaluations and analyses.  - They provide three suggestions to reduce toxicity, including curating training data, mitigating hallucinations, and improving translation robustness.\n\nreasons_to_reject: - The findings are not convincing enough, the statistical numbers are not conclusive. Such as 40.1%, 40.7% in line 391; 45.6%, 54.8% in lines 392-393; 45.7%,54.3% in lines 401-402, etc.   - The authors also point out that the results vary across languages.  - Some experiments lack some motivations such as using translation robustness to interpret added toxicity.  - the organization can be further improved, such as section 3, the methodology and experimental settings are mixed toghether.  - some parts are not clear to me, such as: added toxicity varies from 0 to 5% in line 12 but the authors mentioned that they remove languages with over 5% toxicity in line 303.  - some assumptions are not strong enough, such as the relation between hallucination and low source contribution. Using attention weights to present contribution is also debatable.\n\nquestions_for_the_authors: - line 303 why you remove languages with over 5% toxicity?\n- line 418: what's the motivation of using robustness of translations?\n- figure 3, the right figure, do you have any explainations for the space with Gini 80-90%, source contribution 70-80%, outliers?  How many examples in the cyan box?\n\ntypos_grammar_style_and_presentation_improvements: Title: Maybe \"Added toxicity\" is better than toxicity as you only investigate added toxicity.   - section 2: change the section name to background, there is no need to put dffinitions in the title. You can just give a term (in bold) and its definition in the following text. It would be clearer to readers instead of letting readers searching for terms.  - line 259: change to Experiments?\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: this paper works on toxity in MT.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "0Dubo9Gcqc",
        "length": 450,
        "human_text": "paper_topic_and_main_contributions: This paper studies the error of \"added toxicity\" in Machine Translation both quantitatively through automatic detection and qualitatively through human evaluation.\n\nreasons_to_accept: The authors use a measurement for input attribution to explore the cause and source of added toxicity.\n\nreasons_to_reject: This paper lacks relevant citations and sufficient details to support the claims made: - Key citations on human evaluation are missing despite criticism and discussion of human evaluation in Lines 41-46 and an attempted human evaluation in Section 6.\n- Experimental decisions are not explained or justified; see Questions E and F.  - The results are presented in an imprecise way: no statistics are given for the claim in Line 341; terms used when reporting results are not explained (see Questions A, B, and C); The top graph in Figure 2 is trying to show because the axis labels are confusing (see Question J)\n\nquestions_for_the_authors: -QA: What is the \"nonce (non-sense)\" axis? \n-QB: What does it mean for a noun to be the \"most toxic?\" Does that mean the noun in the source text frequently contributes to a toxic token? \n-QC: Do you have any insight into why the parental nouns are the \"most toxic?\" \n-QD: Where in Section 4 do you explain how the percentages are calculated as mentioned in Line 148? \n-QE: How did you choose your thresholds for high-, mid-, and low-toxicity? \n-QF: Why did you decide to exclude languages with more than 5% detected toxicity as opposed to another threshold? \n-QG: Did you filter the datasets for toxicity that already exists in the source language before translating them? \n-QH: Which model output the examples in Figure 1? \n-QI: Can you provide more context for the HolisticBias dataset? What was it designed for? \n-QJ: Figure 2 questions: Why do the y-ticks on the graph in Figure 2 seem like log scale? What does the x-axis label (Index of languages ordered by toxicity) mean? Since you have already sorted the languages by toxicity, we would expect the fraction of toxic-labeled examples to decrease as languages get \"less\" toxic, so what interesting phenomenon is this graph depicting?\n\nmissing_references: -Multidimensional Quality Metrics, Lommel et al. 2013 https://aclanthology.org/2013.tc-1.6.pdf -Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation, Freitag et al. 2021 https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00437/108866\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "LkQlB34W81",
        "length": 317,
        "human_text": "paper_topic_and_main_contributions: This paper investigates the concept of \"added toxicity\" to translations (i.e., hallucinated or mistranslated sentences that add toxicity from source to target). They translate the HolisticBias dataset, which uses templates to point out toxic translations of a model. Then, measure the layer-wise attention weight contributions of each translated token to possible toxic translations. The analysis consists of 164 languages on 13 axes of toxicity. The paper investigates the robustness of translations with the automatic Gini impurity measure. Last, the paper conducts a human evaluation of the HolisticBias translations for 8 languages.\n\nreasons_to_accept: - A large and detailed empirical study of \"added toxicity\" in translations of a comprehensive set of languages. Strong fit for EMNLP.  - A well-written, easy-to-read paper (even for researchers not in the field), with good takeaways in the conclusion section.\n- Strong analysis of the human evaluation part of the automatic translations.\n\nreasons_to_reject: - Not any strong reasons to reject, only that the research has been conducted with 1 large model (results of one other model put in the appendix). Initially given the title, I expected a study with multiple models from a small # of params to large and whether the findings would hold at scale.\n\nquestions_for_the_authors: - L326--331: I might be missing something: How do you read the bottom of Figure 2? Isn't nonce sharing an almost 10% part of the overall distribution of toxic translations? What about body type (magenta), this also seems to take a large share, but not mentioned in the text.\n\ntypos_grammar_style_and_presentation_improvements: - The abbreviation AT Level in Table 1 is \"Added Toxicity\" right? Wasn't mentioned in the text as far as I know.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "154_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_154_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7383333333333333,
      "max_similarity": 0.7512,
      "avg_coverage": 0.5022000000000001,
      "max_coverage": 0.5238
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 503,
      "avg_human_length": 416.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "zOZSw6ikVP",
        "similarity": 0.7512,
        "coverage": 0.4828,
        "human_length": 481,
        "human_text": "paper_topic_and_main_contributions: This paper investigates the added toxicity (introduced by MT systems) in a large-scale multilingual MT setting. The authors evaluate and analyze added toxicity when translating a large test set, translating from English into 164 languages.  They find that low-resource languages are more likely have added toxicity. \nThey use source contribution and robustness to interpret what causes toxicity, and find that toxic target words lean to have lower source attention weights and are less robust.\n\nreasons_to_accept: - The authors conducted experiments on a large set of language pairs.  - The authors have done a lot of evaluations and analyses.  - They provide three suggestions to reduce toxicity, including curating training data, mitigating hallucinations, and improving translation robustness.\n\nreasons_to_reject: - The findings are not convincing enough, the statistical numbers are not conclusive. Such as 40.1%, 40.7% in line 391; 45.6%, 54.8% in lines 392-393; 45.7%,54.3% in lines 401-402, etc.   - The authors also point out that the results vary across languages.  - Some experiments lack some motivations such as using translation robustness to interpret added toxicity.  - the organization can be further improved, such as section 3, the methodology and experimental settings are mixed toghether.  - some parts are not clear to me, such as: added toxicity varies from 0 to 5% in line 12 but the authors mentioned that they remove languages with over 5% toxicity in line 303.  - some assumptions are not strong enough, such as the relation between hallucination and low source contribution. Using attention weights to present contribution is also debatable.\n\nquestions_for_the_authors: - line 303 why you remove languages with over 5% toxicity?\n- line 418: what's the motivation of using robustness of translations?\n- figure 3, the right figure, do you have any explainations for the space with Gini 80-90%, source contribution 70-80%, outliers?  How many examples in the cyan box?\n\ntypos_grammar_style_and_presentation_improvements: Title: Maybe \"Added toxicity\" is better than toxicity as you only investigate added toxicity.   - section 2: change the section name to background, there is no need to put dffinitions in the title. You can just give a term (in bold) and its definition in the following text. It would be clearer to readers instead of letting readers searching for terms.  - line 259: change to Experiments?\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: this paper works on toxity in MT.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "0Dubo9Gcqc",
        "similarity": 0.7427,
        "coverage": 0.5238,
        "human_length": 450,
        "human_text": "paper_topic_and_main_contributions: This paper studies the error of \"added toxicity\" in Machine Translation both quantitatively through automatic detection and qualitatively through human evaluation.\n\nreasons_to_accept: The authors use a measurement for input attribution to explore the cause and source of added toxicity.\n\nreasons_to_reject: This paper lacks relevant citations and sufficient details to support the claims made: - Key citations on human evaluation are missing despite criticism and discussion of human evaluation in Lines 41-46 and an attempted human evaluation in Section 6.\n- Experimental decisions are not explained or justified; see Questions E and F.  - The results are presented in an imprecise way: no statistics are given for the claim in Line 341; terms used when reporting results are not explained (see Questions A, B, and C); The top graph in Figure 2 is trying to show because the axis labels are confusing (see Question J)\n\nquestions_for_the_authors: -QA: What is the \"nonce (non-sense)\" axis? \n-QB: What does it mean for a noun to be the \"most toxic?\" Does that mean the noun in the source text frequently contributes to a toxic token? \n-QC: Do you have any insight into why the parental nouns are the \"most toxic?\" \n-QD: Where in Section 4 do you explain how the percentages are calculated as mentioned in Line 148? \n-QE: How did you choose your thresholds for high-, mid-, and low-toxicity? \n-QF: Why did you decide to exclude languages with more than 5% detected toxicity as opposed to another threshold? \n-QG: Did you filter the datasets for toxicity that already exists in the source language before translating them? \n-QH: Which model output the examples in Figure 1? \n-QI: Can you provide more context for the HolisticBias dataset? What was it designed for? \n-QJ: Figure 2 questions: Why do the y-ticks on the graph in Figure 2 seem like log scale? What does the x-axis label (Index of languages ordered by toxicity) mean? Since you have already sorted the languages by toxicity, we would expect the fraction of toxic-labeled examples to decrease as languages get \"less\" toxic, so what interesting phenomenon is this graph depicting?\n\nmissing_references: -Multidimensional Quality Metrics, Lommel et al. 2013 https://aclanthology.org/2013.tc-1.6.pdf -Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation, Freitag et al. 2021 https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00437/108866\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "LkQlB34W81",
        "similarity": 0.7211,
        "coverage": 0.5,
        "human_length": 317,
        "human_text": "paper_topic_and_main_contributions: This paper investigates the concept of \"added toxicity\" to translations (i.e., hallucinated or mistranslated sentences that add toxicity from source to target). They translate the HolisticBias dataset, which uses templates to point out toxic translations of a model. Then, measure the layer-wise attention weight contributions of each translated token to possible toxic translations. The analysis consists of 164 languages on 13 axes of toxicity. The paper investigates the robustness of translations with the automatic Gini impurity measure. Last, the paper conducts a human evaluation of the HolisticBias translations for 8 languages.\n\nreasons_to_accept: - A large and detailed empirical study of \"added toxicity\" in translations of a comprehensive set of languages. Strong fit for EMNLP.  - A well-written, easy-to-read paper (even for researchers not in the field), with good takeaways in the conclusion section.\n- Strong analysis of the human evaluation part of the automatic translations.\n\nreasons_to_reject: - Not any strong reasons to reject, only that the research has been conducted with 1 large model (results of one other model put in the appendix). Initially given the title, I expected a study with multiple models from a small # of params to large and whether the findings would hold at scale.\n\nquestions_for_the_authors: - L326--331: I might be missing something: How do you read the bottom of Figure 2? Isn't nonce sharing an almost 10% part of the overall distribution of toxic translations? What about body type (magenta), this also seems to take a large share, but not mentioned in the text.\n\ntypos_grammar_style_and_presentation_improvements: - The abbreviation AT Level in Table 1 is \"Added Toxicity\" right? Wasn't mentioned in the text as far as I know.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "zOZSw6ikVP",
        "length": 481,
        "human_text": "paper_topic_and_main_contributions: This paper investigates the added toxicity (introduced by MT systems) in a large-scale multilingual MT setting. The authors evaluate and analyze added toxicity when translating a large test set, translating from English into 164 languages.  They find that low-resource languages are more likely have added toxicity. \nThey use source contribution and robustness to interpret what causes toxicity, and find that toxic target words lean to have lower source attention weights and are less robust.\n\nreasons_to_accept: - The authors conducted experiments on a large set of language pairs.  - The authors have done a lot of evaluations and analyses.  - They provide three suggestions to reduce toxicity, including curating training data, mitigating hallucinations, and improving translation robustness.\n\nreasons_to_reject: - The findings are not convincing enough, the statistical numbers are not conclusive. Such as 40.1%, 40.7% in line 391; 45.6%, 54.8% in lines 392-393; 45.7%,54.3% in lines 401-402, etc.   - The authors also point out that the results vary across languages.  - Some experiments lack some motivations such as using translation robustness to interpret added toxicity.  - the organization can be further improved, such as section 3, the methodology and experimental settings are mixed toghether.  - some parts are not clear to me, such as: added toxicity varies from 0 to 5% in line 12 but the authors mentioned that they remove languages with over 5% toxicity in line 303.  - some assumptions are not strong enough, such as the relation between hallucination and low source contribution. Using attention weights to present contribution is also debatable.\n\nquestions_for_the_authors: - line 303 why you remove languages with over 5% toxicity?\n- line 418: what's the motivation of using robustness of translations?\n- figure 3, the right figure, do you have any explainations for the space with Gini 80-90%, source contribution 70-80%, outliers?  How many examples in the cyan box?\n\ntypos_grammar_style_and_presentation_improvements: Title: Maybe \"Added toxicity\" is better than toxicity as you only investigate added toxicity.   - section 2: change the section name to background, there is no need to put dffinitions in the title. You can just give a term (in bold) and its definition in the following text. It would be clearer to readers instead of letting readers searching for terms.  - line 259: change to Experiments?\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: this paper works on toxity in MT.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "0Dubo9Gcqc",
        "length": 450,
        "human_text": "paper_topic_and_main_contributions: This paper studies the error of \"added toxicity\" in Machine Translation both quantitatively through automatic detection and qualitatively through human evaluation.\n\nreasons_to_accept: The authors use a measurement for input attribution to explore the cause and source of added toxicity.\n\nreasons_to_reject: This paper lacks relevant citations and sufficient details to support the claims made: - Key citations on human evaluation are missing despite criticism and discussion of human evaluation in Lines 41-46 and an attempted human evaluation in Section 6.\n- Experimental decisions are not explained or justified; see Questions E and F.  - The results are presented in an imprecise way: no statistics are given for the claim in Line 341; terms used when reporting results are not explained (see Questions A, B, and C); The top graph in Figure 2 is trying to show because the axis labels are confusing (see Question J)\n\nquestions_for_the_authors: -QA: What is the \"nonce (non-sense)\" axis? \n-QB: What does it mean for a noun to be the \"most toxic?\" Does that mean the noun in the source text frequently contributes to a toxic token? \n-QC: Do you have any insight into why the parental nouns are the \"most toxic?\" \n-QD: Where in Section 4 do you explain how the percentages are calculated as mentioned in Line 148? \n-QE: How did you choose your thresholds for high-, mid-, and low-toxicity? \n-QF: Why did you decide to exclude languages with more than 5% detected toxicity as opposed to another threshold? \n-QG: Did you filter the datasets for toxicity that already exists in the source language before translating them? \n-QH: Which model output the examples in Figure 1? \n-QI: Can you provide more context for the HolisticBias dataset? What was it designed for? \n-QJ: Figure 2 questions: Why do the y-ticks on the graph in Figure 2 seem like log scale? What does the x-axis label (Index of languages ordered by toxicity) mean? Since you have already sorted the languages by toxicity, we would expect the fraction of toxic-labeled examples to decrease as languages get \"less\" toxic, so what interesting phenomenon is this graph depicting?\n\nmissing_references: -Multidimensional Quality Metrics, Lommel et al. 2013 https://aclanthology.org/2013.tc-1.6.pdf -Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation, Freitag et al. 2021 https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00437/108866\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "LkQlB34W81",
        "length": 317,
        "human_text": "paper_topic_and_main_contributions: This paper investigates the concept of \"added toxicity\" to translations (i.e., hallucinated or mistranslated sentences that add toxicity from source to target). They translate the HolisticBias dataset, which uses templates to point out toxic translations of a model. Then, measure the layer-wise attention weight contributions of each translated token to possible toxic translations. The analysis consists of 164 languages on 13 axes of toxicity. The paper investigates the robustness of translations with the automatic Gini impurity measure. Last, the paper conducts a human evaluation of the HolisticBias translations for 8 languages.\n\nreasons_to_accept: - A large and detailed empirical study of \"added toxicity\" in translations of a comprehensive set of languages. Strong fit for EMNLP.  - A well-written, easy-to-read paper (even for researchers not in the field), with good takeaways in the conclusion section.\n- Strong analysis of the human evaluation part of the automatic translations.\n\nreasons_to_reject: - Not any strong reasons to reject, only that the research has been conducted with 1 large model (results of one other model put in the appendix). Initially given the title, I expected a study with multiple models from a small # of params to large and whether the findings would hold at scale.\n\nquestions_for_the_authors: - L326--331: I might be missing something: How do you read the bottom of Figure 2? Isn't nonce sharing an almost 10% part of the overall distribution of toxic translations? What about body type (magenta), this also seems to take a large share, but not mentioned in the text.\n\ntypos_grammar_style_and_presentation_improvements: - The abbreviation AT Level in Table 1 is \"Added Toxicity\" right? Wasn't mentioned in the text as far as I know.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "111_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_111_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7202000000000001,
      "max_similarity": 0.7301,
      "avg_coverage": 0.4647666666666667,
      "max_coverage": 0.6471
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 449,
      "avg_human_length": 393.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 6,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "QPRPCShLCt",
        "similarity": 0.7301,
        "coverage": 0.2927,
        "human_length": 563,
        "human_text": "paper_topic_and_main_contributions: this work finetuned a multilingual self-supervised speech model using labels generated from off-the-shelf unsupervised word-level speech segmentation systems, namely DPDP, VG-HuBERT, and DP-Parse, for improved word segmentation performance. \nSignificant performance gain is observed for all three types of pseudo-labels. In particular, using pseudo labels from DP-Parse, finetuning XLS-R brings an improvement of 130%.  This work advanced the state-of-the-art of unsupervised word-level speech segmentation by a large margin, which is important for bridging the gap between text-based and speech-based language systems. This work leads to a deeper understanding of multilingual self-supervised speech models\n\nreasons_to_accept: 1. the fact that simple finetuning on pseudo labels via the general cross-entropy loss can bring such significant gain is very interestingly, which reveals interesting properties of the pretrained self-supervised speech models.\n2. this work examined using pseudo labels generated by a range of very different state-of-the-art unsupervised systems, namely DPDP, VG-HuBERT, and DP-Parse, and observed universal improvement. This indicate the robustness of the proposed approach.\n3. Training one model on all languages leads to better performance than training language specific models. Although this has been observed in large scale speech models (e.g. OpenAI's Whisper), it's the first time I observe this phenomenon in small scale studies (the total amount of data is below 80 hours.)\n4. a few tricks has being proposed to (potentially) make their approach works better, namely augmentation, smoothing, loss selection, peak detection. These tricks are not novel, but the usage is new and they make sense intuitively. These tricks could be valuable for researchers working on speech segmentation\n\nreasons_to_reject: Although significant improvements are shown, more explanations are desired:  1. with regard to the impressive performance zero-shot DP-Parse in Table 1, why is zero-shot working so well? What kind of words are being predicted? Or is there a pattern?; \n2. what leads to the discrepancy between using pseudo-labels from DPDP, VG-HuBERT, and DP-Parse\n\nquestions_for_the_authors: A. I'd like to see some ablation studies on how different tricks affect the performance (augmentation, smoothing, loss selection, peak detection) B. When using the time stretch augmentation, does the word boundary label also get stretched? \nC. How many iterations of self-training are needed before the model performance start to decrease?\n\nmissing_references: Citations to two relevant papers are missing: [1] and [2]. But it's worth mentioning that [2] is published within 3 months of this paper.  Both two papers and this work use pseudo-labels to finetune self-supervised speech models for speech segmentation.\n[1] focused on unsupervised phoneme segmentation, by finetuning w2v2 and hubert on pseudo labels generated by off-the-shelf unsup phoneme segmentation models.\n[2] showed that using the features of the SSL model itself to produce pseudo label can already bring significant improvement.\n[1] L. Strgar and D. Harwath, \"Phoneme Segmentation Using Self-Supervised Speech Models,\" 2022 IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar, 2023, pp. 1067-1073, doi: 10.1109/SLT54892.2023.10022827.\n[2] T. S. Fuchs and Y. Hoshen, \"Unsupervised Word Segmentation Using Temporal Gradient Pseudo-Labels,\" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10095363.\n\ntypos_grammar_style_and_presentation_improvements: line 228:  32Go GPU -> 32GB GPU line 257: Germand -> German\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "2d9hcEjXjY",
        "similarity": 0.7012,
        "coverage": 0.4545,
        "human_length": 270,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to finetune the XLS-R model in an \"unsupervised\" way to get better performance on word boundary detection. It compares several systems using their methods.\n\nreasons_to_accept: Exploring unsupervised ways to perform word segmentation in speech is an interesting direction\n\nreasons_to_reject: 1. I wonder if this is unsupervised, as it still receives supervision signals from a trained system. Training that system requires supervised labels, and it is uncertain where we can get those labels if the experiments are unsupervised. \n2. This paper does not compare to any force-alignment systems. The task, in my understanding, can be achieved reasonably easily using force alignment with a GMM-based system. \n3. Why is the system trained on Gold reference achieved 100.0 token-F1? So is that system already perfect? What on earth is the evaluation metric? How is it computed?\n\nquestions_for_the_authors: See RR. \nAlso, I am not convinced why through the described training method, we can get those improvements in token-F1. We did not get any further supervision signal, and there are no new mechanisms introduced. Can you explain why we get that improvement clearly?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "60X75uO91q",
        "similarity": 0.7293,
        "coverage": 0.6471,
        "human_length": 348,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an unsupervised speech segmentation system by fine-tuning the XLS-R speech representation model. Through an iterative training strategy, starting with an off-the-shelf speech segmentation system, the finetuned model shows improved results on the speech segmentation downstream task.\n\nreasons_to_accept: The idea of the proposed method is simple yet yields good results. By fine-tuning the added random feed-forward layer, the paper demonstrates that the pre-trained speech SSL system can be effective for the downstream task of speech segmentation. Additionally, the paper utilizes speech from different languages, which provides a valuable exploration into the understanding of SSL speech models on multilingual speech segmentation tasks.\n\nreasons_to_reject: The proposed method does not convincingly demonstrate its generalization and novelty, mostly showing the effectiveness of fine-tuning a certain speech SSL model on speech segmentation tasks. In addition to the main approach, there are many extra tricks used during training and evaluation, such as post-processing, loss sample selection, and data augmentation steps. It would be clearer to add some analysis or ablation study to convince the readers and show the distinct contributions of these different steps.\n\nquestions_for_the_authors: A. The proposed method is simple but efficient, however, the generalization of the fine-tuning approach is not convincing, as shown in Table 1 that the performance of DPDP and Hubert based ones are worse than the baseline for Mandarin. The discrepancy between different SSL speech models should be investigated to improve the quality of this work. For instance, the authors could dive deep to compare the precision and recall, the effectiveness of data augmentation, etc.\nB. Is the method robust to the accuracy/performance of the initial pseud-label from an off-the-shell speech segmentation system?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "QPRPCShLCt",
        "length": 563,
        "human_text": "paper_topic_and_main_contributions: this work finetuned a multilingual self-supervised speech model using labels generated from off-the-shelf unsupervised word-level speech segmentation systems, namely DPDP, VG-HuBERT, and DP-Parse, for improved word segmentation performance. \nSignificant performance gain is observed for all three types of pseudo-labels. In particular, using pseudo labels from DP-Parse, finetuning XLS-R brings an improvement of 130%.  This work advanced the state-of-the-art of unsupervised word-level speech segmentation by a large margin, which is important for bridging the gap between text-based and speech-based language systems. This work leads to a deeper understanding of multilingual self-supervised speech models\n\nreasons_to_accept: 1. the fact that simple finetuning on pseudo labels via the general cross-entropy loss can bring such significant gain is very interestingly, which reveals interesting properties of the pretrained self-supervised speech models.\n2. this work examined using pseudo labels generated by a range of very different state-of-the-art unsupervised systems, namely DPDP, VG-HuBERT, and DP-Parse, and observed universal improvement. This indicate the robustness of the proposed approach.\n3. Training one model on all languages leads to better performance than training language specific models. Although this has been observed in large scale speech models (e.g. OpenAI's Whisper), it's the first time I observe this phenomenon in small scale studies (the total amount of data is below 80 hours.)\n4. a few tricks has being proposed to (potentially) make their approach works better, namely augmentation, smoothing, loss selection, peak detection. These tricks are not novel, but the usage is new and they make sense intuitively. These tricks could be valuable for researchers working on speech segmentation\n\nreasons_to_reject: Although significant improvements are shown, more explanations are desired:  1. with regard to the impressive performance zero-shot DP-Parse in Table 1, why is zero-shot working so well? What kind of words are being predicted? Or is there a pattern?; \n2. what leads to the discrepancy between using pseudo-labels from DPDP, VG-HuBERT, and DP-Parse\n\nquestions_for_the_authors: A. I'd like to see some ablation studies on how different tricks affect the performance (augmentation, smoothing, loss selection, peak detection) B. When using the time stretch augmentation, does the word boundary label also get stretched? \nC. How many iterations of self-training are needed before the model performance start to decrease?\n\nmissing_references: Citations to two relevant papers are missing: [1] and [2]. But it's worth mentioning that [2] is published within 3 months of this paper.  Both two papers and this work use pseudo-labels to finetune self-supervised speech models for speech segmentation.\n[1] focused on unsupervised phoneme segmentation, by finetuning w2v2 and hubert on pseudo labels generated by off-the-shelf unsup phoneme segmentation models.\n[2] showed that using the features of the SSL model itself to produce pseudo label can already bring significant improvement.\n[1] L. Strgar and D. Harwath, \"Phoneme Segmentation Using Self-Supervised Speech Models,\" 2022 IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar, 2023, pp. 1067-1073, doi: 10.1109/SLT54892.2023.10022827.\n[2] T. S. Fuchs and Y. Hoshen, \"Unsupervised Word Segmentation Using Temporal Gradient Pseudo-Labels,\" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10095363.\n\ntypos_grammar_style_and_presentation_improvements: line 228:  32Go GPU -> 32GB GPU line 257: Germand -> German\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "2d9hcEjXjY",
        "length": 270,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to finetune the XLS-R model in an \"unsupervised\" way to get better performance on word boundary detection. It compares several systems using their methods.\n\nreasons_to_accept: Exploring unsupervised ways to perform word segmentation in speech is an interesting direction\n\nreasons_to_reject: 1. I wonder if this is unsupervised, as it still receives supervision signals from a trained system. Training that system requires supervised labels, and it is uncertain where we can get those labels if the experiments are unsupervised. \n2. This paper does not compare to any force-alignment systems. The task, in my understanding, can be achieved reasonably easily using force alignment with a GMM-based system. \n3. Why is the system trained on Gold reference achieved 100.0 token-F1? So is that system already perfect? What on earth is the evaluation metric? How is it computed?\n\nquestions_for_the_authors: See RR. \nAlso, I am not convinced why through the described training method, we can get those improvements in token-F1. We did not get any further supervision signal, and there are no new mechanisms introduced. Can you explain why we get that improvement clearly?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "60X75uO91q",
        "length": 348,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an unsupervised speech segmentation system by fine-tuning the XLS-R speech representation model. Through an iterative training strategy, starting with an off-the-shelf speech segmentation system, the finetuned model shows improved results on the speech segmentation downstream task.\n\nreasons_to_accept: The idea of the proposed method is simple yet yields good results. By fine-tuning the added random feed-forward layer, the paper demonstrates that the pre-trained speech SSL system can be effective for the downstream task of speech segmentation. Additionally, the paper utilizes speech from different languages, which provides a valuable exploration into the understanding of SSL speech models on multilingual speech segmentation tasks.\n\nreasons_to_reject: The proposed method does not convincingly demonstrate its generalization and novelty, mostly showing the effectiveness of fine-tuning a certain speech SSL model on speech segmentation tasks. In addition to the main approach, there are many extra tricks used during training and evaluation, such as post-processing, loss sample selection, and data augmentation steps. It would be clearer to add some analysis or ablation study to convince the readers and show the distinct contributions of these different steps.\n\nquestions_for_the_authors: A. The proposed method is simple but efficient, however, the generalization of the fine-tuning approach is not convincing, as shown in Table 1 that the performance of DPDP and Hubert based ones are worse than the baseline for Mandarin. The discrepancy between different SSL speech models should be investigated to improve the quality of this work. For instance, the authors could dive deep to compare the precision and recall, the effectiveness of data augmentation, etc.\nB. Is the method robust to the accuracy/performance of the initial pseud-label from an off-the-shell speech segmentation system?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "111_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_111_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7225666666666667,
      "max_similarity": 0.7362,
      "avg_coverage": 0.4647666666666667,
      "max_coverage": 0.6471
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 471,
      "avg_human_length": 393.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 6,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "QPRPCShLCt",
        "similarity": 0.7362,
        "coverage": 0.2927,
        "human_length": 563,
        "human_text": "paper_topic_and_main_contributions: this work finetuned a multilingual self-supervised speech model using labels generated from off-the-shelf unsupervised word-level speech segmentation systems, namely DPDP, VG-HuBERT, and DP-Parse, for improved word segmentation performance. \nSignificant performance gain is observed for all three types of pseudo-labels. In particular, using pseudo labels from DP-Parse, finetuning XLS-R brings an improvement of 130%.  This work advanced the state-of-the-art of unsupervised word-level speech segmentation by a large margin, which is important for bridging the gap between text-based and speech-based language systems. This work leads to a deeper understanding of multilingual self-supervised speech models\n\nreasons_to_accept: 1. the fact that simple finetuning on pseudo labels via the general cross-entropy loss can bring such significant gain is very interestingly, which reveals interesting properties of the pretrained self-supervised speech models.\n2. this work examined using pseudo labels generated by a range of very different state-of-the-art unsupervised systems, namely DPDP, VG-HuBERT, and DP-Parse, and observed universal improvement. This indicate the robustness of the proposed approach.\n3. Training one model on all languages leads to better performance than training language specific models. Although this has been observed in large scale speech models (e.g. OpenAI's Whisper), it's the first time I observe this phenomenon in small scale studies (the total amount of data is below 80 hours.)\n4. a few tricks has being proposed to (potentially) make their approach works better, namely augmentation, smoothing, loss selection, peak detection. These tricks are not novel, but the usage is new and they make sense intuitively. These tricks could be valuable for researchers working on speech segmentation\n\nreasons_to_reject: Although significant improvements are shown, more explanations are desired:  1. with regard to the impressive performance zero-shot DP-Parse in Table 1, why is zero-shot working so well? What kind of words are being predicted? Or is there a pattern?; \n2. what leads to the discrepancy between using pseudo-labels from DPDP, VG-HuBERT, and DP-Parse\n\nquestions_for_the_authors: A. I'd like to see some ablation studies on how different tricks affect the performance (augmentation, smoothing, loss selection, peak detection) B. When using the time stretch augmentation, does the word boundary label also get stretched? \nC. How many iterations of self-training are needed before the model performance start to decrease?\n\nmissing_references: Citations to two relevant papers are missing: [1] and [2]. But it's worth mentioning that [2] is published within 3 months of this paper.  Both two papers and this work use pseudo-labels to finetune self-supervised speech models for speech segmentation.\n[1] focused on unsupervised phoneme segmentation, by finetuning w2v2 and hubert on pseudo labels generated by off-the-shelf unsup phoneme segmentation models.\n[2] showed that using the features of the SSL model itself to produce pseudo label can already bring significant improvement.\n[1] L. Strgar and D. Harwath, \"Phoneme Segmentation Using Self-Supervised Speech Models,\" 2022 IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar, 2023, pp. 1067-1073, doi: 10.1109/SLT54892.2023.10022827.\n[2] T. S. Fuchs and Y. Hoshen, \"Unsupervised Word Segmentation Using Temporal Gradient Pseudo-Labels,\" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10095363.\n\ntypos_grammar_style_and_presentation_improvements: line 228:  32Go GPU -> 32GB GPU line 257: Germand -> German\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "2d9hcEjXjY",
        "similarity": 0.6981,
        "coverage": 0.4545,
        "human_length": 270,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to finetune the XLS-R model in an \"unsupervised\" way to get better performance on word boundary detection. It compares several systems using their methods.\n\nreasons_to_accept: Exploring unsupervised ways to perform word segmentation in speech is an interesting direction\n\nreasons_to_reject: 1. I wonder if this is unsupervised, as it still receives supervision signals from a trained system. Training that system requires supervised labels, and it is uncertain where we can get those labels if the experiments are unsupervised. \n2. This paper does not compare to any force-alignment systems. The task, in my understanding, can be achieved reasonably easily using force alignment with a GMM-based system. \n3. Why is the system trained on Gold reference achieved 100.0 token-F1? So is that system already perfect? What on earth is the evaluation metric? How is it computed?\n\nquestions_for_the_authors: See RR. \nAlso, I am not convinced why through the described training method, we can get those improvements in token-F1. We did not get any further supervision signal, and there are no new mechanisms introduced. Can you explain why we get that improvement clearly?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "60X75uO91q",
        "similarity": 0.7334,
        "coverage": 0.6471,
        "human_length": 348,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an unsupervised speech segmentation system by fine-tuning the XLS-R speech representation model. Through an iterative training strategy, starting with an off-the-shelf speech segmentation system, the finetuned model shows improved results on the speech segmentation downstream task.\n\nreasons_to_accept: The idea of the proposed method is simple yet yields good results. By fine-tuning the added random feed-forward layer, the paper demonstrates that the pre-trained speech SSL system can be effective for the downstream task of speech segmentation. Additionally, the paper utilizes speech from different languages, which provides a valuable exploration into the understanding of SSL speech models on multilingual speech segmentation tasks.\n\nreasons_to_reject: The proposed method does not convincingly demonstrate its generalization and novelty, mostly showing the effectiveness of fine-tuning a certain speech SSL model on speech segmentation tasks. In addition to the main approach, there are many extra tricks used during training and evaluation, such as post-processing, loss sample selection, and data augmentation steps. It would be clearer to add some analysis or ablation study to convince the readers and show the distinct contributions of these different steps.\n\nquestions_for_the_authors: A. The proposed method is simple but efficient, however, the generalization of the fine-tuning approach is not convincing, as shown in Table 1 that the performance of DPDP and Hubert based ones are worse than the baseline for Mandarin. The discrepancy between different SSL speech models should be investigated to improve the quality of this work. For instance, the authors could dive deep to compare the precision and recall, the effectiveness of data augmentation, etc.\nB. Is the method robust to the accuracy/performance of the initial pseud-label from an off-the-shell speech segmentation system?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "QPRPCShLCt",
        "length": 563,
        "human_text": "paper_topic_and_main_contributions: this work finetuned a multilingual self-supervised speech model using labels generated from off-the-shelf unsupervised word-level speech segmentation systems, namely DPDP, VG-HuBERT, and DP-Parse, for improved word segmentation performance. \nSignificant performance gain is observed for all three types of pseudo-labels. In particular, using pseudo labels from DP-Parse, finetuning XLS-R brings an improvement of 130%.  This work advanced the state-of-the-art of unsupervised word-level speech segmentation by a large margin, which is important for bridging the gap between text-based and speech-based language systems. This work leads to a deeper understanding of multilingual self-supervised speech models\n\nreasons_to_accept: 1. the fact that simple finetuning on pseudo labels via the general cross-entropy loss can bring such significant gain is very interestingly, which reveals interesting properties of the pretrained self-supervised speech models.\n2. this work examined using pseudo labels generated by a range of very different state-of-the-art unsupervised systems, namely DPDP, VG-HuBERT, and DP-Parse, and observed universal improvement. This indicate the robustness of the proposed approach.\n3. Training one model on all languages leads to better performance than training language specific models. Although this has been observed in large scale speech models (e.g. OpenAI's Whisper), it's the first time I observe this phenomenon in small scale studies (the total amount of data is below 80 hours.)\n4. a few tricks has being proposed to (potentially) make their approach works better, namely augmentation, smoothing, loss selection, peak detection. These tricks are not novel, but the usage is new and they make sense intuitively. These tricks could be valuable for researchers working on speech segmentation\n\nreasons_to_reject: Although significant improvements are shown, more explanations are desired:  1. with regard to the impressive performance zero-shot DP-Parse in Table 1, why is zero-shot working so well? What kind of words are being predicted? Or is there a pattern?; \n2. what leads to the discrepancy between using pseudo-labels from DPDP, VG-HuBERT, and DP-Parse\n\nquestions_for_the_authors: A. I'd like to see some ablation studies on how different tricks affect the performance (augmentation, smoothing, loss selection, peak detection) B. When using the time stretch augmentation, does the word boundary label also get stretched? \nC. How many iterations of self-training are needed before the model performance start to decrease?\n\nmissing_references: Citations to two relevant papers are missing: [1] and [2]. But it's worth mentioning that [2] is published within 3 months of this paper.  Both two papers and this work use pseudo-labels to finetune self-supervised speech models for speech segmentation.\n[1] focused on unsupervised phoneme segmentation, by finetuning w2v2 and hubert on pseudo labels generated by off-the-shelf unsup phoneme segmentation models.\n[2] showed that using the features of the SSL model itself to produce pseudo label can already bring significant improvement.\n[1] L. Strgar and D. Harwath, \"Phoneme Segmentation Using Self-Supervised Speech Models,\" 2022 IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar, 2023, pp. 1067-1073, doi: 10.1109/SLT54892.2023.10022827.\n[2] T. S. Fuchs and Y. Hoshen, \"Unsupervised Word Segmentation Using Temporal Gradient Pseudo-Labels,\" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10095363.\n\ntypos_grammar_style_and_presentation_improvements: line 228:  32Go GPU -> 32GB GPU line 257: Germand -> German\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "2d9hcEjXjY",
        "length": 270,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to finetune the XLS-R model in an \"unsupervised\" way to get better performance on word boundary detection. It compares several systems using their methods.\n\nreasons_to_accept: Exploring unsupervised ways to perform word segmentation in speech is an interesting direction\n\nreasons_to_reject: 1. I wonder if this is unsupervised, as it still receives supervision signals from a trained system. Training that system requires supervised labels, and it is uncertain where we can get those labels if the experiments are unsupervised. \n2. This paper does not compare to any force-alignment systems. The task, in my understanding, can be achieved reasonably easily using force alignment with a GMM-based system. \n3. Why is the system trained on Gold reference achieved 100.0 token-F1? So is that system already perfect? What on earth is the evaluation metric? How is it computed?\n\nquestions_for_the_authors: See RR. \nAlso, I am not convinced why through the described training method, we can get those improvements in token-F1. We did not get any further supervision signal, and there are no new mechanisms introduced. Can you explain why we get that improvement clearly?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "60X75uO91q",
        "length": 348,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an unsupervised speech segmentation system by fine-tuning the XLS-R speech representation model. Through an iterative training strategy, starting with an off-the-shelf speech segmentation system, the finetuned model shows improved results on the speech segmentation downstream task.\n\nreasons_to_accept: The idea of the proposed method is simple yet yields good results. By fine-tuning the added random feed-forward layer, the paper demonstrates that the pre-trained speech SSL system can be effective for the downstream task of speech segmentation. Additionally, the paper utilizes speech from different languages, which provides a valuable exploration into the understanding of SSL speech models on multilingual speech segmentation tasks.\n\nreasons_to_reject: The proposed method does not convincingly demonstrate its generalization and novelty, mostly showing the effectiveness of fine-tuning a certain speech SSL model on speech segmentation tasks. In addition to the main approach, there are many extra tricks used during training and evaluation, such as post-processing, loss sample selection, and data augmentation steps. It would be clearer to add some analysis or ablation study to convince the readers and show the distinct contributions of these different steps.\n\nquestions_for_the_authors: A. The proposed method is simple but efficient, however, the generalization of the fine-tuning approach is not convincing, as shown in Table 1 that the performance of DPDP and Hubert based ones are worse than the baseline for Mandarin. The discrepancy between different SSL speech models should be investigated to improve the quality of this work. For instance, the authors could dive deep to compare the precision and recall, the effectiveness of data augmentation, etc.\nB. Is the method robust to the accuracy/performance of the initial pseud-label from an off-the-shell speech segmentation system?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "185_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_185_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7451666666666666,
      "max_similarity": 0.7547,
      "avg_coverage": 0.40420000000000006,
      "max_coverage": 0.4333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 449,
      "avg_human_length": 369.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "TKwt6JAaRz",
        "similarity": 0.751,
        "coverage": 0.4333,
        "human_length": 492,
        "human_text": "paper_topic_and_main_contributions: Knowledge Distillation (KD) compresses large pre-trained language models by transferring their knowledge to smaller models, enabling use in resource-constrained settings, but often sacrifices performance. To address this, the authors propose Co-Training and Co-Distillation (CTCD), a framework that co-trains two models of different sizes, mutually distilling knowledge between them. CTCD improves both model performance and inference speed, based on findings that: 1) Distilling knowledge from the smaller to the larger model boosts the larger model's performance, and 2) The improved larger model further enhances the smaller model's performance. CTCD shows promise to combine with techniques like architecture design and data augmentation, replacing one-way KD, for further gains. Extensive ablation studies demonstrate CTCD's effectiveness, where the small model distilled by CTCD significantly outperforms the original larger model by 1.66 on GLUE benchmark.\n\nreasons_to_accept: 1. This paper addresses a critical limitation of knowledge distillation (KD) methods - the tradeoff between model compression and performance degradation. \n2. This paper proposes a novel co-training and bi-directional distillation approach. Most prior work uses one-way distillation which loses knowledge. In contrast, CTCD allows mutual transfer between teacher and student models. \n3. Experimental results indicate two important empirical findings related to co-distillation: 1) Distilling knowledge from smaller to larger model boosts the larger model's performance. 2) The improved larger model further enhances the smaller model's performance. Additionally, the distilled small model significantly outperforms the original large model by 1.66 on GLUE benchmark.\n\nreasons_to_reject: 1. My major concern is about the source of the performance improvement, especially the finding that distilling knowledge from smaller to larger models boosts the larger model's performance. I am wondering if the improvement could be attributed to the values of the hyperparameters \u03b1_h, \u03b1_s, \u03b2_h and \u03b2_s. Although the training epoch is 10 for all different settings, larger values for these hyperparameters result in longer training. I would be convinced if the authors could provide more empirical analysis, e.g. ablation studies or theoretical analysis, to demonstrate that the performance gains are due to the distillation approach rather than the hyperparameter settings.\n2. Another key concern is the selection of teacher and student models. The authors mentioned using 6-layer BERT as the teacher and 4-layer BERT as the student. This seems like an unusual configuration. A more reasonable setting would be to use 12-layer BERT as the teacher and 6-layer or 4-layer BERT as the student model.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "FUcjWp79RY",
        "similarity": 0.7547,
        "coverage": 0.4,
        "human_length": 226,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a novel framework called Co-Training and Co-Distillation (CTCD) for knowledge distillation. This framework simultaneously pre-trains both the teacher and student models, with bidirectional knowledge distillation between the two. Experimental results on the GLUE benchmark demonstrate the superiority of this approach.\n\nreasons_to_accept: 1. The paper is is well written and easy to understand. \n2. The proposed Co-Training and Co-Distillation method is sensible, given the preliminary verification of reversed knowledge distillation. \n3. The authors perform extensive ablation studies to validate the effectiveness of CTCD.\n\nreasons_to_reject: I don\u2019t find significant flaws in this paper. There are some minor suggestions: 1. Providing implementation details for the fine-tuning phase would be beneficial. \n2. It would be interesting to explore whether this framework remains effective during a fine-tuning only stage. I would expect some experiments and analysis of this scenario.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "RGZ68dRChU",
        "similarity": 0.7298,
        "coverage": 0.3793,
        "human_length": 390,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a novel knowledge distillation framework called Co-Training and Co-Distillation (CTCD) to improve model performance while compressing large pre-trained language models (PLMs). The key idea is to co-train a large teacher model and a small student model together, enabling bidirectional knowledge transfer between them. The results on GLUE are reported. Overall, I think the idea of mutual knowledge transfer is interesting but the actual performance gains seem quite small based on the experiments.\n\nreasons_to_accept: 1. The idea of bidirectional knowledge transfer between teacher and student during co-training is novel. \n2. The experiments show distilling knowledge from the student to teacher improves the teacher's performance, which in turn benefits the student. \n3. The student model compressed by CTCD outperforms the standalone trained original teacher model on GLUE.\n\nreasons_to_reject: 1. The paper studies the case where the student distills knowledge to the teacher, which improves the teacher's performance. However, the improvements could potentially be due to regularization effects rather than distillation as claimed, since all fine-tuning is performed for 10 epochs and without early-stopping. The fine-tuning on GLUE without validation early-stopping usually has very high variances, proper ablation studies are needed to verify.\n2. The performance gains (especially, the Community KD) over standard one-way distillation seem quite marginal based on the experiments when compared to other BERT distillation techniques like BERT-PKD, TinyBERT, MobileBERT, or BERT-of-Theseus. This is not a good signal given the training process is more complex with co-training and co-distillation compared to other distillation techniques.\n3. Evaluations are limited to BERT models only. Testing on other PLMs would be more convincing.\n\nquestions_for_the_authors: 1. what does \"training from scratch\" mean for co-training? Are the student and teacher not initialized from pre-trained BERTs? The wording is misleading and not compatible with the figures.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "TKwt6JAaRz",
        "length": 492,
        "human_text": "paper_topic_and_main_contributions: Knowledge Distillation (KD) compresses large pre-trained language models by transferring their knowledge to smaller models, enabling use in resource-constrained settings, but often sacrifices performance. To address this, the authors propose Co-Training and Co-Distillation (CTCD), a framework that co-trains two models of different sizes, mutually distilling knowledge between them. CTCD improves both model performance and inference speed, based on findings that: 1) Distilling knowledge from the smaller to the larger model boosts the larger model's performance, and 2) The improved larger model further enhances the smaller model's performance. CTCD shows promise to combine with techniques like architecture design and data augmentation, replacing one-way KD, for further gains. Extensive ablation studies demonstrate CTCD's effectiveness, where the small model distilled by CTCD significantly outperforms the original larger model by 1.66 on GLUE benchmark.\n\nreasons_to_accept: 1. This paper addresses a critical limitation of knowledge distillation (KD) methods - the tradeoff between model compression and performance degradation. \n2. This paper proposes a novel co-training and bi-directional distillation approach. Most prior work uses one-way distillation which loses knowledge. In contrast, CTCD allows mutual transfer between teacher and student models. \n3. Experimental results indicate two important empirical findings related to co-distillation: 1) Distilling knowledge from smaller to larger model boosts the larger model's performance. 2) The improved larger model further enhances the smaller model's performance. Additionally, the distilled small model significantly outperforms the original large model by 1.66 on GLUE benchmark.\n\nreasons_to_reject: 1. My major concern is about the source of the performance improvement, especially the finding that distilling knowledge from smaller to larger models boosts the larger model's performance. I am wondering if the improvement could be attributed to the values of the hyperparameters \u03b1_h, \u03b1_s, \u03b2_h and \u03b2_s. Although the training epoch is 10 for all different settings, larger values for these hyperparameters result in longer training. I would be convinced if the authors could provide more empirical analysis, e.g. ablation studies or theoretical analysis, to demonstrate that the performance gains are due to the distillation approach rather than the hyperparameter settings.\n2. Another key concern is the selection of teacher and student models. The authors mentioned using 6-layer BERT as the teacher and 4-layer BERT as the student. This seems like an unusual configuration. A more reasonable setting would be to use 12-layer BERT as the teacher and 6-layer or 4-layer BERT as the student model.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "FUcjWp79RY",
        "length": 226,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a novel framework called Co-Training and Co-Distillation (CTCD) for knowledge distillation. This framework simultaneously pre-trains both the teacher and student models, with bidirectional knowledge distillation between the two. Experimental results on the GLUE benchmark demonstrate the superiority of this approach.\n\nreasons_to_accept: 1. The paper is is well written and easy to understand. \n2. The proposed Co-Training and Co-Distillation method is sensible, given the preliminary verification of reversed knowledge distillation. \n3. The authors perform extensive ablation studies to validate the effectiveness of CTCD.\n\nreasons_to_reject: I don\u2019t find significant flaws in this paper. There are some minor suggestions: 1. Providing implementation details for the fine-tuning phase would be beneficial. \n2. It would be interesting to explore whether this framework remains effective during a fine-tuning only stage. I would expect some experiments and analysis of this scenario.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "RGZ68dRChU",
        "length": 390,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a novel knowledge distillation framework called Co-Training and Co-Distillation (CTCD) to improve model performance while compressing large pre-trained language models (PLMs). The key idea is to co-train a large teacher model and a small student model together, enabling bidirectional knowledge transfer between them. The results on GLUE are reported. Overall, I think the idea of mutual knowledge transfer is interesting but the actual performance gains seem quite small based on the experiments.\n\nreasons_to_accept: 1. The idea of bidirectional knowledge transfer between teacher and student during co-training is novel. \n2. The experiments show distilling knowledge from the student to teacher improves the teacher's performance, which in turn benefits the student. \n3. The student model compressed by CTCD outperforms the standalone trained original teacher model on GLUE.\n\nreasons_to_reject: 1. The paper studies the case where the student distills knowledge to the teacher, which improves the teacher's performance. However, the improvements could potentially be due to regularization effects rather than distillation as claimed, since all fine-tuning is performed for 10 epochs and without early-stopping. The fine-tuning on GLUE without validation early-stopping usually has very high variances, proper ablation studies are needed to verify.\n2. The performance gains (especially, the Community KD) over standard one-way distillation seem quite marginal based on the experiments when compared to other BERT distillation techniques like BERT-PKD, TinyBERT, MobileBERT, or BERT-of-Theseus. This is not a good signal given the training process is more complex with co-training and co-distillation compared to other distillation techniques.\n3. Evaluations are limited to BERT models only. Testing on other PLMs would be more convincing.\n\nquestions_for_the_authors: 1. what does \"training from scratch\" mean for co-training? Are the student and teacher not initialized from pre-trained BERTs? The wording is misleading and not compatible with the figures.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "185_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_185_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7451333333333334,
      "max_similarity": 0.7571,
      "avg_coverage": 0.4605333333333333,
      "max_coverage": 0.5
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 391,
      "avg_human_length": 369.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "TKwt6JAaRz",
        "similarity": 0.748,
        "coverage": 0.4333,
        "human_length": 492,
        "human_text": "paper_topic_and_main_contributions: Knowledge Distillation (KD) compresses large pre-trained language models by transferring their knowledge to smaller models, enabling use in resource-constrained settings, but often sacrifices performance. To address this, the authors propose Co-Training and Co-Distillation (CTCD), a framework that co-trains two models of different sizes, mutually distilling knowledge between them. CTCD improves both model performance and inference speed, based on findings that: 1) Distilling knowledge from the smaller to the larger model boosts the larger model's performance, and 2) The improved larger model further enhances the smaller model's performance. CTCD shows promise to combine with techniques like architecture design and data augmentation, replacing one-way KD, for further gains. Extensive ablation studies demonstrate CTCD's effectiveness, where the small model distilled by CTCD significantly outperforms the original larger model by 1.66 on GLUE benchmark.\n\nreasons_to_accept: 1. This paper addresses a critical limitation of knowledge distillation (KD) methods - the tradeoff between model compression and performance degradation. \n2. This paper proposes a novel co-training and bi-directional distillation approach. Most prior work uses one-way distillation which loses knowledge. In contrast, CTCD allows mutual transfer between teacher and student models. \n3. Experimental results indicate two important empirical findings related to co-distillation: 1) Distilling knowledge from smaller to larger model boosts the larger model's performance. 2) The improved larger model further enhances the smaller model's performance. Additionally, the distilled small model significantly outperforms the original large model by 1.66 on GLUE benchmark.\n\nreasons_to_reject: 1. My major concern is about the source of the performance improvement, especially the finding that distilling knowledge from smaller to larger models boosts the larger model's performance. I am wondering if the improvement could be attributed to the values of the hyperparameters \u03b1_h, \u03b1_s, \u03b2_h and \u03b2_s. Although the training epoch is 10 for all different settings, larger values for these hyperparameters result in longer training. I would be convinced if the authors could provide more empirical analysis, e.g. ablation studies or theoretical analysis, to demonstrate that the performance gains are due to the distillation approach rather than the hyperparameter settings.\n2. Another key concern is the selection of teacher and student models. The authors mentioned using 6-layer BERT as the teacher and 4-layer BERT as the student. This seems like an unusual configuration. A more reasonable setting would be to use 12-layer BERT as the teacher and 6-layer or 4-layer BERT as the student model.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "FUcjWp79RY",
        "similarity": 0.7571,
        "coverage": 0.5,
        "human_length": 226,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a novel framework called Co-Training and Co-Distillation (CTCD) for knowledge distillation. This framework simultaneously pre-trains both the teacher and student models, with bidirectional knowledge distillation between the two. Experimental results on the GLUE benchmark demonstrate the superiority of this approach.\n\nreasons_to_accept: 1. The paper is is well written and easy to understand. \n2. The proposed Co-Training and Co-Distillation method is sensible, given the preliminary verification of reversed knowledge distillation. \n3. The authors perform extensive ablation studies to validate the effectiveness of CTCD.\n\nreasons_to_reject: I don\u2019t find significant flaws in this paper. There are some minor suggestions: 1. Providing implementation details for the fine-tuning phase would be beneficial. \n2. It would be interesting to explore whether this framework remains effective during a fine-tuning only stage. I would expect some experiments and analysis of this scenario.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "RGZ68dRChU",
        "similarity": 0.7303,
        "coverage": 0.4483,
        "human_length": 390,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a novel knowledge distillation framework called Co-Training and Co-Distillation (CTCD) to improve model performance while compressing large pre-trained language models (PLMs). The key idea is to co-train a large teacher model and a small student model together, enabling bidirectional knowledge transfer between them. The results on GLUE are reported. Overall, I think the idea of mutual knowledge transfer is interesting but the actual performance gains seem quite small based on the experiments.\n\nreasons_to_accept: 1. The idea of bidirectional knowledge transfer between teacher and student during co-training is novel. \n2. The experiments show distilling knowledge from the student to teacher improves the teacher's performance, which in turn benefits the student. \n3. The student model compressed by CTCD outperforms the standalone trained original teacher model on GLUE.\n\nreasons_to_reject: 1. The paper studies the case where the student distills knowledge to the teacher, which improves the teacher's performance. However, the improvements could potentially be due to regularization effects rather than distillation as claimed, since all fine-tuning is performed for 10 epochs and without early-stopping. The fine-tuning on GLUE without validation early-stopping usually has very high variances, proper ablation studies are needed to verify.\n2. The performance gains (especially, the Community KD) over standard one-way distillation seem quite marginal based on the experiments when compared to other BERT distillation techniques like BERT-PKD, TinyBERT, MobileBERT, or BERT-of-Theseus. This is not a good signal given the training process is more complex with co-training and co-distillation compared to other distillation techniques.\n3. Evaluations are limited to BERT models only. Testing on other PLMs would be more convincing.\n\nquestions_for_the_authors: 1. what does \"training from scratch\" mean for co-training? Are the student and teacher not initialized from pre-trained BERTs? The wording is misleading and not compatible with the figures.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "TKwt6JAaRz",
        "length": 492,
        "human_text": "paper_topic_and_main_contributions: Knowledge Distillation (KD) compresses large pre-trained language models by transferring their knowledge to smaller models, enabling use in resource-constrained settings, but often sacrifices performance. To address this, the authors propose Co-Training and Co-Distillation (CTCD), a framework that co-trains two models of different sizes, mutually distilling knowledge between them. CTCD improves both model performance and inference speed, based on findings that: 1) Distilling knowledge from the smaller to the larger model boosts the larger model's performance, and 2) The improved larger model further enhances the smaller model's performance. CTCD shows promise to combine with techniques like architecture design and data augmentation, replacing one-way KD, for further gains. Extensive ablation studies demonstrate CTCD's effectiveness, where the small model distilled by CTCD significantly outperforms the original larger model by 1.66 on GLUE benchmark.\n\nreasons_to_accept: 1. This paper addresses a critical limitation of knowledge distillation (KD) methods - the tradeoff between model compression and performance degradation. \n2. This paper proposes a novel co-training and bi-directional distillation approach. Most prior work uses one-way distillation which loses knowledge. In contrast, CTCD allows mutual transfer between teacher and student models. \n3. Experimental results indicate two important empirical findings related to co-distillation: 1) Distilling knowledge from smaller to larger model boosts the larger model's performance. 2) The improved larger model further enhances the smaller model's performance. Additionally, the distilled small model significantly outperforms the original large model by 1.66 on GLUE benchmark.\n\nreasons_to_reject: 1. My major concern is about the source of the performance improvement, especially the finding that distilling knowledge from smaller to larger models boosts the larger model's performance. I am wondering if the improvement could be attributed to the values of the hyperparameters \u03b1_h, \u03b1_s, \u03b2_h and \u03b2_s. Although the training epoch is 10 for all different settings, larger values for these hyperparameters result in longer training. I would be convinced if the authors could provide more empirical analysis, e.g. ablation studies or theoretical analysis, to demonstrate that the performance gains are due to the distillation approach rather than the hyperparameter settings.\n2. Another key concern is the selection of teacher and student models. The authors mentioned using 6-layer BERT as the teacher and 4-layer BERT as the student. This seems like an unusual configuration. A more reasonable setting would be to use 12-layer BERT as the teacher and 6-layer or 4-layer BERT as the student model.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "FUcjWp79RY",
        "length": 226,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a novel framework called Co-Training and Co-Distillation (CTCD) for knowledge distillation. This framework simultaneously pre-trains both the teacher and student models, with bidirectional knowledge distillation between the two. Experimental results on the GLUE benchmark demonstrate the superiority of this approach.\n\nreasons_to_accept: 1. The paper is is well written and easy to understand. \n2. The proposed Co-Training and Co-Distillation method is sensible, given the preliminary verification of reversed knowledge distillation. \n3. The authors perform extensive ablation studies to validate the effectiveness of CTCD.\n\nreasons_to_reject: I don\u2019t find significant flaws in this paper. There are some minor suggestions: 1. Providing implementation details for the fine-tuning phase would be beneficial. \n2. It would be interesting to explore whether this framework remains effective during a fine-tuning only stage. I would expect some experiments and analysis of this scenario.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "RGZ68dRChU",
        "length": 390,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a novel knowledge distillation framework called Co-Training and Co-Distillation (CTCD) to improve model performance while compressing large pre-trained language models (PLMs). The key idea is to co-train a large teacher model and a small student model together, enabling bidirectional knowledge transfer between them. The results on GLUE are reported. Overall, I think the idea of mutual knowledge transfer is interesting but the actual performance gains seem quite small based on the experiments.\n\nreasons_to_accept: 1. The idea of bidirectional knowledge transfer between teacher and student during co-training is novel. \n2. The experiments show distilling knowledge from the student to teacher improves the teacher's performance, which in turn benefits the student. \n3. The student model compressed by CTCD outperforms the standalone trained original teacher model on GLUE.\n\nreasons_to_reject: 1. The paper studies the case where the student distills knowledge to the teacher, which improves the teacher's performance. However, the improvements could potentially be due to regularization effects rather than distillation as claimed, since all fine-tuning is performed for 10 epochs and without early-stopping. The fine-tuning on GLUE without validation early-stopping usually has very high variances, proper ablation studies are needed to verify.\n2. The performance gains (especially, the Community KD) over standard one-way distillation seem quite marginal based on the experiments when compared to other BERT distillation techniques like BERT-PKD, TinyBERT, MobileBERT, or BERT-of-Theseus. This is not a good signal given the training process is more complex with co-training and co-distillation compared to other distillation techniques.\n3. Evaluations are limited to BERT models only. Testing on other PLMs would be more convincing.\n\nquestions_for_the_authors: 1. what does \"training from scratch\" mean for co-training? Are the student and teacher not initialized from pre-trained BERTs? The wording is misleading and not compatible with the figures.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "24_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_24_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6964,
      "max_similarity": 0.7123,
      "avg_coverage": 0.4848333333333333,
      "max_coverage": 0.5
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 510,
      "avg_human_length": 318.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "ufHVWDcPw4",
        "similarity": 0.7092,
        "coverage": 0.5,
        "human_length": 412,
        "human_text": "paper_topic_and_main_contributions: This paper explores outputting multiple SRL label sequences with compatible labels, more specifically, jointly outputting PropBank (PB) and VerbNet (VN) labels. The authors propose a joint CRF model that outputs joint PB-VN labels, which is shown to be generally better than a multi-task baseline, especially on the out-of-domain Brown test set. Furthermore, semi-supervised learning with PB-only data and inferencing with SemLink constraints are explored, which could effectively bring benefits.\n\nreasons_to_accept: - I think the direction explored in this work is interesting and important for the future development of unifying semantic resources.\n- The paper is well-written and easy to follow, and the experiments are well-conducted.\n\nreasons_to_reject: - The experiments only involve joint prediction of PB and VN, where there are abundant joint data and high-coverage mapping resources. The paper could be much stronger and more interesting if it could be extended to scenarios where there are less such joint resources (like PropBank/VerbNet & FrameNet).\n- The performance gaps between different methods seem small. Though this is not quite surprising giving strong neural models, it would be more interesting to explore scenarios where the proposed methods could be more helpful (such as low-resource cases and less-frequent predicates).\n\nquestions_for_the_authors: - A: Although the joint label space is pruned, it seems still pretty large, how would this affect training and testing efficiency? How slower would it be compared to the methods that predict separately?\n- B: How effective is CRF in these settings? With strong neural models, maybe simple token-wise labelers would obtain good results (maybe adding BIO constraint enforcing at testing time)?\n- C: The main experimenting dataset only covers 56% of the CoNLL05 predicates, are there any biases for these convertible predicates? For example, are they more frequent ones? Also, it would be better if more analysis can be provided based on predicate frequencies (my intuition is that probably low-frequent predicates would be the ones that benefit more from joint learning).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "s1sxbnlOPT",
        "similarity": 0.6677,
        "coverage": 0.5,
        "human_length": 155,
        "human_text": "paper_topic_and_main_contributions: This paper explores various joint traiing decoding strategies for Propbank and VerbNet based semantic role labeling.\n\nreasons_to_accept: The results are state of the art, the methods sensible and the discussion useful.\n\nreasons_to_reject: The existence of compatible verbnet and PropBank labels is an accident of the history of the field There are not many situations parallel to this, so the result is probably of interest only to a small subset of ACL researchers. The authors make little attempt to draw general lessons from their work.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "gpvduTkDqT",
        "similarity": 0.7123,
        "coverage": 0.4545,
        "human_length": 388,
        "human_text": "paper_topic_and_main_contributions: This paper presents a joint model that simultaneously addresses two distinct SRL tasks with different formats, namely VerbNet SRL and PropBank SRL. \n  By incorporating SEMLINK (Stowe et al. , 2021) during decoding, the authors successfully prevent the generation of conflicting argument labels.  The experimental results indicate that the method proposed by the authors outperforms previous multitasking learning approaches. \nAnd when provided with the gold standard PropBank SRL labels, this method achieves an F1 score of 99 for predicting VerbNet SRL labels.\n\nreasons_to_accept: Experiments conducted within this study are comprehensive, accompanied by a thorough analysis.\nThe proposed approach can be employed to facilitate mutual transformation between SRL data annotated under different standards, which holds significance for the automated construction of annotated datasets.\n\nreasons_to_reject: The writing of this paper requires a little improvement, as certain sections are difficult to comprehend. The foundational aspect of SEMLINK is inadequately introduced. For instance, in section 5, readers less familiar with SEMLINK might struggle to grasp concepts like \"SEML(u)\" and the associated inferences. I believe the authors could enhance understanding by incorporating more illustrative examples.\nAdditionally, as mentioned by the authors in the Limitation section, the model in this study relies on gold predicate positions and predicate attributes. This limitation diminishes the practicality of the model in real-world scenarios.\n\nquestions_for_the_authors: Question A\uff1aWhen constructing \"x_wp,\"  did you predict the predicates and senses first or did you directly use the correct answers? If the predicted answers were used, does that imply that both \"x\" and \"x_wp\" underwent separate encoding, with the encoding result of \"x\" being utilized for predicting predicates?\nQuestion B: In line 406, you mentioned that correct VerbNet classes and PropBank senses were used during constrained decoding. Have you attempted using predicted answers?\n\ntypos_grammar_style_and_presentation_improvements: There is a missing period at line 383.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "ufHVWDcPw4",
        "length": 412,
        "human_text": "paper_topic_and_main_contributions: This paper explores outputting multiple SRL label sequences with compatible labels, more specifically, jointly outputting PropBank (PB) and VerbNet (VN) labels. The authors propose a joint CRF model that outputs joint PB-VN labels, which is shown to be generally better than a multi-task baseline, especially on the out-of-domain Brown test set. Furthermore, semi-supervised learning with PB-only data and inferencing with SemLink constraints are explored, which could effectively bring benefits.\n\nreasons_to_accept: - I think the direction explored in this work is interesting and important for the future development of unifying semantic resources.\n- The paper is well-written and easy to follow, and the experiments are well-conducted.\n\nreasons_to_reject: - The experiments only involve joint prediction of PB and VN, where there are abundant joint data and high-coverage mapping resources. The paper could be much stronger and more interesting if it could be extended to scenarios where there are less such joint resources (like PropBank/VerbNet & FrameNet).\n- The performance gaps between different methods seem small. Though this is not quite surprising giving strong neural models, it would be more interesting to explore scenarios where the proposed methods could be more helpful (such as low-resource cases and less-frequent predicates).\n\nquestions_for_the_authors: - A: Although the joint label space is pruned, it seems still pretty large, how would this affect training and testing efficiency? How slower would it be compared to the methods that predict separately?\n- B: How effective is CRF in these settings? With strong neural models, maybe simple token-wise labelers would obtain good results (maybe adding BIO constraint enforcing at testing time)?\n- C: The main experimenting dataset only covers 56% of the CoNLL05 predicates, are there any biases for these convertible predicates? For example, are they more frequent ones? Also, it would be better if more analysis can be provided based on predicate frequencies (my intuition is that probably low-frequent predicates would be the ones that benefit more from joint learning).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "s1sxbnlOPT",
        "length": 155,
        "human_text": "paper_topic_and_main_contributions: This paper explores various joint traiing decoding strategies for Propbank and VerbNet based semantic role labeling.\n\nreasons_to_accept: The results are state of the art, the methods sensible and the discussion useful.\n\nreasons_to_reject: The existence of compatible verbnet and PropBank labels is an accident of the history of the field There are not many situations parallel to this, so the result is probably of interest only to a small subset of ACL researchers. The authors make little attempt to draw general lessons from their work.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "gpvduTkDqT",
        "length": 388,
        "human_text": "paper_topic_and_main_contributions: This paper presents a joint model that simultaneously addresses two distinct SRL tasks with different formats, namely VerbNet SRL and PropBank SRL. \n  By incorporating SEMLINK (Stowe et al. , 2021) during decoding, the authors successfully prevent the generation of conflicting argument labels.  The experimental results indicate that the method proposed by the authors outperforms previous multitasking learning approaches. \nAnd when provided with the gold standard PropBank SRL labels, this method achieves an F1 score of 99 for predicting VerbNet SRL labels.\n\nreasons_to_accept: Experiments conducted within this study are comprehensive, accompanied by a thorough analysis.\nThe proposed approach can be employed to facilitate mutual transformation between SRL data annotated under different standards, which holds significance for the automated construction of annotated datasets.\n\nreasons_to_reject: The writing of this paper requires a little improvement, as certain sections are difficult to comprehend. The foundational aspect of SEMLINK is inadequately introduced. For instance, in section 5, readers less familiar with SEMLINK might struggle to grasp concepts like \"SEML(u)\" and the associated inferences. I believe the authors could enhance understanding by incorporating more illustrative examples.\nAdditionally, as mentioned by the authors in the Limitation section, the model in this study relies on gold predicate positions and predicate attributes. This limitation diminishes the practicality of the model in real-world scenarios.\n\nquestions_for_the_authors: Question A\uff1aWhen constructing \"x_wp,\"  did you predict the predicates and senses first or did you directly use the correct answers? If the predicted answers were used, does that imply that both \"x\" and \"x_wp\" underwent separate encoding, with the encoding result of \"x\" being utilized for predicting predicates?\nQuestion B: In line 406, you mentioned that correct VerbNet classes and PropBank senses were used during constrained decoding. Have you attempted using predicted answers?\n\ntypos_grammar_style_and_presentation_improvements: There is a missing period at line 383.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "24_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_24_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6941333333333333,
      "max_similarity": 0.7097,
      "avg_coverage": 0.5666666666666667,
      "max_coverage": 0.75
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 554,
      "avg_human_length": 318.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "ufHVWDcPw4",
        "similarity": 0.7044,
        "coverage": 0.45,
        "human_length": 412,
        "human_text": "paper_topic_and_main_contributions: This paper explores outputting multiple SRL label sequences with compatible labels, more specifically, jointly outputting PropBank (PB) and VerbNet (VN) labels. The authors propose a joint CRF model that outputs joint PB-VN labels, which is shown to be generally better than a multi-task baseline, especially on the out-of-domain Brown test set. Furthermore, semi-supervised learning with PB-only data and inferencing with SemLink constraints are explored, which could effectively bring benefits.\n\nreasons_to_accept: - I think the direction explored in this work is interesting and important for the future development of unifying semantic resources.\n- The paper is well-written and easy to follow, and the experiments are well-conducted.\n\nreasons_to_reject: - The experiments only involve joint prediction of PB and VN, where there are abundant joint data and high-coverage mapping resources. The paper could be much stronger and more interesting if it could be extended to scenarios where there are less such joint resources (like PropBank/VerbNet & FrameNet).\n- The performance gaps between different methods seem small. Though this is not quite surprising giving strong neural models, it would be more interesting to explore scenarios where the proposed methods could be more helpful (such as low-resource cases and less-frequent predicates).\n\nquestions_for_the_authors: - A: Although the joint label space is pruned, it seems still pretty large, how would this affect training and testing efficiency? How slower would it be compared to the methods that predict separately?\n- B: How effective is CRF in these settings? With strong neural models, maybe simple token-wise labelers would obtain good results (maybe adding BIO constraint enforcing at testing time)?\n- C: The main experimenting dataset only covers 56% of the CoNLL05 predicates, are there any biases for these convertible predicates? For example, are they more frequent ones? Also, it would be better if more analysis can be provided based on predicate frequencies (my intuition is that probably low-frequent predicates would be the ones that benefit more from joint learning).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "s1sxbnlOPT",
        "similarity": 0.6683,
        "coverage": 0.75,
        "human_length": 155,
        "human_text": "paper_topic_and_main_contributions: This paper explores various joint traiing decoding strategies for Propbank and VerbNet based semantic role labeling.\n\nreasons_to_accept: The results are state of the art, the methods sensible and the discussion useful.\n\nreasons_to_reject: The existence of compatible verbnet and PropBank labels is an accident of the history of the field There are not many situations parallel to this, so the result is probably of interest only to a small subset of ACL researchers. The authors make little attempt to draw general lessons from their work.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "gpvduTkDqT",
        "similarity": 0.7097,
        "coverage": 0.5,
        "human_length": 388,
        "human_text": "paper_topic_and_main_contributions: This paper presents a joint model that simultaneously addresses two distinct SRL tasks with different formats, namely VerbNet SRL and PropBank SRL. \n  By incorporating SEMLINK (Stowe et al. , 2021) during decoding, the authors successfully prevent the generation of conflicting argument labels.  The experimental results indicate that the method proposed by the authors outperforms previous multitasking learning approaches. \nAnd when provided with the gold standard PropBank SRL labels, this method achieves an F1 score of 99 for predicting VerbNet SRL labels.\n\nreasons_to_accept: Experiments conducted within this study are comprehensive, accompanied by a thorough analysis.\nThe proposed approach can be employed to facilitate mutual transformation between SRL data annotated under different standards, which holds significance for the automated construction of annotated datasets.\n\nreasons_to_reject: The writing of this paper requires a little improvement, as certain sections are difficult to comprehend. The foundational aspect of SEMLINK is inadequately introduced. For instance, in section 5, readers less familiar with SEMLINK might struggle to grasp concepts like \"SEML(u)\" and the associated inferences. I believe the authors could enhance understanding by incorporating more illustrative examples.\nAdditionally, as mentioned by the authors in the Limitation section, the model in this study relies on gold predicate positions and predicate attributes. This limitation diminishes the practicality of the model in real-world scenarios.\n\nquestions_for_the_authors: Question A\uff1aWhen constructing \"x_wp,\"  did you predict the predicates and senses first or did you directly use the correct answers? If the predicted answers were used, does that imply that both \"x\" and \"x_wp\" underwent separate encoding, with the encoding result of \"x\" being utilized for predicting predicates?\nQuestion B: In line 406, you mentioned that correct VerbNet classes and PropBank senses were used during constrained decoding. Have you attempted using predicted answers?\n\ntypos_grammar_style_and_presentation_improvements: There is a missing period at line 383.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "ufHVWDcPw4",
        "length": 412,
        "human_text": "paper_topic_and_main_contributions: This paper explores outputting multiple SRL label sequences with compatible labels, more specifically, jointly outputting PropBank (PB) and VerbNet (VN) labels. The authors propose a joint CRF model that outputs joint PB-VN labels, which is shown to be generally better than a multi-task baseline, especially on the out-of-domain Brown test set. Furthermore, semi-supervised learning with PB-only data and inferencing with SemLink constraints are explored, which could effectively bring benefits.\n\nreasons_to_accept: - I think the direction explored in this work is interesting and important for the future development of unifying semantic resources.\n- The paper is well-written and easy to follow, and the experiments are well-conducted.\n\nreasons_to_reject: - The experiments only involve joint prediction of PB and VN, where there are abundant joint data and high-coverage mapping resources. The paper could be much stronger and more interesting if it could be extended to scenarios where there are less such joint resources (like PropBank/VerbNet & FrameNet).\n- The performance gaps between different methods seem small. Though this is not quite surprising giving strong neural models, it would be more interesting to explore scenarios where the proposed methods could be more helpful (such as low-resource cases and less-frequent predicates).\n\nquestions_for_the_authors: - A: Although the joint label space is pruned, it seems still pretty large, how would this affect training and testing efficiency? How slower would it be compared to the methods that predict separately?\n- B: How effective is CRF in these settings? With strong neural models, maybe simple token-wise labelers would obtain good results (maybe adding BIO constraint enforcing at testing time)?\n- C: The main experimenting dataset only covers 56% of the CoNLL05 predicates, are there any biases for these convertible predicates? For example, are they more frequent ones? Also, it would be better if more analysis can be provided based on predicate frequencies (my intuition is that probably low-frequent predicates would be the ones that benefit more from joint learning).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "s1sxbnlOPT",
        "length": 155,
        "human_text": "paper_topic_and_main_contributions: This paper explores various joint traiing decoding strategies for Propbank and VerbNet based semantic role labeling.\n\nreasons_to_accept: The results are state of the art, the methods sensible and the discussion useful.\n\nreasons_to_reject: The existence of compatible verbnet and PropBank labels is an accident of the history of the field There are not many situations parallel to this, so the result is probably of interest only to a small subset of ACL researchers. The authors make little attempt to draw general lessons from their work.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "gpvduTkDqT",
        "length": 388,
        "human_text": "paper_topic_and_main_contributions: This paper presents a joint model that simultaneously addresses two distinct SRL tasks with different formats, namely VerbNet SRL and PropBank SRL. \n  By incorporating SEMLINK (Stowe et al. , 2021) during decoding, the authors successfully prevent the generation of conflicting argument labels.  The experimental results indicate that the method proposed by the authors outperforms previous multitasking learning approaches. \nAnd when provided with the gold standard PropBank SRL labels, this method achieves an F1 score of 99 for predicting VerbNet SRL labels.\n\nreasons_to_accept: Experiments conducted within this study are comprehensive, accompanied by a thorough analysis.\nThe proposed approach can be employed to facilitate mutual transformation between SRL data annotated under different standards, which holds significance for the automated construction of annotated datasets.\n\nreasons_to_reject: The writing of this paper requires a little improvement, as certain sections are difficult to comprehend. The foundational aspect of SEMLINK is inadequately introduced. For instance, in section 5, readers less familiar with SEMLINK might struggle to grasp concepts like \"SEML(u)\" and the associated inferences. I believe the authors could enhance understanding by incorporating more illustrative examples.\nAdditionally, as mentioned by the authors in the Limitation section, the model in this study relies on gold predicate positions and predicate attributes. This limitation diminishes the practicality of the model in real-world scenarios.\n\nquestions_for_the_authors: Question A\uff1aWhen constructing \"x_wp,\"  did you predict the predicates and senses first or did you directly use the correct answers? If the predicted answers were used, does that imply that both \"x\" and \"x_wp\" underwent separate encoding, with the encoding result of \"x\" being utilized for predicting predicates?\nQuestion B: In line 406, you mentioned that correct VerbNet classes and PropBank senses were used during constrained decoding. Have you attempted using predicted answers?\n\ntypos_grammar_style_and_presentation_improvements: There is a missing period at line 383.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "61_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_61_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6865666666666668,
      "max_similarity": 0.7084,
      "avg_coverage": 0.05256666666666667,
      "max_coverage": 0.0625
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 30,
      "avg_human_length": 540.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": false,
      "has_weaknesses": false,
      "has_suggestions": false
    },
    "content_count_metrics": {
      "strengths_count": 0,
      "weaknesses_count": 0,
      "suggestions_count": 0
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "g8sL9rSiAK",
        "similarity": 0.7084,
        "coverage": 0.0526,
        "human_length": 423,
        "human_text": "paper_topic_and_main_contributions: This paper surveys recent developments on out-of-domain detection. They focus specifically on semantic shift, where samples come from unknown categories. The paper organizes based on the availability of in-domain and out-of-domain data. The approaches surveyed are comprehensive and the paper would provide useful pointers to researchers new to the task.\n\nreasons_to_accept: 1. The paper is clearly-written and organized using a meaningful taxonomy to navigate the space 2. The survey covers a large body of recent work using different approaches. \n3. Provides insightful discussions and future work\n\nreasons_to_reject: 1. The survey didn't present (or reference) any empirical result. For someone new to the problem, it's unclear where to start 2. It would be helpful to discuss the pros and cons of the datasets and metrics and propose a standardized benchmark for the task 3. The survey only focuses on semantic shift, but didn't cover non-semantic shift where samples come from different domains and styles. In the real world, semantic- and non-semantic shift often happen together.\n\nquestions_for_the_authors: 1. Section 3.1.2: Why do we need *labeled* OOD data? Since you only classify ID/OOD and discard the OOD ones? Isn't it sufficient to use the unlabeled OOD examples? \n2. How do different approaches for OOD scoring compare? I suppose output-based detecting is more prone to over-confidence of the classifiers than feature-based detecting. Does the latter usually perform better? \n3. Section 3.2.2: The generated pseudo OOD approach appears to me more like data augmentation that makes the model more robust. They will unavoidably differ much from the real-world OOD data. Besides, the generated data also will contain noise. I wonder whether are the generated data used differently during training? \n4. Section 4: the (2) and (3) approach to get datasets are much more trivial than annotating OOD samples (1). I wonder if there's any drawback.\n\nmissing_references: - Li, Chenliang, et al. \"Seed-guided topic model for document filtering and classification.\" ACM Transactions on Information Systems (TOIS) 37.1 (2018): 1-37.\n- Jin, Yiping, Dittaya Wanvarie, and Phu TV Le. \" Learning from noisy out-of-domain corpus using dataless classification.\" Natural Language Engineering 28.1 (2022): 39-69.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "giSojV8d8s",
        "similarity": 0.682,
        "coverage": 0.0426,
        "human_length": 620,
        "human_text": "paper_topic_and_main_contributions: The paper introduces the survey on Out-of-Distribution Detection (OOD) in the NLP field. The authors present the taxonomy of OOD detection methods, based on OOD data availability. A detailed description of the existing approaches is done for each method is provided. Also, there are chapters devoted to datasets, commonly used metrics, and applications. The paper compares the differences between CV and NLP OOD methods and discusses future research directions.   This work is well-written and effectively conveys the main idea.  From my point of view, it covers most of the recent works concerned with OOD in the NLP field. The authors raise the discussion for which setup labeled/unlabeled methods could be applied, so the overview of existing approaches looks comprehensive and could be beneficial to make a clear picture of existing approaches, metrics, and datasets.  On the other hand, I would like to see a more experimental setup. More precisely,  the paper will benefit if it would present which setup and what method achieve better results. I would like to see more discussions about the possible limitations of existing approaches and their pros/cons for different setups (section 6.1 is too short and obvious). The chapter devoted to the comparison between CV and NLP methods doesn`t draw any comparison between methods but just outlines them. Also, it is better to be aligned with previous surveys [1], [2] Strengths: 1) The comprehensive survey of existing OOD methods in NLP may be interesting for researchers in the OOD area. \n2) The suggested taxonomy looks logical for OOD in NLP.\nWeaknesses: 1) There are no experimental results of the observed methods. From the survey, it is not obvious when one should select which type of method.   2) The suggested taxonomy is not well-aligned with the previous surveys [1],[2] [1] Yang, J., Zhou, K., Li, Y., & Liu, Z. (2021). Generalized out-of-distribution detection: A survey.\n[2] Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges.\n\nreasons_to_accept: 1) The comprehensive survey of existing OOD methods in NLP may be interesting for researchers in the OOD area. \n2) The suggested taxonomy looks logical for OOD in NLP.\n\nreasons_to_reject: 1) There are no experimental results of the observed methods. From the survey, it is not obvious when one should select which type of method.   2) The suggested taxonomy is not well-aligned with the previous surveys [1],[2]\n\nquestions_for_the_authors: A. Why pre-training and finetuning are organized as separate classes in Figure 1? All of the further described approaches rely on pre-trained/finetune models. \nB. In [1],[2] is used common-knowledge taxonomy with 1) classification-based methods; 2) density-based methods; 3) distance-based methods; 4) reconstruction-based methods. Why do you prefer to select the other granularity?  [1] Yang, J., Zhou, K., Li, Y., & Liu, Z. (2021). Generalized out-of-distribution detection: A survey.\n[2] Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges.\n\nmissing_references: Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "c9QZIl1KwG",
        "similarity": 0.6693,
        "coverage": 0.0625,
        "human_length": 578,
        "human_text": "paper_topic_and_main_contributions: This survey paper is concerned with the task of Out-of-Distribution Detection in NLP, specifically on the case where the test data contains instances whose label is not in the training data (semantic shift). Methods are classified based on whether OOD labelled data is available. Those methods with access to only labelled ID data are subdivided into those that learn representations that can distinguish between ID and OOD, and methods that generate synthetic OOD data. Completely unsupervised methods (no access to labelled ID data) are also discussed. Datasets, applications and metrics are briefly described.\n\nreasons_to_accept: - Potentially useful survey for someone beginning a project on this task or for someone looking for references to recent work in the area - Generally well written\n\nreasons_to_reject: - The survey is relatively narrow in focus.\n- The structure of the paper could be improved, e.g.  it could benefit from a table which clearly presents the studies included in Section 3 in a structured and easy to access format. This table could include columns referring to availability of ID and OOD labelled data, whether pseudo-OOD data is generated, whether a detection score is used, and what the task is.\n- Section 3.2.3 (Other Methods) is not very satisfactory. I think more effort could have been made to either discuss these methods under existing categories or to create new categories.\n- Some of the information in the Metrics section is widely known and doesn\u2019t need to be included, e.g. the formulae for true positive and false positive rate.\n- Section 6.1 is repetitive. I\u2019m also not sure how necessary the section containing the comparison with Computer Vision (6.2) is. The discussion is somewhat superficial.\n\nquestions_for_the_authors: - 745-747: what does it mean to combine OOD detection with a life-long learning process? More detail should be presented\t\t \t\t - In Table 1, what does \u201cWhether to consider ID performance\u201d mean?\n- Does OOD Detection (with semantic shift) have a role to play in structured prediction tasks such as parsing?\n\nmissing_references: - The Related Research Areas section is a good idea but the references in each section are limited. Consider the Domain Adaptation section which contains only two references! A search for \"Domain Adaptation\" in the ACL Anthology reveals 10 pages of results, with dedicated workshops on the topic, e.g. https://aclanthology.org/W10-26.pdf.  - Lines 729:733 This reminds me of this work which follows outbound links to obtain data for use in domain generalisation in part-of-speech tagging and NER:  https://aclanthology.org/C14-1168.pdf\n\ntypos_grammar_style_and_presentation_improvements: **Presentation** - The focus on OOD Detection with semantic shift should be highlighted in the paper title.\n- Section 3 should not be given the title \u201cMethodology\u201d but rather something like \u201cTaxonomy of Methods\u201d - Applications could benefit from a section or subsection of its own.\n- 59-60 \u201curgently needed\u201d This is an over-statement!\n**Typos** - 33: existing flourishes? Maybe replace with \u201cexisting versions\u201d?\n- 364: Ensembles-based -> Ensemble-based - 630: this stage benefit -> this stage benefits - 782-783: several alias -> several aliases\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "g8sL9rSiAK",
        "length": 423,
        "human_text": "paper_topic_and_main_contributions: This paper surveys recent developments on out-of-domain detection. They focus specifically on semantic shift, where samples come from unknown categories. The paper organizes based on the availability of in-domain and out-of-domain data. The approaches surveyed are comprehensive and the paper would provide useful pointers to researchers new to the task.\n\nreasons_to_accept: 1. The paper is clearly-written and organized using a meaningful taxonomy to navigate the space 2. The survey covers a large body of recent work using different approaches. \n3. Provides insightful discussions and future work\n\nreasons_to_reject: 1. The survey didn't present (or reference) any empirical result. For someone new to the problem, it's unclear where to start 2. It would be helpful to discuss the pros and cons of the datasets and metrics and propose a standardized benchmark for the task 3. The survey only focuses on semantic shift, but didn't cover non-semantic shift where samples come from different domains and styles. In the real world, semantic- and non-semantic shift often happen together.\n\nquestions_for_the_authors: 1. Section 3.1.2: Why do we need *labeled* OOD data? Since you only classify ID/OOD and discard the OOD ones? Isn't it sufficient to use the unlabeled OOD examples? \n2. How do different approaches for OOD scoring compare? I suppose output-based detecting is more prone to over-confidence of the classifiers than feature-based detecting. Does the latter usually perform better? \n3. Section 3.2.2: The generated pseudo OOD approach appears to me more like data augmentation that makes the model more robust. They will unavoidably differ much from the real-world OOD data. Besides, the generated data also will contain noise. I wonder whether are the generated data used differently during training? \n4. Section 4: the (2) and (3) approach to get datasets are much more trivial than annotating OOD samples (1). I wonder if there's any drawback.\n\nmissing_references: - Li, Chenliang, et al. \"Seed-guided topic model for document filtering and classification.\" ACM Transactions on Information Systems (TOIS) 37.1 (2018): 1-37.\n- Jin, Yiping, Dittaya Wanvarie, and Phu TV Le. \" Learning from noisy out-of-domain corpus using dataless classification.\" Natural Language Engineering 28.1 (2022): 39-69.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      },
      {
        "reviewer_id": "giSojV8d8s",
        "length": 620,
        "human_text": "paper_topic_and_main_contributions: The paper introduces the survey on Out-of-Distribution Detection (OOD) in the NLP field. The authors present the taxonomy of OOD detection methods, based on OOD data availability. A detailed description of the existing approaches is done for each method is provided. Also, there are chapters devoted to datasets, commonly used metrics, and applications. The paper compares the differences between CV and NLP OOD methods and discusses future research directions.   This work is well-written and effectively conveys the main idea.  From my point of view, it covers most of the recent works concerned with OOD in the NLP field. The authors raise the discussion for which setup labeled/unlabeled methods could be applied, so the overview of existing approaches looks comprehensive and could be beneficial to make a clear picture of existing approaches, metrics, and datasets.  On the other hand, I would like to see a more experimental setup. More precisely,  the paper will benefit if it would present which setup and what method achieve better results. I would like to see more discussions about the possible limitations of existing approaches and their pros/cons for different setups (section 6.1 is too short and obvious). The chapter devoted to the comparison between CV and NLP methods doesn`t draw any comparison between methods but just outlines them. Also, it is better to be aligned with previous surveys [1], [2] Strengths: 1) The comprehensive survey of existing OOD methods in NLP may be interesting for researchers in the OOD area. \n2) The suggested taxonomy looks logical for OOD in NLP.\nWeaknesses: 1) There are no experimental results of the observed methods. From the survey, it is not obvious when one should select which type of method.   2) The suggested taxonomy is not well-aligned with the previous surveys [1],[2] [1] Yang, J., Zhou, K., Li, Y., & Liu, Z. (2021). Generalized out-of-distribution detection: A survey.\n[2] Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges.\n\nreasons_to_accept: 1) The comprehensive survey of existing OOD methods in NLP may be interesting for researchers in the OOD area. \n2) The suggested taxonomy looks logical for OOD in NLP.\n\nreasons_to_reject: 1) There are no experimental results of the observed methods. From the survey, it is not obvious when one should select which type of method.   2) The suggested taxonomy is not well-aligned with the previous surveys [1],[2]\n\nquestions_for_the_authors: A. Why pre-training and finetuning are organized as separate classes in Figure 1? All of the further described approaches rely on pre-trained/finetune models. \nB. In [1],[2] is used common-knowledge taxonomy with 1) classification-based methods; 2) density-based methods; 3) distance-based methods; 4) reconstruction-based methods. Why do you prefer to select the other granularity?  [1] Yang, J., Zhou, K., Li, Y., & Liu, Z. (2021). Generalized out-of-distribution detection: A survey.\n[2] Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges.\n\nmissing_references: Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      },
      {
        "reviewer_id": "c9QZIl1KwG",
        "length": 578,
        "human_text": "paper_topic_and_main_contributions: This survey paper is concerned with the task of Out-of-Distribution Detection in NLP, specifically on the case where the test data contains instances whose label is not in the training data (semantic shift). Methods are classified based on whether OOD labelled data is available. Those methods with access to only labelled ID data are subdivided into those that learn representations that can distinguish between ID and OOD, and methods that generate synthetic OOD data. Completely unsupervised methods (no access to labelled ID data) are also discussed. Datasets, applications and metrics are briefly described.\n\nreasons_to_accept: - Potentially useful survey for someone beginning a project on this task or for someone looking for references to recent work in the area - Generally well written\n\nreasons_to_reject: - The survey is relatively narrow in focus.\n- The structure of the paper could be improved, e.g.  it could benefit from a table which clearly presents the studies included in Section 3 in a structured and easy to access format. This table could include columns referring to availability of ID and OOD labelled data, whether pseudo-OOD data is generated, whether a detection score is used, and what the task is.\n- Section 3.2.3 (Other Methods) is not very satisfactory. I think more effort could have been made to either discuss these methods under existing categories or to create new categories.\n- Some of the information in the Metrics section is widely known and doesn\u2019t need to be included, e.g. the formulae for true positive and false positive rate.\n- Section 6.1 is repetitive. I\u2019m also not sure how necessary the section containing the comparison with Computer Vision (6.2) is. The discussion is somewhat superficial.\n\nquestions_for_the_authors: - 745-747: what does it mean to combine OOD detection with a life-long learning process? More detail should be presented\t\t \t\t - In Table 1, what does \u201cWhether to consider ID performance\u201d mean?\n- Does OOD Detection (with semantic shift) have a role to play in structured prediction tasks such as parsing?\n\nmissing_references: - The Related Research Areas section is a good idea but the references in each section are limited. Consider the Domain Adaptation section which contains only two references! A search for \"Domain Adaptation\" in the ACL Anthology reveals 10 pages of results, with dedicated workshops on the topic, e.g. https://aclanthology.org/W10-26.pdf.  - Lines 729:733 This reminds me of this work which follows outbound links to obtain data for use in domain generalisation in part-of-speech tagging and NER:  https://aclanthology.org/C14-1168.pdf\n\ntypos_grammar_style_and_presentation_improvements: **Presentation** - The focus on OOD Detection with semantic shift should be highlighted in the paper title.\n- Section 3 should not be given the title \u201cMethodology\u201d but rather something like \u201cTaxonomy of Methods\u201d - Applications could benefit from a section or subsection of its own.\n- 59-60 \u201curgently needed\u201d This is an over-statement!\n**Typos** - 33: existing flourishes? Maybe replace with \u201cexisting versions\u201d?\n- 364: Ensembles-based -> Ensemble-based - 630: this stage benefit -> this stage benefits - 782-783: several alias -> several aliases\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "61_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_61_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7404666666666667,
      "max_similarity": 0.7528,
      "avg_coverage": 0.05256666666666667,
      "max_coverage": 0.0625
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 30,
      "avg_human_length": 540.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": false,
      "has_weaknesses": false,
      "has_suggestions": false
    },
    "content_count_metrics": {
      "strengths_count": 0,
      "weaknesses_count": 0,
      "suggestions_count": 0
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "g8sL9rSiAK",
        "similarity": 0.7258,
        "coverage": 0.0526,
        "human_length": 423,
        "human_text": "paper_topic_and_main_contributions: This paper surveys recent developments on out-of-domain detection. They focus specifically on semantic shift, where samples come from unknown categories. The paper organizes based on the availability of in-domain and out-of-domain data. The approaches surveyed are comprehensive and the paper would provide useful pointers to researchers new to the task.\n\nreasons_to_accept: 1. The paper is clearly-written and organized using a meaningful taxonomy to navigate the space 2. The survey covers a large body of recent work using different approaches. \n3. Provides insightful discussions and future work\n\nreasons_to_reject: 1. The survey didn't present (or reference) any empirical result. For someone new to the problem, it's unclear where to start 2. It would be helpful to discuss the pros and cons of the datasets and metrics and propose a standardized benchmark for the task 3. The survey only focuses on semantic shift, but didn't cover non-semantic shift where samples come from different domains and styles. In the real world, semantic- and non-semantic shift often happen together.\n\nquestions_for_the_authors: 1. Section 3.1.2: Why do we need *labeled* OOD data? Since you only classify ID/OOD and discard the OOD ones? Isn't it sufficient to use the unlabeled OOD examples? \n2. How do different approaches for OOD scoring compare? I suppose output-based detecting is more prone to over-confidence of the classifiers than feature-based detecting. Does the latter usually perform better? \n3. Section 3.2.2: The generated pseudo OOD approach appears to me more like data augmentation that makes the model more robust. They will unavoidably differ much from the real-world OOD data. Besides, the generated data also will contain noise. I wonder whether are the generated data used differently during training? \n4. Section 4: the (2) and (3) approach to get datasets are much more trivial than annotating OOD samples (1). I wonder if there's any drawback.\n\nmissing_references: - Li, Chenliang, et al. \"Seed-guided topic model for document filtering and classification.\" ACM Transactions on Information Systems (TOIS) 37.1 (2018): 1-37.\n- Jin, Yiping, Dittaya Wanvarie, and Phu TV Le. \" Learning from noisy out-of-domain corpus using dataless classification.\" Natural Language Engineering 28.1 (2022): 39-69.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "giSojV8d8s",
        "similarity": 0.7528,
        "coverage": 0.0426,
        "human_length": 620,
        "human_text": "paper_topic_and_main_contributions: The paper introduces the survey on Out-of-Distribution Detection (OOD) in the NLP field. The authors present the taxonomy of OOD detection methods, based on OOD data availability. A detailed description of the existing approaches is done for each method is provided. Also, there are chapters devoted to datasets, commonly used metrics, and applications. The paper compares the differences between CV and NLP OOD methods and discusses future research directions.   This work is well-written and effectively conveys the main idea.  From my point of view, it covers most of the recent works concerned with OOD in the NLP field. The authors raise the discussion for which setup labeled/unlabeled methods could be applied, so the overview of existing approaches looks comprehensive and could be beneficial to make a clear picture of existing approaches, metrics, and datasets.  On the other hand, I would like to see a more experimental setup. More precisely,  the paper will benefit if it would present which setup and what method achieve better results. I would like to see more discussions about the possible limitations of existing approaches and their pros/cons for different setups (section 6.1 is too short and obvious). The chapter devoted to the comparison between CV and NLP methods doesn`t draw any comparison between methods but just outlines them. Also, it is better to be aligned with previous surveys [1], [2] Strengths: 1) The comprehensive survey of existing OOD methods in NLP may be interesting for researchers in the OOD area. \n2) The suggested taxonomy looks logical for OOD in NLP.\nWeaknesses: 1) There are no experimental results of the observed methods. From the survey, it is not obvious when one should select which type of method.   2) The suggested taxonomy is not well-aligned with the previous surveys [1],[2] [1] Yang, J., Zhou, K., Li, Y., & Liu, Z. (2021). Generalized out-of-distribution detection: A survey.\n[2] Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges.\n\nreasons_to_accept: 1) The comprehensive survey of existing OOD methods in NLP may be interesting for researchers in the OOD area. \n2) The suggested taxonomy looks logical for OOD in NLP.\n\nreasons_to_reject: 1) There are no experimental results of the observed methods. From the survey, it is not obvious when one should select which type of method.   2) The suggested taxonomy is not well-aligned with the previous surveys [1],[2]\n\nquestions_for_the_authors: A. Why pre-training and finetuning are organized as separate classes in Figure 1? All of the further described approaches rely on pre-trained/finetune models. \nB. In [1],[2] is used common-knowledge taxonomy with 1) classification-based methods; 2) density-based methods; 3) distance-based methods; 4) reconstruction-based methods. Why do you prefer to select the other granularity?  [1] Yang, J., Zhou, K., Li, Y., & Liu, Z. (2021). Generalized out-of-distribution detection: A survey.\n[2] Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges.\n\nmissing_references: Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "c9QZIl1KwG",
        "similarity": 0.7428,
        "coverage": 0.0625,
        "human_length": 578,
        "human_text": "paper_topic_and_main_contributions: This survey paper is concerned with the task of Out-of-Distribution Detection in NLP, specifically on the case where the test data contains instances whose label is not in the training data (semantic shift). Methods are classified based on whether OOD labelled data is available. Those methods with access to only labelled ID data are subdivided into those that learn representations that can distinguish between ID and OOD, and methods that generate synthetic OOD data. Completely unsupervised methods (no access to labelled ID data) are also discussed. Datasets, applications and metrics are briefly described.\n\nreasons_to_accept: - Potentially useful survey for someone beginning a project on this task or for someone looking for references to recent work in the area - Generally well written\n\nreasons_to_reject: - The survey is relatively narrow in focus.\n- The structure of the paper could be improved, e.g.  it could benefit from a table which clearly presents the studies included in Section 3 in a structured and easy to access format. This table could include columns referring to availability of ID and OOD labelled data, whether pseudo-OOD data is generated, whether a detection score is used, and what the task is.\n- Section 3.2.3 (Other Methods) is not very satisfactory. I think more effort could have been made to either discuss these methods under existing categories or to create new categories.\n- Some of the information in the Metrics section is widely known and doesn\u2019t need to be included, e.g. the formulae for true positive and false positive rate.\n- Section 6.1 is repetitive. I\u2019m also not sure how necessary the section containing the comparison with Computer Vision (6.2) is. The discussion is somewhat superficial.\n\nquestions_for_the_authors: - 745-747: what does it mean to combine OOD detection with a life-long learning process? More detail should be presented\t\t \t\t - In Table 1, what does \u201cWhether to consider ID performance\u201d mean?\n- Does OOD Detection (with semantic shift) have a role to play in structured prediction tasks such as parsing?\n\nmissing_references: - The Related Research Areas section is a good idea but the references in each section are limited. Consider the Domain Adaptation section which contains only two references! A search for \"Domain Adaptation\" in the ACL Anthology reveals 10 pages of results, with dedicated workshops on the topic, e.g. https://aclanthology.org/W10-26.pdf.  - Lines 729:733 This reminds me of this work which follows outbound links to obtain data for use in domain generalisation in part-of-speech tagging and NER:  https://aclanthology.org/C14-1168.pdf\n\ntypos_grammar_style_and_presentation_improvements: **Presentation** - The focus on OOD Detection with semantic shift should be highlighted in the paper title.\n- Section 3 should not be given the title \u201cMethodology\u201d but rather something like \u201cTaxonomy of Methods\u201d - Applications could benefit from a section or subsection of its own.\n- 59-60 \u201curgently needed\u201d This is an over-statement!\n**Typos** - 33: existing flourishes? Maybe replace with \u201cexisting versions\u201d?\n- 364: Ensembles-based -> Ensemble-based - 630: this stage benefit -> this stage benefits - 782-783: several alias -> several aliases\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "g8sL9rSiAK",
        "length": 423,
        "human_text": "paper_topic_and_main_contributions: This paper surveys recent developments on out-of-domain detection. They focus specifically on semantic shift, where samples come from unknown categories. The paper organizes based on the availability of in-domain and out-of-domain data. The approaches surveyed are comprehensive and the paper would provide useful pointers to researchers new to the task.\n\nreasons_to_accept: 1. The paper is clearly-written and organized using a meaningful taxonomy to navigate the space 2. The survey covers a large body of recent work using different approaches. \n3. Provides insightful discussions and future work\n\nreasons_to_reject: 1. The survey didn't present (or reference) any empirical result. For someone new to the problem, it's unclear where to start 2. It would be helpful to discuss the pros and cons of the datasets and metrics and propose a standardized benchmark for the task 3. The survey only focuses on semantic shift, but didn't cover non-semantic shift where samples come from different domains and styles. In the real world, semantic- and non-semantic shift often happen together.\n\nquestions_for_the_authors: 1. Section 3.1.2: Why do we need *labeled* OOD data? Since you only classify ID/OOD and discard the OOD ones? Isn't it sufficient to use the unlabeled OOD examples? \n2. How do different approaches for OOD scoring compare? I suppose output-based detecting is more prone to over-confidence of the classifiers than feature-based detecting. Does the latter usually perform better? \n3. Section 3.2.2: The generated pseudo OOD approach appears to me more like data augmentation that makes the model more robust. They will unavoidably differ much from the real-world OOD data. Besides, the generated data also will contain noise. I wonder whether are the generated data used differently during training? \n4. Section 4: the (2) and (3) approach to get datasets are much more trivial than annotating OOD samples (1). I wonder if there's any drawback.\n\nmissing_references: - Li, Chenliang, et al. \"Seed-guided topic model for document filtering and classification.\" ACM Transactions on Information Systems (TOIS) 37.1 (2018): 1-37.\n- Jin, Yiping, Dittaya Wanvarie, and Phu TV Le. \" Learning from noisy out-of-domain corpus using dataless classification.\" Natural Language Engineering 28.1 (2022): 39-69.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      },
      {
        "reviewer_id": "giSojV8d8s",
        "length": 620,
        "human_text": "paper_topic_and_main_contributions: The paper introduces the survey on Out-of-Distribution Detection (OOD) in the NLP field. The authors present the taxonomy of OOD detection methods, based on OOD data availability. A detailed description of the existing approaches is done for each method is provided. Also, there are chapters devoted to datasets, commonly used metrics, and applications. The paper compares the differences between CV and NLP OOD methods and discusses future research directions.   This work is well-written and effectively conveys the main idea.  From my point of view, it covers most of the recent works concerned with OOD in the NLP field. The authors raise the discussion for which setup labeled/unlabeled methods could be applied, so the overview of existing approaches looks comprehensive and could be beneficial to make a clear picture of existing approaches, metrics, and datasets.  On the other hand, I would like to see a more experimental setup. More precisely,  the paper will benefit if it would present which setup and what method achieve better results. I would like to see more discussions about the possible limitations of existing approaches and their pros/cons for different setups (section 6.1 is too short and obvious). The chapter devoted to the comparison between CV and NLP methods doesn`t draw any comparison between methods but just outlines them. Also, it is better to be aligned with previous surveys [1], [2] Strengths: 1) The comprehensive survey of existing OOD methods in NLP may be interesting for researchers in the OOD area. \n2) The suggested taxonomy looks logical for OOD in NLP.\nWeaknesses: 1) There are no experimental results of the observed methods. From the survey, it is not obvious when one should select which type of method.   2) The suggested taxonomy is not well-aligned with the previous surveys [1],[2] [1] Yang, J., Zhou, K., Li, Y., & Liu, Z. (2021). Generalized out-of-distribution detection: A survey.\n[2] Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges.\n\nreasons_to_accept: 1) The comprehensive survey of existing OOD methods in NLP may be interesting for researchers in the OOD area. \n2) The suggested taxonomy looks logical for OOD in NLP.\n\nreasons_to_reject: 1) There are no experimental results of the observed methods. From the survey, it is not obvious when one should select which type of method.   2) The suggested taxonomy is not well-aligned with the previous surveys [1],[2]\n\nquestions_for_the_authors: A. Why pre-training and finetuning are organized as separate classes in Figure 1? All of the further described approaches rely on pre-trained/finetune models. \nB. In [1],[2] is used common-knowledge taxonomy with 1) classification-based methods; 2) density-based methods; 3) distance-based methods; 4) reconstruction-based methods. Why do you prefer to select the other granularity?  [1] Yang, J., Zhou, K., Li, Y., & Liu, Z. (2021). Generalized out-of-distribution detection: A survey.\n[2] Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges.\n\nmissing_references: Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., & Sabokrou, M. (2021). A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      },
      {
        "reviewer_id": "c9QZIl1KwG",
        "length": 578,
        "human_text": "paper_topic_and_main_contributions: This survey paper is concerned with the task of Out-of-Distribution Detection in NLP, specifically on the case where the test data contains instances whose label is not in the training data (semantic shift). Methods are classified based on whether OOD labelled data is available. Those methods with access to only labelled ID data are subdivided into those that learn representations that can distinguish between ID and OOD, and methods that generate synthetic OOD data. Completely unsupervised methods (no access to labelled ID data) are also discussed. Datasets, applications and metrics are briefly described.\n\nreasons_to_accept: - Potentially useful survey for someone beginning a project on this task or for someone looking for references to recent work in the area - Generally well written\n\nreasons_to_reject: - The survey is relatively narrow in focus.\n- The structure of the paper could be improved, e.g.  it could benefit from a table which clearly presents the studies included in Section 3 in a structured and easy to access format. This table could include columns referring to availability of ID and OOD labelled data, whether pseudo-OOD data is generated, whether a detection score is used, and what the task is.\n- Section 3.2.3 (Other Methods) is not very satisfactory. I think more effort could have been made to either discuss these methods under existing categories or to create new categories.\n- Some of the information in the Metrics section is widely known and doesn\u2019t need to be included, e.g. the formulae for true positive and false positive rate.\n- Section 6.1 is repetitive. I\u2019m also not sure how necessary the section containing the comparison with Computer Vision (6.2) is. The discussion is somewhat superficial.\n\nquestions_for_the_authors: - 745-747: what does it mean to combine OOD detection with a life-long learning process? More detail should be presented\t\t \t\t - In Table 1, what does \u201cWhether to consider ID performance\u201d mean?\n- Does OOD Detection (with semantic shift) have a role to play in structured prediction tasks such as parsing?\n\nmissing_references: - The Related Research Areas section is a good idea but the references in each section are limited. Consider the Domain Adaptation section which contains only two references! A search for \"Domain Adaptation\" in the ACL Anthology reveals 10 pages of results, with dedicated workshops on the topic, e.g. https://aclanthology.org/W10-26.pdf.  - Lines 729:733 This reminds me of this work which follows outbound links to obtain data for use in domain generalisation in part-of-speech tagging and NER:  https://aclanthology.org/C14-1168.pdf\n\ntypos_grammar_style_and_presentation_improvements: **Presentation** - The focus on OOD Detection with semantic shift should be highlighted in the paper title.\n- Section 3 should not be given the title \u201cMethodology\u201d but rather something like \u201cTaxonomy of Methods\u201d - Applications could benefit from a section or subsection of its own.\n- 59-60 \u201curgently needed\u201d This is an over-statement!\n**Typos** - 33: existing flourishes? Maybe replace with \u201cexisting versions\u201d?\n- 364: Ensembles-based -> Ensemble-based - 630: this stage benefit -> this stage benefits - 782-783: several alias -> several aliases\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "55_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_55_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7008333333333333,
      "max_similarity": 0.719,
      "avg_coverage": 0.5583333333333333,
      "max_coverage": 0.8
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 534,
      "avg_human_length": 473.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "rt1cx0wQA6",
        "similarity": 0.719,
        "coverage": 0.4583,
        "human_length": 410,
        "human_text": "paper_topic_and_main_contributions: The paper delves into the incorporation of syntactic knowledge into a language model through additional training. It introduces four pretraining tasks that focus on learning diverse syntactic perspectives. Additionally, the paper explores the utilization of gradient surgery and elastic weight consolidation methods as preventive measures against catastrophic forgetting. The main contributions of this research encompass: (1) the proposition of additional syntactic training incorporating four distinct syntactic tasks for language models, (2) the exploration of gradient surgery and elastic weight consolidation methods to mitigate catastrophic forgetting, and (3) the demonstration of the improved performance of the enhanced model across tasks such as CoLA, RTE, MRPC, and key phrase extraction.\n\nreasons_to_accept: 1. The paper extensively investigates the integration of syntactic knowledge into a language model through additional training. It introduces four pretraining tasks that effectively contribute to this goal. The clarity of the ideas presented and the overall ease of following the paper are commendable. \n2. The ablation study conducted on four optimizers and four syntactic tasks provides a comprehensive analysis. The inclusion of these experiments sufficiently covers and evaluates the impact of these components.\n\nreasons_to_reject: 1. The paper's novelty appears to be somewhat limited, as syntactic training has been widely used in previous methods, and the GS and EWC optimization functions are not entirely new. \n2. The study only explores encoder-only language models, and it may have been beneficial to expand the analysis to also include more commonly used decoder-only language models. For instance, it would have been useful to evaluate the proposed approach on models such as GPT. \n3. Despite leveraging external syntactic training, the study only demonstrates improvement on four of the tasks in GLUE, with no clear advancements observed in the remaining five tasks.\n\nquestions_for_the_authors: Can you please provide information regarding the average performance in GLUE and whether the new method demonstrates any improvement in this score?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "QDZx6l6qD9",
        "similarity": 0.6791,
        "coverage": 0.8,
        "human_length": 275,
        "human_text": "paper_topic_and_main_contributions: To improve the peformance of pretrained transformer (BERT) models on tasks that use syntactic knowledge, the authors examine methods to train the model on a syntax-correlated task (dependency relations, phrases, clause status, coordination) together with a specialized optimizer that is intended to mitigate the catastrophic forgetting problem, namely Gradient Surgery or Elastic Weight Consolidation. \nThe authors find that the syntactic knowledge incorporated in this way improves performance on a subset of the GLUE tasks that they examine as well as for key phrase extraction\n\nreasons_to_accept: The method chosen by the authors seems to be fairly general and the results look good\n\nreasons_to_reject: Given that the main contribution consists in reinforcing the syntactic knowledge in a transformer model, the authors could discuss a bit more of the substantial body of literature that tries to do exactly that - on different tasks, or using techniques that are much more cumbersome, etc. - as a reader it isn't clear to me whether relation extraction or some other syntax-heavy task would profit even more from this.\n\nquestions_for_the_authors: (A) What about the other GLUE tasks? Did you just do experiments on the subset presented in the paper?\n\nmissing_references: Bai et al. (ACL 2021) Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "XBdEmOzeLq",
        "similarity": 0.7044,
        "coverage": 0.4167,
        "human_length": 734,
        "human_text": "paper_topic_and_main_contributions: The authors argue that PLMs lack \"sufficient\" syntactic knowledge, and seek to add this knowledge via fine-tuning. They propose four different syntactic tasks to use during fine-tuning, however, if done naively, fine-tuning on a task which differs from the original pre-training brings the risk of catastrophic forgetting.  Four different methods for optimization (including two which are designed to prevent catastrophic forgetting) are compared on a number of benchmarks from GLUE, and a separate key phrase identification task.  At least one of the catastrophic forgetting-based optimizations typically offer the highest performance on each of the presented GLUE task, and better retain PPL performance.\n\nreasons_to_accept: - Both methods for catastrophic forgetting achieve respectable improvements in performance on GLUE in tasks requiring syntactic knowledge.\n- Good experimental rigor -- Many comparisons -- Many tasks -- Each comparison computed across multiple runs/hyperparameter settings\n\nreasons_to_reject: - The PLMs used (BERT) are by current standards, quite old, and quite small.  As work in scaling PLMs up to sizes orders of magnitude greater, performance on syntactic tasks has shown to improve naturally (along with many other useful emergent forms of knowledge).  Some comparison to larger models / application of this method to such models, is necessary to ensure that the method has any practical purpose.\n- There are also no baselines from existing work.  There are other forms of fine-tuning, such as adapters, which seek to add additional knowledge to PLMs with less chance of catastrophic forgetting.  The authors even cite one of these papers.  This is also a confusing oversight, because the many appropriate inline citations which contrast various decisions in this work to decisions in existing work demonstrate a great familiarity with the literation, so lacking any comparison to any existing methods is an odd oversight.\n- Some questionable design choices.  Perplexity is used as a measure of the model retaining semantic information after fine-tuning, and while that does relate to the original task, there are also aspects of domain drift which are possible and separate from catastrophic forgetting.  How are such factors controlled?\n- Related, there is questionable motivation.  Often when talking about catastrophic forgetting, both the original training and the new task are both relevant.  This is clear in the context of robotics, where learning a new behavior should not result in hindering the robot from performing existing behaviors.  Is this true in this case?  PPL is almost always not a valuable end goal, and the entire PLM/LLM paradigm is built around this notion of pre-training in whatever way leads to learning useful linguistic representations, before fine-tuning, aligning, or few-shotting the model towards the task the user actually cares about.  If users never care about both tasks in approximately equal measure, than what good is retaining the original model weights which were not pertinent to the target task?\n- There's arguably too much going on here.  The focus of the paper aims to be about catastrophic forgetting, but secondary to that, is also the problem of matching the right syntactic fine-tuning task to the right problem.  This is not entirely known a priori, so all possible pairings are explored, but realistically a good guess can be made (as it would likely be if pursued in a more practical setting).  For instance, it is no surprise that the phrase syntax task helps with key phrase identification.  The disadvantage of the exhaustive approach is that it has both distracted from the main takeaway points while cutting into the space available for supporting the main hypothesis.\n- No inclusion of baselines from existing work / SOTA on performance tables - Key extraction F1 results are better than standard optimizers, but negligibly so.\n- No discussion of GC vs EWC.  When a priori would you choose which method?  If the paper included only one such method, traditional optimizers would be the best choice in most situations.\n\nquestions_for_the_authors: a)  Is the EWC procedure something first designed in this work?\n\ntypos_grammar_style_and_presentation_improvements: - Cite GS when discussing it.\n- L345: Missing appendix reference\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "rt1cx0wQA6",
        "length": 410,
        "human_text": "paper_topic_and_main_contributions: The paper delves into the incorporation of syntactic knowledge into a language model through additional training. It introduces four pretraining tasks that focus on learning diverse syntactic perspectives. Additionally, the paper explores the utilization of gradient surgery and elastic weight consolidation methods as preventive measures against catastrophic forgetting. The main contributions of this research encompass: (1) the proposition of additional syntactic training incorporating four distinct syntactic tasks for language models, (2) the exploration of gradient surgery and elastic weight consolidation methods to mitigate catastrophic forgetting, and (3) the demonstration of the improved performance of the enhanced model across tasks such as CoLA, RTE, MRPC, and key phrase extraction.\n\nreasons_to_accept: 1. The paper extensively investigates the integration of syntactic knowledge into a language model through additional training. It introduces four pretraining tasks that effectively contribute to this goal. The clarity of the ideas presented and the overall ease of following the paper are commendable. \n2. The ablation study conducted on four optimizers and four syntactic tasks provides a comprehensive analysis. The inclusion of these experiments sufficiently covers and evaluates the impact of these components.\n\nreasons_to_reject: 1. The paper's novelty appears to be somewhat limited, as syntactic training has been widely used in previous methods, and the GS and EWC optimization functions are not entirely new. \n2. The study only explores encoder-only language models, and it may have been beneficial to expand the analysis to also include more commonly used decoder-only language models. For instance, it would have been useful to evaluate the proposed approach on models such as GPT. \n3. Despite leveraging external syntactic training, the study only demonstrates improvement on four of the tasks in GLUE, with no clear advancements observed in the remaining five tasks.\n\nquestions_for_the_authors: Can you please provide information regarding the average performance in GLUE and whether the new method demonstrates any improvement in this score?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "QDZx6l6qD9",
        "length": 275,
        "human_text": "paper_topic_and_main_contributions: To improve the peformance of pretrained transformer (BERT) models on tasks that use syntactic knowledge, the authors examine methods to train the model on a syntax-correlated task (dependency relations, phrases, clause status, coordination) together with a specialized optimizer that is intended to mitigate the catastrophic forgetting problem, namely Gradient Surgery or Elastic Weight Consolidation. \nThe authors find that the syntactic knowledge incorporated in this way improves performance on a subset of the GLUE tasks that they examine as well as for key phrase extraction\n\nreasons_to_accept: The method chosen by the authors seems to be fairly general and the results look good\n\nreasons_to_reject: Given that the main contribution consists in reinforcing the syntactic knowledge in a transformer model, the authors could discuss a bit more of the substantial body of literature that tries to do exactly that - on different tasks, or using techniques that are much more cumbersome, etc. - as a reader it isn't clear to me whether relation extraction or some other syntax-heavy task would profit even more from this.\n\nquestions_for_the_authors: (A) What about the other GLUE tasks? Did you just do experiments on the subset presented in the paper?\n\nmissing_references: Bai et al. (ACL 2021) Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "XBdEmOzeLq",
        "length": 734,
        "human_text": "paper_topic_and_main_contributions: The authors argue that PLMs lack \"sufficient\" syntactic knowledge, and seek to add this knowledge via fine-tuning. They propose four different syntactic tasks to use during fine-tuning, however, if done naively, fine-tuning on a task which differs from the original pre-training brings the risk of catastrophic forgetting.  Four different methods for optimization (including two which are designed to prevent catastrophic forgetting) are compared on a number of benchmarks from GLUE, and a separate key phrase identification task.  At least one of the catastrophic forgetting-based optimizations typically offer the highest performance on each of the presented GLUE task, and better retain PPL performance.\n\nreasons_to_accept: - Both methods for catastrophic forgetting achieve respectable improvements in performance on GLUE in tasks requiring syntactic knowledge.\n- Good experimental rigor -- Many comparisons -- Many tasks -- Each comparison computed across multiple runs/hyperparameter settings\n\nreasons_to_reject: - The PLMs used (BERT) are by current standards, quite old, and quite small.  As work in scaling PLMs up to sizes orders of magnitude greater, performance on syntactic tasks has shown to improve naturally (along with many other useful emergent forms of knowledge).  Some comparison to larger models / application of this method to such models, is necessary to ensure that the method has any practical purpose.\n- There are also no baselines from existing work.  There are other forms of fine-tuning, such as adapters, which seek to add additional knowledge to PLMs with less chance of catastrophic forgetting.  The authors even cite one of these papers.  This is also a confusing oversight, because the many appropriate inline citations which contrast various decisions in this work to decisions in existing work demonstrate a great familiarity with the literation, so lacking any comparison to any existing methods is an odd oversight.\n- Some questionable design choices.  Perplexity is used as a measure of the model retaining semantic information after fine-tuning, and while that does relate to the original task, there are also aspects of domain drift which are possible and separate from catastrophic forgetting.  How are such factors controlled?\n- Related, there is questionable motivation.  Often when talking about catastrophic forgetting, both the original training and the new task are both relevant.  This is clear in the context of robotics, where learning a new behavior should not result in hindering the robot from performing existing behaviors.  Is this true in this case?  PPL is almost always not a valuable end goal, and the entire PLM/LLM paradigm is built around this notion of pre-training in whatever way leads to learning useful linguistic representations, before fine-tuning, aligning, or few-shotting the model towards the task the user actually cares about.  If users never care about both tasks in approximately equal measure, than what good is retaining the original model weights which were not pertinent to the target task?\n- There's arguably too much going on here.  The focus of the paper aims to be about catastrophic forgetting, but secondary to that, is also the problem of matching the right syntactic fine-tuning task to the right problem.  This is not entirely known a priori, so all possible pairings are explored, but realistically a good guess can be made (as it would likely be if pursued in a more practical setting).  For instance, it is no surprise that the phrase syntax task helps with key phrase identification.  The disadvantage of the exhaustive approach is that it has both distracted from the main takeaway points while cutting into the space available for supporting the main hypothesis.\n- No inclusion of baselines from existing work / SOTA on performance tables - Key extraction F1 results are better than standard optimizers, but negligibly so.\n- No discussion of GC vs EWC.  When a priori would you choose which method?  If the paper included only one such method, traditional optimizers would be the best choice in most situations.\n\nquestions_for_the_authors: a)  Is the EWC procedure something first designed in this work?\n\ntypos_grammar_style_and_presentation_improvements: - Cite GS when discussing it.\n- L345: Missing appendix reference\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "55_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_55_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7028666666666666,
      "max_similarity": 0.7214,
      "avg_coverage": 0.5583333333333333,
      "max_coverage": 0.8
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 569,
      "avg_human_length": 473.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "rt1cx0wQA6",
        "similarity": 0.7214,
        "coverage": 0.4583,
        "human_length": 410,
        "human_text": "paper_topic_and_main_contributions: The paper delves into the incorporation of syntactic knowledge into a language model through additional training. It introduces four pretraining tasks that focus on learning diverse syntactic perspectives. Additionally, the paper explores the utilization of gradient surgery and elastic weight consolidation methods as preventive measures against catastrophic forgetting. The main contributions of this research encompass: (1) the proposition of additional syntactic training incorporating four distinct syntactic tasks for language models, (2) the exploration of gradient surgery and elastic weight consolidation methods to mitigate catastrophic forgetting, and (3) the demonstration of the improved performance of the enhanced model across tasks such as CoLA, RTE, MRPC, and key phrase extraction.\n\nreasons_to_accept: 1. The paper extensively investigates the integration of syntactic knowledge into a language model through additional training. It introduces four pretraining tasks that effectively contribute to this goal. The clarity of the ideas presented and the overall ease of following the paper are commendable. \n2. The ablation study conducted on four optimizers and four syntactic tasks provides a comprehensive analysis. The inclusion of these experiments sufficiently covers and evaluates the impact of these components.\n\nreasons_to_reject: 1. The paper's novelty appears to be somewhat limited, as syntactic training has been widely used in previous methods, and the GS and EWC optimization functions are not entirely new. \n2. The study only explores encoder-only language models, and it may have been beneficial to expand the analysis to also include more commonly used decoder-only language models. For instance, it would have been useful to evaluate the proposed approach on models such as GPT. \n3. Despite leveraging external syntactic training, the study only demonstrates improvement on four of the tasks in GLUE, with no clear advancements observed in the remaining five tasks.\n\nquestions_for_the_authors: Can you please provide information regarding the average performance in GLUE and whether the new method demonstrates any improvement in this score?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "QDZx6l6qD9",
        "similarity": 0.6806,
        "coverage": 0.8,
        "human_length": 275,
        "human_text": "paper_topic_and_main_contributions: To improve the peformance of pretrained transformer (BERT) models on tasks that use syntactic knowledge, the authors examine methods to train the model on a syntax-correlated task (dependency relations, phrases, clause status, coordination) together with a specialized optimizer that is intended to mitigate the catastrophic forgetting problem, namely Gradient Surgery or Elastic Weight Consolidation. \nThe authors find that the syntactic knowledge incorporated in this way improves performance on a subset of the GLUE tasks that they examine as well as for key phrase extraction\n\nreasons_to_accept: The method chosen by the authors seems to be fairly general and the results look good\n\nreasons_to_reject: Given that the main contribution consists in reinforcing the syntactic knowledge in a transformer model, the authors could discuss a bit more of the substantial body of literature that tries to do exactly that - on different tasks, or using techniques that are much more cumbersome, etc. - as a reader it isn't clear to me whether relation extraction or some other syntax-heavy task would profit even more from this.\n\nquestions_for_the_authors: (A) What about the other GLUE tasks? Did you just do experiments on the subset presented in the paper?\n\nmissing_references: Bai et al. (ACL 2021) Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "XBdEmOzeLq",
        "similarity": 0.7066,
        "coverage": 0.4167,
        "human_length": 734,
        "human_text": "paper_topic_and_main_contributions: The authors argue that PLMs lack \"sufficient\" syntactic knowledge, and seek to add this knowledge via fine-tuning. They propose four different syntactic tasks to use during fine-tuning, however, if done naively, fine-tuning on a task which differs from the original pre-training brings the risk of catastrophic forgetting.  Four different methods for optimization (including two which are designed to prevent catastrophic forgetting) are compared on a number of benchmarks from GLUE, and a separate key phrase identification task.  At least one of the catastrophic forgetting-based optimizations typically offer the highest performance on each of the presented GLUE task, and better retain PPL performance.\n\nreasons_to_accept: - Both methods for catastrophic forgetting achieve respectable improvements in performance on GLUE in tasks requiring syntactic knowledge.\n- Good experimental rigor -- Many comparisons -- Many tasks -- Each comparison computed across multiple runs/hyperparameter settings\n\nreasons_to_reject: - The PLMs used (BERT) are by current standards, quite old, and quite small.  As work in scaling PLMs up to sizes orders of magnitude greater, performance on syntactic tasks has shown to improve naturally (along with many other useful emergent forms of knowledge).  Some comparison to larger models / application of this method to such models, is necessary to ensure that the method has any practical purpose.\n- There are also no baselines from existing work.  There are other forms of fine-tuning, such as adapters, which seek to add additional knowledge to PLMs with less chance of catastrophic forgetting.  The authors even cite one of these papers.  This is also a confusing oversight, because the many appropriate inline citations which contrast various decisions in this work to decisions in existing work demonstrate a great familiarity with the literation, so lacking any comparison to any existing methods is an odd oversight.\n- Some questionable design choices.  Perplexity is used as a measure of the model retaining semantic information after fine-tuning, and while that does relate to the original task, there are also aspects of domain drift which are possible and separate from catastrophic forgetting.  How are such factors controlled?\n- Related, there is questionable motivation.  Often when talking about catastrophic forgetting, both the original training and the new task are both relevant.  This is clear in the context of robotics, where learning a new behavior should not result in hindering the robot from performing existing behaviors.  Is this true in this case?  PPL is almost always not a valuable end goal, and the entire PLM/LLM paradigm is built around this notion of pre-training in whatever way leads to learning useful linguistic representations, before fine-tuning, aligning, or few-shotting the model towards the task the user actually cares about.  If users never care about both tasks in approximately equal measure, than what good is retaining the original model weights which were not pertinent to the target task?\n- There's arguably too much going on here.  The focus of the paper aims to be about catastrophic forgetting, but secondary to that, is also the problem of matching the right syntactic fine-tuning task to the right problem.  This is not entirely known a priori, so all possible pairings are explored, but realistically a good guess can be made (as it would likely be if pursued in a more practical setting).  For instance, it is no surprise that the phrase syntax task helps with key phrase identification.  The disadvantage of the exhaustive approach is that it has both distracted from the main takeaway points while cutting into the space available for supporting the main hypothesis.\n- No inclusion of baselines from existing work / SOTA on performance tables - Key extraction F1 results are better than standard optimizers, but negligibly so.\n- No discussion of GC vs EWC.  When a priori would you choose which method?  If the paper included only one such method, traditional optimizers would be the best choice in most situations.\n\nquestions_for_the_authors: a)  Is the EWC procedure something first designed in this work?\n\ntypos_grammar_style_and_presentation_improvements: - Cite GS when discussing it.\n- L345: Missing appendix reference\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "rt1cx0wQA6",
        "length": 410,
        "human_text": "paper_topic_and_main_contributions: The paper delves into the incorporation of syntactic knowledge into a language model through additional training. It introduces four pretraining tasks that focus on learning diverse syntactic perspectives. Additionally, the paper explores the utilization of gradient surgery and elastic weight consolidation methods as preventive measures against catastrophic forgetting. The main contributions of this research encompass: (1) the proposition of additional syntactic training incorporating four distinct syntactic tasks for language models, (2) the exploration of gradient surgery and elastic weight consolidation methods to mitigate catastrophic forgetting, and (3) the demonstration of the improved performance of the enhanced model across tasks such as CoLA, RTE, MRPC, and key phrase extraction.\n\nreasons_to_accept: 1. The paper extensively investigates the integration of syntactic knowledge into a language model through additional training. It introduces four pretraining tasks that effectively contribute to this goal. The clarity of the ideas presented and the overall ease of following the paper are commendable. \n2. The ablation study conducted on four optimizers and four syntactic tasks provides a comprehensive analysis. The inclusion of these experiments sufficiently covers and evaluates the impact of these components.\n\nreasons_to_reject: 1. The paper's novelty appears to be somewhat limited, as syntactic training has been widely used in previous methods, and the GS and EWC optimization functions are not entirely new. \n2. The study only explores encoder-only language models, and it may have been beneficial to expand the analysis to also include more commonly used decoder-only language models. For instance, it would have been useful to evaluate the proposed approach on models such as GPT. \n3. Despite leveraging external syntactic training, the study only demonstrates improvement on four of the tasks in GLUE, with no clear advancements observed in the remaining five tasks.\n\nquestions_for_the_authors: Can you please provide information regarding the average performance in GLUE and whether the new method demonstrates any improvement in this score?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: N/A\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "QDZx6l6qD9",
        "length": 275,
        "human_text": "paper_topic_and_main_contributions: To improve the peformance of pretrained transformer (BERT) models on tasks that use syntactic knowledge, the authors examine methods to train the model on a syntax-correlated task (dependency relations, phrases, clause status, coordination) together with a specialized optimizer that is intended to mitigate the catastrophic forgetting problem, namely Gradient Surgery or Elastic Weight Consolidation. \nThe authors find that the syntactic knowledge incorporated in this way improves performance on a subset of the GLUE tasks that they examine as well as for key phrase extraction\n\nreasons_to_accept: The method chosen by the authors seems to be fairly general and the results look good\n\nreasons_to_reject: Given that the main contribution consists in reinforcing the syntactic knowledge in a transformer model, the authors could discuss a bit more of the substantial body of literature that tries to do exactly that - on different tasks, or using techniques that are much more cumbersome, etc. - as a reader it isn't clear to me whether relation extraction or some other syntax-heavy task would profit even more from this.\n\nquestions_for_the_authors: (A) What about the other GLUE tasks? Did you just do experiments on the subset presented in the paper?\n\nmissing_references: Bai et al. (ACL 2021) Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "XBdEmOzeLq",
        "length": 734,
        "human_text": "paper_topic_and_main_contributions: The authors argue that PLMs lack \"sufficient\" syntactic knowledge, and seek to add this knowledge via fine-tuning. They propose four different syntactic tasks to use during fine-tuning, however, if done naively, fine-tuning on a task which differs from the original pre-training brings the risk of catastrophic forgetting.  Four different methods for optimization (including two which are designed to prevent catastrophic forgetting) are compared on a number of benchmarks from GLUE, and a separate key phrase identification task.  At least one of the catastrophic forgetting-based optimizations typically offer the highest performance on each of the presented GLUE task, and better retain PPL performance.\n\nreasons_to_accept: - Both methods for catastrophic forgetting achieve respectable improvements in performance on GLUE in tasks requiring syntactic knowledge.\n- Good experimental rigor -- Many comparisons -- Many tasks -- Each comparison computed across multiple runs/hyperparameter settings\n\nreasons_to_reject: - The PLMs used (BERT) are by current standards, quite old, and quite small.  As work in scaling PLMs up to sizes orders of magnitude greater, performance on syntactic tasks has shown to improve naturally (along with many other useful emergent forms of knowledge).  Some comparison to larger models / application of this method to such models, is necessary to ensure that the method has any practical purpose.\n- There are also no baselines from existing work.  There are other forms of fine-tuning, such as adapters, which seek to add additional knowledge to PLMs with less chance of catastrophic forgetting.  The authors even cite one of these papers.  This is also a confusing oversight, because the many appropriate inline citations which contrast various decisions in this work to decisions in existing work demonstrate a great familiarity with the literation, so lacking any comparison to any existing methods is an odd oversight.\n- Some questionable design choices.  Perplexity is used as a measure of the model retaining semantic information after fine-tuning, and while that does relate to the original task, there are also aspects of domain drift which are possible and separate from catastrophic forgetting.  How are such factors controlled?\n- Related, there is questionable motivation.  Often when talking about catastrophic forgetting, both the original training and the new task are both relevant.  This is clear in the context of robotics, where learning a new behavior should not result in hindering the robot from performing existing behaviors.  Is this true in this case?  PPL is almost always not a valuable end goal, and the entire PLM/LLM paradigm is built around this notion of pre-training in whatever way leads to learning useful linguistic representations, before fine-tuning, aligning, or few-shotting the model towards the task the user actually cares about.  If users never care about both tasks in approximately equal measure, than what good is retaining the original model weights which were not pertinent to the target task?\n- There's arguably too much going on here.  The focus of the paper aims to be about catastrophic forgetting, but secondary to that, is also the problem of matching the right syntactic fine-tuning task to the right problem.  This is not entirely known a priori, so all possible pairings are explored, but realistically a good guess can be made (as it would likely be if pursued in a more practical setting).  For instance, it is no surprise that the phrase syntax task helps with key phrase identification.  The disadvantage of the exhaustive approach is that it has both distracted from the main takeaway points while cutting into the space available for supporting the main hypothesis.\n- No inclusion of baselines from existing work / SOTA on performance tables - Key extraction F1 results are better than standard optimizers, but negligibly so.\n- No discussion of GC vs EWC.  When a priori would you choose which method?  If the paper included only one such method, traditional optimizers would be the best choice in most situations.\n\nquestions_for_the_authors: a)  Is the EWC procedure something first designed in this work?\n\ntypos_grammar_style_and_presentation_improvements: - Cite GS when discussing it.\n- L345: Missing appendix reference\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "84_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_84_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7776333333333333,
      "max_similarity": 0.7954,
      "avg_coverage": 0.5109666666666667,
      "max_coverage": 0.7
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 271,
      "avg_human_length": 328.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 2,
      "weaknesses_count": 4,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "dJfimrJrLt",
        "similarity": 0.7811,
        "coverage": 0.4211,
        "human_length": 383,
        "human_text": "paper_topic_and_main_contributions: This position paper examines the linguistic factors and psychological mechanisms that affect anthropomorphism in dialogue systems, the harms that may arise, and recommendations for dealing with these aspects. Throughout the paper, the authors also draw on existing efforts around anthropomorphism in dialogue and conversational systems. The main contribution is an organized collection of features, consequences, and recommendations surrounding anthropomorphism in dialogue systems.\n\nreasons_to_accept: Anthropomorphism in dialogue systems is an issue that is increasingly relevant, but there has not been much cohesive work in defining relevant linguistic and psychological features, consequences, and how/where we might begin to address these issues. \nThis paper ties in existing work and organizes a narrative that point the community to how we can take action to move away from anthropomorphic harms.\n\nreasons_to_reject: I don't really have a reason to reject this paper. Some of the paragraphs could be discussed in more detail, and it would be good to clarify the differences between definitions of \"designers\", \"developers\", \"practitioners\", etc (and thus perhaps the targeted audience of different recommendations) used throughout the paper, but those are more minor points.\n\nquestions_for_the_authors: A. Line 268 \"People's expectation is that animate things---such as human beings---and inanimate ones---like machines---have very different functions and capabilities...\" I wonder whether we can cleanly separate out these expectations nowadays? ( Especially as dialogue and other conversational systems have already been integrated with our everyday tools, though it's true that new integrations are constantly forming) B. \"Responses to Direct Questions\": from the paragraph title, I expected this section to go into more types of direct questions. Are there questions beyond \"are you human\" that warrant discussion?\nC. Line 702 \"We therefore recommend...forego embedding humanlike personality traits in dialogue systems\" Are there exceptions? Contexts or scenarios in which the benefits of using humanlike personality traits may be greater than disadvantages (systems for loneliness, education, etc?) Even if the answer is no, it may be good to further contextualize.\n\ntypos_grammar_style_and_presentation_improvements: - Line 638: dollar sign instead of section?\n- Line 686,687: missing spaces\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "xdrAw8vUQ3",
        "similarity": 0.7564,
        "coverage": 0.7,
        "human_length": 364,
        "human_text": "paper_topic_and_main_contributions: This position/survey paper addresses the anthropomorphism of NLP systems: it highlights the linguistic factors that influence people to anthropomorphize systems, discusses the negative consequences of this phenomenon, and makes some broad recommendations for how to address this issue in the future.\n\nreasons_to_accept: This is an excellently written paper that comprehensively unifies a lot of different research findings into an organized narrative. It will make a useful reference for any future work related to the topic. Some of the discussion points I find particularly insightful are: - The cognitive factors that promote anthropomorphism, in particular effectance and sociality - The fact that there is no one single feature of a system that will cause people to anthropomorphize it; instead, this emerges through multiple features - The complicated role of some linguistic factors in anthropomorphism; for example, speech disfluencies can mitigate users\u2019 over-confidence in a system but also make it appear more humanlike - The importance of systems trying to reduce the tendency of users to anthropomorphize them, by avoiding language that presumes human qualities - The recommendation to AI researchers to consider re-framing their research narratives to avoid references to building humanlike AI\n\nreasons_to_reject: I think this paper should be accepted for the reasons above. I do think the outlook expressed by the paper will be met with some opposing viewpoints, since it emphasizes the negative aspects of anthroporphism. A counter viewpoint is that anthropomorphism is not inherently harmful but simply needs to be controlled, and that some of recommendations given in the paper for avoiding it are unrealistic; for example, avoiding the use of first-person pronouns will be extremely difficult to achieve. I don\u2019t see this as a weakness of the paper, but it may mean the authors may need to be prepared to further argue their position with even more detailed discussion (in either the presentation, and/or a follow-up publication, and/or some other venue).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "2L7lNJoLkO",
        "similarity": 0.7954,
        "coverage": 0.4118,
        "human_length": 237,
        "human_text": "paper_topic_and_main_contributions: This is a positional paper arguing about the harms of Anthropomorphism in dialog systems. \n1. The paper shared the Human, agent, and linguistic factors, and roles that contributed to Anthropomorphism. \n2. The paper claimed the consequence of mis-trust & deception, giving machine genders. \n3. This paper recommended 4 ways to reduce  anthropomorphism, including recognizing the tendency, using anthropomorphic tools, update research goal and avoid anthropomorphic system description.\nThe contributions include 1. Reemphasize the harms of anthropomorphism in dialogue system 2. Provided a detailed analysis of what are contributing to anthropomorphism 3. Gave practical recommendations of how to address the issue\n\nreasons_to_accept: 1. Mainly based on the contribution above, with a focus of (2) and (3) in the contributions, as (1) isn't a new idea or arguments. \n2. The paper cited a lot of references to show the point\n\nreasons_to_reject: (1) The harm of anthropomorphism in dialog system isn't a new thing, it isn't a new argument (2) I would hope to see the analysis of anthropomorphism contributor can help guide the recommendation. \n(3) In general, I don't feel I am convinced by this paper about the anthropomorphism's harm\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "dJfimrJrLt",
        "length": 383,
        "human_text": "paper_topic_and_main_contributions: This position paper examines the linguistic factors and psychological mechanisms that affect anthropomorphism in dialogue systems, the harms that may arise, and recommendations for dealing with these aspects. Throughout the paper, the authors also draw on existing efforts around anthropomorphism in dialogue and conversational systems. The main contribution is an organized collection of features, consequences, and recommendations surrounding anthropomorphism in dialogue systems.\n\nreasons_to_accept: Anthropomorphism in dialogue systems is an issue that is increasingly relevant, but there has not been much cohesive work in defining relevant linguistic and psychological features, consequences, and how/where we might begin to address these issues. \nThis paper ties in existing work and organizes a narrative that point the community to how we can take action to move away from anthropomorphic harms.\n\nreasons_to_reject: I don't really have a reason to reject this paper. Some of the paragraphs could be discussed in more detail, and it would be good to clarify the differences between definitions of \"designers\", \"developers\", \"practitioners\", etc (and thus perhaps the targeted audience of different recommendations) used throughout the paper, but those are more minor points.\n\nquestions_for_the_authors: A. Line 268 \"People's expectation is that animate things---such as human beings---and inanimate ones---like machines---have very different functions and capabilities...\" I wonder whether we can cleanly separate out these expectations nowadays? ( Especially as dialogue and other conversational systems have already been integrated with our everyday tools, though it's true that new integrations are constantly forming) B. \"Responses to Direct Questions\": from the paragraph title, I expected this section to go into more types of direct questions. Are there questions beyond \"are you human\" that warrant discussion?\nC. Line 702 \"We therefore recommend...forego embedding humanlike personality traits in dialogue systems\" Are there exceptions? Contexts or scenarios in which the benefits of using humanlike personality traits may be greater than disadvantages (systems for loneliness, education, etc?) Even if the answer is no, it may be good to further contextualize.\n\ntypos_grammar_style_and_presentation_improvements: - Line 638: dollar sign instead of section?\n- Line 686,687: missing spaces\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      },
      {
        "reviewer_id": "xdrAw8vUQ3",
        "length": 364,
        "human_text": "paper_topic_and_main_contributions: This position/survey paper addresses the anthropomorphism of NLP systems: it highlights the linguistic factors that influence people to anthropomorphize systems, discusses the negative consequences of this phenomenon, and makes some broad recommendations for how to address this issue in the future.\n\nreasons_to_accept: This is an excellently written paper that comprehensively unifies a lot of different research findings into an organized narrative. It will make a useful reference for any future work related to the topic. Some of the discussion points I find particularly insightful are: - The cognitive factors that promote anthropomorphism, in particular effectance and sociality - The fact that there is no one single feature of a system that will cause people to anthropomorphize it; instead, this emerges through multiple features - The complicated role of some linguistic factors in anthropomorphism; for example, speech disfluencies can mitigate users\u2019 over-confidence in a system but also make it appear more humanlike - The importance of systems trying to reduce the tendency of users to anthropomorphize them, by avoiding language that presumes human qualities - The recommendation to AI researchers to consider re-framing their research narratives to avoid references to building humanlike AI\n\nreasons_to_reject: I think this paper should be accepted for the reasons above. I do think the outlook expressed by the paper will be met with some opposing viewpoints, since it emphasizes the negative aspects of anthroporphism. A counter viewpoint is that anthropomorphism is not inherently harmful but simply needs to be controlled, and that some of recommendations given in the paper for avoiding it are unrealistic; for example, avoiding the use of first-person pronouns will be extremely difficult to achieve. I don\u2019t see this as a weakness of the paper, but it may mean the authors may need to be prepared to further argue their position with even more detailed discussion (in either the presentation, and/or a follow-up publication, and/or some other venue).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      },
      {
        "reviewer_id": "2L7lNJoLkO",
        "length": 237,
        "human_text": "paper_topic_and_main_contributions: This is a positional paper arguing about the harms of Anthropomorphism in dialog systems. \n1. The paper shared the Human, agent, and linguistic factors, and roles that contributed to Anthropomorphism. \n2. The paper claimed the consequence of mis-trust & deception, giving machine genders. \n3. This paper recommended 4 ways to reduce  anthropomorphism, including recognizing the tendency, using anthropomorphic tools, update research goal and avoid anthropomorphic system description.\nThe contributions include 1. Reemphasize the harms of anthropomorphism in dialogue system 2. Provided a detailed analysis of what are contributing to anthropomorphism 3. Gave practical recommendations of how to address the issue\n\nreasons_to_accept: 1. Mainly based on the contribution above, with a focus of (2) and (3) in the contributions, as (1) isn't a new idea or arguments. \n2. The paper cited a lot of references to show the point\n\nreasons_to_reject: (1) The harm of anthropomorphism in dialog system isn't a new thing, it isn't a new argument (2) I would hope to see the analysis of anthropomorphism contributor can help guide the recommendation. \n(3) In general, I don't feel I am convinced by this paper about the anthropomorphism's harm\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "84_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_84_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7705333333333333,
      "max_similarity": 0.7807,
      "avg_coverage": 0.5443,
      "max_coverage": 0.8
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 300,
      "avg_human_length": 328.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 2,
      "weaknesses_count": 4,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "dJfimrJrLt",
        "similarity": 0.7768,
        "coverage": 0.4211,
        "human_length": 383,
        "human_text": "paper_topic_and_main_contributions: This position paper examines the linguistic factors and psychological mechanisms that affect anthropomorphism in dialogue systems, the harms that may arise, and recommendations for dealing with these aspects. Throughout the paper, the authors also draw on existing efforts around anthropomorphism in dialogue and conversational systems. The main contribution is an organized collection of features, consequences, and recommendations surrounding anthropomorphism in dialogue systems.\n\nreasons_to_accept: Anthropomorphism in dialogue systems is an issue that is increasingly relevant, but there has not been much cohesive work in defining relevant linguistic and psychological features, consequences, and how/where we might begin to address these issues. \nThis paper ties in existing work and organizes a narrative that point the community to how we can take action to move away from anthropomorphic harms.\n\nreasons_to_reject: I don't really have a reason to reject this paper. Some of the paragraphs could be discussed in more detail, and it would be good to clarify the differences between definitions of \"designers\", \"developers\", \"practitioners\", etc (and thus perhaps the targeted audience of different recommendations) used throughout the paper, but those are more minor points.\n\nquestions_for_the_authors: A. Line 268 \"People's expectation is that animate things---such as human beings---and inanimate ones---like machines---have very different functions and capabilities...\" I wonder whether we can cleanly separate out these expectations nowadays? ( Especially as dialogue and other conversational systems have already been integrated with our everyday tools, though it's true that new integrations are constantly forming) B. \"Responses to Direct Questions\": from the paragraph title, I expected this section to go into more types of direct questions. Are there questions beyond \"are you human\" that warrant discussion?\nC. Line 702 \"We therefore recommend...forego embedding humanlike personality traits in dialogue systems\" Are there exceptions? Contexts or scenarios in which the benefits of using humanlike personality traits may be greater than disadvantages (systems for loneliness, education, etc?) Even if the answer is no, it may be good to further contextualize.\n\ntypos_grammar_style_and_presentation_improvements: - Line 638: dollar sign instead of section?\n- Line 686,687: missing spaces\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "xdrAw8vUQ3",
        "similarity": 0.7541,
        "coverage": 0.8,
        "human_length": 364,
        "human_text": "paper_topic_and_main_contributions: This position/survey paper addresses the anthropomorphism of NLP systems: it highlights the linguistic factors that influence people to anthropomorphize systems, discusses the negative consequences of this phenomenon, and makes some broad recommendations for how to address this issue in the future.\n\nreasons_to_accept: This is an excellently written paper that comprehensively unifies a lot of different research findings into an organized narrative. It will make a useful reference for any future work related to the topic. Some of the discussion points I find particularly insightful are: - The cognitive factors that promote anthropomorphism, in particular effectance and sociality - The fact that there is no one single feature of a system that will cause people to anthropomorphize it; instead, this emerges through multiple features - The complicated role of some linguistic factors in anthropomorphism; for example, speech disfluencies can mitigate users\u2019 over-confidence in a system but also make it appear more humanlike - The importance of systems trying to reduce the tendency of users to anthropomorphize them, by avoiding language that presumes human qualities - The recommendation to AI researchers to consider re-framing their research narratives to avoid references to building humanlike AI\n\nreasons_to_reject: I think this paper should be accepted for the reasons above. I do think the outlook expressed by the paper will be met with some opposing viewpoints, since it emphasizes the negative aspects of anthroporphism. A counter viewpoint is that anthropomorphism is not inherently harmful but simply needs to be controlled, and that some of recommendations given in the paper for avoiding it are unrealistic; for example, avoiding the use of first-person pronouns will be extremely difficult to achieve. I don\u2019t see this as a weakness of the paper, but it may mean the authors may need to be prepared to further argue their position with even more detailed discussion (in either the presentation, and/or a follow-up publication, and/or some other venue).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "2L7lNJoLkO",
        "similarity": 0.7807,
        "coverage": 0.4118,
        "human_length": 237,
        "human_text": "paper_topic_and_main_contributions: This is a positional paper arguing about the harms of Anthropomorphism in dialog systems. \n1. The paper shared the Human, agent, and linguistic factors, and roles that contributed to Anthropomorphism. \n2. The paper claimed the consequence of mis-trust & deception, giving machine genders. \n3. This paper recommended 4 ways to reduce  anthropomorphism, including recognizing the tendency, using anthropomorphic tools, update research goal and avoid anthropomorphic system description.\nThe contributions include 1. Reemphasize the harms of anthropomorphism in dialogue system 2. Provided a detailed analysis of what are contributing to anthropomorphism 3. Gave practical recommendations of how to address the issue\n\nreasons_to_accept: 1. Mainly based on the contribution above, with a focus of (2) and (3) in the contributions, as (1) isn't a new idea or arguments. \n2. The paper cited a lot of references to show the point\n\nreasons_to_reject: (1) The harm of anthropomorphism in dialog system isn't a new thing, it isn't a new argument (2) I would hope to see the analysis of anthropomorphism contributor can help guide the recommendation. \n(3) In general, I don't feel I am convinced by this paper about the anthropomorphism's harm\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "dJfimrJrLt",
        "length": 383,
        "human_text": "paper_topic_and_main_contributions: This position paper examines the linguistic factors and psychological mechanisms that affect anthropomorphism in dialogue systems, the harms that may arise, and recommendations for dealing with these aspects. Throughout the paper, the authors also draw on existing efforts around anthropomorphism in dialogue and conversational systems. The main contribution is an organized collection of features, consequences, and recommendations surrounding anthropomorphism in dialogue systems.\n\nreasons_to_accept: Anthropomorphism in dialogue systems is an issue that is increasingly relevant, but there has not been much cohesive work in defining relevant linguistic and psychological features, consequences, and how/where we might begin to address these issues. \nThis paper ties in existing work and organizes a narrative that point the community to how we can take action to move away from anthropomorphic harms.\n\nreasons_to_reject: I don't really have a reason to reject this paper. Some of the paragraphs could be discussed in more detail, and it would be good to clarify the differences between definitions of \"designers\", \"developers\", \"practitioners\", etc (and thus perhaps the targeted audience of different recommendations) used throughout the paper, but those are more minor points.\n\nquestions_for_the_authors: A. Line 268 \"People's expectation is that animate things---such as human beings---and inanimate ones---like machines---have very different functions and capabilities...\" I wonder whether we can cleanly separate out these expectations nowadays? ( Especially as dialogue and other conversational systems have already been integrated with our everyday tools, though it's true that new integrations are constantly forming) B. \"Responses to Direct Questions\": from the paragraph title, I expected this section to go into more types of direct questions. Are there questions beyond \"are you human\" that warrant discussion?\nC. Line 702 \"We therefore recommend...forego embedding humanlike personality traits in dialogue systems\" Are there exceptions? Contexts or scenarios in which the benefits of using humanlike personality traits may be greater than disadvantages (systems for loneliness, education, etc?) Even if the answer is no, it may be good to further contextualize.\n\ntypos_grammar_style_and_presentation_improvements: - Line 638: dollar sign instead of section?\n- Line 686,687: missing spaces\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      },
      {
        "reviewer_id": "xdrAw8vUQ3",
        "length": 364,
        "human_text": "paper_topic_and_main_contributions: This position/survey paper addresses the anthropomorphism of NLP systems: it highlights the linguistic factors that influence people to anthropomorphize systems, discusses the negative consequences of this phenomenon, and makes some broad recommendations for how to address this issue in the future.\n\nreasons_to_accept: This is an excellently written paper that comprehensively unifies a lot of different research findings into an organized narrative. It will make a useful reference for any future work related to the topic. Some of the discussion points I find particularly insightful are: - The cognitive factors that promote anthropomorphism, in particular effectance and sociality - The fact that there is no one single feature of a system that will cause people to anthropomorphize it; instead, this emerges through multiple features - The complicated role of some linguistic factors in anthropomorphism; for example, speech disfluencies can mitigate users\u2019 over-confidence in a system but also make it appear more humanlike - The importance of systems trying to reduce the tendency of users to anthropomorphize them, by avoiding language that presumes human qualities - The recommendation to AI researchers to consider re-framing their research narratives to avoid references to building humanlike AI\n\nreasons_to_reject: I think this paper should be accepted for the reasons above. I do think the outlook expressed by the paper will be met with some opposing viewpoints, since it emphasizes the negative aspects of anthroporphism. A counter viewpoint is that anthropomorphism is not inherently harmful but simply needs to be controlled, and that some of recommendations given in the paper for avoiding it are unrealistic; for example, avoiding the use of first-person pronouns will be extremely difficult to achieve. I don\u2019t see this as a weakness of the paper, but it may mean the authors may need to be prepared to further argue their position with even more detailed discussion (in either the presentation, and/or a follow-up publication, and/or some other venue).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      },
      {
        "reviewer_id": "2L7lNJoLkO",
        "length": 237,
        "human_text": "paper_topic_and_main_contributions: This is a positional paper arguing about the harms of Anthropomorphism in dialog systems. \n1. The paper shared the Human, agent, and linguistic factors, and roles that contributed to Anthropomorphism. \n2. The paper claimed the consequence of mis-trust & deception, giving machine genders. \n3. This paper recommended 4 ways to reduce  anthropomorphism, including recognizing the tendency, using anthropomorphic tools, update research goal and avoid anthropomorphic system description.\nThe contributions include 1. Reemphasize the harms of anthropomorphism in dialogue system 2. Provided a detailed analysis of what are contributing to anthropomorphism 3. Gave practical recommendations of how to address the issue\n\nreasons_to_accept: 1. Mainly based on the contribution above, with a focus of (2) and (3) in the contributions, as (1) isn't a new idea or arguments. \n2. The paper cited a lot of references to show the point\n\nreasons_to_reject: (1) The harm of anthropomorphism in dialog system isn't a new thing, it isn't a new argument (2) I would hope to see the analysis of anthropomorphism contributor can help guide the recommendation. \n(3) In general, I don't feel I am convinced by this paper about the anthropomorphism's harm\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: N/A: Doesn't apply, since the paper does not include empirical results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "209_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_209_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7461333333333333,
      "max_similarity": 0.7695,
      "avg_coverage": 0.5269666666666667,
      "max_coverage": 0.7143
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 543,
      "avg_human_length": 303.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 9,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "6q5ZO12zR0",
        "similarity": 0.7316,
        "coverage": 0.5333,
        "human_length": 236,
        "human_text": "paper_topic_and_main_contributions: The paper reports the procedure for training mLongT5, a transformer model to handle long input sequences with an extensive evaluation of its abilities across a number of languages. The main contribution is in making a publicly available pre-trained model.\n\nreasons_to_accept: 1. Practically useful multilingual language model which can handle long input sequences   2. Extensive evaluation with respect to the number of languages and evaluation scenarios.\n\nreasons_to_reject: 1. Using Rouge for evaluation is not warranted. It has been shown that it is far less reliable with respect to evaluation of neural models as it relies on exact lexical matches. BertScore is more reliable and there is no inherent danger in using BertScore to evaluate mBERT as both use different mechanisms. \n 2. There is little innovation in the pipeline, but having a reporting paper published in a peer-reviewed conference is better than merely an Arxiv publication.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "3ouymsqaFd",
        "similarity": 0.7695,
        "coverage": 0.3333,
        "human_length": 459,
        "human_text": "paper_topic_and_main_contributions: This paper presents a Multilingual LongT5 model (mLongT5), derived and extended from mT5, LongT5 and and UL2 by using a multilingual corpora. The mLongT5 model has been evaluated on  a variety of multilingual summarization and question-answering tasks, where the performance outperforms the existing multilingual models like mBERT and mT5.\n\nreasons_to_accept: 1. This paper integrates and extends the methods from mT5, Long T5 and UL2, and proposes several multilingual versions of LongT5 (base large xl), which achieve improvements on several multilingual downstream tasks.  2. The experiments are elaborated. Not only the model size is discussed, but also the input token length is compared (Table 4).  3. The code and datasets are open, which is good for the community.\n4. Stable improvements are achieved during increasing the model size of mLongT5, as shown in the experiments, showing great potential and insights for larger language models.\n\nreasons_to_reject: 1. Although the experiments are conducted well, there still lack some explanation for some experimental results. Such as, in Table 1, mLongT5(base) is not as good as mBERT(base) on FR evaluation; besides, as for the RU, the author just claims that Russian is the hardest language for language models for its sparsity. The explanation seems simple and even subjective. You could set a ablation study that mLongT5 is tasked to pre-trained on the same datasets that mBERT uses to issue this question.\n2. As for the Table 4, the author compare the mT5 model and the mLongT5 model with the same token lengths to prove the statement that mLongT5 processes long text better. More datasets and existing models like XLM-Roberta should be discussed to extend the generalization.\n\nquestions_for_the_authors: 1. In line 168 \"As noted in the original paper, Russian was the hardest language for language models, due to having a much smaller dataset when compared to the other languages\", cannot explain the results in a straight line. It seems like to display the unfairness between the datasets that mBERT and mLongT5 used. May you use the pre-training datasets that mBERT used to pre-train mLongT5 (or use the Russian part) to issue this problem?\n2. In Table 2, mT5(base) outperforms mLongT5(base) on almost all languages. How you describe this phenomena?\n3. All the experiments are less a discussion of computational cost. May you add a paragraph to compare the costs of pre-training for these models?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "kYl11koC9M",
        "similarity": 0.7373,
        "coverage": 0.7143,
        "human_length": 215,
        "human_text": "paper_topic_and_main_contributions: The paper proposes mLongT5, which extends LongT5 to the multilingual scenario. The paper provides detailed information of model pre-training, and conducts extensive experiments on MLSUM, XL-Sum, Wikilingua, and TyDi QA. The checkpoints will be open-sourced, which will facilitate the research on long text processing in NLP.\n\nreasons_to_accept: Training a multilingual LM that handles long input sequences is an important topic for multilingual research.\nThe open-sourced models will facilitate the research on long text processing in NLP, espcially for low-resource languages.\nExtensive experiments on multiple datasets. The results in Table 4 shows that mLongT5 is good at handling long input sequences.\n\nreasons_to_reject: Directly extending LongT5 to the multilingual one is somewhat lacking in novelty.\n\nquestions_for_the_authors: Would you please provide detailed hyperparameters of mLongT5 and mT5 for fine-tuning?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "6q5ZO12zR0",
        "length": 236,
        "human_text": "paper_topic_and_main_contributions: The paper reports the procedure for training mLongT5, a transformer model to handle long input sequences with an extensive evaluation of its abilities across a number of languages. The main contribution is in making a publicly available pre-trained model.\n\nreasons_to_accept: 1. Practically useful multilingual language model which can handle long input sequences   2. Extensive evaluation with respect to the number of languages and evaluation scenarios.\n\nreasons_to_reject: 1. Using Rouge for evaluation is not warranted. It has been shown that it is far less reliable with respect to evaluation of neural models as it relies on exact lexical matches. BertScore is more reliable and there is no inherent danger in using BertScore to evaluate mBERT as both use different mechanisms. \n 2. There is little innovation in the pipeline, but having a reporting paper published in a peer-reviewed conference is better than merely an Arxiv publication.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "3ouymsqaFd",
        "length": 459,
        "human_text": "paper_topic_and_main_contributions: This paper presents a Multilingual LongT5 model (mLongT5), derived and extended from mT5, LongT5 and and UL2 by using a multilingual corpora. The mLongT5 model has been evaluated on  a variety of multilingual summarization and question-answering tasks, where the performance outperforms the existing multilingual models like mBERT and mT5.\n\nreasons_to_accept: 1. This paper integrates and extends the methods from mT5, Long T5 and UL2, and proposes several multilingual versions of LongT5 (base large xl), which achieve improvements on several multilingual downstream tasks.  2. The experiments are elaborated. Not only the model size is discussed, but also the input token length is compared (Table 4).  3. The code and datasets are open, which is good for the community.\n4. Stable improvements are achieved during increasing the model size of mLongT5, as shown in the experiments, showing great potential and insights for larger language models.\n\nreasons_to_reject: 1. Although the experiments are conducted well, there still lack some explanation for some experimental results. Such as, in Table 1, mLongT5(base) is not as good as mBERT(base) on FR evaluation; besides, as for the RU, the author just claims that Russian is the hardest language for language models for its sparsity. The explanation seems simple and even subjective. You could set a ablation study that mLongT5 is tasked to pre-trained on the same datasets that mBERT uses to issue this question.\n2. As for the Table 4, the author compare the mT5 model and the mLongT5 model with the same token lengths to prove the statement that mLongT5 processes long text better. More datasets and existing models like XLM-Roberta should be discussed to extend the generalization.\n\nquestions_for_the_authors: 1. In line 168 \"As noted in the original paper, Russian was the hardest language for language models, due to having a much smaller dataset when compared to the other languages\", cannot explain the results in a straight line. It seems like to display the unfairness between the datasets that mBERT and mLongT5 used. May you use the pre-training datasets that mBERT used to pre-train mLongT5 (or use the Russian part) to issue this problem?\n2. In Table 2, mT5(base) outperforms mLongT5(base) on almost all languages. How you describe this phenomena?\n3. All the experiments are less a discussion of computational cost. May you add a paragraph to compare the costs of pre-training for these models?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "kYl11koC9M",
        "length": 215,
        "human_text": "paper_topic_and_main_contributions: The paper proposes mLongT5, which extends LongT5 to the multilingual scenario. The paper provides detailed information of model pre-training, and conducts extensive experiments on MLSUM, XL-Sum, Wikilingua, and TyDi QA. The checkpoints will be open-sourced, which will facilitate the research on long text processing in NLP.\n\nreasons_to_accept: Training a multilingual LM that handles long input sequences is an important topic for multilingual research.\nThe open-sourced models will facilitate the research on long text processing in NLP, espcially for low-resource languages.\nExtensive experiments on multiple datasets. The results in Table 4 shows that mLongT5 is good at handling long input sequences.\n\nreasons_to_reject: Directly extending LongT5 to the multilingual one is somewhat lacking in novelty.\n\nquestions_for_the_authors: Would you please provide detailed hyperparameters of mLongT5 and mT5 for fine-tuning?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "209_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_209_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7422666666666666,
      "max_similarity": 0.7679,
      "avg_coverage": 0.5492,
      "max_coverage": 0.7143
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 598,
      "avg_human_length": 303.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 9,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "6q5ZO12zR0",
        "similarity": 0.7259,
        "coverage": 0.6,
        "human_length": 236,
        "human_text": "paper_topic_and_main_contributions: The paper reports the procedure for training mLongT5, a transformer model to handle long input sequences with an extensive evaluation of its abilities across a number of languages. The main contribution is in making a publicly available pre-trained model.\n\nreasons_to_accept: 1. Practically useful multilingual language model which can handle long input sequences   2. Extensive evaluation with respect to the number of languages and evaluation scenarios.\n\nreasons_to_reject: 1. Using Rouge for evaluation is not warranted. It has been shown that it is far less reliable with respect to evaluation of neural models as it relies on exact lexical matches. BertScore is more reliable and there is no inherent danger in using BertScore to evaluate mBERT as both use different mechanisms. \n 2. There is little innovation in the pipeline, but having a reporting paper published in a peer-reviewed conference is better than merely an Arxiv publication.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "3ouymsqaFd",
        "similarity": 0.7679,
        "coverage": 0.3333,
        "human_length": 459,
        "human_text": "paper_topic_and_main_contributions: This paper presents a Multilingual LongT5 model (mLongT5), derived and extended from mT5, LongT5 and and UL2 by using a multilingual corpora. The mLongT5 model has been evaluated on  a variety of multilingual summarization and question-answering tasks, where the performance outperforms the existing multilingual models like mBERT and mT5.\n\nreasons_to_accept: 1. This paper integrates and extends the methods from mT5, Long T5 and UL2, and proposes several multilingual versions of LongT5 (base large xl), which achieve improvements on several multilingual downstream tasks.  2. The experiments are elaborated. Not only the model size is discussed, but also the input token length is compared (Table 4).  3. The code and datasets are open, which is good for the community.\n4. Stable improvements are achieved during increasing the model size of mLongT5, as shown in the experiments, showing great potential and insights for larger language models.\n\nreasons_to_reject: 1. Although the experiments are conducted well, there still lack some explanation for some experimental results. Such as, in Table 1, mLongT5(base) is not as good as mBERT(base) on FR evaluation; besides, as for the RU, the author just claims that Russian is the hardest language for language models for its sparsity. The explanation seems simple and even subjective. You could set a ablation study that mLongT5 is tasked to pre-trained on the same datasets that mBERT uses to issue this question.\n2. As for the Table 4, the author compare the mT5 model and the mLongT5 model with the same token lengths to prove the statement that mLongT5 processes long text better. More datasets and existing models like XLM-Roberta should be discussed to extend the generalization.\n\nquestions_for_the_authors: 1. In line 168 \"As noted in the original paper, Russian was the hardest language for language models, due to having a much smaller dataset when compared to the other languages\", cannot explain the results in a straight line. It seems like to display the unfairness between the datasets that mBERT and mLongT5 used. May you use the pre-training datasets that mBERT used to pre-train mLongT5 (or use the Russian part) to issue this problem?\n2. In Table 2, mT5(base) outperforms mLongT5(base) on almost all languages. How you describe this phenomena?\n3. All the experiments are less a discussion of computational cost. May you add a paragraph to compare the costs of pre-training for these models?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "kYl11koC9M",
        "similarity": 0.733,
        "coverage": 0.7143,
        "human_length": 215,
        "human_text": "paper_topic_and_main_contributions: The paper proposes mLongT5, which extends LongT5 to the multilingual scenario. The paper provides detailed information of model pre-training, and conducts extensive experiments on MLSUM, XL-Sum, Wikilingua, and TyDi QA. The checkpoints will be open-sourced, which will facilitate the research on long text processing in NLP.\n\nreasons_to_accept: Training a multilingual LM that handles long input sequences is an important topic for multilingual research.\nThe open-sourced models will facilitate the research on long text processing in NLP, espcially for low-resource languages.\nExtensive experiments on multiple datasets. The results in Table 4 shows that mLongT5 is good at handling long input sequences.\n\nreasons_to_reject: Directly extending LongT5 to the multilingual one is somewhat lacking in novelty.\n\nquestions_for_the_authors: Would you please provide detailed hyperparameters of mLongT5 and mT5 for fine-tuning?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "6q5ZO12zR0",
        "length": 236,
        "human_text": "paper_topic_and_main_contributions: The paper reports the procedure for training mLongT5, a transformer model to handle long input sequences with an extensive evaluation of its abilities across a number of languages. The main contribution is in making a publicly available pre-trained model.\n\nreasons_to_accept: 1. Practically useful multilingual language model which can handle long input sequences   2. Extensive evaluation with respect to the number of languages and evaluation scenarios.\n\nreasons_to_reject: 1. Using Rouge for evaluation is not warranted. It has been shown that it is far less reliable with respect to evaluation of neural models as it relies on exact lexical matches. BertScore is more reliable and there is no inherent danger in using BertScore to evaluate mBERT as both use different mechanisms. \n 2. There is little innovation in the pipeline, but having a reporting paper published in a peer-reviewed conference is better than merely an Arxiv publication.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "3ouymsqaFd",
        "length": 459,
        "human_text": "paper_topic_and_main_contributions: This paper presents a Multilingual LongT5 model (mLongT5), derived and extended from mT5, LongT5 and and UL2 by using a multilingual corpora. The mLongT5 model has been evaluated on  a variety of multilingual summarization and question-answering tasks, where the performance outperforms the existing multilingual models like mBERT and mT5.\n\nreasons_to_accept: 1. This paper integrates and extends the methods from mT5, Long T5 and UL2, and proposes several multilingual versions of LongT5 (base large xl), which achieve improvements on several multilingual downstream tasks.  2. The experiments are elaborated. Not only the model size is discussed, but also the input token length is compared (Table 4).  3. The code and datasets are open, which is good for the community.\n4. Stable improvements are achieved during increasing the model size of mLongT5, as shown in the experiments, showing great potential and insights for larger language models.\n\nreasons_to_reject: 1. Although the experiments are conducted well, there still lack some explanation for some experimental results. Such as, in Table 1, mLongT5(base) is not as good as mBERT(base) on FR evaluation; besides, as for the RU, the author just claims that Russian is the hardest language for language models for its sparsity. The explanation seems simple and even subjective. You could set a ablation study that mLongT5 is tasked to pre-trained on the same datasets that mBERT uses to issue this question.\n2. As for the Table 4, the author compare the mT5 model and the mLongT5 model with the same token lengths to prove the statement that mLongT5 processes long text better. More datasets and existing models like XLM-Roberta should be discussed to extend the generalization.\n\nquestions_for_the_authors: 1. In line 168 \"As noted in the original paper, Russian was the hardest language for language models, due to having a much smaller dataset when compared to the other languages\", cannot explain the results in a straight line. It seems like to display the unfairness between the datasets that mBERT and mLongT5 used. May you use the pre-training datasets that mBERT used to pre-train mLongT5 (or use the Russian part) to issue this problem?\n2. In Table 2, mT5(base) outperforms mLongT5(base) on almost all languages. How you describe this phenomena?\n3. All the experiments are less a discussion of computational cost. May you add a paragraph to compare the costs of pre-training for these models?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "kYl11koC9M",
        "length": 215,
        "human_text": "paper_topic_and_main_contributions: The paper proposes mLongT5, which extends LongT5 to the multilingual scenario. The paper provides detailed information of model pre-training, and conducts extensive experiments on MLSUM, XL-Sum, Wikilingua, and TyDi QA. The checkpoints will be open-sourced, which will facilitate the research on long text processing in NLP.\n\nreasons_to_accept: Training a multilingual LM that handles long input sequences is an important topic for multilingual research.\nThe open-sourced models will facilitate the research on long text processing in NLP, espcially for low-resource languages.\nExtensive experiments on multiple datasets. The results in Table 4 shows that mLongT5 is good at handling long input sequences.\n\nreasons_to_reject: Directly extending LongT5 to the multilingual one is somewhat lacking in novelty.\n\nquestions_for_the_authors: Would you please provide detailed hyperparameters of mLongT5 and mT5 for fine-tuning?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "217_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_217_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.745375,
      "max_similarity": 0.7671,
      "avg_coverage": 0.513825,
      "max_coverage": 0.6667
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 535,
      "avg_human_length": 443.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 6,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "dMfs0VsPQv",
        "similarity": 0.7507,
        "coverage": 0.3636,
        "human_length": 439,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a formal framework for LLM simulations and introduces the caricature phenomena from the individuation and exaggeration side. The evaluation results from existing works on LLM simulations demonstrate that simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.\nMain Contributions: 1. A new framework for characterizing the dimensions of LLM simulations. \n2. Methods (mainly some metrics) for measuring simulations\u2019 susceptibility to caricatures. \n3. An analysis of existing work on LLM simulations of certain demographics and topics.\n\nreasons_to_accept: 1. The paper provides a framework for evaluating LLM simulations, which can be useful for researchers working with language models in social science. The idea is novel and inspiring, and the findings can inform future work on improving the quality of LLM simulations and reducing the potential for caricature. \n2. The paper provides a useful overview of previous work on LLM simulations and helps to construct a new detection framework on individuation and exaggeration. \n3. This paper concludes several recommendations with abundant experimental evidence for LLM simulations.\n\nreasons_to_reject: 1. The detection framework seems to be limited to specific one-round QA formats and scenarios and may not be applicable to other situations or applications of LLM simulating. \n2. Detection (or classification) methods appear to be simple for Individuation and Exaggeration and may be too biased when based on only sentence embedding features. \n3. More complete samples of person and non-person responses should be provided to validate the accuracy of the test, in addition to the manual (or at least gpt4) results.\n\nquestions_for_the_authors: 1. Tables A1 and A2 do seem to capture some correlation between Pole Seed Words and simulation objectives but also noise a lot. Is there any direct evidence of the exaggeration phenomenon (e.g., some complete transcripts of conversations), and how does your method successfully detect on them? \n2. The correlation between your proposed metrics and the phenomenon of exaggeration does not appear to have been carefully verified. How to demonstrate the relationship between preference for particular words and the model bias\uff1f \n3. How do these metrics work on more complex simulation tasks, I would guess that more complex simulations (e.g. a specific historical character) would be more independent of wordings.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "USkmyx916z",
        "similarity": 0.7671,
        "coverage": 0.625,
        "human_length": 416,
        "human_text": "paper_topic_and_main_contributions: This work introduces a framework, CoMPosT, for characterizing and evaluating caricature (individuation and exaggeration) in LLM simulations. In doing so, the authors propose two different metrics for measuring individuation and exaggeration.  Specifically, they evaluate simulations in the online forum and interview context and find certain demographics and topics are more susceptible to caricature. \nThe main contribution of this paper is to offer a shared language to the field to critique problematic applications of LLM simulations.\n\nreasons_to_accept: - This paper is well written and explained. The motivation to characterize and evaluate caricature in LLM simulations is very clear and important.  - The definition of caricature is sensible and informative. It is important to distinguish caricature and exaggeration as the authors mention in the paper: a caricature not only exaggerates particular features of the subject but also exaggerates in a manner that meaningfully differentiates the subject from others.  - The experiments are quite solid. The authors consider different contexts in LLM simulations such as online forum, interview, and twitter. Besides, they consider a reasonable amount of combinations of persona and topic. The empirical results are convincing.  - The recommendations proposed by the authors are helpful for researchers and stakeholders.\n\nreasons_to_reject: - This paper has a lack of error analysis about false positive cases that seem acceptable and caricature-free based on CoMPosT.  - A power analysis may be needed to determine the number of examples for each simulation setting. In the paper, the authors choose to generate 100 outputs but it is a little unclear where the number comes from.  - It would be nice to include explanations about the differences in individuation scores in various contexts. In Figure 4, the mean individuation scores in the online forum context are consistently lower than in the interview context across all personas.\n\nquestions_for_the_authors: - Question A: For reproducibility purposes, could you please elaborate on the setup of the binary classifier used in measuring individuation?  - Question B: Have you ever considered using persona-topic semantic axes to measure individuation? It would be better if we have the metrics for individuation and exaggeration defined in a comparable way.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "JngkBn0pU1",
        "similarity": 0.7078,
        "coverage": 0.6667,
        "human_length": 361,
        "human_text": "paper_topic_and_main_contributions: Topic: caricature detection in LLM Main contribution: formal formulation of caricature, a novel caricature detection method based on two metrics (individuation and exaggeration), experiment to show how the proposed method can be used to quantify individuation and exaggeration for a variety of personas in two different contexts (online forums and interview), and result that individuation is less helpful than exaggeration in this task.\n\nreasons_to_accept: - Clear formulation of caricature - Novel methods to detect caricature based on two dimensions  - Interesting explanation of experimental results (e.g. marginalized groups are subject to higher exaggeration and thus caricature) - Extended works for other platforms, e.g. Twitter, in the appendix\n\nreasons_to_reject: My only major concern is the lack of evaluation to show the validity of their proposed method in Figure 3 with respect to the caricature detection task (e.g. whether it is valid to use classifier accuracy between default persona and simulation S as a proxy for individuation; whether it is valid to use similarity between simulation S and persona-topic semantic axes) as there seems to be no ground truth for caricature to compare their results with. The experiment seems to be based on the assumption that their method is already valid, and the experiment is just there to show how their method helps social science findings (e.g. which groups are subject to caricature). It will be great if the authors may construct (or use) a small human-annotated dataset of caricatures to further evaluate their proposed method.\n\nquestions_for_the_authors: A: Could you please elaborate on the rationale behind choosing the two contexts: Online Forum vs. Interview (e.g. which contrast you want to make)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "BxumWKeELU",
        "similarity": 0.7559,
        "coverage": 0.4,
        "human_length": 556,
        "human_text": "paper_topic_and_main_contributions: The paper presents a novel framework for identifying and measuring the extent of caricatures in LLM generations/simulations. In their proposed framework, the paper introduces 4 major dimensions: model, context, persona, and topic. The paper defines persona as demographic attributes for a group of people but I believe it can extend to more nuanced views as well. The paper motivates the problem by showcasing examples of how a persona can be exaggerated while simulating a specific topic. Finally, the paper proposes several metrics to measure individuation and exaggeration in LLM generations and finds out several scenarios where LLM simulations are caricatures.\n\nreasons_to_accept: 1. The paper introduces a novel framework to measure caricature, which is a nuanced way of how LLMs can be biased in an open-ended generation setup. \n2. With the growing adaptability of LLMs, measuring caricature is a timely and important problem to be addressed. \n3. The paper shares several recommendations for measuring and mitigating caricatures in LLM simulations in the future.\n\nreasons_to_reject: - The experiments were conducted on a single LLM GPT 4. Although it is one of the popular LLMs with API access, it would be interesting to see small-scale simulations with other commercial or non-commercial LLMs.\n- The description of some of the figures can be improved. \n    * What do 20, 40, 80 refer to in Figure 4? Later I find that these numbers correspond to a person\u2019s age. \n    * It would be nice to label Figure 5 components into subfigures 5(a), 5(b), etc. It is difficult to follow the text (for example in section 6.2.2) at times and figure out which components are being discussed.\n- Some sections of the evaluation are a bit confusing to me. Please find the details in the questions section below.\n\nquestions_for_the_authors: - Measuring individuation: For paper and analysis, using the classifier seems reasonable. I would like to know if the authors had any suggestions for users trying to measure the extent of individuation for a small set of samples. Also, there is a chance that BERT based classifier overfits some characteristic words or starting words from the default simulation. \n    * Will any unsupervised metric like V-measure be helpful to figure out how separable the two sets are? \n    * It would be interesting to have any such unsupervised metric being compared with the classifier performance.  - Measuring exaggeration: I understand that the paper reuses the framework of Lucy et al, 2022 to measure exaggeration. But the intuition behind the measure is a bit unclear to me. It would be great if the authors could clarify the following:     * From my understanding, the generated simulation shouldn\u2019t lean much toward either the persona or topic-related words. For this why can\u2019t we measure the cosine similarity with E[p_i] and E[t_i], and form a normalized score of its sum? Specifically, I\u2019m confused about why subtraction is required in the definition of $V_{p, t}$.     * What does $S^i_{p,t,c}$ refer to? Description of the superscript $i$ is missing?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "dMfs0VsPQv",
        "length": 439,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a formal framework for LLM simulations and introduces the caricature phenomena from the individuation and exaggeration side. The evaluation results from existing works on LLM simulations demonstrate that simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.\nMain Contributions: 1. A new framework for characterizing the dimensions of LLM simulations. \n2. Methods (mainly some metrics) for measuring simulations\u2019 susceptibility to caricatures. \n3. An analysis of existing work on LLM simulations of certain demographics and topics.\n\nreasons_to_accept: 1. The paper provides a framework for evaluating LLM simulations, which can be useful for researchers working with language models in social science. The idea is novel and inspiring, and the findings can inform future work on improving the quality of LLM simulations and reducing the potential for caricature. \n2. The paper provides a useful overview of previous work on LLM simulations and helps to construct a new detection framework on individuation and exaggeration. \n3. This paper concludes several recommendations with abundant experimental evidence for LLM simulations.\n\nreasons_to_reject: 1. The detection framework seems to be limited to specific one-round QA formats and scenarios and may not be applicable to other situations or applications of LLM simulating. \n2. Detection (or classification) methods appear to be simple for Individuation and Exaggeration and may be too biased when based on only sentence embedding features. \n3. More complete samples of person and non-person responses should be provided to validate the accuracy of the test, in addition to the manual (or at least gpt4) results.\n\nquestions_for_the_authors: 1. Tables A1 and A2 do seem to capture some correlation between Pole Seed Words and simulation objectives but also noise a lot. Is there any direct evidence of the exaggeration phenomenon (e.g., some complete transcripts of conversations), and how does your method successfully detect on them? \n2. The correlation between your proposed metrics and the phenomenon of exaggeration does not appear to have been carefully verified. How to demonstrate the relationship between preference for particular words and the model bias\uff1f \n3. How do these metrics work on more complex simulation tasks, I would guess that more complex simulations (e.g. a specific historical character) would be more independent of wordings.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "USkmyx916z",
        "length": 416,
        "human_text": "paper_topic_and_main_contributions: This work introduces a framework, CoMPosT, for characterizing and evaluating caricature (individuation and exaggeration) in LLM simulations. In doing so, the authors propose two different metrics for measuring individuation and exaggeration.  Specifically, they evaluate simulations in the online forum and interview context and find certain demographics and topics are more susceptible to caricature. \nThe main contribution of this paper is to offer a shared language to the field to critique problematic applications of LLM simulations.\n\nreasons_to_accept: - This paper is well written and explained. The motivation to characterize and evaluate caricature in LLM simulations is very clear and important.  - The definition of caricature is sensible and informative. It is important to distinguish caricature and exaggeration as the authors mention in the paper: a caricature not only exaggerates particular features of the subject but also exaggerates in a manner that meaningfully differentiates the subject from others.  - The experiments are quite solid. The authors consider different contexts in LLM simulations such as online forum, interview, and twitter. Besides, they consider a reasonable amount of combinations of persona and topic. The empirical results are convincing.  - The recommendations proposed by the authors are helpful for researchers and stakeholders.\n\nreasons_to_reject: - This paper has a lack of error analysis about false positive cases that seem acceptable and caricature-free based on CoMPosT.  - A power analysis may be needed to determine the number of examples for each simulation setting. In the paper, the authors choose to generate 100 outputs but it is a little unclear where the number comes from.  - It would be nice to include explanations about the differences in individuation scores in various contexts. In Figure 4, the mean individuation scores in the online forum context are consistently lower than in the interview context across all personas.\n\nquestions_for_the_authors: - Question A: For reproducibility purposes, could you please elaborate on the setup of the binary classifier used in measuring individuation?  - Question B: Have you ever considered using persona-topic semantic axes to measure individuation? It would be better if we have the metrics for individuation and exaggeration defined in a comparable way.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "JngkBn0pU1",
        "length": 361,
        "human_text": "paper_topic_and_main_contributions: Topic: caricature detection in LLM Main contribution: formal formulation of caricature, a novel caricature detection method based on two metrics (individuation and exaggeration), experiment to show how the proposed method can be used to quantify individuation and exaggeration for a variety of personas in two different contexts (online forums and interview), and result that individuation is less helpful than exaggeration in this task.\n\nreasons_to_accept: - Clear formulation of caricature - Novel methods to detect caricature based on two dimensions  - Interesting explanation of experimental results (e.g. marginalized groups are subject to higher exaggeration and thus caricature) - Extended works for other platforms, e.g. Twitter, in the appendix\n\nreasons_to_reject: My only major concern is the lack of evaluation to show the validity of their proposed method in Figure 3 with respect to the caricature detection task (e.g. whether it is valid to use classifier accuracy between default persona and simulation S as a proxy for individuation; whether it is valid to use similarity between simulation S and persona-topic semantic axes) as there seems to be no ground truth for caricature to compare their results with. The experiment seems to be based on the assumption that their method is already valid, and the experiment is just there to show how their method helps social science findings (e.g. which groups are subject to caricature). It will be great if the authors may construct (or use) a small human-annotated dataset of caricatures to further evaluate their proposed method.\n\nquestions_for_the_authors: A: Could you please elaborate on the rationale behind choosing the two contexts: Online Forum vs. Interview (e.g. which contrast you want to make)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "BxumWKeELU",
        "length": 556,
        "human_text": "paper_topic_and_main_contributions: The paper presents a novel framework for identifying and measuring the extent of caricatures in LLM generations/simulations. In their proposed framework, the paper introduces 4 major dimensions: model, context, persona, and topic. The paper defines persona as demographic attributes for a group of people but I believe it can extend to more nuanced views as well. The paper motivates the problem by showcasing examples of how a persona can be exaggerated while simulating a specific topic. Finally, the paper proposes several metrics to measure individuation and exaggeration in LLM generations and finds out several scenarios where LLM simulations are caricatures.\n\nreasons_to_accept: 1. The paper introduces a novel framework to measure caricature, which is a nuanced way of how LLMs can be biased in an open-ended generation setup. \n2. With the growing adaptability of LLMs, measuring caricature is a timely and important problem to be addressed. \n3. The paper shares several recommendations for measuring and mitigating caricatures in LLM simulations in the future.\n\nreasons_to_reject: - The experiments were conducted on a single LLM GPT 4. Although it is one of the popular LLMs with API access, it would be interesting to see small-scale simulations with other commercial or non-commercial LLMs.\n- The description of some of the figures can be improved. \n    * What do 20, 40, 80 refer to in Figure 4? Later I find that these numbers correspond to a person\u2019s age. \n    * It would be nice to label Figure 5 components into subfigures 5(a), 5(b), etc. It is difficult to follow the text (for example in section 6.2.2) at times and figure out which components are being discussed.\n- Some sections of the evaluation are a bit confusing to me. Please find the details in the questions section below.\n\nquestions_for_the_authors: - Measuring individuation: For paper and analysis, using the classifier seems reasonable. I would like to know if the authors had any suggestions for users trying to measure the extent of individuation for a small set of samples. Also, there is a chance that BERT based classifier overfits some characteristic words or starting words from the default simulation. \n    * Will any unsupervised metric like V-measure be helpful to figure out how separable the two sets are? \n    * It would be interesting to have any such unsupervised metric being compared with the classifier performance.  - Measuring exaggeration: I understand that the paper reuses the framework of Lucy et al, 2022 to measure exaggeration. But the intuition behind the measure is a bit unclear to me. It would be great if the authors could clarify the following:     * From my understanding, the generated simulation shouldn\u2019t lean much toward either the persona or topic-related words. For this why can\u2019t we measure the cosine similarity with E[p_i] and E[t_i], and form a normalized score of its sum? Specifically, I\u2019m confused about why subtraction is required in the definition of $V_{p, t}$.     * What does $S^i_{p,t,c}$ refer to? Description of the superscript $i$ is missing?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "217_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_217_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.74405,
      "max_similarity": 0.7653,
      "avg_coverage": 0.472,
      "max_coverage": 0.6
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 488,
      "avg_human_length": 443.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 6,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "dMfs0VsPQv",
        "similarity": 0.749,
        "coverage": 0.3333,
        "human_length": 439,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a formal framework for LLM simulations and introduces the caricature phenomena from the individuation and exaggeration side. The evaluation results from existing works on LLM simulations demonstrate that simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.\nMain Contributions: 1. A new framework for characterizing the dimensions of LLM simulations. \n2. Methods (mainly some metrics) for measuring simulations\u2019 susceptibility to caricatures. \n3. An analysis of existing work on LLM simulations of certain demographics and topics.\n\nreasons_to_accept: 1. The paper provides a framework for evaluating LLM simulations, which can be useful for researchers working with language models in social science. The idea is novel and inspiring, and the findings can inform future work on improving the quality of LLM simulations and reducing the potential for caricature. \n2. The paper provides a useful overview of previous work on LLM simulations and helps to construct a new detection framework on individuation and exaggeration. \n3. This paper concludes several recommendations with abundant experimental evidence for LLM simulations.\n\nreasons_to_reject: 1. The detection framework seems to be limited to specific one-round QA formats and scenarios and may not be applicable to other situations or applications of LLM simulating. \n2. Detection (or classification) methods appear to be simple for Individuation and Exaggeration and may be too biased when based on only sentence embedding features. \n3. More complete samples of person and non-person responses should be provided to validate the accuracy of the test, in addition to the manual (or at least gpt4) results.\n\nquestions_for_the_authors: 1. Tables A1 and A2 do seem to capture some correlation between Pole Seed Words and simulation objectives but also noise a lot. Is there any direct evidence of the exaggeration phenomenon (e.g., some complete transcripts of conversations), and how does your method successfully detect on them? \n2. The correlation between your proposed metrics and the phenomenon of exaggeration does not appear to have been carefully verified. How to demonstrate the relationship between preference for particular words and the model bias\uff1f \n3. How do these metrics work on more complex simulation tasks, I would guess that more complex simulations (e.g. a specific historical character) would be more independent of wordings.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "USkmyx916z",
        "similarity": 0.7653,
        "coverage": 0.5833,
        "human_length": 416,
        "human_text": "paper_topic_and_main_contributions: This work introduces a framework, CoMPosT, for characterizing and evaluating caricature (individuation and exaggeration) in LLM simulations. In doing so, the authors propose two different metrics for measuring individuation and exaggeration.  Specifically, they evaluate simulations in the online forum and interview context and find certain demographics and topics are more susceptible to caricature. \nThe main contribution of this paper is to offer a shared language to the field to critique problematic applications of LLM simulations.\n\nreasons_to_accept: - This paper is well written and explained. The motivation to characterize and evaluate caricature in LLM simulations is very clear and important.  - The definition of caricature is sensible and informative. It is important to distinguish caricature and exaggeration as the authors mention in the paper: a caricature not only exaggerates particular features of the subject but also exaggerates in a manner that meaningfully differentiates the subject from others.  - The experiments are quite solid. The authors consider different contexts in LLM simulations such as online forum, interview, and twitter. Besides, they consider a reasonable amount of combinations of persona and topic. The empirical results are convincing.  - The recommendations proposed by the authors are helpful for researchers and stakeholders.\n\nreasons_to_reject: - This paper has a lack of error analysis about false positive cases that seem acceptable and caricature-free based on CoMPosT.  - A power analysis may be needed to determine the number of examples for each simulation setting. In the paper, the authors choose to generate 100 outputs but it is a little unclear where the number comes from.  - It would be nice to include explanations about the differences in individuation scores in various contexts. In Figure 4, the mean individuation scores in the online forum context are consistently lower than in the interview context across all personas.\n\nquestions_for_the_authors: - Question A: For reproducibility purposes, could you please elaborate on the setup of the binary classifier used in measuring individuation?  - Question B: Have you ever considered using persona-topic semantic axes to measure individuation? It would be better if we have the metrics for individuation and exaggeration defined in a comparable way.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "JngkBn0pU1",
        "similarity": 0.703,
        "coverage": 0.6,
        "human_length": 361,
        "human_text": "paper_topic_and_main_contributions: Topic: caricature detection in LLM Main contribution: formal formulation of caricature, a novel caricature detection method based on two metrics (individuation and exaggeration), experiment to show how the proposed method can be used to quantify individuation and exaggeration for a variety of personas in two different contexts (online forums and interview), and result that individuation is less helpful than exaggeration in this task.\n\nreasons_to_accept: - Clear formulation of caricature - Novel methods to detect caricature based on two dimensions  - Interesting explanation of experimental results (e.g. marginalized groups are subject to higher exaggeration and thus caricature) - Extended works for other platforms, e.g. Twitter, in the appendix\n\nreasons_to_reject: My only major concern is the lack of evaluation to show the validity of their proposed method in Figure 3 with respect to the caricature detection task (e.g. whether it is valid to use classifier accuracy between default persona and simulation S as a proxy for individuation; whether it is valid to use similarity between simulation S and persona-topic semantic axes) as there seems to be no ground truth for caricature to compare their results with. The experiment seems to be based on the assumption that their method is already valid, and the experiment is just there to show how their method helps social science findings (e.g. which groups are subject to caricature). It will be great if the authors may construct (or use) a small human-annotated dataset of caricatures to further evaluate their proposed method.\n\nquestions_for_the_authors: A: Could you please elaborate on the rationale behind choosing the two contexts: Online Forum vs. Interview (e.g. which contrast you want to make)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "BxumWKeELU",
        "similarity": 0.7589,
        "coverage": 0.3714,
        "human_length": 556,
        "human_text": "paper_topic_and_main_contributions: The paper presents a novel framework for identifying and measuring the extent of caricatures in LLM generations/simulations. In their proposed framework, the paper introduces 4 major dimensions: model, context, persona, and topic. The paper defines persona as demographic attributes for a group of people but I believe it can extend to more nuanced views as well. The paper motivates the problem by showcasing examples of how a persona can be exaggerated while simulating a specific topic. Finally, the paper proposes several metrics to measure individuation and exaggeration in LLM generations and finds out several scenarios where LLM simulations are caricatures.\n\nreasons_to_accept: 1. The paper introduces a novel framework to measure caricature, which is a nuanced way of how LLMs can be biased in an open-ended generation setup. \n2. With the growing adaptability of LLMs, measuring caricature is a timely and important problem to be addressed. \n3. The paper shares several recommendations for measuring and mitigating caricatures in LLM simulations in the future.\n\nreasons_to_reject: - The experiments were conducted on a single LLM GPT 4. Although it is one of the popular LLMs with API access, it would be interesting to see small-scale simulations with other commercial or non-commercial LLMs.\n- The description of some of the figures can be improved. \n    * What do 20, 40, 80 refer to in Figure 4? Later I find that these numbers correspond to a person\u2019s age. \n    * It would be nice to label Figure 5 components into subfigures 5(a), 5(b), etc. It is difficult to follow the text (for example in section 6.2.2) at times and figure out which components are being discussed.\n- Some sections of the evaluation are a bit confusing to me. Please find the details in the questions section below.\n\nquestions_for_the_authors: - Measuring individuation: For paper and analysis, using the classifier seems reasonable. I would like to know if the authors had any suggestions for users trying to measure the extent of individuation for a small set of samples. Also, there is a chance that BERT based classifier overfits some characteristic words or starting words from the default simulation. \n    * Will any unsupervised metric like V-measure be helpful to figure out how separable the two sets are? \n    * It would be interesting to have any such unsupervised metric being compared with the classifier performance.  - Measuring exaggeration: I understand that the paper reuses the framework of Lucy et al, 2022 to measure exaggeration. But the intuition behind the measure is a bit unclear to me. It would be great if the authors could clarify the following:     * From my understanding, the generated simulation shouldn\u2019t lean much toward either the persona or topic-related words. For this why can\u2019t we measure the cosine similarity with E[p_i] and E[t_i], and form a normalized score of its sum? Specifically, I\u2019m confused about why subtraction is required in the definition of $V_{p, t}$.     * What does $S^i_{p,t,c}$ refer to? Description of the superscript $i$ is missing?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "dMfs0VsPQv",
        "length": 439,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a formal framework for LLM simulations and introduces the caricature phenomena from the individuation and exaggeration side. The evaluation results from existing works on LLM simulations demonstrate that simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.\nMain Contributions: 1. A new framework for characterizing the dimensions of LLM simulations. \n2. Methods (mainly some metrics) for measuring simulations\u2019 susceptibility to caricatures. \n3. An analysis of existing work on LLM simulations of certain demographics and topics.\n\nreasons_to_accept: 1. The paper provides a framework for evaluating LLM simulations, which can be useful for researchers working with language models in social science. The idea is novel and inspiring, and the findings can inform future work on improving the quality of LLM simulations and reducing the potential for caricature. \n2. The paper provides a useful overview of previous work on LLM simulations and helps to construct a new detection framework on individuation and exaggeration. \n3. This paper concludes several recommendations with abundant experimental evidence for LLM simulations.\n\nreasons_to_reject: 1. The detection framework seems to be limited to specific one-round QA formats and scenarios and may not be applicable to other situations or applications of LLM simulating. \n2. Detection (or classification) methods appear to be simple for Individuation and Exaggeration and may be too biased when based on only sentence embedding features. \n3. More complete samples of person and non-person responses should be provided to validate the accuracy of the test, in addition to the manual (or at least gpt4) results.\n\nquestions_for_the_authors: 1. Tables A1 and A2 do seem to capture some correlation between Pole Seed Words and simulation objectives but also noise a lot. Is there any direct evidence of the exaggeration phenomenon (e.g., some complete transcripts of conversations), and how does your method successfully detect on them? \n2. The correlation between your proposed metrics and the phenomenon of exaggeration does not appear to have been carefully verified. How to demonstrate the relationship between preference for particular words and the model bias\uff1f \n3. How do these metrics work on more complex simulation tasks, I would guess that more complex simulations (e.g. a specific historical character) would be more independent of wordings.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "USkmyx916z",
        "length": 416,
        "human_text": "paper_topic_and_main_contributions: This work introduces a framework, CoMPosT, for characterizing and evaluating caricature (individuation and exaggeration) in LLM simulations. In doing so, the authors propose two different metrics for measuring individuation and exaggeration.  Specifically, they evaluate simulations in the online forum and interview context and find certain demographics and topics are more susceptible to caricature. \nThe main contribution of this paper is to offer a shared language to the field to critique problematic applications of LLM simulations.\n\nreasons_to_accept: - This paper is well written and explained. The motivation to characterize and evaluate caricature in LLM simulations is very clear and important.  - The definition of caricature is sensible and informative. It is important to distinguish caricature and exaggeration as the authors mention in the paper: a caricature not only exaggerates particular features of the subject but also exaggerates in a manner that meaningfully differentiates the subject from others.  - The experiments are quite solid. The authors consider different contexts in LLM simulations such as online forum, interview, and twitter. Besides, they consider a reasonable amount of combinations of persona and topic. The empirical results are convincing.  - The recommendations proposed by the authors are helpful for researchers and stakeholders.\n\nreasons_to_reject: - This paper has a lack of error analysis about false positive cases that seem acceptable and caricature-free based on CoMPosT.  - A power analysis may be needed to determine the number of examples for each simulation setting. In the paper, the authors choose to generate 100 outputs but it is a little unclear where the number comes from.  - It would be nice to include explanations about the differences in individuation scores in various contexts. In Figure 4, the mean individuation scores in the online forum context are consistently lower than in the interview context across all personas.\n\nquestions_for_the_authors: - Question A: For reproducibility purposes, could you please elaborate on the setup of the binary classifier used in measuring individuation?  - Question B: Have you ever considered using persona-topic semantic axes to measure individuation? It would be better if we have the metrics for individuation and exaggeration defined in a comparable way.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "JngkBn0pU1",
        "length": 361,
        "human_text": "paper_topic_and_main_contributions: Topic: caricature detection in LLM Main contribution: formal formulation of caricature, a novel caricature detection method based on two metrics (individuation and exaggeration), experiment to show how the proposed method can be used to quantify individuation and exaggeration for a variety of personas in two different contexts (online forums and interview), and result that individuation is less helpful than exaggeration in this task.\n\nreasons_to_accept: - Clear formulation of caricature - Novel methods to detect caricature based on two dimensions  - Interesting explanation of experimental results (e.g. marginalized groups are subject to higher exaggeration and thus caricature) - Extended works for other platforms, e.g. Twitter, in the appendix\n\nreasons_to_reject: My only major concern is the lack of evaluation to show the validity of their proposed method in Figure 3 with respect to the caricature detection task (e.g. whether it is valid to use classifier accuracy between default persona and simulation S as a proxy for individuation; whether it is valid to use similarity between simulation S and persona-topic semantic axes) as there seems to be no ground truth for caricature to compare their results with. The experiment seems to be based on the assumption that their method is already valid, and the experiment is just there to show how their method helps social science findings (e.g. which groups are subject to caricature). It will be great if the authors may construct (or use) a small human-annotated dataset of caricatures to further evaluate their proposed method.\n\nquestions_for_the_authors: A: Could you please elaborate on the rationale behind choosing the two contexts: Online Forum vs. Interview (e.g. which contrast you want to make)?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "BxumWKeELU",
        "length": 556,
        "human_text": "paper_topic_and_main_contributions: The paper presents a novel framework for identifying and measuring the extent of caricatures in LLM generations/simulations. In their proposed framework, the paper introduces 4 major dimensions: model, context, persona, and topic. The paper defines persona as demographic attributes for a group of people but I believe it can extend to more nuanced views as well. The paper motivates the problem by showcasing examples of how a persona can be exaggerated while simulating a specific topic. Finally, the paper proposes several metrics to measure individuation and exaggeration in LLM generations and finds out several scenarios where LLM simulations are caricatures.\n\nreasons_to_accept: 1. The paper introduces a novel framework to measure caricature, which is a nuanced way of how LLMs can be biased in an open-ended generation setup. \n2. With the growing adaptability of LLMs, measuring caricature is a timely and important problem to be addressed. \n3. The paper shares several recommendations for measuring and mitigating caricatures in LLM simulations in the future.\n\nreasons_to_reject: - The experiments were conducted on a single LLM GPT 4. Although it is one of the popular LLMs with API access, it would be interesting to see small-scale simulations with other commercial or non-commercial LLMs.\n- The description of some of the figures can be improved. \n    * What do 20, 40, 80 refer to in Figure 4? Later I find that these numbers correspond to a person\u2019s age. \n    * It would be nice to label Figure 5 components into subfigures 5(a), 5(b), etc. It is difficult to follow the text (for example in section 6.2.2) at times and figure out which components are being discussed.\n- Some sections of the evaluation are a bit confusing to me. Please find the details in the questions section below.\n\nquestions_for_the_authors: - Measuring individuation: For paper and analysis, using the classifier seems reasonable. I would like to know if the authors had any suggestions for users trying to measure the extent of individuation for a small set of samples. Also, there is a chance that BERT based classifier overfits some characteristic words or starting words from the default simulation. \n    * Will any unsupervised metric like V-measure be helpful to figure out how separable the two sets are? \n    * It would be interesting to have any such unsupervised metric being compared with the classifier performance.  - Measuring exaggeration: I understand that the paper reuses the framework of Lucy et al, 2022 to measure exaggeration. But the intuition behind the measure is a bit unclear to me. It would be great if the authors could clarify the following:     * From my understanding, the generated simulation shouldn\u2019t lean much toward either the persona or topic-related words. For this why can\u2019t we measure the cosine similarity with E[p_i] and E[t_i], and form a normalized score of its sum? Specifically, I\u2019m confused about why subtraction is required in the definition of $V_{p, t}$.     * What does $S^i_{p,t,c}$ refer to? Description of the superscript $i$ is missing?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "8_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_8_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7052,
      "max_similarity": 0.7182,
      "avg_coverage": 0.5103666666666666,
      "max_coverage": 0.6
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 603,
      "avg_human_length": 360.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "0Ar4x1A9XF",
        "similarity": 0.6821,
        "coverage": 0.6,
        "human_length": 266,
        "human_text": "paper_topic_and_main_contributions: This paper performs video moment retrieval under the training of image-noun pairs and video-verb pairs. Transfer learning is applied to incorporate the alignment knowledge.\n\nreasons_to_accept: [+] Unsupervised setting for TSG [+] Sufficient experiments [+] Enhanced retrieval performances\n\nreasons_to_reject: [-] Although this paper is not the first paper to perform unsupervised TSG, the introduction is unnecessarily written as if it were the first time performing an unsupervised task. There are several citations about TSG and weakly-supervised TSG, but why did the author not cite the previous works about unsupervised TSG in Introduction. See below recent works of unsupervised TSG - Zero-shot natural language video localization. ICCV'21 - Prompt-based zero-shot video moment retrieval ACM MM'22 [-] This work is more closed to weakly-supervised training, the model trained with a video retrieval dataset and performing retrieval on the video is the process of weakly-supervised setting. Therefore this work has a fatal misunderstanding of contributions.\n[-] The figure is too complicated, which ruins the readability.\n[-] Many recent works about unsupervised TSG are not cited.\n- Zero-shot natural language video localization. ICCV'21 - Prompt-based zero-shot video moment retrieval ACM MM'22\n\nquestions_for_the_authors: see weakness\n\nmissing_references: see weakness\n\ntypos_grammar_style_and_presentation_improvements: presentations are too complicated.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 1: Poor: This study is not yet sufficiently thorough to warrant publication or is not relevant to EMNLP.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "wpeMlizB0Q",
        "similarity": 0.7182,
        "coverage": 0.4828,
        "human_length": 405,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a novel approach for unsupervised temporal sentence grounding (TSG) without relying on expensive video-query annotations. The proposed Cross Modal Knowledge Transferring (CMKT) network leverages simple cross-modal alignment knowledge from other tasks to model appearance and action information in videos. The extracted knowledge is then fine-tuned and transferred to the TSG task for inference without further training. Extensive experiments on two challenging datasets demonstrate the effectiveness of the proposed approach, outperforming existing unsupervised methods and even competitively beating supervised methods.\n\nreasons_to_accept: 1. The proposed approach leverages simple and cheap cross-modal alignment knowledge from other tasks to model appearance and action information in videos, which significantly reduces the reliance on expensive video-query annotations and the need for large-scale training data.\n2. The proposed approach introduces a novel copy-paste approach to synthesize various multi-action clips and improve the generalization ability of the model, which enables it to handle complicated videos with multiple action contexts.\n3. The proposed approach achieves state-of-the-art performance on two challenging datasets, outperforming existing unsupervised TSG methods and even competing with supervised methods, demonstrating its effectiveness and potential for practical applications.\n\nreasons_to_reject: 1. The content of the model diagram is too complicated, and the font is too small, which greatly affects readability.\n2. It can be observed that the methodology section occupies a large portion of the paper, containing a significant amount of content with insufficient emphasis on key information. It might be worth considering highlighting the crucial information and moving some of the content to the appendix.\n3. The methodology involves a highly complex process, and the model lacks simplicity.\n4. There is a lack of organization and analysis of important references in the paper.\n\nmissing_references: [1] Mun J, Cho M, Han B. Local-global video-text interactions for temporal grounding[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 10810-10819.\n[2] Li M, Wang T, Zhang H, et al. End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding[C]//Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022: 8707-8717.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "lUVnb3CYFy",
        "similarity": 0.7153,
        "coverage": 0.4483,
        "human_length": 409,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the task of unsupervised temporal sentence grounding (TSG). The main contributions of this paper are as follows: 1. This work is the first attempt to use transfer learning knowledge to tackle the unsupervised temporal sentence grounding task. It eliminates the need for any annotations related to TSG data and simplifies the training process. \n2. The authors adopted carefully designed techniques to acquire and utilize cross-modal knowledge obtained through transfer learning, aiming to achieve better generalization performance. \n3. The model performs well on both datasets, especially in the unsupervised task setting. It also demonstrates competitive performance compared to some models that adopt weakly supervised approaches.\n\nreasons_to_accept: 1. There has been limited research on unsupervised temporal sentence grounding in the past. Previous works often focused on meticulously designing models and dealing with complex training processes. However, this work takes a different approach by leveraging transfer learning, which directly eliminates the need for an extensive training process. \n2. This article\u2019s approach involves decomposing the original task, enabling the direct transfer of knowledge from other simpler tasks to the unsupervised Temporal Sentence Grounding (TSG) task. During the knowledge acquisition stage, the authors introduced several techniques to enhance the generalization of the acquired knowledge. In the knowledge inference stage, they also addressed the issue of knowledge applicability and proposed specific optimization measures to utilize the knowledge effectively. \n3. The experimental section of this work is comprehensive, and the model\u2019s performance shows significant improvements. Moreover, various ablation experiments conducted in the study demonstrate the effectiveness of each proposed design.\n\nreasons_to_reject: 1. Although this work eliminates the complex training process for Temporal Sentence Grounding (TSG), there will still be some degree of training cost during the stages of acquiring transfer knowledge and improving knowledge applicability, both in terms of computational resources and time cost. So, compared to the original training process, does it show improvement? \n2. The model framework diagram presented in the paper is not sufficiently clear and aesthetically pleasing. There is an excessive use of symbols instead of explicit explanations, which significantly impacts the reading experience.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "0Ar4x1A9XF",
        "length": 266,
        "human_text": "paper_topic_and_main_contributions: This paper performs video moment retrieval under the training of image-noun pairs and video-verb pairs. Transfer learning is applied to incorporate the alignment knowledge.\n\nreasons_to_accept: [+] Unsupervised setting for TSG [+] Sufficient experiments [+] Enhanced retrieval performances\n\nreasons_to_reject: [-] Although this paper is not the first paper to perform unsupervised TSG, the introduction is unnecessarily written as if it were the first time performing an unsupervised task. There are several citations about TSG and weakly-supervised TSG, but why did the author not cite the previous works about unsupervised TSG in Introduction. See below recent works of unsupervised TSG - Zero-shot natural language video localization. ICCV'21 - Prompt-based zero-shot video moment retrieval ACM MM'22 [-] This work is more closed to weakly-supervised training, the model trained with a video retrieval dataset and performing retrieval on the video is the process of weakly-supervised setting. Therefore this work has a fatal misunderstanding of contributions.\n[-] The figure is too complicated, which ruins the readability.\n[-] Many recent works about unsupervised TSG are not cited.\n- Zero-shot natural language video localization. ICCV'21 - Prompt-based zero-shot video moment retrieval ACM MM'22\n\nquestions_for_the_authors: see weakness\n\nmissing_references: see weakness\n\ntypos_grammar_style_and_presentation_improvements: presentations are too complicated.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 1: Poor: This study is not yet sufficiently thorough to warrant publication or is not relevant to EMNLP.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "wpeMlizB0Q",
        "length": 405,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a novel approach for unsupervised temporal sentence grounding (TSG) without relying on expensive video-query annotations. The proposed Cross Modal Knowledge Transferring (CMKT) network leverages simple cross-modal alignment knowledge from other tasks to model appearance and action information in videos. The extracted knowledge is then fine-tuned and transferred to the TSG task for inference without further training. Extensive experiments on two challenging datasets demonstrate the effectiveness of the proposed approach, outperforming existing unsupervised methods and even competitively beating supervised methods.\n\nreasons_to_accept: 1. The proposed approach leverages simple and cheap cross-modal alignment knowledge from other tasks to model appearance and action information in videos, which significantly reduces the reliance on expensive video-query annotations and the need for large-scale training data.\n2. The proposed approach introduces a novel copy-paste approach to synthesize various multi-action clips and improve the generalization ability of the model, which enables it to handle complicated videos with multiple action contexts.\n3. The proposed approach achieves state-of-the-art performance on two challenging datasets, outperforming existing unsupervised TSG methods and even competing with supervised methods, demonstrating its effectiveness and potential for practical applications.\n\nreasons_to_reject: 1. The content of the model diagram is too complicated, and the font is too small, which greatly affects readability.\n2. It can be observed that the methodology section occupies a large portion of the paper, containing a significant amount of content with insufficient emphasis on key information. It might be worth considering highlighting the crucial information and moving some of the content to the appendix.\n3. The methodology involves a highly complex process, and the model lacks simplicity.\n4. There is a lack of organization and analysis of important references in the paper.\n\nmissing_references: [1] Mun J, Cho M, Han B. Local-global video-text interactions for temporal grounding[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 10810-10819.\n[2] Li M, Wang T, Zhang H, et al. End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding[C]//Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022: 8707-8717.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "lUVnb3CYFy",
        "length": 409,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the task of unsupervised temporal sentence grounding (TSG). The main contributions of this paper are as follows: 1. This work is the first attempt to use transfer learning knowledge to tackle the unsupervised temporal sentence grounding task. It eliminates the need for any annotations related to TSG data and simplifies the training process. \n2. The authors adopted carefully designed techniques to acquire and utilize cross-modal knowledge obtained through transfer learning, aiming to achieve better generalization performance. \n3. The model performs well on both datasets, especially in the unsupervised task setting. It also demonstrates competitive performance compared to some models that adopt weakly supervised approaches.\n\nreasons_to_accept: 1. There has been limited research on unsupervised temporal sentence grounding in the past. Previous works often focused on meticulously designing models and dealing with complex training processes. However, this work takes a different approach by leveraging transfer learning, which directly eliminates the need for an extensive training process. \n2. This article\u2019s approach involves decomposing the original task, enabling the direct transfer of knowledge from other simpler tasks to the unsupervised Temporal Sentence Grounding (TSG) task. During the knowledge acquisition stage, the authors introduced several techniques to enhance the generalization of the acquired knowledge. In the knowledge inference stage, they also addressed the issue of knowledge applicability and proposed specific optimization measures to utilize the knowledge effectively. \n3. The experimental section of this work is comprehensive, and the model\u2019s performance shows significant improvements. Moreover, various ablation experiments conducted in the study demonstrate the effectiveness of each proposed design.\n\nreasons_to_reject: 1. Although this work eliminates the complex training process for Temporal Sentence Grounding (TSG), there will still be some degree of training cost during the stages of acquiring transfer knowledge and improving knowledge applicability, both in terms of computational resources and time cost. So, compared to the original training process, does it show improvement? \n2. The model framework diagram presented in the paper is not sufficiently clear and aesthetically pleasing. There is an excessive use of symbols instead of explicit explanations, which significantly impacts the reading experience.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "8_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_8_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7071000000000001,
      "max_similarity": 0.7195,
      "avg_coverage": 0.4988666666666666,
      "max_coverage": 0.6
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 586,
      "avg_human_length": 360.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "0Ar4x1A9XF",
        "similarity": 0.6852,
        "coverage": 0.6,
        "human_length": 266,
        "human_text": "paper_topic_and_main_contributions: This paper performs video moment retrieval under the training of image-noun pairs and video-verb pairs. Transfer learning is applied to incorporate the alignment knowledge.\n\nreasons_to_accept: [+] Unsupervised setting for TSG [+] Sufficient experiments [+] Enhanced retrieval performances\n\nreasons_to_reject: [-] Although this paper is not the first paper to perform unsupervised TSG, the introduction is unnecessarily written as if it were the first time performing an unsupervised task. There are several citations about TSG and weakly-supervised TSG, but why did the author not cite the previous works about unsupervised TSG in Introduction. See below recent works of unsupervised TSG - Zero-shot natural language video localization. ICCV'21 - Prompt-based zero-shot video moment retrieval ACM MM'22 [-] This work is more closed to weakly-supervised training, the model trained with a video retrieval dataset and performing retrieval on the video is the process of weakly-supervised setting. Therefore this work has a fatal misunderstanding of contributions.\n[-] The figure is too complicated, which ruins the readability.\n[-] Many recent works about unsupervised TSG are not cited.\n- Zero-shot natural language video localization. ICCV'21 - Prompt-based zero-shot video moment retrieval ACM MM'22\n\nquestions_for_the_authors: see weakness\n\nmissing_references: see weakness\n\ntypos_grammar_style_and_presentation_improvements: presentations are too complicated.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 1: Poor: This study is not yet sufficiently thorough to warrant publication or is not relevant to EMNLP.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "wpeMlizB0Q",
        "similarity": 0.7195,
        "coverage": 0.4828,
        "human_length": 405,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a novel approach for unsupervised temporal sentence grounding (TSG) without relying on expensive video-query annotations. The proposed Cross Modal Knowledge Transferring (CMKT) network leverages simple cross-modal alignment knowledge from other tasks to model appearance and action information in videos. The extracted knowledge is then fine-tuned and transferred to the TSG task for inference without further training. Extensive experiments on two challenging datasets demonstrate the effectiveness of the proposed approach, outperforming existing unsupervised methods and even competitively beating supervised methods.\n\nreasons_to_accept: 1. The proposed approach leverages simple and cheap cross-modal alignment knowledge from other tasks to model appearance and action information in videos, which significantly reduces the reliance on expensive video-query annotations and the need for large-scale training data.\n2. The proposed approach introduces a novel copy-paste approach to synthesize various multi-action clips and improve the generalization ability of the model, which enables it to handle complicated videos with multiple action contexts.\n3. The proposed approach achieves state-of-the-art performance on two challenging datasets, outperforming existing unsupervised TSG methods and even competing with supervised methods, demonstrating its effectiveness and potential for practical applications.\n\nreasons_to_reject: 1. The content of the model diagram is too complicated, and the font is too small, which greatly affects readability.\n2. It can be observed that the methodology section occupies a large portion of the paper, containing a significant amount of content with insufficient emphasis on key information. It might be worth considering highlighting the crucial information and moving some of the content to the appendix.\n3. The methodology involves a highly complex process, and the model lacks simplicity.\n4. There is a lack of organization and analysis of important references in the paper.\n\nmissing_references: [1] Mun J, Cho M, Han B. Local-global video-text interactions for temporal grounding[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 10810-10819.\n[2] Li M, Wang T, Zhang H, et al. End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding[C]//Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022: 8707-8717.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "lUVnb3CYFy",
        "similarity": 0.7166,
        "coverage": 0.4138,
        "human_length": 409,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the task of unsupervised temporal sentence grounding (TSG). The main contributions of this paper are as follows: 1. This work is the first attempt to use transfer learning knowledge to tackle the unsupervised temporal sentence grounding task. It eliminates the need for any annotations related to TSG data and simplifies the training process. \n2. The authors adopted carefully designed techniques to acquire and utilize cross-modal knowledge obtained through transfer learning, aiming to achieve better generalization performance. \n3. The model performs well on both datasets, especially in the unsupervised task setting. It also demonstrates competitive performance compared to some models that adopt weakly supervised approaches.\n\nreasons_to_accept: 1. There has been limited research on unsupervised temporal sentence grounding in the past. Previous works often focused on meticulously designing models and dealing with complex training processes. However, this work takes a different approach by leveraging transfer learning, which directly eliminates the need for an extensive training process. \n2. This article\u2019s approach involves decomposing the original task, enabling the direct transfer of knowledge from other simpler tasks to the unsupervised Temporal Sentence Grounding (TSG) task. During the knowledge acquisition stage, the authors introduced several techniques to enhance the generalization of the acquired knowledge. In the knowledge inference stage, they also addressed the issue of knowledge applicability and proposed specific optimization measures to utilize the knowledge effectively. \n3. The experimental section of this work is comprehensive, and the model\u2019s performance shows significant improvements. Moreover, various ablation experiments conducted in the study demonstrate the effectiveness of each proposed design.\n\nreasons_to_reject: 1. Although this work eliminates the complex training process for Temporal Sentence Grounding (TSG), there will still be some degree of training cost during the stages of acquiring transfer knowledge and improving knowledge applicability, both in terms of computational resources and time cost. So, compared to the original training process, does it show improvement? \n2. The model framework diagram presented in the paper is not sufficiently clear and aesthetically pleasing. There is an excessive use of symbols instead of explicit explanations, which significantly impacts the reading experience.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "0Ar4x1A9XF",
        "length": 266,
        "human_text": "paper_topic_and_main_contributions: This paper performs video moment retrieval under the training of image-noun pairs and video-verb pairs. Transfer learning is applied to incorporate the alignment knowledge.\n\nreasons_to_accept: [+] Unsupervised setting for TSG [+] Sufficient experiments [+] Enhanced retrieval performances\n\nreasons_to_reject: [-] Although this paper is not the first paper to perform unsupervised TSG, the introduction is unnecessarily written as if it were the first time performing an unsupervised task. There are several citations about TSG and weakly-supervised TSG, but why did the author not cite the previous works about unsupervised TSG in Introduction. See below recent works of unsupervised TSG - Zero-shot natural language video localization. ICCV'21 - Prompt-based zero-shot video moment retrieval ACM MM'22 [-] This work is more closed to weakly-supervised training, the model trained with a video retrieval dataset and performing retrieval on the video is the process of weakly-supervised setting. Therefore this work has a fatal misunderstanding of contributions.\n[-] The figure is too complicated, which ruins the readability.\n[-] Many recent works about unsupervised TSG are not cited.\n- Zero-shot natural language video localization. ICCV'21 - Prompt-based zero-shot video moment retrieval ACM MM'22\n\nquestions_for_the_authors: see weakness\n\nmissing_references: see weakness\n\ntypos_grammar_style_and_presentation_improvements: presentations are too complicated.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 1: Poor: This study is not yet sufficiently thorough to warrant publication or is not relevant to EMNLP.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "wpeMlizB0Q",
        "length": 405,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a novel approach for unsupervised temporal sentence grounding (TSG) without relying on expensive video-query annotations. The proposed Cross Modal Knowledge Transferring (CMKT) network leverages simple cross-modal alignment knowledge from other tasks to model appearance and action information in videos. The extracted knowledge is then fine-tuned and transferred to the TSG task for inference without further training. Extensive experiments on two challenging datasets demonstrate the effectiveness of the proposed approach, outperforming existing unsupervised methods and even competitively beating supervised methods.\n\nreasons_to_accept: 1. The proposed approach leverages simple and cheap cross-modal alignment knowledge from other tasks to model appearance and action information in videos, which significantly reduces the reliance on expensive video-query annotations and the need for large-scale training data.\n2. The proposed approach introduces a novel copy-paste approach to synthesize various multi-action clips and improve the generalization ability of the model, which enables it to handle complicated videos with multiple action contexts.\n3. The proposed approach achieves state-of-the-art performance on two challenging datasets, outperforming existing unsupervised TSG methods and even competing with supervised methods, demonstrating its effectiveness and potential for practical applications.\n\nreasons_to_reject: 1. The content of the model diagram is too complicated, and the font is too small, which greatly affects readability.\n2. It can be observed that the methodology section occupies a large portion of the paper, containing a significant amount of content with insufficient emphasis on key information. It might be worth considering highlighting the crucial information and moving some of the content to the appendix.\n3. The methodology involves a highly complex process, and the model lacks simplicity.\n4. There is a lack of organization and analysis of important references in the paper.\n\nmissing_references: [1] Mun J, Cho M, Han B. Local-global video-text interactions for temporal grounding[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 10810-10819.\n[2] Li M, Wang T, Zhang H, et al. End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding[C]//Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022: 8707-8717.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "lUVnb3CYFy",
        "length": 409,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the task of unsupervised temporal sentence grounding (TSG). The main contributions of this paper are as follows: 1. This work is the first attempt to use transfer learning knowledge to tackle the unsupervised temporal sentence grounding task. It eliminates the need for any annotations related to TSG data and simplifies the training process. \n2. The authors adopted carefully designed techniques to acquire and utilize cross-modal knowledge obtained through transfer learning, aiming to achieve better generalization performance. \n3. The model performs well on both datasets, especially in the unsupervised task setting. It also demonstrates competitive performance compared to some models that adopt weakly supervised approaches.\n\nreasons_to_accept: 1. There has been limited research on unsupervised temporal sentence grounding in the past. Previous works often focused on meticulously designing models and dealing with complex training processes. However, this work takes a different approach by leveraging transfer learning, which directly eliminates the need for an extensive training process. \n2. This article\u2019s approach involves decomposing the original task, enabling the direct transfer of knowledge from other simpler tasks to the unsupervised Temporal Sentence Grounding (TSG) task. During the knowledge acquisition stage, the authors introduced several techniques to enhance the generalization of the acquired knowledge. In the knowledge inference stage, they also addressed the issue of knowledge applicability and proposed specific optimization measures to utilize the knowledge effectively. \n3. The experimental section of this work is comprehensive, and the model\u2019s performance shows significant improvements. Moreover, various ablation experiments conducted in the study demonstrate the effectiveness of each proposed design.\n\nreasons_to_reject: 1. Although this work eliminates the complex training process for Temporal Sentence Grounding (TSG), there will still be some degree of training cost during the stages of acquiring transfer knowledge and improving knowledge applicability, both in terms of computational resources and time cost. So, compared to the original training process, does it show improvement? \n2. The model framework diagram presented in the paper is not sufficiently clear and aesthetically pleasing. There is an excessive use of symbols instead of explicit explanations, which significantly impacts the reading experience.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "70_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_70_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7330666666666666,
      "max_similarity": 0.7451,
      "avg_coverage": 0.525,
      "max_coverage": 0.8571
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 481,
      "avg_human_length": 362.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 7,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "T9HlKDT9T5",
        "similarity": 0.7451,
        "coverage": 0.3333,
        "human_length": 420,
        "human_text": "paper_topic_and_main_contributions: This paper studies how to improve the chain of thought (CoT) prompting to enhance the reasoning abilities of large language models (LLMs). The main contribution of this paper is the proposal and demonstration of the self-verification abilities of large language models. Specifically, the authors introduce a self-verification method that includes a forward reasoning generation and a backward verification. Forward Reasoning involves LLM reasoners generating candidate answers using CoT.  In Backward Verification, the original condition is masked, and its result is predicted using another CoT. Candidate conclusions are ranked based on a verification score that assesses the consistency between the predicted and original condition values. The paper also presents experimental results that demonstrate the effectiveness of the proposed method in solving arithmetic, commonsense, and logical reasoning tasks.\n\nreasons_to_accept: 1) This paper improves the chain of thought (CoT) prompting by introducing backward verification. \n2) The proposed method allows LLMs to verify whether the generated CoTs are correct or not, simulating human thinking and verifying processes in solving complex reasoning problems. \n3) Experimental results demonstrate the effectiveness of the backward verification on arithmetic, commonsense, and logical reasoning tasks.\n\nreasons_to_reject: 1) The improvement over CoT baselines, especially Self-Consistency Decoding CoTs, is not very significant. With Self-Consistency Decoding, the average improvement is usually around 0.5%, which may limit the real application of the proposed method considering the higher computing usage of backward verification. \n2) The method does not work very effectively on general reasoning tasks compared with mathematic reasoning. \n3) Lack of deep analysis on when back verification would work and when would not. From the current results, we can only conclude that the proposed method may work better in mathematical reasoning tasks. One possible reason is the masked conditions might be more effective than True-False Item Verification. Some deep qualitative analysis is needed here, which may also further help to know how to generalize similar verification approaches to other reasoning tasks beyond arithmetic reasoning.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "QgzXGxg7Cn",
        "similarity": 0.7232,
        "coverage": 0.8571,
        "human_length": 157,
        "human_text": "paper_topic_and_main_contributions: This work proposes to let LLMs self-verify through three steps: mask the values provided in the question; treat the answers generated in the forward inference as known conditions; ask the LLM to answer the backward question, that is, provide the masked value.\n\nreasons_to_accept: The self-verification method is novel, and verification results can improve performance.\n\nreasons_to_reject: Lack of analysis of self-verification accuracy, e.g. LLMs can return wrong answers to forward questions and also to generated backward questions; The proposed method proceeds in an iterative manner resulting in high cost of querying the LLM.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "oVmw6YM9rL",
        "similarity": 0.7309,
        "coverage": 0.3846,
        "human_length": 510,
        "human_text": "paper_topic_and_main_contributions: This paper analyzes the problems that current LLMs is prone to in multi-step reasoning, that is, it is sensitive to a certain error and easy to cause error accumulation. The authors propose a method of backward verification after forward reasoning, which alleviates this phenomenon and verifies that LLMs itself has the ability to self-verify the generated results.\nThe main contributions of this paper are: (1) The ability of LLMs to self-verify results with the help of specific methods is verified; (2) The feasibility of the proposed method is verified on multiple LLMs and multiple mathematical, commonsense, and logical reasoning datasets. ( 3) Two methods of self-validation are proposed. True-False Item Verification for General Tasks in the backward verification stage and Condition Mask Verification based on the characteristics of Arithmetic Tasks.\n\nreasons_to_accept: 1. The authors analyze the problem with the current method of inference using the prompting method, that is, LLMs may be sensitive to a certain error in multi-step inference and accumulate errors in subsequent inference.\n2. The authors propose a method to solve this problem by using the self-verification ability of the model to enhance the reasoning ability of the model, which includes two stages: forward reasoning and backward verification.\n3. The authors clearly show the process of their proposed method (Fig. 2).\n4. The author conduct a detailed analytical experiment (Sec.5).\n5. The method proposed by the author can be combined with some existing methods (CoT, self-consistency (SC) and PAL are demonstrated in the paper).\n\nreasons_to_reject: 1. The benefits from backward process are not analyzed separately (after all, the forward reasoning process is like getting benefits from self-consistency).\n2. The models used in the experiment are slightly outdated, and the experiment lacks analysis of new models, such as text-davinci-003 and gpt-3.5-turbo, and even gpt-4\n\nquestions_for_the_authors: Q1: The proposed method includes forward reasoning and backward verification, wherein K candidate answers are generated in the forward reasoning process, and P times of reverse verification will be performed for each answer. Is there any deduplication process in the process of obtaining K candidate answers?\nQ2: If the answer to Q1 is no, can it be understood that this process of forward reasoning is equivalent to a self-consistency? ( Equivalent to the effect shown by this method to a large extent with the strength of the idea of self-consistency, so this also explains why the effect is weak when SV and SC methods are combined)\n\ntypos_grammar_style_and_presentation_improvements: Line 444: \"PoL\" should be \"PAL\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "T9HlKDT9T5",
        "length": 420,
        "human_text": "paper_topic_and_main_contributions: This paper studies how to improve the chain of thought (CoT) prompting to enhance the reasoning abilities of large language models (LLMs). The main contribution of this paper is the proposal and demonstration of the self-verification abilities of large language models. Specifically, the authors introduce a self-verification method that includes a forward reasoning generation and a backward verification. Forward Reasoning involves LLM reasoners generating candidate answers using CoT.  In Backward Verification, the original condition is masked, and its result is predicted using another CoT. Candidate conclusions are ranked based on a verification score that assesses the consistency between the predicted and original condition values. The paper also presents experimental results that demonstrate the effectiveness of the proposed method in solving arithmetic, commonsense, and logical reasoning tasks.\n\nreasons_to_accept: 1) This paper improves the chain of thought (CoT) prompting by introducing backward verification. \n2) The proposed method allows LLMs to verify whether the generated CoTs are correct or not, simulating human thinking and verifying processes in solving complex reasoning problems. \n3) Experimental results demonstrate the effectiveness of the backward verification on arithmetic, commonsense, and logical reasoning tasks.\n\nreasons_to_reject: 1) The improvement over CoT baselines, especially Self-Consistency Decoding CoTs, is not very significant. With Self-Consistency Decoding, the average improvement is usually around 0.5%, which may limit the real application of the proposed method considering the higher computing usage of backward verification. \n2) The method does not work very effectively on general reasoning tasks compared with mathematic reasoning. \n3) Lack of deep analysis on when back verification would work and when would not. From the current results, we can only conclude that the proposed method may work better in mathematical reasoning tasks. One possible reason is the masked conditions might be more effective than True-False Item Verification. Some deep qualitative analysis is needed here, which may also further help to know how to generalize similar verification approaches to other reasoning tasks beyond arithmetic reasoning.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "QgzXGxg7Cn",
        "length": 157,
        "human_text": "paper_topic_and_main_contributions: This work proposes to let LLMs self-verify through three steps: mask the values provided in the question; treat the answers generated in the forward inference as known conditions; ask the LLM to answer the backward question, that is, provide the masked value.\n\nreasons_to_accept: The self-verification method is novel, and verification results can improve performance.\n\nreasons_to_reject: Lack of analysis of self-verification accuracy, e.g. LLMs can return wrong answers to forward questions and also to generated backward questions; The proposed method proceeds in an iterative manner resulting in high cost of querying the LLM.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "oVmw6YM9rL",
        "length": 510,
        "human_text": "paper_topic_and_main_contributions: This paper analyzes the problems that current LLMs is prone to in multi-step reasoning, that is, it is sensitive to a certain error and easy to cause error accumulation. The authors propose a method of backward verification after forward reasoning, which alleviates this phenomenon and verifies that LLMs itself has the ability to self-verify the generated results.\nThe main contributions of this paper are: (1) The ability of LLMs to self-verify results with the help of specific methods is verified; (2) The feasibility of the proposed method is verified on multiple LLMs and multiple mathematical, commonsense, and logical reasoning datasets. ( 3) Two methods of self-validation are proposed. True-False Item Verification for General Tasks in the backward verification stage and Condition Mask Verification based on the characteristics of Arithmetic Tasks.\n\nreasons_to_accept: 1. The authors analyze the problem with the current method of inference using the prompting method, that is, LLMs may be sensitive to a certain error in multi-step inference and accumulate errors in subsequent inference.\n2. The authors propose a method to solve this problem by using the self-verification ability of the model to enhance the reasoning ability of the model, which includes two stages: forward reasoning and backward verification.\n3. The authors clearly show the process of their proposed method (Fig. 2).\n4. The author conduct a detailed analytical experiment (Sec.5).\n5. The method proposed by the author can be combined with some existing methods (CoT, self-consistency (SC) and PAL are demonstrated in the paper).\n\nreasons_to_reject: 1. The benefits from backward process are not analyzed separately (after all, the forward reasoning process is like getting benefits from self-consistency).\n2. The models used in the experiment are slightly outdated, and the experiment lacks analysis of new models, such as text-davinci-003 and gpt-3.5-turbo, and even gpt-4\n\nquestions_for_the_authors: Q1: The proposed method includes forward reasoning and backward verification, wherein K candidate answers are generated in the forward reasoning process, and P times of reverse verification will be performed for each answer. Is there any deduplication process in the process of obtaining K candidate answers?\nQ2: If the answer to Q1 is no, can it be understood that this process of forward reasoning is equivalent to a self-consistency? ( Equivalent to the effect shown by this method to a large extent with the strength of the idea of self-consistency, so this also explains why the effect is weak when SV and SC methods are combined)\n\ntypos_grammar_style_and_presentation_improvements: Line 444: \"PoL\" should be \"PAL\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "70_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_70_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7313666666666667,
      "max_similarity": 0.7435,
      "avg_coverage": 0.525,
      "max_coverage": 0.8571
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 488,
      "avg_human_length": 362.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 7,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "T9HlKDT9T5",
        "similarity": 0.7435,
        "coverage": 0.3333,
        "human_length": 420,
        "human_text": "paper_topic_and_main_contributions: This paper studies how to improve the chain of thought (CoT) prompting to enhance the reasoning abilities of large language models (LLMs). The main contribution of this paper is the proposal and demonstration of the self-verification abilities of large language models. Specifically, the authors introduce a self-verification method that includes a forward reasoning generation and a backward verification. Forward Reasoning involves LLM reasoners generating candidate answers using CoT.  In Backward Verification, the original condition is masked, and its result is predicted using another CoT. Candidate conclusions are ranked based on a verification score that assesses the consistency between the predicted and original condition values. The paper also presents experimental results that demonstrate the effectiveness of the proposed method in solving arithmetic, commonsense, and logical reasoning tasks.\n\nreasons_to_accept: 1) This paper improves the chain of thought (CoT) prompting by introducing backward verification. \n2) The proposed method allows LLMs to verify whether the generated CoTs are correct or not, simulating human thinking and verifying processes in solving complex reasoning problems. \n3) Experimental results demonstrate the effectiveness of the backward verification on arithmetic, commonsense, and logical reasoning tasks.\n\nreasons_to_reject: 1) The improvement over CoT baselines, especially Self-Consistency Decoding CoTs, is not very significant. With Self-Consistency Decoding, the average improvement is usually around 0.5%, which may limit the real application of the proposed method considering the higher computing usage of backward verification. \n2) The method does not work very effectively on general reasoning tasks compared with mathematic reasoning. \n3) Lack of deep analysis on when back verification would work and when would not. From the current results, we can only conclude that the proposed method may work better in mathematical reasoning tasks. One possible reason is the masked conditions might be more effective than True-False Item Verification. Some deep qualitative analysis is needed here, which may also further help to know how to generalize similar verification approaches to other reasoning tasks beyond arithmetic reasoning.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "QgzXGxg7Cn",
        "similarity": 0.7216,
        "coverage": 0.8571,
        "human_length": 157,
        "human_text": "paper_topic_and_main_contributions: This work proposes to let LLMs self-verify through three steps: mask the values provided in the question; treat the answers generated in the forward inference as known conditions; ask the LLM to answer the backward question, that is, provide the masked value.\n\nreasons_to_accept: The self-verification method is novel, and verification results can improve performance.\n\nreasons_to_reject: Lack of analysis of self-verification accuracy, e.g. LLMs can return wrong answers to forward questions and also to generated backward questions; The proposed method proceeds in an iterative manner resulting in high cost of querying the LLM.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "oVmw6YM9rL",
        "similarity": 0.729,
        "coverage": 0.3846,
        "human_length": 510,
        "human_text": "paper_topic_and_main_contributions: This paper analyzes the problems that current LLMs is prone to in multi-step reasoning, that is, it is sensitive to a certain error and easy to cause error accumulation. The authors propose a method of backward verification after forward reasoning, which alleviates this phenomenon and verifies that LLMs itself has the ability to self-verify the generated results.\nThe main contributions of this paper are: (1) The ability of LLMs to self-verify results with the help of specific methods is verified; (2) The feasibility of the proposed method is verified on multiple LLMs and multiple mathematical, commonsense, and logical reasoning datasets. ( 3) Two methods of self-validation are proposed. True-False Item Verification for General Tasks in the backward verification stage and Condition Mask Verification based on the characteristics of Arithmetic Tasks.\n\nreasons_to_accept: 1. The authors analyze the problem with the current method of inference using the prompting method, that is, LLMs may be sensitive to a certain error in multi-step inference and accumulate errors in subsequent inference.\n2. The authors propose a method to solve this problem by using the self-verification ability of the model to enhance the reasoning ability of the model, which includes two stages: forward reasoning and backward verification.\n3. The authors clearly show the process of their proposed method (Fig. 2).\n4. The author conduct a detailed analytical experiment (Sec.5).\n5. The method proposed by the author can be combined with some existing methods (CoT, self-consistency (SC) and PAL are demonstrated in the paper).\n\nreasons_to_reject: 1. The benefits from backward process are not analyzed separately (after all, the forward reasoning process is like getting benefits from self-consistency).\n2. The models used in the experiment are slightly outdated, and the experiment lacks analysis of new models, such as text-davinci-003 and gpt-3.5-turbo, and even gpt-4\n\nquestions_for_the_authors: Q1: The proposed method includes forward reasoning and backward verification, wherein K candidate answers are generated in the forward reasoning process, and P times of reverse verification will be performed for each answer. Is there any deduplication process in the process of obtaining K candidate answers?\nQ2: If the answer to Q1 is no, can it be understood that this process of forward reasoning is equivalent to a self-consistency? ( Equivalent to the effect shown by this method to a large extent with the strength of the idea of self-consistency, so this also explains why the effect is weak when SV and SC methods are combined)\n\ntypos_grammar_style_and_presentation_improvements: Line 444: \"PoL\" should be \"PAL\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "T9HlKDT9T5",
        "length": 420,
        "human_text": "paper_topic_and_main_contributions: This paper studies how to improve the chain of thought (CoT) prompting to enhance the reasoning abilities of large language models (LLMs). The main contribution of this paper is the proposal and demonstration of the self-verification abilities of large language models. Specifically, the authors introduce a self-verification method that includes a forward reasoning generation and a backward verification. Forward Reasoning involves LLM reasoners generating candidate answers using CoT.  In Backward Verification, the original condition is masked, and its result is predicted using another CoT. Candidate conclusions are ranked based on a verification score that assesses the consistency between the predicted and original condition values. The paper also presents experimental results that demonstrate the effectiveness of the proposed method in solving arithmetic, commonsense, and logical reasoning tasks.\n\nreasons_to_accept: 1) This paper improves the chain of thought (CoT) prompting by introducing backward verification. \n2) The proposed method allows LLMs to verify whether the generated CoTs are correct or not, simulating human thinking and verifying processes in solving complex reasoning problems. \n3) Experimental results demonstrate the effectiveness of the backward verification on arithmetic, commonsense, and logical reasoning tasks.\n\nreasons_to_reject: 1) The improvement over CoT baselines, especially Self-Consistency Decoding CoTs, is not very significant. With Self-Consistency Decoding, the average improvement is usually around 0.5%, which may limit the real application of the proposed method considering the higher computing usage of backward verification. \n2) The method does not work very effectively on general reasoning tasks compared with mathematic reasoning. \n3) Lack of deep analysis on when back verification would work and when would not. From the current results, we can only conclude that the proposed method may work better in mathematical reasoning tasks. One possible reason is the masked conditions might be more effective than True-False Item Verification. Some deep qualitative analysis is needed here, which may also further help to know how to generalize similar verification approaches to other reasoning tasks beyond arithmetic reasoning.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "QgzXGxg7Cn",
        "length": 157,
        "human_text": "paper_topic_and_main_contributions: This work proposes to let LLMs self-verify through three steps: mask the values provided in the question; treat the answers generated in the forward inference as known conditions; ask the LLM to answer the backward question, that is, provide the masked value.\n\nreasons_to_accept: The self-verification method is novel, and verification results can improve performance.\n\nreasons_to_reject: Lack of analysis of self-verification accuracy, e.g. LLMs can return wrong answers to forward questions and also to generated backward questions; The proposed method proceeds in an iterative manner resulting in high cost of querying the LLM.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "oVmw6YM9rL",
        "length": 510,
        "human_text": "paper_topic_and_main_contributions: This paper analyzes the problems that current LLMs is prone to in multi-step reasoning, that is, it is sensitive to a certain error and easy to cause error accumulation. The authors propose a method of backward verification after forward reasoning, which alleviates this phenomenon and verifies that LLMs itself has the ability to self-verify the generated results.\nThe main contributions of this paper are: (1) The ability of LLMs to self-verify results with the help of specific methods is verified; (2) The feasibility of the proposed method is verified on multiple LLMs and multiple mathematical, commonsense, and logical reasoning datasets. ( 3) Two methods of self-validation are proposed. True-False Item Verification for General Tasks in the backward verification stage and Condition Mask Verification based on the characteristics of Arithmetic Tasks.\n\nreasons_to_accept: 1. The authors analyze the problem with the current method of inference using the prompting method, that is, LLMs may be sensitive to a certain error in multi-step inference and accumulate errors in subsequent inference.\n2. The authors propose a method to solve this problem by using the self-verification ability of the model to enhance the reasoning ability of the model, which includes two stages: forward reasoning and backward verification.\n3. The authors clearly show the process of their proposed method (Fig. 2).\n4. The author conduct a detailed analytical experiment (Sec.5).\n5. The method proposed by the author can be combined with some existing methods (CoT, self-consistency (SC) and PAL are demonstrated in the paper).\n\nreasons_to_reject: 1. The benefits from backward process are not analyzed separately (after all, the forward reasoning process is like getting benefits from self-consistency).\n2. The models used in the experiment are slightly outdated, and the experiment lacks analysis of new models, such as text-davinci-003 and gpt-3.5-turbo, and even gpt-4\n\nquestions_for_the_authors: Q1: The proposed method includes forward reasoning and backward verification, wherein K candidate answers are generated in the forward reasoning process, and P times of reverse verification will be performed for each answer. Is there any deduplication process in the process of obtaining K candidate answers?\nQ2: If the answer to Q1 is no, can it be understood that this process of forward reasoning is equivalent to a self-consistency? ( Equivalent to the effect shown by this method to a large extent with the strength of the idea of self-consistency, so this also explains why the effect is weak when SV and SC methods are combined)\n\ntypos_grammar_style_and_presentation_improvements: Line 444: \"PoL\" should be \"PAL\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "100_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_100_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7101666666666667,
      "max_similarity": 0.7237,
      "avg_coverage": 0.6337,
      "max_coverage": 0.7143
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 659,
      "avg_human_length": 249.66666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "sPeIAS5MuS",
        "similarity": 0.6889,
        "coverage": 0.6154,
        "human_length": 180,
        "human_text": "paper_topic_and_main_contributions: This paper is about Adaptive Language-guided Multimodal Transformer, which incorporates an Adaptive Hyper-modality Learning module to learn an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales for multimodal sentiment analysis task. The main contributions of the paper are: Proposed a novel multimodal sentiment analysis method, namely Adaptive Language-guided Multimodal Transformer, and explored a novel Adaptive Hyper-modality Learning module for representation learning.\n\nreasons_to_accept: 1. The paper is written well. \n2. Explored ALMT with AHL module which is quite interesting. \n3. Ablation study is good.\n\nreasons_to_reject: 1. The approach looks fine, but only one minor thing is it is good to share the results different fusion techniques as well.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Gc9gy0Xnp2",
        "similarity": 0.7179,
        "coverage": 0.7143,
        "human_length": 313,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel approach to Multimodal Sentiment Analysis (MSA) by introducing the Adaptive Language-guided Multimodal Transformer (ALMT). The primary problem addressed by this paper is the potential sentiment-irrelevant and conflicting information across different modalities (language, video, and audio) that may hinder the performance of MSA. The authors propose an Adaptive Hyper-modality Learning (AHL) module that learns an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales. The paper claims that the proposed model achieves state-of-the-art performance on several popular datasets such as MOSI, MOSEI, and CH-SIMS.\n\nreasons_to_accept: 1) The paper presents a novel approach to tackle the issue of sentiment-irrelevant and conflicting information across different modalities in MSA, which is a significant contribution to the field. \n2) The proposed model, ALMT, incorporates an Adaptive Hyper-modality Learning (AHL) module, which is a novel concept that could potentially inspire future research in this area. \n3) The paper provides empirical evidence of the model's effectiveness by demonstrating state-of-the-art performance on several popular datasets.\n\nreasons_to_reject: It appears that the improvement of ALMT over the CHFN method on the MOSI dataset is quite marginal. It would be beneficial if the authors could conduct significance tests to validate the statistical significance of the observed improvements.\n\nquestions_for_the_authors: Can the authors provide more details on how the Adaptive Hyper-modality Learning (AHL) module works, specifically how it suppresses sentiment-irrelevant information? Is there any theoretical justification for the chosen architecture, especially the design of AHL in the proposed method?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "CXDSd1NENn",
        "similarity": 0.7237,
        "coverage": 0.5714,
        "human_length": 256,
        "human_text": "paper_topic_and_main_contributions: This work proposes Adaptive Language-guided Multimodal Transformer (ALMT) to better model sentiment cues for robust Multimodal Sentiment Analysis (MSA). ALMT consists of three major components, i.e., modality embedding, adaptive hyper-modality learning, and multimodal fusion. Due to effectively suppressing the adverse effects of redundant information in visual and audio modalities, the proposed method achieved highly improved performance on several popular datasets. Detail experiments and analyses are provided to prove the effectiveness of the ALMT.\n\nreasons_to_accept: - This work presents a novel multimodal sentiment analysis method, namely Adaptive Language-guided Multimodal Transformer (ALMT), which for the first time explicitly tackles the adverse effects of redundant and conflicting information in auxiliary modalities (i.e., visual and audio modalities), achieving a more robust sentiment understanding performance.\n- This work devises a novel Adaptive Hyper-modality Learning (AHL) module for representation learning. The AHL uses different scales of language features to guide the visual and audio modalities to form a hyper modality that complements the language modality.\n- State-of-the-art performance and detailed analysis in several public and widely are provided.\n\nreasons_to_reject: - Novelty limitations. This is an incremental job. It is common to use Transformers to fuse multiple modalities.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "sPeIAS5MuS",
        "length": 180,
        "human_text": "paper_topic_and_main_contributions: This paper is about Adaptive Language-guided Multimodal Transformer, which incorporates an Adaptive Hyper-modality Learning module to learn an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales for multimodal sentiment analysis task. The main contributions of the paper are: Proposed a novel multimodal sentiment analysis method, namely Adaptive Language-guided Multimodal Transformer, and explored a novel Adaptive Hyper-modality Learning module for representation learning.\n\nreasons_to_accept: 1. The paper is written well. \n2. Explored ALMT with AHL module which is quite interesting. \n3. Ablation study is good.\n\nreasons_to_reject: 1. The approach looks fine, but only one minor thing is it is good to share the results different fusion techniques as well.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "Gc9gy0Xnp2",
        "length": 313,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel approach to Multimodal Sentiment Analysis (MSA) by introducing the Adaptive Language-guided Multimodal Transformer (ALMT). The primary problem addressed by this paper is the potential sentiment-irrelevant and conflicting information across different modalities (language, video, and audio) that may hinder the performance of MSA. The authors propose an Adaptive Hyper-modality Learning (AHL) module that learns an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales. The paper claims that the proposed model achieves state-of-the-art performance on several popular datasets such as MOSI, MOSEI, and CH-SIMS.\n\nreasons_to_accept: 1) The paper presents a novel approach to tackle the issue of sentiment-irrelevant and conflicting information across different modalities in MSA, which is a significant contribution to the field. \n2) The proposed model, ALMT, incorporates an Adaptive Hyper-modality Learning (AHL) module, which is a novel concept that could potentially inspire future research in this area. \n3) The paper provides empirical evidence of the model's effectiveness by demonstrating state-of-the-art performance on several popular datasets.\n\nreasons_to_reject: It appears that the improvement of ALMT over the CHFN method on the MOSI dataset is quite marginal. It would be beneficial if the authors could conduct significance tests to validate the statistical significance of the observed improvements.\n\nquestions_for_the_authors: Can the authors provide more details on how the Adaptive Hyper-modality Learning (AHL) module works, specifically how it suppresses sentiment-irrelevant information? Is there any theoretical justification for the chosen architecture, especially the design of AHL in the proposed method?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "CXDSd1NENn",
        "length": 256,
        "human_text": "paper_topic_and_main_contributions: This work proposes Adaptive Language-guided Multimodal Transformer (ALMT) to better model sentiment cues for robust Multimodal Sentiment Analysis (MSA). ALMT consists of three major components, i.e., modality embedding, adaptive hyper-modality learning, and multimodal fusion. Due to effectively suppressing the adverse effects of redundant information in visual and audio modalities, the proposed method achieved highly improved performance on several popular datasets. Detail experiments and analyses are provided to prove the effectiveness of the ALMT.\n\nreasons_to_accept: - This work presents a novel multimodal sentiment analysis method, namely Adaptive Language-guided Multimodal Transformer (ALMT), which for the first time explicitly tackles the adverse effects of redundant and conflicting information in auxiliary modalities (i.e., visual and audio modalities), achieving a more robust sentiment understanding performance.\n- This work devises a novel Adaptive Hyper-modality Learning (AHL) module for representation learning. The AHL uses different scales of language features to guide the visual and audio modalities to form a hyper modality that complements the language modality.\n- State-of-the-art performance and detailed analysis in several public and widely are provided.\n\nreasons_to_reject: - Novelty limitations. This is an incremental job. It is common to use Transformers to fuse multiple modalities.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "100_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_100_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7091,
      "max_similarity": 0.725,
      "avg_coverage": 0.6575333333333333,
      "max_coverage": 0.7143
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 666,
      "avg_human_length": 249.66666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "sPeIAS5MuS",
        "similarity": 0.689,
        "coverage": 0.6154,
        "human_length": 180,
        "human_text": "paper_topic_and_main_contributions: This paper is about Adaptive Language-guided Multimodal Transformer, which incorporates an Adaptive Hyper-modality Learning module to learn an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales for multimodal sentiment analysis task. The main contributions of the paper are: Proposed a novel multimodal sentiment analysis method, namely Adaptive Language-guided Multimodal Transformer, and explored a novel Adaptive Hyper-modality Learning module for representation learning.\n\nreasons_to_accept: 1. The paper is written well. \n2. Explored ALMT with AHL module which is quite interesting. \n3. Ablation study is good.\n\nreasons_to_reject: 1. The approach looks fine, but only one minor thing is it is good to share the results different fusion techniques as well.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Gc9gy0Xnp2",
        "similarity": 0.7133,
        "coverage": 0.7143,
        "human_length": 313,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel approach to Multimodal Sentiment Analysis (MSA) by introducing the Adaptive Language-guided Multimodal Transformer (ALMT). The primary problem addressed by this paper is the potential sentiment-irrelevant and conflicting information across different modalities (language, video, and audio) that may hinder the performance of MSA. The authors propose an Adaptive Hyper-modality Learning (AHL) module that learns an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales. The paper claims that the proposed model achieves state-of-the-art performance on several popular datasets such as MOSI, MOSEI, and CH-SIMS.\n\nreasons_to_accept: 1) The paper presents a novel approach to tackle the issue of sentiment-irrelevant and conflicting information across different modalities in MSA, which is a significant contribution to the field. \n2) The proposed model, ALMT, incorporates an Adaptive Hyper-modality Learning (AHL) module, which is a novel concept that could potentially inspire future research in this area. \n3) The paper provides empirical evidence of the model's effectiveness by demonstrating state-of-the-art performance on several popular datasets.\n\nreasons_to_reject: It appears that the improvement of ALMT over the CHFN method on the MOSI dataset is quite marginal. It would be beneficial if the authors could conduct significance tests to validate the statistical significance of the observed improvements.\n\nquestions_for_the_authors: Can the authors provide more details on how the Adaptive Hyper-modality Learning (AHL) module works, specifically how it suppresses sentiment-irrelevant information? Is there any theoretical justification for the chosen architecture, especially the design of AHL in the proposed method?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "CXDSd1NENn",
        "similarity": 0.725,
        "coverage": 0.6429,
        "human_length": 256,
        "human_text": "paper_topic_and_main_contributions: This work proposes Adaptive Language-guided Multimodal Transformer (ALMT) to better model sentiment cues for robust Multimodal Sentiment Analysis (MSA). ALMT consists of three major components, i.e., modality embedding, adaptive hyper-modality learning, and multimodal fusion. Due to effectively suppressing the adverse effects of redundant information in visual and audio modalities, the proposed method achieved highly improved performance on several popular datasets. Detail experiments and analyses are provided to prove the effectiveness of the ALMT.\n\nreasons_to_accept: - This work presents a novel multimodal sentiment analysis method, namely Adaptive Language-guided Multimodal Transformer (ALMT), which for the first time explicitly tackles the adverse effects of redundant and conflicting information in auxiliary modalities (i.e., visual and audio modalities), achieving a more robust sentiment understanding performance.\n- This work devises a novel Adaptive Hyper-modality Learning (AHL) module for representation learning. The AHL uses different scales of language features to guide the visual and audio modalities to form a hyper modality that complements the language modality.\n- State-of-the-art performance and detailed analysis in several public and widely are provided.\n\nreasons_to_reject: - Novelty limitations. This is an incremental job. It is common to use Transformers to fuse multiple modalities.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "sPeIAS5MuS",
        "length": 180,
        "human_text": "paper_topic_and_main_contributions: This paper is about Adaptive Language-guided Multimodal Transformer, which incorporates an Adaptive Hyper-modality Learning module to learn an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales for multimodal sentiment analysis task. The main contributions of the paper are: Proposed a novel multimodal sentiment analysis method, namely Adaptive Language-guided Multimodal Transformer, and explored a novel Adaptive Hyper-modality Learning module for representation learning.\n\nreasons_to_accept: 1. The paper is written well. \n2. Explored ALMT with AHL module which is quite interesting. \n3. Ablation study is good.\n\nreasons_to_reject: 1. The approach looks fine, but only one minor thing is it is good to share the results different fusion techniques as well.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "Gc9gy0Xnp2",
        "length": 313,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel approach to Multimodal Sentiment Analysis (MSA) by introducing the Adaptive Language-guided Multimodal Transformer (ALMT). The primary problem addressed by this paper is the potential sentiment-irrelevant and conflicting information across different modalities (language, video, and audio) that may hinder the performance of MSA. The authors propose an Adaptive Hyper-modality Learning (AHL) module that learns an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales. The paper claims that the proposed model achieves state-of-the-art performance on several popular datasets such as MOSI, MOSEI, and CH-SIMS.\n\nreasons_to_accept: 1) The paper presents a novel approach to tackle the issue of sentiment-irrelevant and conflicting information across different modalities in MSA, which is a significant contribution to the field. \n2) The proposed model, ALMT, incorporates an Adaptive Hyper-modality Learning (AHL) module, which is a novel concept that could potentially inspire future research in this area. \n3) The paper provides empirical evidence of the model's effectiveness by demonstrating state-of-the-art performance on several popular datasets.\n\nreasons_to_reject: It appears that the improvement of ALMT over the CHFN method on the MOSI dataset is quite marginal. It would be beneficial if the authors could conduct significance tests to validate the statistical significance of the observed improvements.\n\nquestions_for_the_authors: Can the authors provide more details on how the Adaptive Hyper-modality Learning (AHL) module works, specifically how it suppresses sentiment-irrelevant information? Is there any theoretical justification for the chosen architecture, especially the design of AHL in the proposed method?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "CXDSd1NENn",
        "length": 256,
        "human_text": "paper_topic_and_main_contributions: This work proposes Adaptive Language-guided Multimodal Transformer (ALMT) to better model sentiment cues for robust Multimodal Sentiment Analysis (MSA). ALMT consists of three major components, i.e., modality embedding, adaptive hyper-modality learning, and multimodal fusion. Due to effectively suppressing the adverse effects of redundant information in visual and audio modalities, the proposed method achieved highly improved performance on several popular datasets. Detail experiments and analyses are provided to prove the effectiveness of the ALMT.\n\nreasons_to_accept: - This work presents a novel multimodal sentiment analysis method, namely Adaptive Language-guided Multimodal Transformer (ALMT), which for the first time explicitly tackles the adverse effects of redundant and conflicting information in auxiliary modalities (i.e., visual and audio modalities), achieving a more robust sentiment understanding performance.\n- This work devises a novel Adaptive Hyper-modality Learning (AHL) module for representation learning. The AHL uses different scales of language features to guide the visual and audio modalities to form a hyper modality that complements the language modality.\n- State-of-the-art performance and detailed analysis in several public and widely are provided.\n\nreasons_to_reject: - Novelty limitations. This is an incremental job. It is common to use Transformers to fuse multiple modalities.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "201_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_201_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7012666666666667,
      "max_similarity": 0.7214,
      "avg_coverage": 0.11193333333333333,
      "max_coverage": 0.1429
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 16,
      "avg_human_length": 380.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": false,
      "has_weaknesses": false,
      "has_suggestions": false
    },
    "content_count_metrics": {
      "strengths_count": 0,
      "weaknesses_count": 0,
      "suggestions_count": 0
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "YvLrlQHDzR",
        "similarity": 0.7214,
        "coverage": 0.1429,
        "human_length": 250,
        "human_text": "paper_topic_and_main_contributions: This paper builds a new benchmark for the Arabic grammatical error correction (GEC) area. To build the benchmark, it starts with the grammatical error detection (GED) task. In this section, the paper proposes a new alignment algorithm for error type annotation and achieves better results compared with previous methods. In the GEC section, it finds that the pre-trained model BART can be enhanced with GED and Morphological Disambiguation output, and this combination performs best among most of the settings. It also shows how GED granularity affects final GEC performance.\n\nreasons_to_accept: The paper is well-written and easy to understand; the readers who do not speak Arabic, such as me, can also understand the nature and background of Arabic GEC quickly by reading Sections 2 and 3. The newly proposed alignment algorithm and the strong baseline are essential contributions to the Arabic GEC community.\n\nreasons_to_reject: I only have one concern about the missing explanations of results. In Section 7, Line 509, the paper only describes the results. In Section 5, we can see that AraT5 has a larger pre-trained data size than AraBART but performs worse in most of the situations on the GEC dataset. It would be better to provide some explanations about this kind of result.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "QZRng7EQYl",
        "similarity": 0.6777,
        "coverage": 0.1429,
        "human_length": 246,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on investigating the effectiveness of Grammatical Error Detection (GED) on Arabic Grammatical Error Correction (GEC). For the investigation process, they depend on three datasets (QALB-2014, QALB-2015, and ZAEBUC). They claim that they were the first to benchmark newly developed pretrained Seq2Seq models on Arabic Grammatical Error Correction. As for the GED task, they used their own alignment algorithm which shows that they were superior across all metrics. They could show that using GED information in GEC models improves the performance across GEC datasets. In addition, they discussed the importance of leveraging contextual morphological preprocessing in improving the GEC performance. Their experiments achieved SOTA results on the two (L1 and L2) datasets of (QALB-2014 and QALB-21015).\n\nreasons_to_accept: Grammatical error correction (GEC) is still problematic for morphologically rich languages, such as Arabic, due to the complexity and nature of such languages. Having a paper that focuses on GEC for Arabic, helps the NLP community in developing GEC systems for Arabic.\n\nreasons_to_reject: No reasons\n\ntypos_grammar_style_and_presentation_improvements: It would be better if you add the structure of the paper in the last part of the introduction.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "qK9BghaIps",
        "similarity": 0.7047,
        "coverage": 0.05,
        "human_length": 646,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of grammatical error correction (GEC) and grammatical error detection (GED) in Arabic, more specifically on Modern Standard Arabic (no dialectal Arabic). Arabic is a morphologically rich language therefore the tasks is more difficult compared to e.g. English.  The authors work with three existing datasets for GEC in MSA, for all of them the erroneous text is provided with the corrected one.  For GED, the authors extend the three existing datasets with automatic annotation of errors types. This mainly requires alignment of the two texts. The authors propose a novel method for obtaining automatic word alignment of erroneous and correct text in Arabic, achieving almost 100% Precision and Recall. After alignment, the errors are automatically classified into 7 error classes and 32 subtypes (assigned individually or in a combination). An Arabic PLM CAMeLBERT is finetuned for the classification tasks with varying number of classes (43/13/2) and evaluated on the three data sets.  For GEC, where each error must be detected and a replacement produced.  The authors proposes two models, one based on AraBART, one on AraT5, each in three configurations, one includes morphological analysis of each Arabic word, one detecting error type, the last one combining both. The performance of those models are compared with other solutions and a few baselines (including e.g. ChatGPT). The results show a superior performance of the models combining both GED and Morphological analysis.\n\nreasons_to_accept: Nicely written paper on an interesting topic. Showing how explicit GED and Morphological analysis can help in GEC in Arabic. \nStrong evaluation on three diverse datasets, outperforming the current SOTA. Comparison with several baselines, also including ChatGPT. \nThe list of references is extremely rich and can well serve as a source of relevant references on this topic\n\nreasons_to_reject: While the authors outperform the current SOTA on all the datasets, the main reason for that is the large pretrained models - AraT5 and AraBART (the latter showed better perfomrance). The effect of including the morhological analysis and error classification is rather small (measured on the test sets, Table 6), most of the results are reported as not statistically significant.  Only the overall (average) improvements of the models combining GAD and Morphological analysis is provided. It is not clear, whether there are some error types where the models can benefit from knowing the explicit information of the type of the error.   It is not clear, from the paper, how acurrat is the automatic method for error type classification. At least a small qualitative study should have been performed to analyse, whether the errors are well classified. The authors report the results on the automatically obtained annotations, but the performance measured w.r.t. ground truth is not discussed.\n\nquestions_for_the_authors: The problem of Arabic dialects is not really discussed in the paper. The large pretrained models are probably trained also on dialectal data. Are dialectal words considered as errors in MSA texts? Is there a special error type for them?  Please, provide better description of the alignment algorithm. It is not clear, whether the Levenshtein algorithm is applied to tokens or to the whole sentences.  The greedy part of the algorithm is not explained well.  Line 565 refers to a statistical test of significance. Please, provide details on the test results also for other experiments (mainly presented in Table 6).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "YvLrlQHDzR",
        "length": 250,
        "human_text": "paper_topic_and_main_contributions: This paper builds a new benchmark for the Arabic grammatical error correction (GEC) area. To build the benchmark, it starts with the grammatical error detection (GED) task. In this section, the paper proposes a new alignment algorithm for error type annotation and achieves better results compared with previous methods. In the GEC section, it finds that the pre-trained model BART can be enhanced with GED and Morphological Disambiguation output, and this combination performs best among most of the settings. It also shows how GED granularity affects final GEC performance.\n\nreasons_to_accept: The paper is well-written and easy to understand; the readers who do not speak Arabic, such as me, can also understand the nature and background of Arabic GEC quickly by reading Sections 2 and 3. The newly proposed alignment algorithm and the strong baseline are essential contributions to the Arabic GEC community.\n\nreasons_to_reject: I only have one concern about the missing explanations of results. In Section 7, Line 509, the paper only describes the results. In Section 5, we can see that AraT5 has a larger pre-trained data size than AraBART but performs worse in most of the situations on the GEC dataset. It would be better to provide some explanations about this kind of result.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "QZRng7EQYl",
        "length": 246,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on investigating the effectiveness of Grammatical Error Detection (GED) on Arabic Grammatical Error Correction (GEC). For the investigation process, they depend on three datasets (QALB-2014, QALB-2015, and ZAEBUC). They claim that they were the first to benchmark newly developed pretrained Seq2Seq models on Arabic Grammatical Error Correction. As for the GED task, they used their own alignment algorithm which shows that they were superior across all metrics. They could show that using GED information in GEC models improves the performance across GEC datasets. In addition, they discussed the importance of leveraging contextual morphological preprocessing in improving the GEC performance. Their experiments achieved SOTA results on the two (L1 and L2) datasets of (QALB-2014 and QALB-21015).\n\nreasons_to_accept: Grammatical error correction (GEC) is still problematic for morphologically rich languages, such as Arabic, due to the complexity and nature of such languages. Having a paper that focuses on GEC for Arabic, helps the NLP community in developing GEC systems for Arabic.\n\nreasons_to_reject: No reasons\n\ntypos_grammar_style_and_presentation_improvements: It would be better if you add the structure of the paper in the last part of the introduction.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "qK9BghaIps",
        "length": 646,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of grammatical error correction (GEC) and grammatical error detection (GED) in Arabic, more specifically on Modern Standard Arabic (no dialectal Arabic). Arabic is a morphologically rich language therefore the tasks is more difficult compared to e.g. English.  The authors work with three existing datasets for GEC in MSA, for all of them the erroneous text is provided with the corrected one.  For GED, the authors extend the three existing datasets with automatic annotation of errors types. This mainly requires alignment of the two texts. The authors propose a novel method for obtaining automatic word alignment of erroneous and correct text in Arabic, achieving almost 100% Precision and Recall. After alignment, the errors are automatically classified into 7 error classes and 32 subtypes (assigned individually or in a combination). An Arabic PLM CAMeLBERT is finetuned for the classification tasks with varying number of classes (43/13/2) and evaluated on the three data sets.  For GEC, where each error must be detected and a replacement produced.  The authors proposes two models, one based on AraBART, one on AraT5, each in three configurations, one includes morphological analysis of each Arabic word, one detecting error type, the last one combining both. The performance of those models are compared with other solutions and a few baselines (including e.g. ChatGPT). The results show a superior performance of the models combining both GED and Morphological analysis.\n\nreasons_to_accept: Nicely written paper on an interesting topic. Showing how explicit GED and Morphological analysis can help in GEC in Arabic. \nStrong evaluation on three diverse datasets, outperforming the current SOTA. Comparison with several baselines, also including ChatGPT. \nThe list of references is extremely rich and can well serve as a source of relevant references on this topic\n\nreasons_to_reject: While the authors outperform the current SOTA on all the datasets, the main reason for that is the large pretrained models - AraT5 and AraBART (the latter showed better perfomrance). The effect of including the morhological analysis and error classification is rather small (measured on the test sets, Table 6), most of the results are reported as not statistically significant.  Only the overall (average) improvements of the models combining GAD and Morphological analysis is provided. It is not clear, whether there are some error types where the models can benefit from knowing the explicit information of the type of the error.   It is not clear, from the paper, how acurrat is the automatic method for error type classification. At least a small qualitative study should have been performed to analyse, whether the errors are well classified. The authors report the results on the automatically obtained annotations, but the performance measured w.r.t. ground truth is not discussed.\n\nquestions_for_the_authors: The problem of Arabic dialects is not really discussed in the paper. The large pretrained models are probably trained also on dialectal data. Are dialectal words considered as errors in MSA texts? Is there a special error type for them?  Please, provide better description of the alignment algorithm. It is not clear, whether the Levenshtein algorithm is applied to tokens or to the whole sentences.  The greedy part of the algorithm is not explained well.  Line 565 refers to a statistical test of significance. Please, provide details on the test results also for other experiments (mainly presented in Table 6).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "201_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_201_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6988666666666666,
      "max_similarity": 0.7126,
      "avg_coverage": 0.11193333333333333,
      "max_coverage": 0.1429
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 21,
      "avg_human_length": 380.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": false,
      "has_weaknesses": false,
      "has_suggestions": false
    },
    "content_count_metrics": {
      "strengths_count": 0,
      "weaknesses_count": 0,
      "suggestions_count": 0
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "YvLrlQHDzR",
        "similarity": 0.7126,
        "coverage": 0.1429,
        "human_length": 250,
        "human_text": "paper_topic_and_main_contributions: This paper builds a new benchmark for the Arabic grammatical error correction (GEC) area. To build the benchmark, it starts with the grammatical error detection (GED) task. In this section, the paper proposes a new alignment algorithm for error type annotation and achieves better results compared with previous methods. In the GEC section, it finds that the pre-trained model BART can be enhanced with GED and Morphological Disambiguation output, and this combination performs best among most of the settings. It also shows how GED granularity affects final GEC performance.\n\nreasons_to_accept: The paper is well-written and easy to understand; the readers who do not speak Arabic, such as me, can also understand the nature and background of Arabic GEC quickly by reading Sections 2 and 3. The newly proposed alignment algorithm and the strong baseline are essential contributions to the Arabic GEC community.\n\nreasons_to_reject: I only have one concern about the missing explanations of results. In Section 7, Line 509, the paper only describes the results. In Section 5, we can see that AraT5 has a larger pre-trained data size than AraBART but performs worse in most of the situations on the GEC dataset. It would be better to provide some explanations about this kind of result.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "QZRng7EQYl",
        "similarity": 0.6821,
        "coverage": 0.1429,
        "human_length": 246,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on investigating the effectiveness of Grammatical Error Detection (GED) on Arabic Grammatical Error Correction (GEC). For the investigation process, they depend on three datasets (QALB-2014, QALB-2015, and ZAEBUC). They claim that they were the first to benchmark newly developed pretrained Seq2Seq models on Arabic Grammatical Error Correction. As for the GED task, they used their own alignment algorithm which shows that they were superior across all metrics. They could show that using GED information in GEC models improves the performance across GEC datasets. In addition, they discussed the importance of leveraging contextual morphological preprocessing in improving the GEC performance. Their experiments achieved SOTA results on the two (L1 and L2) datasets of (QALB-2014 and QALB-21015).\n\nreasons_to_accept: Grammatical error correction (GEC) is still problematic for morphologically rich languages, such as Arabic, due to the complexity and nature of such languages. Having a paper that focuses on GEC for Arabic, helps the NLP community in developing GEC systems for Arabic.\n\nreasons_to_reject: No reasons\n\ntypos_grammar_style_and_presentation_improvements: It would be better if you add the structure of the paper in the last part of the introduction.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "qK9BghaIps",
        "similarity": 0.7019,
        "coverage": 0.05,
        "human_length": 646,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of grammatical error correction (GEC) and grammatical error detection (GED) in Arabic, more specifically on Modern Standard Arabic (no dialectal Arabic). Arabic is a morphologically rich language therefore the tasks is more difficult compared to e.g. English.  The authors work with three existing datasets for GEC in MSA, for all of them the erroneous text is provided with the corrected one.  For GED, the authors extend the three existing datasets with automatic annotation of errors types. This mainly requires alignment of the two texts. The authors propose a novel method for obtaining automatic word alignment of erroneous and correct text in Arabic, achieving almost 100% Precision and Recall. After alignment, the errors are automatically classified into 7 error classes and 32 subtypes (assigned individually or in a combination). An Arabic PLM CAMeLBERT is finetuned for the classification tasks with varying number of classes (43/13/2) and evaluated on the three data sets.  For GEC, where each error must be detected and a replacement produced.  The authors proposes two models, one based on AraBART, one on AraT5, each in three configurations, one includes morphological analysis of each Arabic word, one detecting error type, the last one combining both. The performance of those models are compared with other solutions and a few baselines (including e.g. ChatGPT). The results show a superior performance of the models combining both GED and Morphological analysis.\n\nreasons_to_accept: Nicely written paper on an interesting topic. Showing how explicit GED and Morphological analysis can help in GEC in Arabic. \nStrong evaluation on three diverse datasets, outperforming the current SOTA. Comparison with several baselines, also including ChatGPT. \nThe list of references is extremely rich and can well serve as a source of relevant references on this topic\n\nreasons_to_reject: While the authors outperform the current SOTA on all the datasets, the main reason for that is the large pretrained models - AraT5 and AraBART (the latter showed better perfomrance). The effect of including the morhological analysis and error classification is rather small (measured on the test sets, Table 6), most of the results are reported as not statistically significant.  Only the overall (average) improvements of the models combining GAD and Morphological analysis is provided. It is not clear, whether there are some error types where the models can benefit from knowing the explicit information of the type of the error.   It is not clear, from the paper, how acurrat is the automatic method for error type classification. At least a small qualitative study should have been performed to analyse, whether the errors are well classified. The authors report the results on the automatically obtained annotations, but the performance measured w.r.t. ground truth is not discussed.\n\nquestions_for_the_authors: The problem of Arabic dialects is not really discussed in the paper. The large pretrained models are probably trained also on dialectal data. Are dialectal words considered as errors in MSA texts? Is there a special error type for them?  Please, provide better description of the alignment algorithm. It is not clear, whether the Levenshtein algorithm is applied to tokens or to the whole sentences.  The greedy part of the algorithm is not explained well.  Line 565 refers to a statistical test of significance. Please, provide details on the test results also for other experiments (mainly presented in Table 6).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "YvLrlQHDzR",
        "length": 250,
        "human_text": "paper_topic_and_main_contributions: This paper builds a new benchmark for the Arabic grammatical error correction (GEC) area. To build the benchmark, it starts with the grammatical error detection (GED) task. In this section, the paper proposes a new alignment algorithm for error type annotation and achieves better results compared with previous methods. In the GEC section, it finds that the pre-trained model BART can be enhanced with GED and Morphological Disambiguation output, and this combination performs best among most of the settings. It also shows how GED granularity affects final GEC performance.\n\nreasons_to_accept: The paper is well-written and easy to understand; the readers who do not speak Arabic, such as me, can also understand the nature and background of Arabic GEC quickly by reading Sections 2 and 3. The newly proposed alignment algorithm and the strong baseline are essential contributions to the Arabic GEC community.\n\nreasons_to_reject: I only have one concern about the missing explanations of results. In Section 7, Line 509, the paper only describes the results. In Section 5, we can see that AraT5 has a larger pre-trained data size than AraBART but performs worse in most of the situations on the GEC dataset. It would be better to provide some explanations about this kind of result.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "QZRng7EQYl",
        "length": 246,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on investigating the effectiveness of Grammatical Error Detection (GED) on Arabic Grammatical Error Correction (GEC). For the investigation process, they depend on three datasets (QALB-2014, QALB-2015, and ZAEBUC). They claim that they were the first to benchmark newly developed pretrained Seq2Seq models on Arabic Grammatical Error Correction. As for the GED task, they used their own alignment algorithm which shows that they were superior across all metrics. They could show that using GED information in GEC models improves the performance across GEC datasets. In addition, they discussed the importance of leveraging contextual morphological preprocessing in improving the GEC performance. Their experiments achieved SOTA results on the two (L1 and L2) datasets of (QALB-2014 and QALB-21015).\n\nreasons_to_accept: Grammatical error correction (GEC) is still problematic for morphologically rich languages, such as Arabic, due to the complexity and nature of such languages. Having a paper that focuses on GEC for Arabic, helps the NLP community in developing GEC systems for Arabic.\n\nreasons_to_reject: No reasons\n\ntypos_grammar_style_and_presentation_improvements: It would be better if you add the structure of the paper in the last part of the introduction.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "qK9BghaIps",
        "length": 646,
        "human_text": "paper_topic_and_main_contributions: The paper addresses the problem of grammatical error correction (GEC) and grammatical error detection (GED) in Arabic, more specifically on Modern Standard Arabic (no dialectal Arabic). Arabic is a morphologically rich language therefore the tasks is more difficult compared to e.g. English.  The authors work with three existing datasets for GEC in MSA, for all of them the erroneous text is provided with the corrected one.  For GED, the authors extend the three existing datasets with automatic annotation of errors types. This mainly requires alignment of the two texts. The authors propose a novel method for obtaining automatic word alignment of erroneous and correct text in Arabic, achieving almost 100% Precision and Recall. After alignment, the errors are automatically classified into 7 error classes and 32 subtypes (assigned individually or in a combination). An Arabic PLM CAMeLBERT is finetuned for the classification tasks with varying number of classes (43/13/2) and evaluated on the three data sets.  For GEC, where each error must be detected and a replacement produced.  The authors proposes two models, one based on AraBART, one on AraT5, each in three configurations, one includes morphological analysis of each Arabic word, one detecting error type, the last one combining both. The performance of those models are compared with other solutions and a few baselines (including e.g. ChatGPT). The results show a superior performance of the models combining both GED and Morphological analysis.\n\nreasons_to_accept: Nicely written paper on an interesting topic. Showing how explicit GED and Morphological analysis can help in GEC in Arabic. \nStrong evaluation on three diverse datasets, outperforming the current SOTA. Comparison with several baselines, also including ChatGPT. \nThe list of references is extremely rich and can well serve as a source of relevant references on this topic\n\nreasons_to_reject: While the authors outperform the current SOTA on all the datasets, the main reason for that is the large pretrained models - AraT5 and AraBART (the latter showed better perfomrance). The effect of including the morhological analysis and error classification is rather small (measured on the test sets, Table 6), most of the results are reported as not statistically significant.  Only the overall (average) improvements of the models combining GAD and Morphological analysis is provided. It is not clear, whether there are some error types where the models can benefit from knowing the explicit information of the type of the error.   It is not clear, from the paper, how acurrat is the automatic method for error type classification. At least a small qualitative study should have been performed to analyse, whether the errors are well classified. The authors report the results on the automatically obtained annotations, but the performance measured w.r.t. ground truth is not discussed.\n\nquestions_for_the_authors: The problem of Arabic dialects is not really discussed in the paper. The large pretrained models are probably trained also on dialectal data. Are dialectal words considered as errors in MSA texts? Is there a special error type for them?  Please, provide better description of the alignment algorithm. It is not clear, whether the Levenshtein algorithm is applied to tokens or to the whole sentences.  The greedy part of the algorithm is not explained well.  Line 565 refers to a statistical test of significance. Please, provide details on the test results also for other experiments (mainly presented in Table 6).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "18_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_18_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7247666666666666,
      "max_similarity": 0.7311,
      "avg_coverage": 0.6164666666666667,
      "max_coverage": 0.8333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 804,
      "avg_human_length": 437.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "J6WeppqCO7",
        "similarity": 0.7311,
        "coverage": 0.6087,
        "human_length": 426,
        "human_text": "paper_topic_and_main_contributions: The work proposes to teach vision-language Transformers structured information leveraging scene graphs (SGs). Supervision signals are obtained from SGs in two ways: 1)  positive and negative captions are generated with different compositional substituents; 2) object labels, relations and coordinates are used to supervise a set of special \"scene graph tokens\".\nThe learnable scene graph tokens possess a separate set of attention and feedforward-layer parameters. They are allowed to interact with regular patch tokens through self-attention, which can be later leveraged for predicting the text embedding of objects or relations.\nThe entire model is finetuned from CLIP/BLIP via LoRA and outperforms various baselines in hard VL benchmarks requiring compositional scene understanding.\n\nreasons_to_accept: This paper demonstrates that the standard contrastive language-image pretraining cannot sufficiently imbue a model with compositional understanding capabilities. Therefore, the authors propose to leverage information about relations and attributes from scene graphs. It's demonstrated that a small number of scene graph annotations could successfully compensate for the lack of compositional understanding. The results would encourage the community to adapt models for better compositional scene understanding with moderate effort.\n\nreasons_to_reject: The proposed method does not consistently improve performances. As mentioned in Line 485-496, zero-shot performance was degraded. Table 2&3 also show that the  proposed method harmed performances on certain partitions of the evaluation benchmark. I'm expecting more error analysis on the cause of such degradation. For example, is there a commonality of all tasks where performances were harmed by SGVL, such that during application, users could wisely choose when to adopt BLIP vs. BLIP-SGVL?\n\nquestions_for_the_authors: Line 263: How do you represent locations for relationship tokens?\n\ntypos_grammar_style_and_presentation_improvements: Line603 qualitative annotations --> high-quality annotations Line605-606: I don't think unsupervised training is a natural extension of this work. The main insight brought by this work is the necessity of a small amount of densely annotated data. Future directions might include generalizing this approach beyond the VL domain or using other types of dense structured data (e.g. segmentation masks, sketch)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "iVwqiqnn9i",
        "similarity": 0.7171,
        "coverage": 0.8333,
        "human_length": 259,
        "human_text": "paper_topic_and_main_contributions: The paper is about enhancing structural understanding of images by Vision Language Models(VLMs) using additional signals contained in scene graphs associated with the image. Since scene graph annotation is a costly process, the authors specifically look at whether small scene graph datasets can provide sufficient information to the VLMs during training/finetuning. The authors present a novel way to incorporate scene graph information into visual and textual representations by using adaptive scene graph tokens, positive and negative captions, modifications to the transformer architecture and introduce related losses. The authors try their methodology on various popular VLMs/Datasets and present encouraging results.\n\nreasons_to_accept: This paper is well written and the diagrams clearly capture what the authors intended to show in their architectural improvements. Utilizing small scene graph datasets to better enable VLMs to create more granular multimodal representations can benefit the research community immensely. The authors provide comprehensive ablation studies and test their methodology using popular VLMs/Datasets. The loss functions and the architectural changes introduced are uncomplicated and easy to follow/implement.\n\nreasons_to_reject: It would have been better if more scene graph datasets were considered other than VG and 1% of LAION.\n\nquestions_for_the_authors: NA\n\nmissing_references: NA\n\ntypos_grammar_style_and_presentation_improvements: NA\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "jJF2yZ26kY",
        "similarity": 0.7261,
        "coverage": 0.4074,
        "human_length": 626,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an interesting vision-language pre-training method, which leverages the scene graph pairs to jointly train the text and image encoder. \nTechnically, the impact of scene graph is reflected from the adaptive tokens and the prompts. \nIn the output stage, it can additional yield the scene graph output. \nThe framework is pre-trained on LAION, and benchmarked on four scene understanding datasets and one zero-shot classification dataset. \nCompared with conventional VLM, such as CLIP and BLIP, it achieves a better performance.\n\nreasons_to_accept: - Techniqually, the proposed framework is reasonable and effective.\n- Compared with conventional VLM such as CLIP, BLIP, it shows a significant improvement on the proposed task.\n- This paper is well-written and easy-to-follow.\n- The ablation studies are very extensive.\n\nreasons_to_reject: - The idea to leverage scene graph for vision-language model is not very enough now. In the past few years, there are already some references. Unfortunately, the authors do not discuss their diffference in the reference, which may in turn negatively impact the novelties of this work. For example: [1] Yu, Fei, et al. \"Ernie-vil: Knowledge enhanced vision-language representations through scene graphs.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 4. 2021.\n[2] Doveh, Sivan, et al. \"Teaching structured vision & language concepts to vision & language models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[3] Ma, Zixian, et al. \"CREPE: Can Vision-Language Foundation Models Reason Compositionally?.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n- Lack state-of-the-art comparison in the experimental section. Not only the above reference, but also some state-of-the-art VLM on the scene understanding task.\n- Another minor issue is the presentation of this paper: I noticed the visual prediction are all put into the supplementary materials. It would be better to put several in the main text, as this is one of the key results for this paper.\n\nquestions_for_the_authors: I would appreciate it if the authors can address the questions below, in the rebuttal stage: **Q1:** Using scene graph, or more boardly structured information, is not a very novel idea now. The difference of the proposed method against some prior works need to be clarified.\n**Q2:** More state-of-the-art comparison, including these latest methods and more advanced VLM.\n\nmissing_references: Yes, as I mentioned above, some key references on leveraging scene graph and using it to train VLM are missing. Not only the related work section but also the experimental comparison need to enrich them. For example: [1] Yu, Fei, et al. \"Ernie-vil: Knowledge enhanced vision-language representations through scene graphs.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 4. 2021.\n[2] Doveh, Sivan, et al. \"Teaching structured vision & language concepts to vision & language models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[3] Ma, Zixian, et al. \"CREPE: Can Vision-Language Foundation Models Reason Compositionally?.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\ntypos_grammar_style_and_presentation_improvements: Just one presentation issue: The visual prediction are all put into the supplementary materials. It would be better to put several in the main text, as this is one of the key results for this paper.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: No ethical issues.\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "J6WeppqCO7",
        "length": 426,
        "human_text": "paper_topic_and_main_contributions: The work proposes to teach vision-language Transformers structured information leveraging scene graphs (SGs). Supervision signals are obtained from SGs in two ways: 1)  positive and negative captions are generated with different compositional substituents; 2) object labels, relations and coordinates are used to supervise a set of special \"scene graph tokens\".\nThe learnable scene graph tokens possess a separate set of attention and feedforward-layer parameters. They are allowed to interact with regular patch tokens through self-attention, which can be later leveraged for predicting the text embedding of objects or relations.\nThe entire model is finetuned from CLIP/BLIP via LoRA and outperforms various baselines in hard VL benchmarks requiring compositional scene understanding.\n\nreasons_to_accept: This paper demonstrates that the standard contrastive language-image pretraining cannot sufficiently imbue a model with compositional understanding capabilities. Therefore, the authors propose to leverage information about relations and attributes from scene graphs. It's demonstrated that a small number of scene graph annotations could successfully compensate for the lack of compositional understanding. The results would encourage the community to adapt models for better compositional scene understanding with moderate effort.\n\nreasons_to_reject: The proposed method does not consistently improve performances. As mentioned in Line 485-496, zero-shot performance was degraded. Table 2&3 also show that the  proposed method harmed performances on certain partitions of the evaluation benchmark. I'm expecting more error analysis on the cause of such degradation. For example, is there a commonality of all tasks where performances were harmed by SGVL, such that during application, users could wisely choose when to adopt BLIP vs. BLIP-SGVL?\n\nquestions_for_the_authors: Line 263: How do you represent locations for relationship tokens?\n\ntypos_grammar_style_and_presentation_improvements: Line603 qualitative annotations --> high-quality annotations Line605-606: I don't think unsupervised training is a natural extension of this work. The main insight brought by this work is the necessity of a small amount of densely annotated data. Future directions might include generalizing this approach beyond the VL domain or using other types of dense structured data (e.g. segmentation masks, sketch)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "iVwqiqnn9i",
        "length": 259,
        "human_text": "paper_topic_and_main_contributions: The paper is about enhancing structural understanding of images by Vision Language Models(VLMs) using additional signals contained in scene graphs associated with the image. Since scene graph annotation is a costly process, the authors specifically look at whether small scene graph datasets can provide sufficient information to the VLMs during training/finetuning. The authors present a novel way to incorporate scene graph information into visual and textual representations by using adaptive scene graph tokens, positive and negative captions, modifications to the transformer architecture and introduce related losses. The authors try their methodology on various popular VLMs/Datasets and present encouraging results.\n\nreasons_to_accept: This paper is well written and the diagrams clearly capture what the authors intended to show in their architectural improvements. Utilizing small scene graph datasets to better enable VLMs to create more granular multimodal representations can benefit the research community immensely. The authors provide comprehensive ablation studies and test their methodology using popular VLMs/Datasets. The loss functions and the architectural changes introduced are uncomplicated and easy to follow/implement.\n\nreasons_to_reject: It would have been better if more scene graph datasets were considered other than VG and 1% of LAION.\n\nquestions_for_the_authors: NA\n\nmissing_references: NA\n\ntypos_grammar_style_and_presentation_improvements: NA\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "jJF2yZ26kY",
        "length": 626,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an interesting vision-language pre-training method, which leverages the scene graph pairs to jointly train the text and image encoder. \nTechnically, the impact of scene graph is reflected from the adaptive tokens and the prompts. \nIn the output stage, it can additional yield the scene graph output. \nThe framework is pre-trained on LAION, and benchmarked on four scene understanding datasets and one zero-shot classification dataset. \nCompared with conventional VLM, such as CLIP and BLIP, it achieves a better performance.\n\nreasons_to_accept: - Techniqually, the proposed framework is reasonable and effective.\n- Compared with conventional VLM such as CLIP, BLIP, it shows a significant improvement on the proposed task.\n- This paper is well-written and easy-to-follow.\n- The ablation studies are very extensive.\n\nreasons_to_reject: - The idea to leverage scene graph for vision-language model is not very enough now. In the past few years, there are already some references. Unfortunately, the authors do not discuss their diffference in the reference, which may in turn negatively impact the novelties of this work. For example: [1] Yu, Fei, et al. \"Ernie-vil: Knowledge enhanced vision-language representations through scene graphs.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 4. 2021.\n[2] Doveh, Sivan, et al. \"Teaching structured vision & language concepts to vision & language models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[3] Ma, Zixian, et al. \"CREPE: Can Vision-Language Foundation Models Reason Compositionally?.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n- Lack state-of-the-art comparison in the experimental section. Not only the above reference, but also some state-of-the-art VLM on the scene understanding task.\n- Another minor issue is the presentation of this paper: I noticed the visual prediction are all put into the supplementary materials. It would be better to put several in the main text, as this is one of the key results for this paper.\n\nquestions_for_the_authors: I would appreciate it if the authors can address the questions below, in the rebuttal stage: **Q1:** Using scene graph, or more boardly structured information, is not a very novel idea now. The difference of the proposed method against some prior works need to be clarified.\n**Q2:** More state-of-the-art comparison, including these latest methods and more advanced VLM.\n\nmissing_references: Yes, as I mentioned above, some key references on leveraging scene graph and using it to train VLM are missing. Not only the related work section but also the experimental comparison need to enrich them. For example: [1] Yu, Fei, et al. \"Ernie-vil: Knowledge enhanced vision-language representations through scene graphs.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 4. 2021.\n[2] Doveh, Sivan, et al. \"Teaching structured vision & language concepts to vision & language models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[3] Ma, Zixian, et al. \"CREPE: Can Vision-Language Foundation Models Reason Compositionally?.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\ntypos_grammar_style_and_presentation_improvements: Just one presentation issue: The visual prediction are all put into the supplementary materials. It would be better to put several in the main text, as this is one of the key results for this paper.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: No ethical issues.\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "18_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_18_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7252333333333333,
      "max_similarity": 0.7326,
      "avg_coverage": 0.5741999999999999,
      "max_coverage": 0.75
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 758,
      "avg_human_length": 437.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 8,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "J6WeppqCO7",
        "similarity": 0.7326,
        "coverage": 0.5652,
        "human_length": 426,
        "human_text": "paper_topic_and_main_contributions: The work proposes to teach vision-language Transformers structured information leveraging scene graphs (SGs). Supervision signals are obtained from SGs in two ways: 1)  positive and negative captions are generated with different compositional substituents; 2) object labels, relations and coordinates are used to supervise a set of special \"scene graph tokens\".\nThe learnable scene graph tokens possess a separate set of attention and feedforward-layer parameters. They are allowed to interact with regular patch tokens through self-attention, which can be later leveraged for predicting the text embedding of objects or relations.\nThe entire model is finetuned from CLIP/BLIP via LoRA and outperforms various baselines in hard VL benchmarks requiring compositional scene understanding.\n\nreasons_to_accept: This paper demonstrates that the standard contrastive language-image pretraining cannot sufficiently imbue a model with compositional understanding capabilities. Therefore, the authors propose to leverage information about relations and attributes from scene graphs. It's demonstrated that a small number of scene graph annotations could successfully compensate for the lack of compositional understanding. The results would encourage the community to adapt models for better compositional scene understanding with moderate effort.\n\nreasons_to_reject: The proposed method does not consistently improve performances. As mentioned in Line 485-496, zero-shot performance was degraded. Table 2&3 also show that the  proposed method harmed performances on certain partitions of the evaluation benchmark. I'm expecting more error analysis on the cause of such degradation. For example, is there a commonality of all tasks where performances were harmed by SGVL, such that during application, users could wisely choose when to adopt BLIP vs. BLIP-SGVL?\n\nquestions_for_the_authors: Line 263: How do you represent locations for relationship tokens?\n\ntypos_grammar_style_and_presentation_improvements: Line603 qualitative annotations --> high-quality annotations Line605-606: I don't think unsupervised training is a natural extension of this work. The main insight brought by this work is the necessity of a small amount of densely annotated data. Future directions might include generalizing this approach beyond the VL domain or using other types of dense structured data (e.g. segmentation masks, sketch)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "iVwqiqnn9i",
        "similarity": 0.7127,
        "coverage": 0.75,
        "human_length": 259,
        "human_text": "paper_topic_and_main_contributions: The paper is about enhancing structural understanding of images by Vision Language Models(VLMs) using additional signals contained in scene graphs associated with the image. Since scene graph annotation is a costly process, the authors specifically look at whether small scene graph datasets can provide sufficient information to the VLMs during training/finetuning. The authors present a novel way to incorporate scene graph information into visual and textual representations by using adaptive scene graph tokens, positive and negative captions, modifications to the transformer architecture and introduce related losses. The authors try their methodology on various popular VLMs/Datasets and present encouraging results.\n\nreasons_to_accept: This paper is well written and the diagrams clearly capture what the authors intended to show in their architectural improvements. Utilizing small scene graph datasets to better enable VLMs to create more granular multimodal representations can benefit the research community immensely. The authors provide comprehensive ablation studies and test their methodology using popular VLMs/Datasets. The loss functions and the architectural changes introduced are uncomplicated and easy to follow/implement.\n\nreasons_to_reject: It would have been better if more scene graph datasets were considered other than VG and 1% of LAION.\n\nquestions_for_the_authors: NA\n\nmissing_references: NA\n\ntypos_grammar_style_and_presentation_improvements: NA\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "jJF2yZ26kY",
        "similarity": 0.7304,
        "coverage": 0.4074,
        "human_length": 626,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an interesting vision-language pre-training method, which leverages the scene graph pairs to jointly train the text and image encoder. \nTechnically, the impact of scene graph is reflected from the adaptive tokens and the prompts. \nIn the output stage, it can additional yield the scene graph output. \nThe framework is pre-trained on LAION, and benchmarked on four scene understanding datasets and one zero-shot classification dataset. \nCompared with conventional VLM, such as CLIP and BLIP, it achieves a better performance.\n\nreasons_to_accept: - Techniqually, the proposed framework is reasonable and effective.\n- Compared with conventional VLM such as CLIP, BLIP, it shows a significant improvement on the proposed task.\n- This paper is well-written and easy-to-follow.\n- The ablation studies are very extensive.\n\nreasons_to_reject: - The idea to leverage scene graph for vision-language model is not very enough now. In the past few years, there are already some references. Unfortunately, the authors do not discuss their diffference in the reference, which may in turn negatively impact the novelties of this work. For example: [1] Yu, Fei, et al. \"Ernie-vil: Knowledge enhanced vision-language representations through scene graphs.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 4. 2021.\n[2] Doveh, Sivan, et al. \"Teaching structured vision & language concepts to vision & language models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[3] Ma, Zixian, et al. \"CREPE: Can Vision-Language Foundation Models Reason Compositionally?.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n- Lack state-of-the-art comparison in the experimental section. Not only the above reference, but also some state-of-the-art VLM on the scene understanding task.\n- Another minor issue is the presentation of this paper: I noticed the visual prediction are all put into the supplementary materials. It would be better to put several in the main text, as this is one of the key results for this paper.\n\nquestions_for_the_authors: I would appreciate it if the authors can address the questions below, in the rebuttal stage: **Q1:** Using scene graph, or more boardly structured information, is not a very novel idea now. The difference of the proposed method against some prior works need to be clarified.\n**Q2:** More state-of-the-art comparison, including these latest methods and more advanced VLM.\n\nmissing_references: Yes, as I mentioned above, some key references on leveraging scene graph and using it to train VLM are missing. Not only the related work section but also the experimental comparison need to enrich them. For example: [1] Yu, Fei, et al. \"Ernie-vil: Knowledge enhanced vision-language representations through scene graphs.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 4. 2021.\n[2] Doveh, Sivan, et al. \"Teaching structured vision & language concepts to vision & language models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[3] Ma, Zixian, et al. \"CREPE: Can Vision-Language Foundation Models Reason Compositionally?.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\ntypos_grammar_style_and_presentation_improvements: Just one presentation issue: The visual prediction are all put into the supplementary materials. It would be better to put several in the main text, as this is one of the key results for this paper.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: No ethical issues.\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "J6WeppqCO7",
        "length": 426,
        "human_text": "paper_topic_and_main_contributions: The work proposes to teach vision-language Transformers structured information leveraging scene graphs (SGs). Supervision signals are obtained from SGs in two ways: 1)  positive and negative captions are generated with different compositional substituents; 2) object labels, relations and coordinates are used to supervise a set of special \"scene graph tokens\".\nThe learnable scene graph tokens possess a separate set of attention and feedforward-layer parameters. They are allowed to interact with regular patch tokens through self-attention, which can be later leveraged for predicting the text embedding of objects or relations.\nThe entire model is finetuned from CLIP/BLIP via LoRA and outperforms various baselines in hard VL benchmarks requiring compositional scene understanding.\n\nreasons_to_accept: This paper demonstrates that the standard contrastive language-image pretraining cannot sufficiently imbue a model with compositional understanding capabilities. Therefore, the authors propose to leverage information about relations and attributes from scene graphs. It's demonstrated that a small number of scene graph annotations could successfully compensate for the lack of compositional understanding. The results would encourage the community to adapt models for better compositional scene understanding with moderate effort.\n\nreasons_to_reject: The proposed method does not consistently improve performances. As mentioned in Line 485-496, zero-shot performance was degraded. Table 2&3 also show that the  proposed method harmed performances on certain partitions of the evaluation benchmark. I'm expecting more error analysis on the cause of such degradation. For example, is there a commonality of all tasks where performances were harmed by SGVL, such that during application, users could wisely choose when to adopt BLIP vs. BLIP-SGVL?\n\nquestions_for_the_authors: Line 263: How do you represent locations for relationship tokens?\n\ntypos_grammar_style_and_presentation_improvements: Line603 qualitative annotations --> high-quality annotations Line605-606: I don't think unsupervised training is a natural extension of this work. The main insight brought by this work is the necessity of a small amount of densely annotated data. Future directions might include generalizing this approach beyond the VL domain or using other types of dense structured data (e.g. segmentation masks, sketch)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "iVwqiqnn9i",
        "length": 259,
        "human_text": "paper_topic_and_main_contributions: The paper is about enhancing structural understanding of images by Vision Language Models(VLMs) using additional signals contained in scene graphs associated with the image. Since scene graph annotation is a costly process, the authors specifically look at whether small scene graph datasets can provide sufficient information to the VLMs during training/finetuning. The authors present a novel way to incorporate scene graph information into visual and textual representations by using adaptive scene graph tokens, positive and negative captions, modifications to the transformer architecture and introduce related losses. The authors try their methodology on various popular VLMs/Datasets and present encouraging results.\n\nreasons_to_accept: This paper is well written and the diagrams clearly capture what the authors intended to show in their architectural improvements. Utilizing small scene graph datasets to better enable VLMs to create more granular multimodal representations can benefit the research community immensely. The authors provide comprehensive ablation studies and test their methodology using popular VLMs/Datasets. The loss functions and the architectural changes introduced are uncomplicated and easy to follow/implement.\n\nreasons_to_reject: It would have been better if more scene graph datasets were considered other than VG and 1% of LAION.\n\nquestions_for_the_authors: NA\n\nmissing_references: NA\n\ntypos_grammar_style_and_presentation_improvements: NA\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "jJF2yZ26kY",
        "length": 626,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an interesting vision-language pre-training method, which leverages the scene graph pairs to jointly train the text and image encoder. \nTechnically, the impact of scene graph is reflected from the adaptive tokens and the prompts. \nIn the output stage, it can additional yield the scene graph output. \nThe framework is pre-trained on LAION, and benchmarked on four scene understanding datasets and one zero-shot classification dataset. \nCompared with conventional VLM, such as CLIP and BLIP, it achieves a better performance.\n\nreasons_to_accept: - Techniqually, the proposed framework is reasonable and effective.\n- Compared with conventional VLM such as CLIP, BLIP, it shows a significant improvement on the proposed task.\n- This paper is well-written and easy-to-follow.\n- The ablation studies are very extensive.\n\nreasons_to_reject: - The idea to leverage scene graph for vision-language model is not very enough now. In the past few years, there are already some references. Unfortunately, the authors do not discuss their diffference in the reference, which may in turn negatively impact the novelties of this work. For example: [1] Yu, Fei, et al. \"Ernie-vil: Knowledge enhanced vision-language representations through scene graphs.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 4. 2021.\n[2] Doveh, Sivan, et al. \"Teaching structured vision & language concepts to vision & language models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[3] Ma, Zixian, et al. \"CREPE: Can Vision-Language Foundation Models Reason Compositionally?.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n- Lack state-of-the-art comparison in the experimental section. Not only the above reference, but also some state-of-the-art VLM on the scene understanding task.\n- Another minor issue is the presentation of this paper: I noticed the visual prediction are all put into the supplementary materials. It would be better to put several in the main text, as this is one of the key results for this paper.\n\nquestions_for_the_authors: I would appreciate it if the authors can address the questions below, in the rebuttal stage: **Q1:** Using scene graph, or more boardly structured information, is not a very novel idea now. The difference of the proposed method against some prior works need to be clarified.\n**Q2:** More state-of-the-art comparison, including these latest methods and more advanced VLM.\n\nmissing_references: Yes, as I mentioned above, some key references on leveraging scene graph and using it to train VLM are missing. Not only the related work section but also the experimental comparison need to enrich them. For example: [1] Yu, Fei, et al. \"Ernie-vil: Knowledge enhanced vision-language representations through scene graphs.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 4. 2021.\n[2] Doveh, Sivan, et al. \"Teaching structured vision & language concepts to vision & language models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[3] Ma, Zixian, et al. \"CREPE: Can Vision-Language Foundation Models Reason Compositionally?.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\ntypos_grammar_style_and_presentation_improvements: Just one presentation issue: The visual prediction are all put into the supplementary materials. It would be better to put several in the main text, as this is one of the key results for this paper.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: No ethical issues.\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "176_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_176_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7087,
      "max_similarity": 0.7186,
      "avg_coverage": 0.5573,
      "max_coverage": 0.6667
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 639,
      "avg_human_length": 363.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 12,
      "suggestions_count": 12
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "pXpYHwQuz7",
        "similarity": 0.7186,
        "coverage": 0.5385,
        "human_length": 358,
        "human_text": "paper_topic_and_main_contributions: This paper investigates how to simulate a specific person with large language models (LLMs). Different from previous works that prompt existing LLM, the authors construct the experience of that person and then fine-tune an LLM on the experience dataset. The experience consists of (1) character experience generated by another LLM based on the profile, and (2) protective experience that shows the person has no idea about unrelated knowledge. Evaluated by GPT 3.5, the trained model performs better than Alpaca, Vicuna, and is comparable with ChatGPT, in several aspects such as memorizing the character profile and reducing hallucination.\n\nreasons_to_accept: 1. This paper proposes an experience reconstruction method to extend a character's profile to many detailed scenes that mimic interactions between the target character and other characters. The experience can provide more information for character simulation. This method may inspire future work on character data construction. \n2. This paper proposes constructing and using protective experience to reduce hallucination during role-playing. Trained on protective experience, the model can pretend to be ignorant of the knowledge that the character does not know. The character hallucination problem is important, and this work gives some insight.\n\nreasons_to_reject: 1. Lack of important details. It's unclear which LLM did the authors use for scene extraction and experience generation (both character experience and protective experience). Does the selection of this model important? Does the generated experience faithful and without hallucination? How many scenes are there in the protective experience for training? For evaluation, how are the questions/topics selected? \n2. Lack of ablation study to show the superiority of the proposed experience construction method. I think adding a comparison between models fine-tuned on different kinds of character data helps.\n\nquestions_for_the_authors: See above\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "LdCxndmrbd",
        "similarity": 0.6992,
        "coverage": 0.6667,
        "human_length": 336,
        "human_text": "paper_topic_and_main_contributions: The paper presents a novel approach for training trainable agents to simulate specific individuals by providing them with profiles and experiences. The authors propose a framework that includes experience reconstruction, experience upload, and protective experiences. They evaluate the trained agents through interviews and compare them with baseline models. The results show that the trainable agents outperform the baselines in terms of personality, memorization, hallucination, and stability. The paper provides valuable insights into building better character simulacra.\n\nreasons_to_accept: (1) The paper introduces a novel approach for training trainable agents to simulate specific individuals, which is a step closer to character simulacra. \n(2) The framework proposed by the authors, including experience reconstruction, experience upload, and protective experiences, is well-structured and practical. \n(3) The evaluation process, including interviews and AI-based judging, provides comprehensive insights into the performance of the trained agents.\n\nreasons_to_reject: (1) The paper lacks implementation details, such as the specific architecture and training settings used for the trainable agents. \n(2) The evaluation could benefit from comparisons with more baselines and a larger number of interview questions. \n(3) The paper could provide more in-depth discussions on the limitations and potential ethical concerns of using trainable agents.\n\nquestions_for_the_authors: (1) Could you please provide more details about the implementation of the trainable agents, such as the specific architecture and training settings used? \n(2) Have you considered comparing the trained agents with more baselines, such as other instruction-tuned models or models trained with different methodologies? \n(3) Can you discuss the potential ethical concerns of using trainable agents and how you plan to address them?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "WNkPFljv4L",
        "similarity": 0.7083,
        "coverage": 0.4667,
        "human_length": 395,
        "human_text": "paper_topic_and_main_contributions: This paper introduces an innovative pipeline designed to construct trainable agents on LLMs that can convincingly emulate specific historical figures, such as Beethoven. The pipeline commences with the collection of comprehensive profiles for the chosen characters. These profiles are then utilized to prompt the LLMs to generate detailed scenes and experiences that are characteristic of these figures. Subsequently, a specialized LLM is fine-tuned based on these character-specific experiences, resulting in a convincing simulacrum of the character. Experimental evaluations, conducted using ChatGPT as a benchmark, demonstrate that the performance of these character simulacra is comparable to ChatGPT.\n\nreasons_to_accept: 1. The paper introduces a novel pipeline for creating trainable agents on LLMs that can emulate specific characters. This approach is innovative and could have wide-ranging applications in various fields, such as virtual assistants, education, and entertainment.\n2. This paper presents a thorough process that covers everything from gathering character profiles to refining LLMs using those profiles. This meticulous method could serve as a guide for other scholars in the same field.\n3. The paper also discusses potential flaws and areas for improvement in the proposed approach, indicating a thorough and thoughtful analysis.\n\nreasons_to_reject: 1. While the Protective Experience method is used to reduce hallucination issues, there is a possibility that it may result in bias or incorrect information being incorporated into the agent. This is a matter of concern for future usage.\n2. Including a human evaluation could potentially increase its credibility.\n\nquestions_for_the_authors: 1. In your experiments with RLHF-trained or distilled from RLHF models such as ChatGPT and Vicuna, have you noticed them generating safe responses like \"As an AI, I can't...\"? If so, how do you handle these situations in your evaluation?\n2. Do you notice a preference for ChatGPT agents evaluated by humans since you use GPT as the evaluator?\n3. Do you observe hallucinations in scenes? If so, how do you prevent them?\n\nmissing_references: SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization - Kim et al.\n\ntypos_grammar_style_and_presentation_improvements: L447 OpenAI\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "pXpYHwQuz7",
        "length": 358,
        "human_text": "paper_topic_and_main_contributions: This paper investigates how to simulate a specific person with large language models (LLMs). Different from previous works that prompt existing LLM, the authors construct the experience of that person and then fine-tune an LLM on the experience dataset. The experience consists of (1) character experience generated by another LLM based on the profile, and (2) protective experience that shows the person has no idea about unrelated knowledge. Evaluated by GPT 3.5, the trained model performs better than Alpaca, Vicuna, and is comparable with ChatGPT, in several aspects such as memorizing the character profile and reducing hallucination.\n\nreasons_to_accept: 1. This paper proposes an experience reconstruction method to extend a character's profile to many detailed scenes that mimic interactions between the target character and other characters. The experience can provide more information for character simulation. This method may inspire future work on character data construction. \n2. This paper proposes constructing and using protective experience to reduce hallucination during role-playing. Trained on protective experience, the model can pretend to be ignorant of the knowledge that the character does not know. The character hallucination problem is important, and this work gives some insight.\n\nreasons_to_reject: 1. Lack of important details. It's unclear which LLM did the authors use for scene extraction and experience generation (both character experience and protective experience). Does the selection of this model important? Does the generated experience faithful and without hallucination? How many scenes are there in the protective experience for training? For evaluation, how are the questions/topics selected? \n2. Lack of ablation study to show the superiority of the proposed experience construction method. I think adding a comparison between models fine-tuned on different kinds of character data helps.\n\nquestions_for_the_authors: See above\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "LdCxndmrbd",
        "length": 336,
        "human_text": "paper_topic_and_main_contributions: The paper presents a novel approach for training trainable agents to simulate specific individuals by providing them with profiles and experiences. The authors propose a framework that includes experience reconstruction, experience upload, and protective experiences. They evaluate the trained agents through interviews and compare them with baseline models. The results show that the trainable agents outperform the baselines in terms of personality, memorization, hallucination, and stability. The paper provides valuable insights into building better character simulacra.\n\nreasons_to_accept: (1) The paper introduces a novel approach for training trainable agents to simulate specific individuals, which is a step closer to character simulacra. \n(2) The framework proposed by the authors, including experience reconstruction, experience upload, and protective experiences, is well-structured and practical. \n(3) The evaluation process, including interviews and AI-based judging, provides comprehensive insights into the performance of the trained agents.\n\nreasons_to_reject: (1) The paper lacks implementation details, such as the specific architecture and training settings used for the trainable agents. \n(2) The evaluation could benefit from comparisons with more baselines and a larger number of interview questions. \n(3) The paper could provide more in-depth discussions on the limitations and potential ethical concerns of using trainable agents.\n\nquestions_for_the_authors: (1) Could you please provide more details about the implementation of the trainable agents, such as the specific architecture and training settings used? \n(2) Have you considered comparing the trained agents with more baselines, such as other instruction-tuned models or models trained with different methodologies? \n(3) Can you discuss the potential ethical concerns of using trainable agents and how you plan to address them?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "WNkPFljv4L",
        "length": 395,
        "human_text": "paper_topic_and_main_contributions: This paper introduces an innovative pipeline designed to construct trainable agents on LLMs that can convincingly emulate specific historical figures, such as Beethoven. The pipeline commences with the collection of comprehensive profiles for the chosen characters. These profiles are then utilized to prompt the LLMs to generate detailed scenes and experiences that are characteristic of these figures. Subsequently, a specialized LLM is fine-tuned based on these character-specific experiences, resulting in a convincing simulacrum of the character. Experimental evaluations, conducted using ChatGPT as a benchmark, demonstrate that the performance of these character simulacra is comparable to ChatGPT.\n\nreasons_to_accept: 1. The paper introduces a novel pipeline for creating trainable agents on LLMs that can emulate specific characters. This approach is innovative and could have wide-ranging applications in various fields, such as virtual assistants, education, and entertainment.\n2. This paper presents a thorough process that covers everything from gathering character profiles to refining LLMs using those profiles. This meticulous method could serve as a guide for other scholars in the same field.\n3. The paper also discusses potential flaws and areas for improvement in the proposed approach, indicating a thorough and thoughtful analysis.\n\nreasons_to_reject: 1. While the Protective Experience method is used to reduce hallucination issues, there is a possibility that it may result in bias or incorrect information being incorporated into the agent. This is a matter of concern for future usage.\n2. Including a human evaluation could potentially increase its credibility.\n\nquestions_for_the_authors: 1. In your experiments with RLHF-trained or distilled from RLHF models such as ChatGPT and Vicuna, have you noticed them generating safe responses like \"As an AI, I can't...\"? If so, how do you handle these situations in your evaluation?\n2. Do you notice a preference for ChatGPT agents evaluated by humans since you use GPT as the evaluator?\n3. Do you observe hallucinations in scenes? If so, how do you prevent them?\n\nmissing_references: SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization - Kim et al.\n\ntypos_grammar_style_and_presentation_improvements: L447 OpenAI\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "176_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_176_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7089666666666666,
      "max_similarity": 0.7182,
      "avg_coverage": 0.5573,
      "max_coverage": 0.6667
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 648,
      "avg_human_length": 363.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 12,
      "suggestions_count": 12
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "pXpYHwQuz7",
        "similarity": 0.7182,
        "coverage": 0.5385,
        "human_length": 358,
        "human_text": "paper_topic_and_main_contributions: This paper investigates how to simulate a specific person with large language models (LLMs). Different from previous works that prompt existing LLM, the authors construct the experience of that person and then fine-tune an LLM on the experience dataset. The experience consists of (1) character experience generated by another LLM based on the profile, and (2) protective experience that shows the person has no idea about unrelated knowledge. Evaluated by GPT 3.5, the trained model performs better than Alpaca, Vicuna, and is comparable with ChatGPT, in several aspects such as memorizing the character profile and reducing hallucination.\n\nreasons_to_accept: 1. This paper proposes an experience reconstruction method to extend a character's profile to many detailed scenes that mimic interactions between the target character and other characters. The experience can provide more information for character simulation. This method may inspire future work on character data construction. \n2. This paper proposes constructing and using protective experience to reduce hallucination during role-playing. Trained on protective experience, the model can pretend to be ignorant of the knowledge that the character does not know. The character hallucination problem is important, and this work gives some insight.\n\nreasons_to_reject: 1. Lack of important details. It's unclear which LLM did the authors use for scene extraction and experience generation (both character experience and protective experience). Does the selection of this model important? Does the generated experience faithful and without hallucination? How many scenes are there in the protective experience for training? For evaluation, how are the questions/topics selected? \n2. Lack of ablation study to show the superiority of the proposed experience construction method. I think adding a comparison between models fine-tuned on different kinds of character data helps.\n\nquestions_for_the_authors: See above\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "LdCxndmrbd",
        "similarity": 0.6986,
        "coverage": 0.6667,
        "human_length": 336,
        "human_text": "paper_topic_and_main_contributions: The paper presents a novel approach for training trainable agents to simulate specific individuals by providing them with profiles and experiences. The authors propose a framework that includes experience reconstruction, experience upload, and protective experiences. They evaluate the trained agents through interviews and compare them with baseline models. The results show that the trainable agents outperform the baselines in terms of personality, memorization, hallucination, and stability. The paper provides valuable insights into building better character simulacra.\n\nreasons_to_accept: (1) The paper introduces a novel approach for training trainable agents to simulate specific individuals, which is a step closer to character simulacra. \n(2) The framework proposed by the authors, including experience reconstruction, experience upload, and protective experiences, is well-structured and practical. \n(3) The evaluation process, including interviews and AI-based judging, provides comprehensive insights into the performance of the trained agents.\n\nreasons_to_reject: (1) The paper lacks implementation details, such as the specific architecture and training settings used for the trainable agents. \n(2) The evaluation could benefit from comparisons with more baselines and a larger number of interview questions. \n(3) The paper could provide more in-depth discussions on the limitations and potential ethical concerns of using trainable agents.\n\nquestions_for_the_authors: (1) Could you please provide more details about the implementation of the trainable agents, such as the specific architecture and training settings used? \n(2) Have you considered comparing the trained agents with more baselines, such as other instruction-tuned models or models trained with different methodologies? \n(3) Can you discuss the potential ethical concerns of using trainable agents and how you plan to address them?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "WNkPFljv4L",
        "similarity": 0.7101,
        "coverage": 0.4667,
        "human_length": 395,
        "human_text": "paper_topic_and_main_contributions: This paper introduces an innovative pipeline designed to construct trainable agents on LLMs that can convincingly emulate specific historical figures, such as Beethoven. The pipeline commences with the collection of comprehensive profiles for the chosen characters. These profiles are then utilized to prompt the LLMs to generate detailed scenes and experiences that are characteristic of these figures. Subsequently, a specialized LLM is fine-tuned based on these character-specific experiences, resulting in a convincing simulacrum of the character. Experimental evaluations, conducted using ChatGPT as a benchmark, demonstrate that the performance of these character simulacra is comparable to ChatGPT.\n\nreasons_to_accept: 1. The paper introduces a novel pipeline for creating trainable agents on LLMs that can emulate specific characters. This approach is innovative and could have wide-ranging applications in various fields, such as virtual assistants, education, and entertainment.\n2. This paper presents a thorough process that covers everything from gathering character profiles to refining LLMs using those profiles. This meticulous method could serve as a guide for other scholars in the same field.\n3. The paper also discusses potential flaws and areas for improvement in the proposed approach, indicating a thorough and thoughtful analysis.\n\nreasons_to_reject: 1. While the Protective Experience method is used to reduce hallucination issues, there is a possibility that it may result in bias or incorrect information being incorporated into the agent. This is a matter of concern for future usage.\n2. Including a human evaluation could potentially increase its credibility.\n\nquestions_for_the_authors: 1. In your experiments with RLHF-trained or distilled from RLHF models such as ChatGPT and Vicuna, have you noticed them generating safe responses like \"As an AI, I can't...\"? If so, how do you handle these situations in your evaluation?\n2. Do you notice a preference for ChatGPT agents evaluated by humans since you use GPT as the evaluator?\n3. Do you observe hallucinations in scenes? If so, how do you prevent them?\n\nmissing_references: SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization - Kim et al.\n\ntypos_grammar_style_and_presentation_improvements: L447 OpenAI\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "pXpYHwQuz7",
        "length": 358,
        "human_text": "paper_topic_and_main_contributions: This paper investigates how to simulate a specific person with large language models (LLMs). Different from previous works that prompt existing LLM, the authors construct the experience of that person and then fine-tune an LLM on the experience dataset. The experience consists of (1) character experience generated by another LLM based on the profile, and (2) protective experience that shows the person has no idea about unrelated knowledge. Evaluated by GPT 3.5, the trained model performs better than Alpaca, Vicuna, and is comparable with ChatGPT, in several aspects such as memorizing the character profile and reducing hallucination.\n\nreasons_to_accept: 1. This paper proposes an experience reconstruction method to extend a character's profile to many detailed scenes that mimic interactions between the target character and other characters. The experience can provide more information for character simulation. This method may inspire future work on character data construction. \n2. This paper proposes constructing and using protective experience to reduce hallucination during role-playing. Trained on protective experience, the model can pretend to be ignorant of the knowledge that the character does not know. The character hallucination problem is important, and this work gives some insight.\n\nreasons_to_reject: 1. Lack of important details. It's unclear which LLM did the authors use for scene extraction and experience generation (both character experience and protective experience). Does the selection of this model important? Does the generated experience faithful and without hallucination? How many scenes are there in the protective experience for training? For evaluation, how are the questions/topics selected? \n2. Lack of ablation study to show the superiority of the proposed experience construction method. I think adding a comparison between models fine-tuned on different kinds of character data helps.\n\nquestions_for_the_authors: See above\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "LdCxndmrbd",
        "length": 336,
        "human_text": "paper_topic_and_main_contributions: The paper presents a novel approach for training trainable agents to simulate specific individuals by providing them with profiles and experiences. The authors propose a framework that includes experience reconstruction, experience upload, and protective experiences. They evaluate the trained agents through interviews and compare them with baseline models. The results show that the trainable agents outperform the baselines in terms of personality, memorization, hallucination, and stability. The paper provides valuable insights into building better character simulacra.\n\nreasons_to_accept: (1) The paper introduces a novel approach for training trainable agents to simulate specific individuals, which is a step closer to character simulacra. \n(2) The framework proposed by the authors, including experience reconstruction, experience upload, and protective experiences, is well-structured and practical. \n(3) The evaluation process, including interviews and AI-based judging, provides comprehensive insights into the performance of the trained agents.\n\nreasons_to_reject: (1) The paper lacks implementation details, such as the specific architecture and training settings used for the trainable agents. \n(2) The evaluation could benefit from comparisons with more baselines and a larger number of interview questions. \n(3) The paper could provide more in-depth discussions on the limitations and potential ethical concerns of using trainable agents.\n\nquestions_for_the_authors: (1) Could you please provide more details about the implementation of the trainable agents, such as the specific architecture and training settings used? \n(2) Have you considered comparing the trained agents with more baselines, such as other instruction-tuned models or models trained with different methodologies? \n(3) Can you discuss the potential ethical concerns of using trainable agents and how you plan to address them?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "WNkPFljv4L",
        "length": 395,
        "human_text": "paper_topic_and_main_contributions: This paper introduces an innovative pipeline designed to construct trainable agents on LLMs that can convincingly emulate specific historical figures, such as Beethoven. The pipeline commences with the collection of comprehensive profiles for the chosen characters. These profiles are then utilized to prompt the LLMs to generate detailed scenes and experiences that are characteristic of these figures. Subsequently, a specialized LLM is fine-tuned based on these character-specific experiences, resulting in a convincing simulacrum of the character. Experimental evaluations, conducted using ChatGPT as a benchmark, demonstrate that the performance of these character simulacra is comparable to ChatGPT.\n\nreasons_to_accept: 1. The paper introduces a novel pipeline for creating trainable agents on LLMs that can emulate specific characters. This approach is innovative and could have wide-ranging applications in various fields, such as virtual assistants, education, and entertainment.\n2. This paper presents a thorough process that covers everything from gathering character profiles to refining LLMs using those profiles. This meticulous method could serve as a guide for other scholars in the same field.\n3. The paper also discusses potential flaws and areas for improvement in the proposed approach, indicating a thorough and thoughtful analysis.\n\nreasons_to_reject: 1. While the Protective Experience method is used to reduce hallucination issues, there is a possibility that it may result in bias or incorrect information being incorporated into the agent. This is a matter of concern for future usage.\n2. Including a human evaluation could potentially increase its credibility.\n\nquestions_for_the_authors: 1. In your experiments with RLHF-trained or distilled from RLHF models such as ChatGPT and Vicuna, have you noticed them generating safe responses like \"As an AI, I can't...\"? If so, how do you handle these situations in your evaluation?\n2. Do you notice a preference for ChatGPT agents evaluated by humans since you use GPT as the evaluator?\n3. Do you observe hallucinations in scenes? If so, how do you prevent them?\n\nmissing_references: SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization - Kim et al.\n\ntypos_grammar_style_and_presentation_improvements: L447 OpenAI\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "78_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_78_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7072333333333333,
      "max_similarity": 0.7244,
      "avg_coverage": 0.6084,
      "max_coverage": 0.6471
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 546,
      "avg_human_length": 339.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 9,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "vocEqkBxaX",
        "similarity": 0.7244,
        "coverage": 0.5417,
        "human_length": 444,
        "human_text": "paper_topic_and_main_contributions: This paper aims to improve the compositional reasoning ability of vision-language models (VLMs). To address this problem, a coarse-to-fine contrastive learning framework (MosaiCLIP) is proposed, which has three major technical innovations: - **Scene graph guided text decomposition**: The image caption is converted into a graph structure using a text scene graph parser and the sub-graphs are extracted as positive samples in image-text contrastive learning. Specifically, the sub-graphs are converted to sentences based on a template before being fed to the VLM text encoder.\n- **Negative sub-graph creation**: Hard negative sub-graphs are created by three operations, i.e., (1) node swapping and replacement, (2) edge replacement and (3) Connecting sub-graphs.\n- **Curriculum and robust fine-tuning**: To bridge the gap in training objectives between conventional CLIP model and MosaiCLIP, a two-stage fine-tuning strategy is proposed. In the first stage, each image is only associated with a single positive sub-graph and a negative sub-graph. In the second stage, each image has multi-stage positive and negative sub-graphs.\nThe empirical studies demonstrate that MosaiCLIP outperforms existing CLIP-based VLMs in terms of visio-linguistic compositional reasoning. Moreover, MosaiCLIP is more robust when the image captions in the training data are noisy. The reason behind the improvement of MosaiCLIP is also explained based on the improved Tree-Score of the text encoder.\n\nreasons_to_accept: - The proposed coarse-to-fine contrastive learning framework, which targets the viso-linguistic compositional reasoning problem, is intuitive and each component of MosaiCLIP is well-designed.\n- The experiments are comprehensive and the results are convincing, which validate the superiority of MosaiCLIP in viso-linguistic compositional reasoning. The ablation studies suggest that every component of MosaiCLIP is conducive.\n- The analysis on the reason behind MosaiCLIP\u2019s improvement is instructive.\n- The limitations of this work are properly recognized and discussed.  - The paper is well-written and easy to follow.\n\nreasons_to_reject: - As discussed in the Limitation section, it is unclear whether the proposed method can bring improvement to more advanced VLMs like BLIP.\n- There is no evaluation on standard image-text retrieval tasks (e.g., on COCO), in addition to the evaluation on compositional reasoning benchmarks.\n\nquestions_for_the_authors: What are the templates when there are multiple nodes in the sub-graphs? Could you provide some specific examples?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: Some abbreviations are inconsistent, e.g., Fig.3 (line 452) and Figures 4 (line 467).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "nVWkUP3FTG",
        "similarity": 0.7013,
        "coverage": 0.6471,
        "human_length": 259,
        "human_text": "paper_topic_and_main_contributions: This article presents a scene graph-based image-text contrastive learning method. By incorporating scene graphs, the fine-grained control of contrastive learning is achieved, and experimental results demonstrate performance improvement compared to the baseline.\n\nreasons_to_accept: 1. It is commendable that the experiments in this study were conducted in a thorough and reliable manner, providing substantial evidence for the model's performance. The validation of the motivation behind the proposed approach adds further credibility to the research findings.\n2. The method is indeed novel and inspiring, offering fresh perspectives in the field.\n\nreasons_to_reject: 1. Indeed, the method's success heavily relies on the quality of scene graph generation. If errors occur during scene graph generation, it may lead to subsequent inaccuracies in the results. Ensuring a reliable and accurate scene graph generation process is crucial for the overall effectiveness of the approach.\n2.The process of extracting scene graphs may consume significant computational resources, and in situations where the scene is complex, it might not be possible to obtain complete or accurate scene graph information. This can potentially harm the model's performance.\n3.Compared to other state-of-the-art models in the same field, the performance of this method is not particularly outstanding.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "zjQZ0XO3Yu",
        "similarity": 0.696,
        "coverage": 0.6364,
        "human_length": 315,
        "human_text": "paper_topic_and_main_contributions: The paper proposed MosaiCLIP, a framework to decompose text into scene graphs for image-text contrastive learning. It incorporates hard-negative mining via text scene graph transformations and provides a coarse-to-fine contrastive learning strategy. The efficacy of MosaiCLIP is validated through comprehensive experiments across multiple architectures, datasets, training fashions, and compositional benchmarks.\n\nreasons_to_accept: 1. They proposed to parse scene graphs from the text for contrastive Image-text pre-training. The text scene graphs enable multiple positive samples and hard negative mining, which facilitate contrastive training. The idea is interesting and novel to some degree. \n2. The experimental result is impressive, showing a decent gain of the proposed model over previous methods.\n\nreasons_to_reject: 1. Lack of enough comparison with previous works. There are also other works utilizing more types of negative samples such as DeCLIP, etc, which is not compared in the experiments. \n2. I wonder if the performance improvement is brought by the proposed method or just a larger batch size brought by more negative samples. Are NegCLIP/CLIP and the proposed method in comparison using the same text batch size? If not so, it's a necessary comparison that adds more negative text samples in the original CLIP or NegCLIP so that the total text bz is the same as the proposed method. \n3. Evaluation on image-text retrieval is missed.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "vocEqkBxaX",
        "length": 444,
        "human_text": "paper_topic_and_main_contributions: This paper aims to improve the compositional reasoning ability of vision-language models (VLMs). To address this problem, a coarse-to-fine contrastive learning framework (MosaiCLIP) is proposed, which has three major technical innovations: - **Scene graph guided text decomposition**: The image caption is converted into a graph structure using a text scene graph parser and the sub-graphs are extracted as positive samples in image-text contrastive learning. Specifically, the sub-graphs are converted to sentences based on a template before being fed to the VLM text encoder.\n- **Negative sub-graph creation**: Hard negative sub-graphs are created by three operations, i.e., (1) node swapping and replacement, (2) edge replacement and (3) Connecting sub-graphs.\n- **Curriculum and robust fine-tuning**: To bridge the gap in training objectives between conventional CLIP model and MosaiCLIP, a two-stage fine-tuning strategy is proposed. In the first stage, each image is only associated with a single positive sub-graph and a negative sub-graph. In the second stage, each image has multi-stage positive and negative sub-graphs.\nThe empirical studies demonstrate that MosaiCLIP outperforms existing CLIP-based VLMs in terms of visio-linguistic compositional reasoning. Moreover, MosaiCLIP is more robust when the image captions in the training data are noisy. The reason behind the improvement of MosaiCLIP is also explained based on the improved Tree-Score of the text encoder.\n\nreasons_to_accept: - The proposed coarse-to-fine contrastive learning framework, which targets the viso-linguistic compositional reasoning problem, is intuitive and each component of MosaiCLIP is well-designed.\n- The experiments are comprehensive and the results are convincing, which validate the superiority of MosaiCLIP in viso-linguistic compositional reasoning. The ablation studies suggest that every component of MosaiCLIP is conducive.\n- The analysis on the reason behind MosaiCLIP\u2019s improvement is instructive.\n- The limitations of this work are properly recognized and discussed.  - The paper is well-written and easy to follow.\n\nreasons_to_reject: - As discussed in the Limitation section, it is unclear whether the proposed method can bring improvement to more advanced VLMs like BLIP.\n- There is no evaluation on standard image-text retrieval tasks (e.g., on COCO), in addition to the evaluation on compositional reasoning benchmarks.\n\nquestions_for_the_authors: What are the templates when there are multiple nodes in the sub-graphs? Could you provide some specific examples?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: Some abbreviations are inconsistent, e.g., Fig.3 (line 452) and Figures 4 (line 467).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "nVWkUP3FTG",
        "length": 259,
        "human_text": "paper_topic_and_main_contributions: This article presents a scene graph-based image-text contrastive learning method. By incorporating scene graphs, the fine-grained control of contrastive learning is achieved, and experimental results demonstrate performance improvement compared to the baseline.\n\nreasons_to_accept: 1. It is commendable that the experiments in this study were conducted in a thorough and reliable manner, providing substantial evidence for the model's performance. The validation of the motivation behind the proposed approach adds further credibility to the research findings.\n2. The method is indeed novel and inspiring, offering fresh perspectives in the field.\n\nreasons_to_reject: 1. Indeed, the method's success heavily relies on the quality of scene graph generation. If errors occur during scene graph generation, it may lead to subsequent inaccuracies in the results. Ensuring a reliable and accurate scene graph generation process is crucial for the overall effectiveness of the approach.\n2.The process of extracting scene graphs may consume significant computational resources, and in situations where the scene is complex, it might not be possible to obtain complete or accurate scene graph information. This can potentially harm the model's performance.\n3.Compared to other state-of-the-art models in the same field, the performance of this method is not particularly outstanding.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "zjQZ0XO3Yu",
        "length": 315,
        "human_text": "paper_topic_and_main_contributions: The paper proposed MosaiCLIP, a framework to decompose text into scene graphs for image-text contrastive learning. It incorporates hard-negative mining via text scene graph transformations and provides a coarse-to-fine contrastive learning strategy. The efficacy of MosaiCLIP is validated through comprehensive experiments across multiple architectures, datasets, training fashions, and compositional benchmarks.\n\nreasons_to_accept: 1. They proposed to parse scene graphs from the text for contrastive Image-text pre-training. The text scene graphs enable multiple positive samples and hard negative mining, which facilitate contrastive training. The idea is interesting and novel to some degree. \n2. The experimental result is impressive, showing a decent gain of the proposed model over previous methods.\n\nreasons_to_reject: 1. Lack of enough comparison with previous works. There are also other works utilizing more types of negative samples such as DeCLIP, etc, which is not compared in the experiments. \n2. I wonder if the performance improvement is brought by the proposed method or just a larger batch size brought by more negative samples. Are NegCLIP/CLIP and the proposed method in comparison using the same text batch size? If not so, it's a necessary comparison that adds more negative text samples in the original CLIP or NegCLIP so that the total text bz is the same as the proposed method. \n3. Evaluation on image-text retrieval is missed.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "78_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_78_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7223333333333333,
      "max_similarity": 0.7424,
      "avg_coverage": 0.5294,
      "max_coverage": 0.5882
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 439,
      "avg_human_length": 339.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 7,
      "suggestions_count": 3
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "vocEqkBxaX",
        "similarity": 0.7424,
        "coverage": 0.5,
        "human_length": 444,
        "human_text": "paper_topic_and_main_contributions: This paper aims to improve the compositional reasoning ability of vision-language models (VLMs). To address this problem, a coarse-to-fine contrastive learning framework (MosaiCLIP) is proposed, which has three major technical innovations: - **Scene graph guided text decomposition**: The image caption is converted into a graph structure using a text scene graph parser and the sub-graphs are extracted as positive samples in image-text contrastive learning. Specifically, the sub-graphs are converted to sentences based on a template before being fed to the VLM text encoder.\n- **Negative sub-graph creation**: Hard negative sub-graphs are created by three operations, i.e., (1) node swapping and replacement, (2) edge replacement and (3) Connecting sub-graphs.\n- **Curriculum and robust fine-tuning**: To bridge the gap in training objectives between conventional CLIP model and MosaiCLIP, a two-stage fine-tuning strategy is proposed. In the first stage, each image is only associated with a single positive sub-graph and a negative sub-graph. In the second stage, each image has multi-stage positive and negative sub-graphs.\nThe empirical studies demonstrate that MosaiCLIP outperforms existing CLIP-based VLMs in terms of visio-linguistic compositional reasoning. Moreover, MosaiCLIP is more robust when the image captions in the training data are noisy. The reason behind the improvement of MosaiCLIP is also explained based on the improved Tree-Score of the text encoder.\n\nreasons_to_accept: - The proposed coarse-to-fine contrastive learning framework, which targets the viso-linguistic compositional reasoning problem, is intuitive and each component of MosaiCLIP is well-designed.\n- The experiments are comprehensive and the results are convincing, which validate the superiority of MosaiCLIP in viso-linguistic compositional reasoning. The ablation studies suggest that every component of MosaiCLIP is conducive.\n- The analysis on the reason behind MosaiCLIP\u2019s improvement is instructive.\n- The limitations of this work are properly recognized and discussed.  - The paper is well-written and easy to follow.\n\nreasons_to_reject: - As discussed in the Limitation section, it is unclear whether the proposed method can bring improvement to more advanced VLMs like BLIP.\n- There is no evaluation on standard image-text retrieval tasks (e.g., on COCO), in addition to the evaluation on compositional reasoning benchmarks.\n\nquestions_for_the_authors: What are the templates when there are multiple nodes in the sub-graphs? Could you provide some specific examples?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: Some abbreviations are inconsistent, e.g., Fig.3 (line 452) and Figures 4 (line 467).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "nVWkUP3FTG",
        "similarity": 0.7125,
        "coverage": 0.5882,
        "human_length": 259,
        "human_text": "paper_topic_and_main_contributions: This article presents a scene graph-based image-text contrastive learning method. By incorporating scene graphs, the fine-grained control of contrastive learning is achieved, and experimental results demonstrate performance improvement compared to the baseline.\n\nreasons_to_accept: 1. It is commendable that the experiments in this study were conducted in a thorough and reliable manner, providing substantial evidence for the model's performance. The validation of the motivation behind the proposed approach adds further credibility to the research findings.\n2. The method is indeed novel and inspiring, offering fresh perspectives in the field.\n\nreasons_to_reject: 1. Indeed, the method's success heavily relies on the quality of scene graph generation. If errors occur during scene graph generation, it may lead to subsequent inaccuracies in the results. Ensuring a reliable and accurate scene graph generation process is crucial for the overall effectiveness of the approach.\n2.The process of extracting scene graphs may consume significant computational resources, and in situations where the scene is complex, it might not be possible to obtain complete or accurate scene graph information. This can potentially harm the model's performance.\n3.Compared to other state-of-the-art models in the same field, the performance of this method is not particularly outstanding.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "zjQZ0XO3Yu",
        "similarity": 0.7121,
        "coverage": 0.5,
        "human_length": 315,
        "human_text": "paper_topic_and_main_contributions: The paper proposed MosaiCLIP, a framework to decompose text into scene graphs for image-text contrastive learning. It incorporates hard-negative mining via text scene graph transformations and provides a coarse-to-fine contrastive learning strategy. The efficacy of MosaiCLIP is validated through comprehensive experiments across multiple architectures, datasets, training fashions, and compositional benchmarks.\n\nreasons_to_accept: 1. They proposed to parse scene graphs from the text for contrastive Image-text pre-training. The text scene graphs enable multiple positive samples and hard negative mining, which facilitate contrastive training. The idea is interesting and novel to some degree. \n2. The experimental result is impressive, showing a decent gain of the proposed model over previous methods.\n\nreasons_to_reject: 1. Lack of enough comparison with previous works. There are also other works utilizing more types of negative samples such as DeCLIP, etc, which is not compared in the experiments. \n2. I wonder if the performance improvement is brought by the proposed method or just a larger batch size brought by more negative samples. Are NegCLIP/CLIP and the proposed method in comparison using the same text batch size? If not so, it's a necessary comparison that adds more negative text samples in the original CLIP or NegCLIP so that the total text bz is the same as the proposed method. \n3. Evaluation on image-text retrieval is missed.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "vocEqkBxaX",
        "length": 444,
        "human_text": "paper_topic_and_main_contributions: This paper aims to improve the compositional reasoning ability of vision-language models (VLMs). To address this problem, a coarse-to-fine contrastive learning framework (MosaiCLIP) is proposed, which has three major technical innovations: - **Scene graph guided text decomposition**: The image caption is converted into a graph structure using a text scene graph parser and the sub-graphs are extracted as positive samples in image-text contrastive learning. Specifically, the sub-graphs are converted to sentences based on a template before being fed to the VLM text encoder.\n- **Negative sub-graph creation**: Hard negative sub-graphs are created by three operations, i.e., (1) node swapping and replacement, (2) edge replacement and (3) Connecting sub-graphs.\n- **Curriculum and robust fine-tuning**: To bridge the gap in training objectives between conventional CLIP model and MosaiCLIP, a two-stage fine-tuning strategy is proposed. In the first stage, each image is only associated with a single positive sub-graph and a negative sub-graph. In the second stage, each image has multi-stage positive and negative sub-graphs.\nThe empirical studies demonstrate that MosaiCLIP outperforms existing CLIP-based VLMs in terms of visio-linguistic compositional reasoning. Moreover, MosaiCLIP is more robust when the image captions in the training data are noisy. The reason behind the improvement of MosaiCLIP is also explained based on the improved Tree-Score of the text encoder.\n\nreasons_to_accept: - The proposed coarse-to-fine contrastive learning framework, which targets the viso-linguistic compositional reasoning problem, is intuitive and each component of MosaiCLIP is well-designed.\n- The experiments are comprehensive and the results are convincing, which validate the superiority of MosaiCLIP in viso-linguistic compositional reasoning. The ablation studies suggest that every component of MosaiCLIP is conducive.\n- The analysis on the reason behind MosaiCLIP\u2019s improvement is instructive.\n- The limitations of this work are properly recognized and discussed.  - The paper is well-written and easy to follow.\n\nreasons_to_reject: - As discussed in the Limitation section, it is unclear whether the proposed method can bring improvement to more advanced VLMs like BLIP.\n- There is no evaluation on standard image-text retrieval tasks (e.g., on COCO), in addition to the evaluation on compositional reasoning benchmarks.\n\nquestions_for_the_authors: What are the templates when there are multiple nodes in the sub-graphs? Could you provide some specific examples?\n\nmissing_references: N/A\n\ntypos_grammar_style_and_presentation_improvements: Some abbreviations are inconsistent, e.g., Fig.3 (line 452) and Figures 4 (line 467).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "nVWkUP3FTG",
        "length": 259,
        "human_text": "paper_topic_and_main_contributions: This article presents a scene graph-based image-text contrastive learning method. By incorporating scene graphs, the fine-grained control of contrastive learning is achieved, and experimental results demonstrate performance improvement compared to the baseline.\n\nreasons_to_accept: 1. It is commendable that the experiments in this study were conducted in a thorough and reliable manner, providing substantial evidence for the model's performance. The validation of the motivation behind the proposed approach adds further credibility to the research findings.\n2. The method is indeed novel and inspiring, offering fresh perspectives in the field.\n\nreasons_to_reject: 1. Indeed, the method's success heavily relies on the quality of scene graph generation. If errors occur during scene graph generation, it may lead to subsequent inaccuracies in the results. Ensuring a reliable and accurate scene graph generation process is crucial for the overall effectiveness of the approach.\n2.The process of extracting scene graphs may consume significant computational resources, and in situations where the scene is complex, it might not be possible to obtain complete or accurate scene graph information. This can potentially harm the model's performance.\n3.Compared to other state-of-the-art models in the same field, the performance of this method is not particularly outstanding.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "zjQZ0XO3Yu",
        "length": 315,
        "human_text": "paper_topic_and_main_contributions: The paper proposed MosaiCLIP, a framework to decompose text into scene graphs for image-text contrastive learning. It incorporates hard-negative mining via text scene graph transformations and provides a coarse-to-fine contrastive learning strategy. The efficacy of MosaiCLIP is validated through comprehensive experiments across multiple architectures, datasets, training fashions, and compositional benchmarks.\n\nreasons_to_accept: 1. They proposed to parse scene graphs from the text for contrastive Image-text pre-training. The text scene graphs enable multiple positive samples and hard negative mining, which facilitate contrastive training. The idea is interesting and novel to some degree. \n2. The experimental result is impressive, showing a decent gain of the proposed model over previous methods.\n\nreasons_to_reject: 1. Lack of enough comparison with previous works. There are also other works utilizing more types of negative samples such as DeCLIP, etc, which is not compared in the experiments. \n2. I wonder if the performance improvement is brought by the proposed method or just a larger batch size brought by more negative samples. Are NegCLIP/CLIP and the proposed method in comparison using the same text batch size? If not so, it's a necessary comparison that adds more negative text samples in the original CLIP or NegCLIP so that the total text bz is the same as the proposed method. \n3. Evaluation on image-text retrieval is missed.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "153_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_153_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7491,
      "max_similarity": 0.7607,
      "avg_coverage": 0.3854,
      "max_coverage": 0.5
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 385,
      "avg_human_length": 497.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 6,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "5oKNfku6S9",
        "similarity": 0.7482,
        "coverage": 0.5,
        "human_length": 396,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on spatial role extraction and spatial relation extraction. Its main contribution is to disentange the processes of spatial relation extraction to different stages. In details, it proposes three different models: a pipeline of extraction and symbolic reasoning, an end-to-end PLM in a QA format, and  an end-to-end neural model with explicit layers of extraction and reasoning. The experimental results on multiple datasets show the effectiveness of the proposed models.\n\nreasons_to_accept: 1) This paper proposed three different model on spatial relation extraction.\n2) The proposed models achieve good performance on multiple datasets.\n3) This paper is well-written.\n\nreasons_to_reject: 1) I mainly concern its novelty. The three proposed models are not new, because pipeline models and QA-style models are widely used in IE field.\n2) Which model is the best for a IE task? Pipeline model? Unified model? Joint model? or multitask model? I think this paper should compare   the proposed models with other framework, e.g., joint or multitask framework.\n3) The baselines are weak. This paper only compared the proposed models with some basic models (e.g., Majority, BERT,GPT3) and did not compare them with the SOTA models.\n4) PISTAQ is better than SREQA on SPART datasets, However, it performs worse than SREQA on RESQ. I would like to see the reason.\n5) The motivation is not clear. This paper did not answer the question: Why is disentangling extraction and reasoning better than joint  extraction and reasoning in multi-hop spatial reasoning?\n\nquestions_for_the_authors: 1)  PISTAQ is better than SREQA on SPART datasets, However, it performs worse than SREQA on RESQ. Why?\n2) The performance of entity coreference is still poor and most models are less than 70 in F1-score. Why can the accuracy of the proposed Coref module acheive 99? How about the Precision and Recall?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "9GwC7fonMJ",
        "similarity": 0.7384,
        "coverage": 0.5,
        "human_length": 368,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to disentangle the information extraction process with reasoning process for multihop spatial reasoning dataset, and design both symbolic pipeline disentangling and end-to-end neural disentangling to achieve the objective. To showcase the effect of the disentanglement, the authors compare with PLMs such as BERT and GPT-3 which produce direct inference given the story and question.\nExtensive experiments are conducted over different datasets, automatic or human-authored, with or without SPRL labels, to empirically verify the effect of different components, and analyze the models' generalization towards unseen datasets during training.\n\nreasons_to_accept: 1. The paper raises an interesting and valuable problem of whether disentangling the extraction and reasoning process leads to improved spatial reasoning. \n2. Comprehensive model designs and comparisons are given to ablate specific functionalities with empirical results, including direct black-box inference, neural disentanglement, and symbolic disentanglement. \n3. The results indicate the potential of using specific reasoning models to replace LLMs in reasoning-intensive tasks.\n\nreasons_to_reject: 1. The descriptions are intensive, but lack clearer organizations which makes it a bit challenging to follow. A running example could be added to make description much easier. \n2. The settings of different proposed models are not so clear, as well as some of the illustrations. For example: - What are logic rules used in the prolog for the reasoning part?  - Is BERT-EQ essentially similar to BERT but with data augmentations using SPRL labels?  - For SREQA, how is entity selection  (via BIO, similar to PISTAQ?) and the final answer produced given the predicted pairs? \n3. The proposed models still require SPRL labels for training the extraction module. The authors mention that it is beneficial to use LLMs to produce zero-shot or few-shot extractions, but is there any empirical results for the final performance?\n\nquestions_for_the_authors: Refer to the above.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "7uLKGE3apI",
        "similarity": 0.7607,
        "coverage": 0.1562,
        "human_length": 727,
        "human_text": "paper_topic_and_main_contributions: To show benefits of segregating the processes of information extraction and reasoning in the task of spatial question answering, the paper proposes 3 models: PistaQ, BERT-EQ and SREQA. In brief, PistaQ is a model based on extracting relations and then perform symbolic reasoning. BERT-EQ is an end-to-end pre-trained language model that uses the same spatial information supervision but in question-answer format. Lastly, SREQA model is again an end-to-end neural model with explicit layers of extraction and reasoning. Various experiments are conducted over 3 datasets: SPARTQA, SPARTUN and RESQ, which show the efficacy of separating the processes information extraction and reasoning.\n\nreasons_to_accept: 1. \tThis paper demonstrates the effectiveness of separating the process of information extraction from reasoning while performing spatial question answering. \n2. \tAlthough the quantitative and qualitative are strong against the baseline models, yet, they may lead a reader to a dilemma on which model, PISTA-Q or SREQA, to use for any use- case.\n\nreasons_to_reject: 1. \tThe structure of the paper is not good. For instance,        a.\tText below section 3 should briefly introduce all the 3 models (with names of models) and should refer to subsections 3.1, 3.2 and 3.3 to provide the reader an idea of what to expect. \n      b.\tNames of the models should be easy to remember. For example, what is the purpose of EQ in BERT-EQ. As a reader, I don\u2019t understand it until I read section 3.2. \n      c.\tHeading of Section 3.1 should be PISTAQ: \u2026.. to make it evident that the section talks about first model. \n      d.\tHeading of Section 3.2 should be BERT-EQ: \u2026.. to make it evident that the section talks about second model. \n      e.\tHeading of Section 3.3 should be SREQA: \u2026.. to make it evident that the section talks about second model. \n      f.\tIs the section starting at Line 525 a subsection or something different?\n2. \tTable 2 does not help in dissecting the attributes of proposed models among each other and with the baselines. The authors are requested to make it easy for the readers to get an understanding of the various models. For instance, a suggestion is to make a table where every row is a model name and columns are 4 or 5 attributes which the authors feel critical to distinguish the various models. This will help in the ablation analysis aswell. Moreover, current Table 2 does not contain baselines: Majority Baseline and GT-PISTAQ. The purpose of this table is to give readers a quick glance about all the models discussed in the paper with their attributes.\n3. \tSections discussing results simply report the results without any interesting insights. My suggestion will be to discuss the results with respect to the new Table 2 mentioned in point 2. This will keep readers interested in reading the results with some insights about attributes of the proposed models.\n4. \tThe paper uses too many acronyms, and some acronyms have an unusual mention. For eg. SPARTUN where size of \u201cS\u201d is smaller than \u201cRTUN\u201d but bigger than \u201cPA\u201d.\n5. \tThe authors are requested to create separate sections for quantitative results which can have Table 3-6 at one place and a qualitative section which can have Figures 4 and 5.\n\ntypos_grammar_style_and_presentation_improvements: 1. \tIn Table 3, dataset MSPRL is mentioned. However, I don\u2019t see MSPRL described in section 4.1 2. \tLine140: Reasoning -> reasoning.\n3. \tInconsistent usage of reference to artifacts. For instance, \u201cFigure 1\u201d in Line 058 and \u201cFig 1\u201d in Line 156.\n4. \tFigure 2 and the text in section 3.1 should match to make the reader understand the model easily. For instance:         a.\tIn Figure 2, why is \u201cCoref Resolution\u201d missing in Question Processing? \n        b.\tWhat process does Spatial Reasoner follow to generate the answer? Currently, its not clear either from the Figure 2 or paragraph in Line222-229.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "5oKNfku6S9",
        "length": 396,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on spatial role extraction and spatial relation extraction. Its main contribution is to disentange the processes of spatial relation extraction to different stages. In details, it proposes three different models: a pipeline of extraction and symbolic reasoning, an end-to-end PLM in a QA format, and  an end-to-end neural model with explicit layers of extraction and reasoning. The experimental results on multiple datasets show the effectiveness of the proposed models.\n\nreasons_to_accept: 1) This paper proposed three different model on spatial relation extraction.\n2) The proposed models achieve good performance on multiple datasets.\n3) This paper is well-written.\n\nreasons_to_reject: 1) I mainly concern its novelty. The three proposed models are not new, because pipeline models and QA-style models are widely used in IE field.\n2) Which model is the best for a IE task? Pipeline model? Unified model? Joint model? or multitask model? I think this paper should compare   the proposed models with other framework, e.g., joint or multitask framework.\n3) The baselines are weak. This paper only compared the proposed models with some basic models (e.g., Majority, BERT,GPT3) and did not compare them with the SOTA models.\n4) PISTAQ is better than SREQA on SPART datasets, However, it performs worse than SREQA on RESQ. I would like to see the reason.\n5) The motivation is not clear. This paper did not answer the question: Why is disentangling extraction and reasoning better than joint  extraction and reasoning in multi-hop spatial reasoning?\n\nquestions_for_the_authors: 1)  PISTAQ is better than SREQA on SPART datasets, However, it performs worse than SREQA on RESQ. Why?\n2) The performance of entity coreference is still poor and most models are less than 70 in F1-score. Why can the accuracy of the proposed Coref module acheive 99? How about the Precision and Recall?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "9GwC7fonMJ",
        "length": 368,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to disentangle the information extraction process with reasoning process for multihop spatial reasoning dataset, and design both symbolic pipeline disentangling and end-to-end neural disentangling to achieve the objective. To showcase the effect of the disentanglement, the authors compare with PLMs such as BERT and GPT-3 which produce direct inference given the story and question.\nExtensive experiments are conducted over different datasets, automatic or human-authored, with or without SPRL labels, to empirically verify the effect of different components, and analyze the models' generalization towards unseen datasets during training.\n\nreasons_to_accept: 1. The paper raises an interesting and valuable problem of whether disentangling the extraction and reasoning process leads to improved spatial reasoning. \n2. Comprehensive model designs and comparisons are given to ablate specific functionalities with empirical results, including direct black-box inference, neural disentanglement, and symbolic disentanglement. \n3. The results indicate the potential of using specific reasoning models to replace LLMs in reasoning-intensive tasks.\n\nreasons_to_reject: 1. The descriptions are intensive, but lack clearer organizations which makes it a bit challenging to follow. A running example could be added to make description much easier. \n2. The settings of different proposed models are not so clear, as well as some of the illustrations. For example: - What are logic rules used in the prolog for the reasoning part?  - Is BERT-EQ essentially similar to BERT but with data augmentations using SPRL labels?  - For SREQA, how is entity selection  (via BIO, similar to PISTAQ?) and the final answer produced given the predicted pairs? \n3. The proposed models still require SPRL labels for training the extraction module. The authors mention that it is beneficial to use LLMs to produce zero-shot or few-shot extractions, but is there any empirical results for the final performance?\n\nquestions_for_the_authors: Refer to the above.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "7uLKGE3apI",
        "length": 727,
        "human_text": "paper_topic_and_main_contributions: To show benefits of segregating the processes of information extraction and reasoning in the task of spatial question answering, the paper proposes 3 models: PistaQ, BERT-EQ and SREQA. In brief, PistaQ is a model based on extracting relations and then perform symbolic reasoning. BERT-EQ is an end-to-end pre-trained language model that uses the same spatial information supervision but in question-answer format. Lastly, SREQA model is again an end-to-end neural model with explicit layers of extraction and reasoning. Various experiments are conducted over 3 datasets: SPARTQA, SPARTUN and RESQ, which show the efficacy of separating the processes information extraction and reasoning.\n\nreasons_to_accept: 1. \tThis paper demonstrates the effectiveness of separating the process of information extraction from reasoning while performing spatial question answering. \n2. \tAlthough the quantitative and qualitative are strong against the baseline models, yet, they may lead a reader to a dilemma on which model, PISTA-Q or SREQA, to use for any use- case.\n\nreasons_to_reject: 1. \tThe structure of the paper is not good. For instance,        a.\tText below section 3 should briefly introduce all the 3 models (with names of models) and should refer to subsections 3.1, 3.2 and 3.3 to provide the reader an idea of what to expect. \n      b.\tNames of the models should be easy to remember. For example, what is the purpose of EQ in BERT-EQ. As a reader, I don\u2019t understand it until I read section 3.2. \n      c.\tHeading of Section 3.1 should be PISTAQ: \u2026.. to make it evident that the section talks about first model. \n      d.\tHeading of Section 3.2 should be BERT-EQ: \u2026.. to make it evident that the section talks about second model. \n      e.\tHeading of Section 3.3 should be SREQA: \u2026.. to make it evident that the section talks about second model. \n      f.\tIs the section starting at Line 525 a subsection or something different?\n2. \tTable 2 does not help in dissecting the attributes of proposed models among each other and with the baselines. The authors are requested to make it easy for the readers to get an understanding of the various models. For instance, a suggestion is to make a table where every row is a model name and columns are 4 or 5 attributes which the authors feel critical to distinguish the various models. This will help in the ablation analysis aswell. Moreover, current Table 2 does not contain baselines: Majority Baseline and GT-PISTAQ. The purpose of this table is to give readers a quick glance about all the models discussed in the paper with their attributes.\n3. \tSections discussing results simply report the results without any interesting insights. My suggestion will be to discuss the results with respect to the new Table 2 mentioned in point 2. This will keep readers interested in reading the results with some insights about attributes of the proposed models.\n4. \tThe paper uses too many acronyms, and some acronyms have an unusual mention. For eg. SPARTUN where size of \u201cS\u201d is smaller than \u201cRTUN\u201d but bigger than \u201cPA\u201d.\n5. \tThe authors are requested to create separate sections for quantitative results which can have Table 3-6 at one place and a qualitative section which can have Figures 4 and 5.\n\ntypos_grammar_style_and_presentation_improvements: 1. \tIn Table 3, dataset MSPRL is mentioned. However, I don\u2019t see MSPRL described in section 4.1 2. \tLine140: Reasoning -> reasoning.\n3. \tInconsistent usage of reference to artifacts. For instance, \u201cFigure 1\u201d in Line 058 and \u201cFig 1\u201d in Line 156.\n4. \tFigure 2 and the text in section 3.1 should match to make the reader understand the model easily. For instance:         a.\tIn Figure 2, why is \u201cCoref Resolution\u201d missing in Question Processing? \n        b.\tWhat process does Spatial Reasoner follow to generate the answer? Currently, its not clear either from the Figure 2 or paragraph in Line222-229.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "153_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_153_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7538999999999999,
      "max_similarity": 0.7666,
      "avg_coverage": 0.3520666666666667,
      "max_coverage": 0.5
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 430,
      "avg_human_length": 497.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 5,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "5oKNfku6S9",
        "similarity": 0.7502,
        "coverage": 0.4,
        "human_length": 396,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on spatial role extraction and spatial relation extraction. Its main contribution is to disentange the processes of spatial relation extraction to different stages. In details, it proposes three different models: a pipeline of extraction and symbolic reasoning, an end-to-end PLM in a QA format, and  an end-to-end neural model with explicit layers of extraction and reasoning. The experimental results on multiple datasets show the effectiveness of the proposed models.\n\nreasons_to_accept: 1) This paper proposed three different model on spatial relation extraction.\n2) The proposed models achieve good performance on multiple datasets.\n3) This paper is well-written.\n\nreasons_to_reject: 1) I mainly concern its novelty. The three proposed models are not new, because pipeline models and QA-style models are widely used in IE field.\n2) Which model is the best for a IE task? Pipeline model? Unified model? Joint model? or multitask model? I think this paper should compare   the proposed models with other framework, e.g., joint or multitask framework.\n3) The baselines are weak. This paper only compared the proposed models with some basic models (e.g., Majority, BERT,GPT3) and did not compare them with the SOTA models.\n4) PISTAQ is better than SREQA on SPART datasets, However, it performs worse than SREQA on RESQ. I would like to see the reason.\n5) The motivation is not clear. This paper did not answer the question: Why is disentangling extraction and reasoning better than joint  extraction and reasoning in multi-hop spatial reasoning?\n\nquestions_for_the_authors: 1)  PISTAQ is better than SREQA on SPART datasets, However, it performs worse than SREQA on RESQ. Why?\n2) The performance of entity coreference is still poor and most models are less than 70 in F1-score. Why can the accuracy of the proposed Coref module acheive 99? How about the Precision and Recall?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "9GwC7fonMJ",
        "similarity": 0.7449,
        "coverage": 0.5,
        "human_length": 368,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to disentangle the information extraction process with reasoning process for multihop spatial reasoning dataset, and design both symbolic pipeline disentangling and end-to-end neural disentangling to achieve the objective. To showcase the effect of the disentanglement, the authors compare with PLMs such as BERT and GPT-3 which produce direct inference given the story and question.\nExtensive experiments are conducted over different datasets, automatic or human-authored, with or without SPRL labels, to empirically verify the effect of different components, and analyze the models' generalization towards unseen datasets during training.\n\nreasons_to_accept: 1. The paper raises an interesting and valuable problem of whether disentangling the extraction and reasoning process leads to improved spatial reasoning. \n2. Comprehensive model designs and comparisons are given to ablate specific functionalities with empirical results, including direct black-box inference, neural disentanglement, and symbolic disentanglement. \n3. The results indicate the potential of using specific reasoning models to replace LLMs in reasoning-intensive tasks.\n\nreasons_to_reject: 1. The descriptions are intensive, but lack clearer organizations which makes it a bit challenging to follow. A running example could be added to make description much easier. \n2. The settings of different proposed models are not so clear, as well as some of the illustrations. For example: - What are logic rules used in the prolog for the reasoning part?  - Is BERT-EQ essentially similar to BERT but with data augmentations using SPRL labels?  - For SREQA, how is entity selection  (via BIO, similar to PISTAQ?) and the final answer produced given the predicted pairs? \n3. The proposed models still require SPRL labels for training the extraction module. The authors mention that it is beneficial to use LLMs to produce zero-shot or few-shot extractions, but is there any empirical results for the final performance?\n\nquestions_for_the_authors: Refer to the above.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "7uLKGE3apI",
        "similarity": 0.7666,
        "coverage": 0.1562,
        "human_length": 727,
        "human_text": "paper_topic_and_main_contributions: To show benefits of segregating the processes of information extraction and reasoning in the task of spatial question answering, the paper proposes 3 models: PistaQ, BERT-EQ and SREQA. In brief, PistaQ is a model based on extracting relations and then perform symbolic reasoning. BERT-EQ is an end-to-end pre-trained language model that uses the same spatial information supervision but in question-answer format. Lastly, SREQA model is again an end-to-end neural model with explicit layers of extraction and reasoning. Various experiments are conducted over 3 datasets: SPARTQA, SPARTUN and RESQ, which show the efficacy of separating the processes information extraction and reasoning.\n\nreasons_to_accept: 1. \tThis paper demonstrates the effectiveness of separating the process of information extraction from reasoning while performing spatial question answering. \n2. \tAlthough the quantitative and qualitative are strong against the baseline models, yet, they may lead a reader to a dilemma on which model, PISTA-Q or SREQA, to use for any use- case.\n\nreasons_to_reject: 1. \tThe structure of the paper is not good. For instance,        a.\tText below section 3 should briefly introduce all the 3 models (with names of models) and should refer to subsections 3.1, 3.2 and 3.3 to provide the reader an idea of what to expect. \n      b.\tNames of the models should be easy to remember. For example, what is the purpose of EQ in BERT-EQ. As a reader, I don\u2019t understand it until I read section 3.2. \n      c.\tHeading of Section 3.1 should be PISTAQ: \u2026.. to make it evident that the section talks about first model. \n      d.\tHeading of Section 3.2 should be BERT-EQ: \u2026.. to make it evident that the section talks about second model. \n      e.\tHeading of Section 3.3 should be SREQA: \u2026.. to make it evident that the section talks about second model. \n      f.\tIs the section starting at Line 525 a subsection or something different?\n2. \tTable 2 does not help in dissecting the attributes of proposed models among each other and with the baselines. The authors are requested to make it easy for the readers to get an understanding of the various models. For instance, a suggestion is to make a table where every row is a model name and columns are 4 or 5 attributes which the authors feel critical to distinguish the various models. This will help in the ablation analysis aswell. Moreover, current Table 2 does not contain baselines: Majority Baseline and GT-PISTAQ. The purpose of this table is to give readers a quick glance about all the models discussed in the paper with their attributes.\n3. \tSections discussing results simply report the results without any interesting insights. My suggestion will be to discuss the results with respect to the new Table 2 mentioned in point 2. This will keep readers interested in reading the results with some insights about attributes of the proposed models.\n4. \tThe paper uses too many acronyms, and some acronyms have an unusual mention. For eg. SPARTUN where size of \u201cS\u201d is smaller than \u201cRTUN\u201d but bigger than \u201cPA\u201d.\n5. \tThe authors are requested to create separate sections for quantitative results which can have Table 3-6 at one place and a qualitative section which can have Figures 4 and 5.\n\ntypos_grammar_style_and_presentation_improvements: 1. \tIn Table 3, dataset MSPRL is mentioned. However, I don\u2019t see MSPRL described in section 4.1 2. \tLine140: Reasoning -> reasoning.\n3. \tInconsistent usage of reference to artifacts. For instance, \u201cFigure 1\u201d in Line 058 and \u201cFig 1\u201d in Line 156.\n4. \tFigure 2 and the text in section 3.1 should match to make the reader understand the model easily. For instance:         a.\tIn Figure 2, why is \u201cCoref Resolution\u201d missing in Question Processing? \n        b.\tWhat process does Spatial Reasoner follow to generate the answer? Currently, its not clear either from the Figure 2 or paragraph in Line222-229.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "5oKNfku6S9",
        "length": 396,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on spatial role extraction and spatial relation extraction. Its main contribution is to disentange the processes of spatial relation extraction to different stages. In details, it proposes three different models: a pipeline of extraction and symbolic reasoning, an end-to-end PLM in a QA format, and  an end-to-end neural model with explicit layers of extraction and reasoning. The experimental results on multiple datasets show the effectiveness of the proposed models.\n\nreasons_to_accept: 1) This paper proposed three different model on spatial relation extraction.\n2) The proposed models achieve good performance on multiple datasets.\n3) This paper is well-written.\n\nreasons_to_reject: 1) I mainly concern its novelty. The three proposed models are not new, because pipeline models and QA-style models are widely used in IE field.\n2) Which model is the best for a IE task? Pipeline model? Unified model? Joint model? or multitask model? I think this paper should compare   the proposed models with other framework, e.g., joint or multitask framework.\n3) The baselines are weak. This paper only compared the proposed models with some basic models (e.g., Majority, BERT,GPT3) and did not compare them with the SOTA models.\n4) PISTAQ is better than SREQA on SPART datasets, However, it performs worse than SREQA on RESQ. I would like to see the reason.\n5) The motivation is not clear. This paper did not answer the question: Why is disentangling extraction and reasoning better than joint  extraction and reasoning in multi-hop spatial reasoning?\n\nquestions_for_the_authors: 1)  PISTAQ is better than SREQA on SPART datasets, However, it performs worse than SREQA on RESQ. Why?\n2) The performance of entity coreference is still poor and most models are less than 70 in F1-score. Why can the accuracy of the proposed Coref module acheive 99? How about the Precision and Recall?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "9GwC7fonMJ",
        "length": 368,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to disentangle the information extraction process with reasoning process for multihop spatial reasoning dataset, and design both symbolic pipeline disentangling and end-to-end neural disentangling to achieve the objective. To showcase the effect of the disentanglement, the authors compare with PLMs such as BERT and GPT-3 which produce direct inference given the story and question.\nExtensive experiments are conducted over different datasets, automatic or human-authored, with or without SPRL labels, to empirically verify the effect of different components, and analyze the models' generalization towards unseen datasets during training.\n\nreasons_to_accept: 1. The paper raises an interesting and valuable problem of whether disentangling the extraction and reasoning process leads to improved spatial reasoning. \n2. Comprehensive model designs and comparisons are given to ablate specific functionalities with empirical results, including direct black-box inference, neural disentanglement, and symbolic disentanglement. \n3. The results indicate the potential of using specific reasoning models to replace LLMs in reasoning-intensive tasks.\n\nreasons_to_reject: 1. The descriptions are intensive, but lack clearer organizations which makes it a bit challenging to follow. A running example could be added to make description much easier. \n2. The settings of different proposed models are not so clear, as well as some of the illustrations. For example: - What are logic rules used in the prolog for the reasoning part?  - Is BERT-EQ essentially similar to BERT but with data augmentations using SPRL labels?  - For SREQA, how is entity selection  (via BIO, similar to PISTAQ?) and the final answer produced given the predicted pairs? \n3. The proposed models still require SPRL labels for training the extraction module. The authors mention that it is beneficial to use LLMs to produce zero-shot or few-shot extractions, but is there any empirical results for the final performance?\n\nquestions_for_the_authors: Refer to the above.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "7uLKGE3apI",
        "length": 727,
        "human_text": "paper_topic_and_main_contributions: To show benefits of segregating the processes of information extraction and reasoning in the task of spatial question answering, the paper proposes 3 models: PistaQ, BERT-EQ and SREQA. In brief, PistaQ is a model based on extracting relations and then perform symbolic reasoning. BERT-EQ is an end-to-end pre-trained language model that uses the same spatial information supervision but in question-answer format. Lastly, SREQA model is again an end-to-end neural model with explicit layers of extraction and reasoning. Various experiments are conducted over 3 datasets: SPARTQA, SPARTUN and RESQ, which show the efficacy of separating the processes information extraction and reasoning.\n\nreasons_to_accept: 1. \tThis paper demonstrates the effectiveness of separating the process of information extraction from reasoning while performing spatial question answering. \n2. \tAlthough the quantitative and qualitative are strong against the baseline models, yet, they may lead a reader to a dilemma on which model, PISTA-Q or SREQA, to use for any use- case.\n\nreasons_to_reject: 1. \tThe structure of the paper is not good. For instance,        a.\tText below section 3 should briefly introduce all the 3 models (with names of models) and should refer to subsections 3.1, 3.2 and 3.3 to provide the reader an idea of what to expect. \n      b.\tNames of the models should be easy to remember. For example, what is the purpose of EQ in BERT-EQ. As a reader, I don\u2019t understand it until I read section 3.2. \n      c.\tHeading of Section 3.1 should be PISTAQ: \u2026.. to make it evident that the section talks about first model. \n      d.\tHeading of Section 3.2 should be BERT-EQ: \u2026.. to make it evident that the section talks about second model. \n      e.\tHeading of Section 3.3 should be SREQA: \u2026.. to make it evident that the section talks about second model. \n      f.\tIs the section starting at Line 525 a subsection or something different?\n2. \tTable 2 does not help in dissecting the attributes of proposed models among each other and with the baselines. The authors are requested to make it easy for the readers to get an understanding of the various models. For instance, a suggestion is to make a table where every row is a model name and columns are 4 or 5 attributes which the authors feel critical to distinguish the various models. This will help in the ablation analysis aswell. Moreover, current Table 2 does not contain baselines: Majority Baseline and GT-PISTAQ. The purpose of this table is to give readers a quick glance about all the models discussed in the paper with their attributes.\n3. \tSections discussing results simply report the results without any interesting insights. My suggestion will be to discuss the results with respect to the new Table 2 mentioned in point 2. This will keep readers interested in reading the results with some insights about attributes of the proposed models.\n4. \tThe paper uses too many acronyms, and some acronyms have an unusual mention. For eg. SPARTUN where size of \u201cS\u201d is smaller than \u201cRTUN\u201d but bigger than \u201cPA\u201d.\n5. \tThe authors are requested to create separate sections for quantitative results which can have Table 3-6 at one place and a qualitative section which can have Figures 4 and 5.\n\ntypos_grammar_style_and_presentation_improvements: 1. \tIn Table 3, dataset MSPRL is mentioned. However, I don\u2019t see MSPRL described in section 4.1 2. \tLine140: Reasoning -> reasoning.\n3. \tInconsistent usage of reference to artifacts. For instance, \u201cFigure 1\u201d in Line 058 and \u201cFig 1\u201d in Line 156.\n4. \tFigure 2 and the text in section 3.1 should match to make the reader understand the model easily. For instance:         a.\tIn Figure 2, why is \u201cCoref Resolution\u201d missing in Question Processing? \n        b.\tWhat process does Spatial Reasoner follow to generate the answer? Currently, its not clear either from the Figure 2 or paragraph in Line222-229.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "116_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_116_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7149,
      "max_similarity": 0.7233,
      "avg_coverage": 0.4889333333333334,
      "max_coverage": 0.5238
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 463,
      "avg_human_length": 481.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 7,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "Y1KYBcpFCw",
        "similarity": 0.7233,
        "coverage": 0.4615,
        "human_length": 310,
        "human_text": "paper_topic_and_main_contributions: 1. The authors demonstrate the effectiveness of their approach in affective reasoning 2. They curate a synthetic dataset with implicit causes to visualize the latent variable in our implementation. \n3. They exhibit a significant reduction in false predictions for negative samples across three causal discrimination scenarios. \n4. They formulate the dialogue process and analyze the cogn skeleton with encoder-decoder structure, GNN, and graph attention.\n\nreasons_to_accept: 1. This paper is well-organized, clearly describing the task and problem.\n2. The proposed method is based on a high level of understanding of existing research and includes the definition of a valid hypothesis with technical descriptions.\n3. It clearly describes the limitations and provides appropriate analysis through experiments.\n\nreasons_to_reject: 1. The authors have partly addressed this in the limitations section, but considering confounding-bias and the backdoor problem is relatively weak compared to previous studies on causality.\n2. Isn't the proposed method also not a fundamental solution to the challenge of affective reasoning? If there are labels and structures that can represent causality in a given conversation situation, I think it is natural to see the effect of investing more layers and resources.\n\ntypos_grammar_style_and_presentation_improvements: Figure 3. The description of the colors and shading is missing and hard to see, which is confusing.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "bflEv1doWI",
        "similarity": 0.7066,
        "coverage": 0.4815,
        "human_length": 713,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an approach to \"affective reasoning in conversation\" formulated as a \"causal discrimination\" task, which essentially aims to classify whether a pair of conversation turns has a causal relationship (and if so, in what directionality).\nThe underlying premise of this work (which is assumed but not formally or empirically demonstrated) is that this particular task formulation of emotional causal relation classification in a pair of conversation turns is important.\nThe key thesis of this paper is that handling of i.i.d. noise is important in reasoning about such structural causal relationships, and the paper thus proposes an autoencoder architecture that can incorporate such i.i.d. noise, which presumably corresponds to the \"implicit causes\".\n\nreasons_to_accept: - the paper addresses causal reasoning, an important and open research challenge in today's AI literature and proposes an approach for structural causal reasoning, demonstrating stronger performance over baselines\n\nreasons_to_reject: - I wish the paper provided a better description of the task itself. Based just on Figure 1 example, I couldn't help but wonder if the correct answer has a trivial pattern --- that there's a causal relationship between the most recent utterance of between two speakers (100% cases?), and also that there's a causal relationship between two most recent utterances of each speaker (the vast majority of the cases?), which makes me wonder whether simple majority baselines of some sorts could've worked surprisingly well. Detecting when the causality misses between two adjacent utterances (which can happen, though less frequently) seems relatively easy due to the abrupt change of the topic. Fundamentally, I came away with the suspicion --- whether the classification formulation experimented in this paper justifies the relatively complex method proposed in this work, and I wish there was a more convincing benchmark explored in this paper.  - Concurrently, I'm not convinced if the relatively simplistic classification of causal relationship explored in this paper helps advancing the field in terms of affective understanding in conversation, especially in the context of the currently powerful LLMs that can carry about an amazing level of conversation. While causal reasoning is generally an interesting and open research question, I wonder if the particular task formulation explored in this paper is rather too narrow to be relevant in the current AI SOTA capabilities.\n- I wish the authors provided more intuition about why \"implicit causes\" correspond to \"i.i.d. noise\". While I understand to some degree that implicit causes may seem like \"noise\", various statements around this don't seem well justified or even precisely stated.  - The lack of clarity of writing is among the weakest points of this paper. At least for me, of the couple dozen papers across NLP and ML venues that I have reviewed in recent years, this one by far was the most opaque to unpack. As one of the many such examples, I found the following sentence hard to parse and hard to fully understand [line 073 - line 079]: \"In order to discriminate different causal relationships between two similar embeddings, we construct the dialogue process as a Structural Causal Model (SCM) stemming from many endeavors supporting that i.i.d. noise of SCM could facilitate the discrimination of causal relationships when finning two variables.\" In fact, I found almost half of the remaining sentences of the introduction section somewhat troublesome, sometimes because the sentences make claims that are not yet defined or justified.\n- In the same vein, the technical writing seems a bit sloppy at times. For example in Table 3, DD and IE were never defined, and the caption talks about ECPE-MLL, while it doesn't even appear in the rows of the methods listed in that Table. Moreover, the boldfaced numbers seem inconsistent. For example, GPT-4's IE score is higher than that of ECPE-2D, but not boldfaced.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "gUgIvYrh7E",
        "similarity": 0.7148,
        "coverage": 0.5238,
        "human_length": 420,
        "human_text": "paper_topic_and_main_contributions: This study concentrates on the challenge of causal discrimination, specifically on the task of affective reasoning in *conversations*. The paper reveals that almost all existing models excel at capturing semantic relatedness within utterance embeddings but fall short in determining the specific causal relationship underlying the association between two utterances on the task of causal discrimination. To overcome this limitation, the authors constructed a SCM with i.i.d. noise terms, and proposed the *cogn* framework to address the unstructured nature of conversation data. Experimental results show that the proposed approaches significantly outperform existing methods on two affective reasoning tasks including Emotion-Cause Pair Extraction (ECPE) and Emotion-Cause Span Recognition (ECSR) and one emotion recognition task (ERC), demonstrating its effectiveness in affective reasoning.\nThe main contributions are as follows: 1) The authors incorporated i.i.d. noise terms, and formulated the dialogue process as a structural causal model (SCM); 2) The authors devised the *cogn* skeleton to address the problems of variable-length and unstructured dialogue samples; 3) The authors adopted an autoencoder architecture to overcome the unobservability of implicit causes and make it learnable; and 4) The authors constructed a synthetic dataset with implicit causes and conducted extensive evaluations of the proposed method.\n\nreasons_to_accept: 1. The study reveals that when it comes to more specific causal relationships within semantically similar sentences (such as reasoning tasks), both unsupervised and supervised methods may not exhibit the same level of \u201cintelligence\u201d and output some \u201cpseudo-correlation\u201d. \n2. The discussion of the methods proposed in this work is thorough and detailed. \n3. The authors propose new methods to address the problem of causal discrimination in affective reasoning. Their approaches significantly outperform other state-of-the-art methods, including GPT-3.5 and GPT-4.\n\nreasons_to_reject: There are too many abbreviations throughout the paper (e.g., ECP, ERC, ECPE, ECSR), which can be a bit confusing sometimes. Otherwise, I see no reasons to reject this paper.\n\ntypos_grammar_style_and_presentation_improvements: There are too many abbreviations throughout the paper (e.g., ECP, ERC, ECPE, ECSR), which can be a bit confusing sometimes.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "Y1KYBcpFCw",
        "length": 310,
        "human_text": "paper_topic_and_main_contributions: 1. The authors demonstrate the effectiveness of their approach in affective reasoning 2. They curate a synthetic dataset with implicit causes to visualize the latent variable in our implementation. \n3. They exhibit a significant reduction in false predictions for negative samples across three causal discrimination scenarios. \n4. They formulate the dialogue process and analyze the cogn skeleton with encoder-decoder structure, GNN, and graph attention.\n\nreasons_to_accept: 1. This paper is well-organized, clearly describing the task and problem.\n2. The proposed method is based on a high level of understanding of existing research and includes the definition of a valid hypothesis with technical descriptions.\n3. It clearly describes the limitations and provides appropriate analysis through experiments.\n\nreasons_to_reject: 1. The authors have partly addressed this in the limitations section, but considering confounding-bias and the backdoor problem is relatively weak compared to previous studies on causality.\n2. Isn't the proposed method also not a fundamental solution to the challenge of affective reasoning? If there are labels and structures that can represent causality in a given conversation situation, I think it is natural to see the effect of investing more layers and resources.\n\ntypos_grammar_style_and_presentation_improvements: Figure 3. The description of the colors and shading is missing and hard to see, which is confusing.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "bflEv1doWI",
        "length": 713,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an approach to \"affective reasoning in conversation\" formulated as a \"causal discrimination\" task, which essentially aims to classify whether a pair of conversation turns has a causal relationship (and if so, in what directionality).\nThe underlying premise of this work (which is assumed but not formally or empirically demonstrated) is that this particular task formulation of emotional causal relation classification in a pair of conversation turns is important.\nThe key thesis of this paper is that handling of i.i.d. noise is important in reasoning about such structural causal relationships, and the paper thus proposes an autoencoder architecture that can incorporate such i.i.d. noise, which presumably corresponds to the \"implicit causes\".\n\nreasons_to_accept: - the paper addresses causal reasoning, an important and open research challenge in today's AI literature and proposes an approach for structural causal reasoning, demonstrating stronger performance over baselines\n\nreasons_to_reject: - I wish the paper provided a better description of the task itself. Based just on Figure 1 example, I couldn't help but wonder if the correct answer has a trivial pattern --- that there's a causal relationship between the most recent utterance of between two speakers (100% cases?), and also that there's a causal relationship between two most recent utterances of each speaker (the vast majority of the cases?), which makes me wonder whether simple majority baselines of some sorts could've worked surprisingly well. Detecting when the causality misses between two adjacent utterances (which can happen, though less frequently) seems relatively easy due to the abrupt change of the topic. Fundamentally, I came away with the suspicion --- whether the classification formulation experimented in this paper justifies the relatively complex method proposed in this work, and I wish there was a more convincing benchmark explored in this paper.  - Concurrently, I'm not convinced if the relatively simplistic classification of causal relationship explored in this paper helps advancing the field in terms of affective understanding in conversation, especially in the context of the currently powerful LLMs that can carry about an amazing level of conversation. While causal reasoning is generally an interesting and open research question, I wonder if the particular task formulation explored in this paper is rather too narrow to be relevant in the current AI SOTA capabilities.\n- I wish the authors provided more intuition about why \"implicit causes\" correspond to \"i.i.d. noise\". While I understand to some degree that implicit causes may seem like \"noise\", various statements around this don't seem well justified or even precisely stated.  - The lack of clarity of writing is among the weakest points of this paper. At least for me, of the couple dozen papers across NLP and ML venues that I have reviewed in recent years, this one by far was the most opaque to unpack. As one of the many such examples, I found the following sentence hard to parse and hard to fully understand [line 073 - line 079]: \"In order to discriminate different causal relationships between two similar embeddings, we construct the dialogue process as a Structural Causal Model (SCM) stemming from many endeavors supporting that i.i.d. noise of SCM could facilitate the discrimination of causal relationships when finning two variables.\" In fact, I found almost half of the remaining sentences of the introduction section somewhat troublesome, sometimes because the sentences make claims that are not yet defined or justified.\n- In the same vein, the technical writing seems a bit sloppy at times. For example in Table 3, DD and IE were never defined, and the caption talks about ECPE-MLL, while it doesn't even appear in the rows of the methods listed in that Table. Moreover, the boldfaced numbers seem inconsistent. For example, GPT-4's IE score is higher than that of ECPE-2D, but not boldfaced.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "gUgIvYrh7E",
        "length": 420,
        "human_text": "paper_topic_and_main_contributions: This study concentrates on the challenge of causal discrimination, specifically on the task of affective reasoning in *conversations*. The paper reveals that almost all existing models excel at capturing semantic relatedness within utterance embeddings but fall short in determining the specific causal relationship underlying the association between two utterances on the task of causal discrimination. To overcome this limitation, the authors constructed a SCM with i.i.d. noise terms, and proposed the *cogn* framework to address the unstructured nature of conversation data. Experimental results show that the proposed approaches significantly outperform existing methods on two affective reasoning tasks including Emotion-Cause Pair Extraction (ECPE) and Emotion-Cause Span Recognition (ECSR) and one emotion recognition task (ERC), demonstrating its effectiveness in affective reasoning.\nThe main contributions are as follows: 1) The authors incorporated i.i.d. noise terms, and formulated the dialogue process as a structural causal model (SCM); 2) The authors devised the *cogn* skeleton to address the problems of variable-length and unstructured dialogue samples; 3) The authors adopted an autoencoder architecture to overcome the unobservability of implicit causes and make it learnable; and 4) The authors constructed a synthetic dataset with implicit causes and conducted extensive evaluations of the proposed method.\n\nreasons_to_accept: 1. The study reveals that when it comes to more specific causal relationships within semantically similar sentences (such as reasoning tasks), both unsupervised and supervised methods may not exhibit the same level of \u201cintelligence\u201d and output some \u201cpseudo-correlation\u201d. \n2. The discussion of the methods proposed in this work is thorough and detailed. \n3. The authors propose new methods to address the problem of causal discrimination in affective reasoning. Their approaches significantly outperform other state-of-the-art methods, including GPT-3.5 and GPT-4.\n\nreasons_to_reject: There are too many abbreviations throughout the paper (e.g., ECP, ERC, ECPE, ECSR), which can be a bit confusing sometimes. Otherwise, I see no reasons to reject this paper.\n\ntypos_grammar_style_and_presentation_improvements: There are too many abbreviations throughout the paper (e.g., ECP, ERC, ECPE, ECSR), which can be a bit confusing sometimes.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "116_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_116_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7287666666666667,
      "max_similarity": 0.7341,
      "avg_coverage": 0.4677666666666667,
      "max_coverage": 0.5714
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 539,
      "avg_human_length": 481.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 7,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "Y1KYBcpFCw",
        "similarity": 0.7341,
        "coverage": 0.4615,
        "human_length": 310,
        "human_text": "paper_topic_and_main_contributions: 1. The authors demonstrate the effectiveness of their approach in affective reasoning 2. They curate a synthetic dataset with implicit causes to visualize the latent variable in our implementation. \n3. They exhibit a significant reduction in false predictions for negative samples across three causal discrimination scenarios. \n4. They formulate the dialogue process and analyze the cogn skeleton with encoder-decoder structure, GNN, and graph attention.\n\nreasons_to_accept: 1. This paper is well-organized, clearly describing the task and problem.\n2. The proposed method is based on a high level of understanding of existing research and includes the definition of a valid hypothesis with technical descriptions.\n3. It clearly describes the limitations and provides appropriate analysis through experiments.\n\nreasons_to_reject: 1. The authors have partly addressed this in the limitations section, but considering confounding-bias and the backdoor problem is relatively weak compared to previous studies on causality.\n2. Isn't the proposed method also not a fundamental solution to the challenge of affective reasoning? If there are labels and structures that can represent causality in a given conversation situation, I think it is natural to see the effect of investing more layers and resources.\n\ntypos_grammar_style_and_presentation_improvements: Figure 3. The description of the colors and shading is missing and hard to see, which is confusing.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "bflEv1doWI",
        "similarity": 0.7188,
        "coverage": 0.3704,
        "human_length": 713,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an approach to \"affective reasoning in conversation\" formulated as a \"causal discrimination\" task, which essentially aims to classify whether a pair of conversation turns has a causal relationship (and if so, in what directionality).\nThe underlying premise of this work (which is assumed but not formally or empirically demonstrated) is that this particular task formulation of emotional causal relation classification in a pair of conversation turns is important.\nThe key thesis of this paper is that handling of i.i.d. noise is important in reasoning about such structural causal relationships, and the paper thus proposes an autoencoder architecture that can incorporate such i.i.d. noise, which presumably corresponds to the \"implicit causes\".\n\nreasons_to_accept: - the paper addresses causal reasoning, an important and open research challenge in today's AI literature and proposes an approach for structural causal reasoning, demonstrating stronger performance over baselines\n\nreasons_to_reject: - I wish the paper provided a better description of the task itself. Based just on Figure 1 example, I couldn't help but wonder if the correct answer has a trivial pattern --- that there's a causal relationship between the most recent utterance of between two speakers (100% cases?), and also that there's a causal relationship between two most recent utterances of each speaker (the vast majority of the cases?), which makes me wonder whether simple majority baselines of some sorts could've worked surprisingly well. Detecting when the causality misses between two adjacent utterances (which can happen, though less frequently) seems relatively easy due to the abrupt change of the topic. Fundamentally, I came away with the suspicion --- whether the classification formulation experimented in this paper justifies the relatively complex method proposed in this work, and I wish there was a more convincing benchmark explored in this paper.  - Concurrently, I'm not convinced if the relatively simplistic classification of causal relationship explored in this paper helps advancing the field in terms of affective understanding in conversation, especially in the context of the currently powerful LLMs that can carry about an amazing level of conversation. While causal reasoning is generally an interesting and open research question, I wonder if the particular task formulation explored in this paper is rather too narrow to be relevant in the current AI SOTA capabilities.\n- I wish the authors provided more intuition about why \"implicit causes\" correspond to \"i.i.d. noise\". While I understand to some degree that implicit causes may seem like \"noise\", various statements around this don't seem well justified or even precisely stated.  - The lack of clarity of writing is among the weakest points of this paper. At least for me, of the couple dozen papers across NLP and ML venues that I have reviewed in recent years, this one by far was the most opaque to unpack. As one of the many such examples, I found the following sentence hard to parse and hard to fully understand [line 073 - line 079]: \"In order to discriminate different causal relationships between two similar embeddings, we construct the dialogue process as a Structural Causal Model (SCM) stemming from many endeavors supporting that i.i.d. noise of SCM could facilitate the discrimination of causal relationships when finning two variables.\" In fact, I found almost half of the remaining sentences of the introduction section somewhat troublesome, sometimes because the sentences make claims that are not yet defined or justified.\n- In the same vein, the technical writing seems a bit sloppy at times. For example in Table 3, DD and IE were never defined, and the caption talks about ECPE-MLL, while it doesn't even appear in the rows of the methods listed in that Table. Moreover, the boldfaced numbers seem inconsistent. For example, GPT-4's IE score is higher than that of ECPE-2D, but not boldfaced.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "gUgIvYrh7E",
        "similarity": 0.7334,
        "coverage": 0.5714,
        "human_length": 420,
        "human_text": "paper_topic_and_main_contributions: This study concentrates on the challenge of causal discrimination, specifically on the task of affective reasoning in *conversations*. The paper reveals that almost all existing models excel at capturing semantic relatedness within utterance embeddings but fall short in determining the specific causal relationship underlying the association between two utterances on the task of causal discrimination. To overcome this limitation, the authors constructed a SCM with i.i.d. noise terms, and proposed the *cogn* framework to address the unstructured nature of conversation data. Experimental results show that the proposed approaches significantly outperform existing methods on two affective reasoning tasks including Emotion-Cause Pair Extraction (ECPE) and Emotion-Cause Span Recognition (ECSR) and one emotion recognition task (ERC), demonstrating its effectiveness in affective reasoning.\nThe main contributions are as follows: 1) The authors incorporated i.i.d. noise terms, and formulated the dialogue process as a structural causal model (SCM); 2) The authors devised the *cogn* skeleton to address the problems of variable-length and unstructured dialogue samples; 3) The authors adopted an autoencoder architecture to overcome the unobservability of implicit causes and make it learnable; and 4) The authors constructed a synthetic dataset with implicit causes and conducted extensive evaluations of the proposed method.\n\nreasons_to_accept: 1. The study reveals that when it comes to more specific causal relationships within semantically similar sentences (such as reasoning tasks), both unsupervised and supervised methods may not exhibit the same level of \u201cintelligence\u201d and output some \u201cpseudo-correlation\u201d. \n2. The discussion of the methods proposed in this work is thorough and detailed. \n3. The authors propose new methods to address the problem of causal discrimination in affective reasoning. Their approaches significantly outperform other state-of-the-art methods, including GPT-3.5 and GPT-4.\n\nreasons_to_reject: There are too many abbreviations throughout the paper (e.g., ECP, ERC, ECPE, ECSR), which can be a bit confusing sometimes. Otherwise, I see no reasons to reject this paper.\n\ntypos_grammar_style_and_presentation_improvements: There are too many abbreviations throughout the paper (e.g., ECP, ERC, ECPE, ECSR), which can be a bit confusing sometimes.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "Y1KYBcpFCw",
        "length": 310,
        "human_text": "paper_topic_and_main_contributions: 1. The authors demonstrate the effectiveness of their approach in affective reasoning 2. They curate a synthetic dataset with implicit causes to visualize the latent variable in our implementation. \n3. They exhibit a significant reduction in false predictions for negative samples across three causal discrimination scenarios. \n4. They formulate the dialogue process and analyze the cogn skeleton with encoder-decoder structure, GNN, and graph attention.\n\nreasons_to_accept: 1. This paper is well-organized, clearly describing the task and problem.\n2. The proposed method is based on a high level of understanding of existing research and includes the definition of a valid hypothesis with technical descriptions.\n3. It clearly describes the limitations and provides appropriate analysis through experiments.\n\nreasons_to_reject: 1. The authors have partly addressed this in the limitations section, but considering confounding-bias and the backdoor problem is relatively weak compared to previous studies on causality.\n2. Isn't the proposed method also not a fundamental solution to the challenge of affective reasoning? If there are labels and structures that can represent causality in a given conversation situation, I think it is natural to see the effect of investing more layers and resources.\n\ntypos_grammar_style_and_presentation_improvements: Figure 3. The description of the colors and shading is missing and hard to see, which is confusing.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "bflEv1doWI",
        "length": 713,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an approach to \"affective reasoning in conversation\" formulated as a \"causal discrimination\" task, which essentially aims to classify whether a pair of conversation turns has a causal relationship (and if so, in what directionality).\nThe underlying premise of this work (which is assumed but not formally or empirically demonstrated) is that this particular task formulation of emotional causal relation classification in a pair of conversation turns is important.\nThe key thesis of this paper is that handling of i.i.d. noise is important in reasoning about such structural causal relationships, and the paper thus proposes an autoencoder architecture that can incorporate such i.i.d. noise, which presumably corresponds to the \"implicit causes\".\n\nreasons_to_accept: - the paper addresses causal reasoning, an important and open research challenge in today's AI literature and proposes an approach for structural causal reasoning, demonstrating stronger performance over baselines\n\nreasons_to_reject: - I wish the paper provided a better description of the task itself. Based just on Figure 1 example, I couldn't help but wonder if the correct answer has a trivial pattern --- that there's a causal relationship between the most recent utterance of between two speakers (100% cases?), and also that there's a causal relationship between two most recent utterances of each speaker (the vast majority of the cases?), which makes me wonder whether simple majority baselines of some sorts could've worked surprisingly well. Detecting when the causality misses between two adjacent utterances (which can happen, though less frequently) seems relatively easy due to the abrupt change of the topic. Fundamentally, I came away with the suspicion --- whether the classification formulation experimented in this paper justifies the relatively complex method proposed in this work, and I wish there was a more convincing benchmark explored in this paper.  - Concurrently, I'm not convinced if the relatively simplistic classification of causal relationship explored in this paper helps advancing the field in terms of affective understanding in conversation, especially in the context of the currently powerful LLMs that can carry about an amazing level of conversation. While causal reasoning is generally an interesting and open research question, I wonder if the particular task formulation explored in this paper is rather too narrow to be relevant in the current AI SOTA capabilities.\n- I wish the authors provided more intuition about why \"implicit causes\" correspond to \"i.i.d. noise\". While I understand to some degree that implicit causes may seem like \"noise\", various statements around this don't seem well justified or even precisely stated.  - The lack of clarity of writing is among the weakest points of this paper. At least for me, of the couple dozen papers across NLP and ML venues that I have reviewed in recent years, this one by far was the most opaque to unpack. As one of the many such examples, I found the following sentence hard to parse and hard to fully understand [line 073 - line 079]: \"In order to discriminate different causal relationships between two similar embeddings, we construct the dialogue process as a Structural Causal Model (SCM) stemming from many endeavors supporting that i.i.d. noise of SCM could facilitate the discrimination of causal relationships when finning two variables.\" In fact, I found almost half of the remaining sentences of the introduction section somewhat troublesome, sometimes because the sentences make claims that are not yet defined or justified.\n- In the same vein, the technical writing seems a bit sloppy at times. For example in Table 3, DD and IE were never defined, and the caption talks about ECPE-MLL, while it doesn't even appear in the rows of the methods listed in that Table. Moreover, the boldfaced numbers seem inconsistent. For example, GPT-4's IE score is higher than that of ECPE-2D, but not boldfaced.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "gUgIvYrh7E",
        "length": 420,
        "human_text": "paper_topic_and_main_contributions: This study concentrates on the challenge of causal discrimination, specifically on the task of affective reasoning in *conversations*. The paper reveals that almost all existing models excel at capturing semantic relatedness within utterance embeddings but fall short in determining the specific causal relationship underlying the association between two utterances on the task of causal discrimination. To overcome this limitation, the authors constructed a SCM with i.i.d. noise terms, and proposed the *cogn* framework to address the unstructured nature of conversation data. Experimental results show that the proposed approaches significantly outperform existing methods on two affective reasoning tasks including Emotion-Cause Pair Extraction (ECPE) and Emotion-Cause Span Recognition (ECSR) and one emotion recognition task (ERC), demonstrating its effectiveness in affective reasoning.\nThe main contributions are as follows: 1) The authors incorporated i.i.d. noise terms, and formulated the dialogue process as a structural causal model (SCM); 2) The authors devised the *cogn* skeleton to address the problems of variable-length and unstructured dialogue samples; 3) The authors adopted an autoencoder architecture to overcome the unobservability of implicit causes and make it learnable; and 4) The authors constructed a synthetic dataset with implicit causes and conducted extensive evaluations of the proposed method.\n\nreasons_to_accept: 1. The study reveals that when it comes to more specific causal relationships within semantically similar sentences (such as reasoning tasks), both unsupervised and supervised methods may not exhibit the same level of \u201cintelligence\u201d and output some \u201cpseudo-correlation\u201d. \n2. The discussion of the methods proposed in this work is thorough and detailed. \n3. The authors propose new methods to address the problem of causal discrimination in affective reasoning. Their approaches significantly outperform other state-of-the-art methods, including GPT-3.5 and GPT-4.\n\nreasons_to_reject: There are too many abbreviations throughout the paper (e.g., ECP, ERC, ECPE, ECSR), which can be a bit confusing sometimes. Otherwise, I see no reasons to reject this paper.\n\ntypos_grammar_style_and_presentation_improvements: There are too many abbreviations throughout the paper (e.g., ECP, ERC, ECPE, ECSR), which can be a bit confusing sometimes.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "182_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_182_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6623666666666667,
      "max_similarity": 0.6977,
      "avg_coverage": 0.6801666666666666,
      "max_coverage": 0.8182
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 510,
      "avg_human_length": 283.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 7,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "VUu0JTUExK",
        "similarity": 0.6977,
        "coverage": 0.6667,
        "human_length": 380,
        "human_text": "paper_topic_and_main_contributions: This paper contributes a strategy to boost multiple-choice QA accuracy for language models by first eliminating low-score answers then rescoring the top-scoring answers. They also provide an analysis of different design choices for this procedure.\nThe authors hypothesize that the MCQA requires two skills: eliminating choices and choosing the correct choice. Their results give evidence for the claim by showing PoE usually performs comparably with MCP for most tasks, but for some tasks beats MCP by a large margin.\n\nreasons_to_accept: This paper is clearly written and shows promising results for their method. Their contribution seems fairly scoped for a short paper. I appreciate their thoroughness in considering the space of design decisions for their method.\n\nreasons_to_reject: What would make this paper interesting to me would be to have some idea of *when* PoE can be expected to dramatically outperform MCP (i.e., what makes LD and CC tasks unique). Why does the scoring order change after masking? Without answering, analyzing, or addressing this question the contribution seems uninformative. For instance, in Holtzman et al. (2021) the authors provide a hypothesis for a mechanism for exactly how their method works (surface form competition).\nA more easily resolved criticism I have is that, since MCP is the closest method to the paper's method, the authors should show a clear advantage to using PoE over MCP in some cases. Therefore, in Table 2 I would be more interested in seeing the results for LD or CC to see if the large gains from using PoE for these tasks persists in the few-shot setting. This should be easily addressable with an added experiment.\n\nquestions_for_the_authors: 1. Why do you mask the incorrect answers rather than simply removing them from the prompt? \n2. Do you have any idea why the top-choice ordering changes in some cases after elimination?\n\ntypos_grammar_style_and_presentation_improvements: 133: What does \"$T$ to 1\" mean?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "57QMIMpP1t",
        "similarity": 0.653,
        "coverage": 0.5556,
        "human_length": 260,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an approach to improve LLM's accuracy for multiple choice questions. The approach divides the LLM reasoning process into two steps, first eliminates unlikely choices, then choose the best option among the remaining items.\n\nreasons_to_accept: The motivation of the paper, namely dividing the multiple-choice solution process into two steps, makes a lot of sense.\n\nreasons_to_reject: The scope of the proposed approach is very limited (only fits multiple-choice questions). What is more, it remains unclear how effective this approach is, compared with the straightforward multiple choice prompting.\n\nquestions_for_the_authors: 1. For a question with N choices, instead of simply decompose the solution process into two steps, what if you decompose it into N-1 steps (first remove most unlikely solution, then remove the 2nd unlikely solution etc.)? \n2. How does the option elimination process compare with directly picking the top k scores as candidates (i.e. si >= top_k(s1, ..., si) in Eq. 5)? \n3. It remains unclear why the approach needs to replace eliminated option with the token \"[MASK]\". Does it matter if we use a different token to indicate invalid options?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "kMN9C8oEML",
        "similarity": 0.6364,
        "coverage": 0.8182,
        "human_length": 211,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a simple trick for multi-choice QA: eliminate some choices first, then re-answer the question with eliminated choices masked. It is shown with both ChatGPT and FLAN-T5-XL on multiple datasets to work (somehow).\n\nreasons_to_accept: the technique is well motivated and reasonable, and multiple benchmarks and design choices are considered for FLAN-T5-XL.\n\nreasons_to_reject: - ChatGPT (which model, gpt-3.5-turbo?) experiment is too simple, just two datasets, and on one of them it doesn't really work well. I feel ChatGPT should be (obviously) more important than FLAN-T5-XL?\n- FLAN-T5-XL across many datasets, seems some show bigger improvements, while others show very little improvement, any intuition?\n- few-shot performance gain seems small overall, not sure if added computation step is worthy.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "VUu0JTUExK",
        "length": 380,
        "human_text": "paper_topic_and_main_contributions: This paper contributes a strategy to boost multiple-choice QA accuracy for language models by first eliminating low-score answers then rescoring the top-scoring answers. They also provide an analysis of different design choices for this procedure.\nThe authors hypothesize that the MCQA requires two skills: eliminating choices and choosing the correct choice. Their results give evidence for the claim by showing PoE usually performs comparably with MCP for most tasks, but for some tasks beats MCP by a large margin.\n\nreasons_to_accept: This paper is clearly written and shows promising results for their method. Their contribution seems fairly scoped for a short paper. I appreciate their thoroughness in considering the space of design decisions for their method.\n\nreasons_to_reject: What would make this paper interesting to me would be to have some idea of *when* PoE can be expected to dramatically outperform MCP (i.e., what makes LD and CC tasks unique). Why does the scoring order change after masking? Without answering, analyzing, or addressing this question the contribution seems uninformative. For instance, in Holtzman et al. (2021) the authors provide a hypothesis for a mechanism for exactly how their method works (surface form competition).\nA more easily resolved criticism I have is that, since MCP is the closest method to the paper's method, the authors should show a clear advantage to using PoE over MCP in some cases. Therefore, in Table 2 I would be more interested in seeing the results for LD or CC to see if the large gains from using PoE for these tasks persists in the few-shot setting. This should be easily addressable with an added experiment.\n\nquestions_for_the_authors: 1. Why do you mask the incorrect answers rather than simply removing them from the prompt? \n2. Do you have any idea why the top-choice ordering changes in some cases after elimination?\n\ntypos_grammar_style_and_presentation_improvements: 133: What does \"$T$ to 1\" mean?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "57QMIMpP1t",
        "length": 260,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an approach to improve LLM's accuracy for multiple choice questions. The approach divides the LLM reasoning process into two steps, first eliminates unlikely choices, then choose the best option among the remaining items.\n\nreasons_to_accept: The motivation of the paper, namely dividing the multiple-choice solution process into two steps, makes a lot of sense.\n\nreasons_to_reject: The scope of the proposed approach is very limited (only fits multiple-choice questions). What is more, it remains unclear how effective this approach is, compared with the straightforward multiple choice prompting.\n\nquestions_for_the_authors: 1. For a question with N choices, instead of simply decompose the solution process into two steps, what if you decompose it into N-1 steps (first remove most unlikely solution, then remove the 2nd unlikely solution etc.)? \n2. How does the option elimination process compare with directly picking the top k scores as candidates (i.e. si >= top_k(s1, ..., si) in Eq. 5)? \n3. It remains unclear why the approach needs to replace eliminated option with the token \"[MASK]\". Does it matter if we use a different token to indicate invalid options?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "kMN9C8oEML",
        "length": 211,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a simple trick for multi-choice QA: eliminate some choices first, then re-answer the question with eliminated choices masked. It is shown with both ChatGPT and FLAN-T5-XL on multiple datasets to work (somehow).\n\nreasons_to_accept: the technique is well motivated and reasonable, and multiple benchmarks and design choices are considered for FLAN-T5-XL.\n\nreasons_to_reject: - ChatGPT (which model, gpt-3.5-turbo?) experiment is too simple, just two datasets, and on one of them it doesn't really work well. I feel ChatGPT should be (obviously) more important than FLAN-T5-XL?\n- FLAN-T5-XL across many datasets, seems some show bigger improvements, while others show very little improvement, any intuition?\n- few-shot performance gain seems small overall, not sure if added computation step is worthy.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "182_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_182_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6667333333333333,
      "max_similarity": 0.7028,
      "avg_coverage": 0.7104666666666667,
      "max_coverage": 0.9091
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 517,
      "avg_human_length": 283.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 7,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "VUu0JTUExK",
        "similarity": 0.7028,
        "coverage": 0.6667,
        "human_length": 380,
        "human_text": "paper_topic_and_main_contributions: This paper contributes a strategy to boost multiple-choice QA accuracy for language models by first eliminating low-score answers then rescoring the top-scoring answers. They also provide an analysis of different design choices for this procedure.\nThe authors hypothesize that the MCQA requires two skills: eliminating choices and choosing the correct choice. Their results give evidence for the claim by showing PoE usually performs comparably with MCP for most tasks, but for some tasks beats MCP by a large margin.\n\nreasons_to_accept: This paper is clearly written and shows promising results for their method. Their contribution seems fairly scoped for a short paper. I appreciate their thoroughness in considering the space of design decisions for their method.\n\nreasons_to_reject: What would make this paper interesting to me would be to have some idea of *when* PoE can be expected to dramatically outperform MCP (i.e., what makes LD and CC tasks unique). Why does the scoring order change after masking? Without answering, analyzing, or addressing this question the contribution seems uninformative. For instance, in Holtzman et al. (2021) the authors provide a hypothesis for a mechanism for exactly how their method works (surface form competition).\nA more easily resolved criticism I have is that, since MCP is the closest method to the paper's method, the authors should show a clear advantage to using PoE over MCP in some cases. Therefore, in Table 2 I would be more interested in seeing the results for LD or CC to see if the large gains from using PoE for these tasks persists in the few-shot setting. This should be easily addressable with an added experiment.\n\nquestions_for_the_authors: 1. Why do you mask the incorrect answers rather than simply removing them from the prompt? \n2. Do you have any idea why the top-choice ordering changes in some cases after elimination?\n\ntypos_grammar_style_and_presentation_improvements: 133: What does \"$T$ to 1\" mean?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "57QMIMpP1t",
        "similarity": 0.6589,
        "coverage": 0.5556,
        "human_length": 260,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an approach to improve LLM's accuracy for multiple choice questions. The approach divides the LLM reasoning process into two steps, first eliminates unlikely choices, then choose the best option among the remaining items.\n\nreasons_to_accept: The motivation of the paper, namely dividing the multiple-choice solution process into two steps, makes a lot of sense.\n\nreasons_to_reject: The scope of the proposed approach is very limited (only fits multiple-choice questions). What is more, it remains unclear how effective this approach is, compared with the straightforward multiple choice prompting.\n\nquestions_for_the_authors: 1. For a question with N choices, instead of simply decompose the solution process into two steps, what if you decompose it into N-1 steps (first remove most unlikely solution, then remove the 2nd unlikely solution etc.)? \n2. How does the option elimination process compare with directly picking the top k scores as candidates (i.e. si >= top_k(s1, ..., si) in Eq. 5)? \n3. It remains unclear why the approach needs to replace eliminated option with the token \"[MASK]\". Does it matter if we use a different token to indicate invalid options?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "kMN9C8oEML",
        "similarity": 0.6385,
        "coverage": 0.9091,
        "human_length": 211,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a simple trick for multi-choice QA: eliminate some choices first, then re-answer the question with eliminated choices masked. It is shown with both ChatGPT and FLAN-T5-XL on multiple datasets to work (somehow).\n\nreasons_to_accept: the technique is well motivated and reasonable, and multiple benchmarks and design choices are considered for FLAN-T5-XL.\n\nreasons_to_reject: - ChatGPT (which model, gpt-3.5-turbo?) experiment is too simple, just two datasets, and on one of them it doesn't really work well. I feel ChatGPT should be (obviously) more important than FLAN-T5-XL?\n- FLAN-T5-XL across many datasets, seems some show bigger improvements, while others show very little improvement, any intuition?\n- few-shot performance gain seems small overall, not sure if added computation step is worthy.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "VUu0JTUExK",
        "length": 380,
        "human_text": "paper_topic_and_main_contributions: This paper contributes a strategy to boost multiple-choice QA accuracy for language models by first eliminating low-score answers then rescoring the top-scoring answers. They also provide an analysis of different design choices for this procedure.\nThe authors hypothesize that the MCQA requires two skills: eliminating choices and choosing the correct choice. Their results give evidence for the claim by showing PoE usually performs comparably with MCP for most tasks, but for some tasks beats MCP by a large margin.\n\nreasons_to_accept: This paper is clearly written and shows promising results for their method. Their contribution seems fairly scoped for a short paper. I appreciate their thoroughness in considering the space of design decisions for their method.\n\nreasons_to_reject: What would make this paper interesting to me would be to have some idea of *when* PoE can be expected to dramatically outperform MCP (i.e., what makes LD and CC tasks unique). Why does the scoring order change after masking? Without answering, analyzing, or addressing this question the contribution seems uninformative. For instance, in Holtzman et al. (2021) the authors provide a hypothesis for a mechanism for exactly how their method works (surface form competition).\nA more easily resolved criticism I have is that, since MCP is the closest method to the paper's method, the authors should show a clear advantage to using PoE over MCP in some cases. Therefore, in Table 2 I would be more interested in seeing the results for LD or CC to see if the large gains from using PoE for these tasks persists in the few-shot setting. This should be easily addressable with an added experiment.\n\nquestions_for_the_authors: 1. Why do you mask the incorrect answers rather than simply removing them from the prompt? \n2. Do you have any idea why the top-choice ordering changes in some cases after elimination?\n\ntypos_grammar_style_and_presentation_improvements: 133: What does \"$T$ to 1\" mean?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "57QMIMpP1t",
        "length": 260,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an approach to improve LLM's accuracy for multiple choice questions. The approach divides the LLM reasoning process into two steps, first eliminates unlikely choices, then choose the best option among the remaining items.\n\nreasons_to_accept: The motivation of the paper, namely dividing the multiple-choice solution process into two steps, makes a lot of sense.\n\nreasons_to_reject: The scope of the proposed approach is very limited (only fits multiple-choice questions). What is more, it remains unclear how effective this approach is, compared with the straightforward multiple choice prompting.\n\nquestions_for_the_authors: 1. For a question with N choices, instead of simply decompose the solution process into two steps, what if you decompose it into N-1 steps (first remove most unlikely solution, then remove the 2nd unlikely solution etc.)? \n2. How does the option elimination process compare with directly picking the top k scores as candidates (i.e. si >= top_k(s1, ..., si) in Eq. 5)? \n3. It remains unclear why the approach needs to replace eliminated option with the token \"[MASK]\". Does it matter if we use a different token to indicate invalid options?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "kMN9C8oEML",
        "length": 211,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a simple trick for multi-choice QA: eliminate some choices first, then re-answer the question with eliminated choices masked. It is shown with both ChatGPT and FLAN-T5-XL on multiple datasets to work (somehow).\n\nreasons_to_accept: the technique is well motivated and reasonable, and multiple benchmarks and design choices are considered for FLAN-T5-XL.\n\nreasons_to_reject: - ChatGPT (which model, gpt-3.5-turbo?) experiment is too simple, just two datasets, and on one of them it doesn't really work well. I feel ChatGPT should be (obviously) more important than FLAN-T5-XL?\n- FLAN-T5-XL across many datasets, seems some show bigger improvements, while others show very little improvement, any intuition?\n- few-shot performance gain seems small overall, not sure if added computation step is worthy.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "179_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_179_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7103,
      "max_similarity": 0.7155,
      "avg_coverage": 0.5306666666666667,
      "max_coverage": 0.6522
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 547,
      "avg_human_length": 615.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 8,
      "weaknesses_count": 9,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "n6blR5W7cv",
        "similarity": 0.7116,
        "coverage": 0.5143,
        "human_length": 576,
        "human_text": "paper_topic_and_main_contributions: This paper constructs an open-domain QA dataset (CAMBIGNQ) with clarification questions for resolve the ambiguity. The dataset is built upon an existing dataset (AMBIGNQ). The only difference between these two datasets are that the newly constructed one asks clarification question while the existing one asks disambiguated questions for each possible interpretation. This paper reports the human evaluation on the preference between these two ways for resolving ambiguity in QAs. The results support their arguments that this new setting is more preferable.  In addition to the dataset, this work establishes benchmark performance on three tasks: ambiguity detection, clarification generation and clarification-based question answering. The experimental results reveal the difficulty of these tasks, thus show the need of this new dataset  for future development.\n\nreasons_to_accept: - Construct a new open-domain QA dataset for resolving ambiguity by asking clarification questions. This dataset can be used for following work to improve the modeling performance.\n- Demonstrate the advantage of asking clarification questions than disambiguated questions. This effort encourages future work towards generating clarification question.\n- Report solid benchmark performance. These experiments reveal the challenge of these tasks.\n- Evaluate InstructGPT's ability on generate clarification questions from disambiguated questions through in-context few-shot learning. The insufficient performance demands better methods.\n\nreasons_to_reject: - Some experimental design is not convincing. For example, in Sec 6.1, they use BERT model for one setting and BART model for the other, so the difference between these two results may not come from whether the predicted answers are useful or not, since it may come from the different capacity of these two models. Another one is that in Table 5, the experiment with ground truth CQ is required, because this will identify whether the low performance in Table 5 is caused by the QA modeling or the generated CQ quality.\n- The benchmark experiments are conducted with BERT, BART which don't represent a SOTA performance. These experimental results are not convincing that these tasks are difficult for more SOTA models such as XLNET, GPT-3, GPT-3.5, GPT-4.  - They only consider one fixed template for asking clarification question, which limits the scope of this work.\n\nquestions_for_the_authors: - One key contribution is to generate clarification questions from disambiguated questions. The InstructGPT with few-shot learning shows limited performance. Performance of some intuitive models would be necessary to be reported, such as rule-based approach which extracts new words in disambiguated questions compared to the ambiguous questions and then consider them as \"options\". Other approaches to be considered is to fine-tune BERT for example. Did you conduct them?\n- The clarification question follows a fixed template, i.e. \u201cWhich [category]: [option1], [option2], ..., or [optionn]?\u201d In human communication, there could be other ways which may be more natural and fluent given a certain context. Why did you choose this template, and how to compare with other possibilities?\n- What do you mean by \"reranked related paragraphs\" in line 242 and 504? ( Reranked by what?)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "JKArhgoYSJ",
        "similarity": 0.7155,
        "coverage": 0.4255,
        "human_length": 780,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an innovative approach to handle ambiguous questions (AQ) in open-domain QA by introducing clarifying ambiguous natural questions (CAmbigNQ), a dataset consisting of 5,653 AQs with relevant passages, possible answers, and clarification questions (CQ). Unlike previous methods that resolved ambiguities by **directly** generating disambiguated questions (DQs), the authors present a novel method that first ask CQ before generating an answer. The pipeline consists of three tasks: ambiguity detection, clarification question generation, and clarification-based QA. Experimental results are presented, emphasizing the need for further enhancements.\n\nreasons_to_accept: **-- Novel Approach:** The authors introduce a new method by shifting from conventional disambiguated question generation to a clarification questions-based method, aligning more closely with real-world applications.\n**-- New Dataset:** The creation of the CAmbigNQ dataset to support research in ambiguity in QA represents a valuable asset for the community.\n**-- Writing Quality:** The paper is clear, well-written, and effectively uses examples to explain the concepts, enhancing readability and understanding.\n\nreasons_to_reject: **-- Doubtful Motivation:** While CQ improves user preference (as shown in Experiment 1), 33% of people preferred not to generate a CQ, which raises questions about the necessity and effectiveness of producing these CQs.\n**-- Insufficient Comparison with SoTA:** The paper lacks comparison with state-of-the-art methods, which is essential to gauge the true effectiveness of the proposed approach.\n**-- Questions about Methodology:** Several questions arise concerning the methodology, including the choice of evaluation metrics, the contradiction in observations, and the real necessity of CQ generation for end-to-end performance. These need to be clearly addressed to strengthen the paper (see questions for more details).\n\nquestions_for_the_authors: 1. While CQ improves user preference (as shown in experiment 1), 33% of people preferred not to generate a CQ. Could this indicate the generated CQs could be misleading or incorrect? Why do human annotators do not prefer those CQs? It would be beneficial to understand the underlying reasons for this preference.\nAnd at the very least, generating CQs should ideally (at least) not alter human preference, as more information is provided compared to only AQ and DQ provided. However, the fact that human preference decreases by 33% when generating CQs raises questions about the necessity and effectiveness of producing these CQs.\n2. The paper relies on large language models such as ChatGPT to generate CQs. I am curious about the ability of a model like ChatGPT to **directly** solve AQ, instead of using generated CQs to train small language model? Can ChatGPT solve AQ in a form similar to chain-of-thought, for example, a chain consisting of (1) ask CQ (2) generate DQ (3) answer the AQ. To summarize, what is the ChatGPT performance on such task? Could ChatGPT perform better than this complex pipeline?\n3. line 394 stated *\u201csince predicted answers for AQ have been shown to be helpful for DQ based approaches\u201d*, however, as shown in Table  3 and Table 6, it seems *No Answers for AQ* makes better performance than *Predicted Answers for AQ* on the ambiguity detection and end-QA task. Does it mean your observation contradict to the observation from existing literature? Can you explain this discrepancy? Or please correct me if I misunderstand this.  4. Since *category* name are mostly short, so, does the use of BLEU or EM score as evaluation metrics reflect the correctness of generated CQs accurately? How does it align with human evaluations? And also same question as the evaluation of CQs, did authors perform human evaluation or semantic-based evaluation on CQs, since the CQs is much longer than *category*?\n5. As shown in Table 5, it seems adding this CQ generation, as an intermediate process, does not help much on end-to-end QA performance, i.e., predicting answers for AQ. So, why do authors think generating CQ is necessary if people still mostly care about the end-to-end QA performance?  6. The proposed method makes good ablation study on using or not using CQ in the question answering generation process. However, the paper does not compare with SoTA performance on AmbigQA.  Leaderboard: https://nlp.cs.washington.edu/ambigqa/leaderboard.html Minor:  7. For CQ generation evaluation, how many references are provided? Do authors construct multi-reference for evaluation to improve the robustness?  8. Do authors try different prompts (as shown in line 239 and line 289) to generate CQ using ChatGPT?\n\nmissing_references: n/a\n\ntypos_grammar_style_and_presentation_improvements: n/a\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "bitOc3EZcv",
        "similarity": 0.7038,
        "coverage": 0.6522,
        "human_length": 489,
        "human_text": "paper_topic_and_main_contributions: When confronted with an ambiguous question, rather than have the alternatives spelled out as a list of separate questions, it is more natural to produce a clarification question with concisely expressed options. In this paper the authors describe the addition to an existing dataset of ambiguous questions, relevant passages, and possible answers, a clarification question of the preferred type. This was done using InstructGPT with some few-shot examples to generate possible candidates which were then manually checked and revised.\nThey distinguish two subtasks relevant to the creation of this dataset: ambiguity detection and clarification question generation, and one which uses the results of these subtasks, namely finding answers to the clarification question.\nAnnotators were asked to state a preference between a number of the original verbose individual questions and the more compact alternative version. This latter was usually preferred, and was invariably preferred when more than three interpretations were possible.\nThe paper presents preliminary results for ambiguity detection: under one condition, a BERT-based classifier is trained to distinguish ambiguous from non-ambiguous questions (trained and tested on the original dataset) and under another a BART-based model is used to predict answers, with the input classified as ambiguous if more than one is predicted. The first classifier achieved 61.3 F1, the second 34.3.\nThere are also preliminary results for clarification question generation. Various BART models are trained to predict the clarification question given (1) the ambiguous one and relevant passages, or (2) the ambiguous question, answers as predicted by one of the models just described as well as the relevant passages. For comparison, in one condition (3) the predicted answers were replaced by the actual answers, as a kind of best case. Naturally, the latter case produced the best results, with no significant variation between 1 and 2.\nThe final set of experiments tests QA performance on clarification questions, testing four conditions: ambiguity detection with and without predicted answers and clarification questions created with or without the predicted answers. Perhaps surprisingly the best results were obtained with no answers for either stage.\nThe main contributions of the paper are (1) a clear description of the problem (2) provision (public?) of a dataset and (3) initial results for the accuracy of automation of some of the stages, and utility in a QA setting.\n\nreasons_to_accept: This is an interesting set of problems, which the paper presents one type of solution to.\n\nreasons_to_reject: I can't see any reason to reject this paper.\n\ntypos_grammar_style_and_presentation_improvements: I could not see any typos. The paper is clear and well written.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "n6blR5W7cv",
        "length": 576,
        "human_text": "paper_topic_and_main_contributions: This paper constructs an open-domain QA dataset (CAMBIGNQ) with clarification questions for resolve the ambiguity. The dataset is built upon an existing dataset (AMBIGNQ). The only difference between these two datasets are that the newly constructed one asks clarification question while the existing one asks disambiguated questions for each possible interpretation. This paper reports the human evaluation on the preference between these two ways for resolving ambiguity in QAs. The results support their arguments that this new setting is more preferable.  In addition to the dataset, this work establishes benchmark performance on three tasks: ambiguity detection, clarification generation and clarification-based question answering. The experimental results reveal the difficulty of these tasks, thus show the need of this new dataset  for future development.\n\nreasons_to_accept: - Construct a new open-domain QA dataset for resolving ambiguity by asking clarification questions. This dataset can be used for following work to improve the modeling performance.\n- Demonstrate the advantage of asking clarification questions than disambiguated questions. This effort encourages future work towards generating clarification question.\n- Report solid benchmark performance. These experiments reveal the challenge of these tasks.\n- Evaluate InstructGPT's ability on generate clarification questions from disambiguated questions through in-context few-shot learning. The insufficient performance demands better methods.\n\nreasons_to_reject: - Some experimental design is not convincing. For example, in Sec 6.1, they use BERT model for one setting and BART model for the other, so the difference between these two results may not come from whether the predicted answers are useful or not, since it may come from the different capacity of these two models. Another one is that in Table 5, the experiment with ground truth CQ is required, because this will identify whether the low performance in Table 5 is caused by the QA modeling or the generated CQ quality.\n- The benchmark experiments are conducted with BERT, BART which don't represent a SOTA performance. These experimental results are not convincing that these tasks are difficult for more SOTA models such as XLNET, GPT-3, GPT-3.5, GPT-4.  - They only consider one fixed template for asking clarification question, which limits the scope of this work.\n\nquestions_for_the_authors: - One key contribution is to generate clarification questions from disambiguated questions. The InstructGPT with few-shot learning shows limited performance. Performance of some intuitive models would be necessary to be reported, such as rule-based approach which extracts new words in disambiguated questions compared to the ambiguous questions and then consider them as \"options\". Other approaches to be considered is to fine-tune BERT for example. Did you conduct them?\n- The clarification question follows a fixed template, i.e. \u201cWhich [category]: [option1], [option2], ..., or [optionn]?\u201d In human communication, there could be other ways which may be more natural and fluent given a certain context. Why did you choose this template, and how to compare with other possibilities?\n- What do you mean by \"reranked related paragraphs\" in line 242 and 504? ( Reranked by what?)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "JKArhgoYSJ",
        "length": 780,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an innovative approach to handle ambiguous questions (AQ) in open-domain QA by introducing clarifying ambiguous natural questions (CAmbigNQ), a dataset consisting of 5,653 AQs with relevant passages, possible answers, and clarification questions (CQ). Unlike previous methods that resolved ambiguities by **directly** generating disambiguated questions (DQs), the authors present a novel method that first ask CQ before generating an answer. The pipeline consists of three tasks: ambiguity detection, clarification question generation, and clarification-based QA. Experimental results are presented, emphasizing the need for further enhancements.\n\nreasons_to_accept: **-- Novel Approach:** The authors introduce a new method by shifting from conventional disambiguated question generation to a clarification questions-based method, aligning more closely with real-world applications.\n**-- New Dataset:** The creation of the CAmbigNQ dataset to support research in ambiguity in QA represents a valuable asset for the community.\n**-- Writing Quality:** The paper is clear, well-written, and effectively uses examples to explain the concepts, enhancing readability and understanding.\n\nreasons_to_reject: **-- Doubtful Motivation:** While CQ improves user preference (as shown in Experiment 1), 33% of people preferred not to generate a CQ, which raises questions about the necessity and effectiveness of producing these CQs.\n**-- Insufficient Comparison with SoTA:** The paper lacks comparison with state-of-the-art methods, which is essential to gauge the true effectiveness of the proposed approach.\n**-- Questions about Methodology:** Several questions arise concerning the methodology, including the choice of evaluation metrics, the contradiction in observations, and the real necessity of CQ generation for end-to-end performance. These need to be clearly addressed to strengthen the paper (see questions for more details).\n\nquestions_for_the_authors: 1. While CQ improves user preference (as shown in experiment 1), 33% of people preferred not to generate a CQ. Could this indicate the generated CQs could be misleading or incorrect? Why do human annotators do not prefer those CQs? It would be beneficial to understand the underlying reasons for this preference.\nAnd at the very least, generating CQs should ideally (at least) not alter human preference, as more information is provided compared to only AQ and DQ provided. However, the fact that human preference decreases by 33% when generating CQs raises questions about the necessity and effectiveness of producing these CQs.\n2. The paper relies on large language models such as ChatGPT to generate CQs. I am curious about the ability of a model like ChatGPT to **directly** solve AQ, instead of using generated CQs to train small language model? Can ChatGPT solve AQ in a form similar to chain-of-thought, for example, a chain consisting of (1) ask CQ (2) generate DQ (3) answer the AQ. To summarize, what is the ChatGPT performance on such task? Could ChatGPT perform better than this complex pipeline?\n3. line 394 stated *\u201csince predicted answers for AQ have been shown to be helpful for DQ based approaches\u201d*, however, as shown in Table  3 and Table 6, it seems *No Answers for AQ* makes better performance than *Predicted Answers for AQ* on the ambiguity detection and end-QA task. Does it mean your observation contradict to the observation from existing literature? Can you explain this discrepancy? Or please correct me if I misunderstand this.  4. Since *category* name are mostly short, so, does the use of BLEU or EM score as evaluation metrics reflect the correctness of generated CQs accurately? How does it align with human evaluations? And also same question as the evaluation of CQs, did authors perform human evaluation or semantic-based evaluation on CQs, since the CQs is much longer than *category*?\n5. As shown in Table 5, it seems adding this CQ generation, as an intermediate process, does not help much on end-to-end QA performance, i.e., predicting answers for AQ. So, why do authors think generating CQ is necessary if people still mostly care about the end-to-end QA performance?  6. The proposed method makes good ablation study on using or not using CQ in the question answering generation process. However, the paper does not compare with SoTA performance on AmbigQA.  Leaderboard: https://nlp.cs.washington.edu/ambigqa/leaderboard.html Minor:  7. For CQ generation evaluation, how many references are provided? Do authors construct multi-reference for evaluation to improve the robustness?  8. Do authors try different prompts (as shown in line 239 and line 289) to generate CQ using ChatGPT?\n\nmissing_references: n/a\n\ntypos_grammar_style_and_presentation_improvements: n/a\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "bitOc3EZcv",
        "length": 489,
        "human_text": "paper_topic_and_main_contributions: When confronted with an ambiguous question, rather than have the alternatives spelled out as a list of separate questions, it is more natural to produce a clarification question with concisely expressed options. In this paper the authors describe the addition to an existing dataset of ambiguous questions, relevant passages, and possible answers, a clarification question of the preferred type. This was done using InstructGPT with some few-shot examples to generate possible candidates which were then manually checked and revised.\nThey distinguish two subtasks relevant to the creation of this dataset: ambiguity detection and clarification question generation, and one which uses the results of these subtasks, namely finding answers to the clarification question.\nAnnotators were asked to state a preference between a number of the original verbose individual questions and the more compact alternative version. This latter was usually preferred, and was invariably preferred when more than three interpretations were possible.\nThe paper presents preliminary results for ambiguity detection: under one condition, a BERT-based classifier is trained to distinguish ambiguous from non-ambiguous questions (trained and tested on the original dataset) and under another a BART-based model is used to predict answers, with the input classified as ambiguous if more than one is predicted. The first classifier achieved 61.3 F1, the second 34.3.\nThere are also preliminary results for clarification question generation. Various BART models are trained to predict the clarification question given (1) the ambiguous one and relevant passages, or (2) the ambiguous question, answers as predicted by one of the models just described as well as the relevant passages. For comparison, in one condition (3) the predicted answers were replaced by the actual answers, as a kind of best case. Naturally, the latter case produced the best results, with no significant variation between 1 and 2.\nThe final set of experiments tests QA performance on clarification questions, testing four conditions: ambiguity detection with and without predicted answers and clarification questions created with or without the predicted answers. Perhaps surprisingly the best results were obtained with no answers for either stage.\nThe main contributions of the paper are (1) a clear description of the problem (2) provision (public?) of a dataset and (3) initial results for the accuracy of automation of some of the stages, and utility in a QA setting.\n\nreasons_to_accept: This is an interesting set of problems, which the paper presents one type of solution to.\n\nreasons_to_reject: I can't see any reason to reject this paper.\n\ntypos_grammar_style_and_presentation_improvements: I could not see any typos. The paper is clear and well written.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "179_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_179_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7172999999999999,
      "max_similarity": 0.724,
      "avg_coverage": 0.48069999999999996,
      "max_coverage": 0.6087
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 383,
      "avg_human_length": 615.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 8,
      "weaknesses_count": 7,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "n6blR5W7cv",
        "similarity": 0.7142,
        "coverage": 0.5143,
        "human_length": 576,
        "human_text": "paper_topic_and_main_contributions: This paper constructs an open-domain QA dataset (CAMBIGNQ) with clarification questions for resolve the ambiguity. The dataset is built upon an existing dataset (AMBIGNQ). The only difference between these two datasets are that the newly constructed one asks clarification question while the existing one asks disambiguated questions for each possible interpretation. This paper reports the human evaluation on the preference between these two ways for resolving ambiguity in QAs. The results support their arguments that this new setting is more preferable.  In addition to the dataset, this work establishes benchmark performance on three tasks: ambiguity detection, clarification generation and clarification-based question answering. The experimental results reveal the difficulty of these tasks, thus show the need of this new dataset  for future development.\n\nreasons_to_accept: - Construct a new open-domain QA dataset for resolving ambiguity by asking clarification questions. This dataset can be used for following work to improve the modeling performance.\n- Demonstrate the advantage of asking clarification questions than disambiguated questions. This effort encourages future work towards generating clarification question.\n- Report solid benchmark performance. These experiments reveal the challenge of these tasks.\n- Evaluate InstructGPT's ability on generate clarification questions from disambiguated questions through in-context few-shot learning. The insufficient performance demands better methods.\n\nreasons_to_reject: - Some experimental design is not convincing. For example, in Sec 6.1, they use BERT model for one setting and BART model for the other, so the difference between these two results may not come from whether the predicted answers are useful or not, since it may come from the different capacity of these two models. Another one is that in Table 5, the experiment with ground truth CQ is required, because this will identify whether the low performance in Table 5 is caused by the QA modeling or the generated CQ quality.\n- The benchmark experiments are conducted with BERT, BART which don't represent a SOTA performance. These experimental results are not convincing that these tasks are difficult for more SOTA models such as XLNET, GPT-3, GPT-3.5, GPT-4.  - They only consider one fixed template for asking clarification question, which limits the scope of this work.\n\nquestions_for_the_authors: - One key contribution is to generate clarification questions from disambiguated questions. The InstructGPT with few-shot learning shows limited performance. Performance of some intuitive models would be necessary to be reported, such as rule-based approach which extracts new words in disambiguated questions compared to the ambiguous questions and then consider them as \"options\". Other approaches to be considered is to fine-tune BERT for example. Did you conduct them?\n- The clarification question follows a fixed template, i.e. \u201cWhich [category]: [option1], [option2], ..., or [optionn]?\u201d In human communication, there could be other ways which may be more natural and fluent given a certain context. Why did you choose this template, and how to compare with other possibilities?\n- What do you mean by \"reranked related paragraphs\" in line 242 and 504? ( Reranked by what?)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "JKArhgoYSJ",
        "similarity": 0.724,
        "coverage": 0.3191,
        "human_length": 780,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an innovative approach to handle ambiguous questions (AQ) in open-domain QA by introducing clarifying ambiguous natural questions (CAmbigNQ), a dataset consisting of 5,653 AQs with relevant passages, possible answers, and clarification questions (CQ). Unlike previous methods that resolved ambiguities by **directly** generating disambiguated questions (DQs), the authors present a novel method that first ask CQ before generating an answer. The pipeline consists of three tasks: ambiguity detection, clarification question generation, and clarification-based QA. Experimental results are presented, emphasizing the need for further enhancements.\n\nreasons_to_accept: **-- Novel Approach:** The authors introduce a new method by shifting from conventional disambiguated question generation to a clarification questions-based method, aligning more closely with real-world applications.\n**-- New Dataset:** The creation of the CAmbigNQ dataset to support research in ambiguity in QA represents a valuable asset for the community.\n**-- Writing Quality:** The paper is clear, well-written, and effectively uses examples to explain the concepts, enhancing readability and understanding.\n\nreasons_to_reject: **-- Doubtful Motivation:** While CQ improves user preference (as shown in Experiment 1), 33% of people preferred not to generate a CQ, which raises questions about the necessity and effectiveness of producing these CQs.\n**-- Insufficient Comparison with SoTA:** The paper lacks comparison with state-of-the-art methods, which is essential to gauge the true effectiveness of the proposed approach.\n**-- Questions about Methodology:** Several questions arise concerning the methodology, including the choice of evaluation metrics, the contradiction in observations, and the real necessity of CQ generation for end-to-end performance. These need to be clearly addressed to strengthen the paper (see questions for more details).\n\nquestions_for_the_authors: 1. While CQ improves user preference (as shown in experiment 1), 33% of people preferred not to generate a CQ. Could this indicate the generated CQs could be misleading or incorrect? Why do human annotators do not prefer those CQs? It would be beneficial to understand the underlying reasons for this preference.\nAnd at the very least, generating CQs should ideally (at least) not alter human preference, as more information is provided compared to only AQ and DQ provided. However, the fact that human preference decreases by 33% when generating CQs raises questions about the necessity and effectiveness of producing these CQs.\n2. The paper relies on large language models such as ChatGPT to generate CQs. I am curious about the ability of a model like ChatGPT to **directly** solve AQ, instead of using generated CQs to train small language model? Can ChatGPT solve AQ in a form similar to chain-of-thought, for example, a chain consisting of (1) ask CQ (2) generate DQ (3) answer the AQ. To summarize, what is the ChatGPT performance on such task? Could ChatGPT perform better than this complex pipeline?\n3. line 394 stated *\u201csince predicted answers for AQ have been shown to be helpful for DQ based approaches\u201d*, however, as shown in Table  3 and Table 6, it seems *No Answers for AQ* makes better performance than *Predicted Answers for AQ* on the ambiguity detection and end-QA task. Does it mean your observation contradict to the observation from existing literature? Can you explain this discrepancy? Or please correct me if I misunderstand this.  4. Since *category* name are mostly short, so, does the use of BLEU or EM score as evaluation metrics reflect the correctness of generated CQs accurately? How does it align with human evaluations? And also same question as the evaluation of CQs, did authors perform human evaluation or semantic-based evaluation on CQs, since the CQs is much longer than *category*?\n5. As shown in Table 5, it seems adding this CQ generation, as an intermediate process, does not help much on end-to-end QA performance, i.e., predicting answers for AQ. So, why do authors think generating CQ is necessary if people still mostly care about the end-to-end QA performance?  6. The proposed method makes good ablation study on using or not using CQ in the question answering generation process. However, the paper does not compare with SoTA performance on AmbigQA.  Leaderboard: https://nlp.cs.washington.edu/ambigqa/leaderboard.html Minor:  7. For CQ generation evaluation, how many references are provided? Do authors construct multi-reference for evaluation to improve the robustness?  8. Do authors try different prompts (as shown in line 239 and line 289) to generate CQ using ChatGPT?\n\nmissing_references: n/a\n\ntypos_grammar_style_and_presentation_improvements: n/a\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "bitOc3EZcv",
        "similarity": 0.7137,
        "coverage": 0.6087,
        "human_length": 489,
        "human_text": "paper_topic_and_main_contributions: When confronted with an ambiguous question, rather than have the alternatives spelled out as a list of separate questions, it is more natural to produce a clarification question with concisely expressed options. In this paper the authors describe the addition to an existing dataset of ambiguous questions, relevant passages, and possible answers, a clarification question of the preferred type. This was done using InstructGPT with some few-shot examples to generate possible candidates which were then manually checked and revised.\nThey distinguish two subtasks relevant to the creation of this dataset: ambiguity detection and clarification question generation, and one which uses the results of these subtasks, namely finding answers to the clarification question.\nAnnotators were asked to state a preference between a number of the original verbose individual questions and the more compact alternative version. This latter was usually preferred, and was invariably preferred when more than three interpretations were possible.\nThe paper presents preliminary results for ambiguity detection: under one condition, a BERT-based classifier is trained to distinguish ambiguous from non-ambiguous questions (trained and tested on the original dataset) and under another a BART-based model is used to predict answers, with the input classified as ambiguous if more than one is predicted. The first classifier achieved 61.3 F1, the second 34.3.\nThere are also preliminary results for clarification question generation. Various BART models are trained to predict the clarification question given (1) the ambiguous one and relevant passages, or (2) the ambiguous question, answers as predicted by one of the models just described as well as the relevant passages. For comparison, in one condition (3) the predicted answers were replaced by the actual answers, as a kind of best case. Naturally, the latter case produced the best results, with no significant variation between 1 and 2.\nThe final set of experiments tests QA performance on clarification questions, testing four conditions: ambiguity detection with and without predicted answers and clarification questions created with or without the predicted answers. Perhaps surprisingly the best results were obtained with no answers for either stage.\nThe main contributions of the paper are (1) a clear description of the problem (2) provision (public?) of a dataset and (3) initial results for the accuracy of automation of some of the stages, and utility in a QA setting.\n\nreasons_to_accept: This is an interesting set of problems, which the paper presents one type of solution to.\n\nreasons_to_reject: I can't see any reason to reject this paper.\n\ntypos_grammar_style_and_presentation_improvements: I could not see any typos. The paper is clear and well written.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "n6blR5W7cv",
        "length": 576,
        "human_text": "paper_topic_and_main_contributions: This paper constructs an open-domain QA dataset (CAMBIGNQ) with clarification questions for resolve the ambiguity. The dataset is built upon an existing dataset (AMBIGNQ). The only difference between these two datasets are that the newly constructed one asks clarification question while the existing one asks disambiguated questions for each possible interpretation. This paper reports the human evaluation on the preference between these two ways for resolving ambiguity in QAs. The results support their arguments that this new setting is more preferable.  In addition to the dataset, this work establishes benchmark performance on three tasks: ambiguity detection, clarification generation and clarification-based question answering. The experimental results reveal the difficulty of these tasks, thus show the need of this new dataset  for future development.\n\nreasons_to_accept: - Construct a new open-domain QA dataset for resolving ambiguity by asking clarification questions. This dataset can be used for following work to improve the modeling performance.\n- Demonstrate the advantage of asking clarification questions than disambiguated questions. This effort encourages future work towards generating clarification question.\n- Report solid benchmark performance. These experiments reveal the challenge of these tasks.\n- Evaluate InstructGPT's ability on generate clarification questions from disambiguated questions through in-context few-shot learning. The insufficient performance demands better methods.\n\nreasons_to_reject: - Some experimental design is not convincing. For example, in Sec 6.1, they use BERT model for one setting and BART model for the other, so the difference between these two results may not come from whether the predicted answers are useful or not, since it may come from the different capacity of these two models. Another one is that in Table 5, the experiment with ground truth CQ is required, because this will identify whether the low performance in Table 5 is caused by the QA modeling or the generated CQ quality.\n- The benchmark experiments are conducted with BERT, BART which don't represent a SOTA performance. These experimental results are not convincing that these tasks are difficult for more SOTA models such as XLNET, GPT-3, GPT-3.5, GPT-4.  - They only consider one fixed template for asking clarification question, which limits the scope of this work.\n\nquestions_for_the_authors: - One key contribution is to generate clarification questions from disambiguated questions. The InstructGPT with few-shot learning shows limited performance. Performance of some intuitive models would be necessary to be reported, such as rule-based approach which extracts new words in disambiguated questions compared to the ambiguous questions and then consider them as \"options\". Other approaches to be considered is to fine-tune BERT for example. Did you conduct them?\n- The clarification question follows a fixed template, i.e. \u201cWhich [category]: [option1], [option2], ..., or [optionn]?\u201d In human communication, there could be other ways which may be more natural and fluent given a certain context. Why did you choose this template, and how to compare with other possibilities?\n- What do you mean by \"reranked related paragraphs\" in line 242 and 504? ( Reranked by what?)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "JKArhgoYSJ",
        "length": 780,
        "human_text": "paper_topic_and_main_contributions: The paper proposes an innovative approach to handle ambiguous questions (AQ) in open-domain QA by introducing clarifying ambiguous natural questions (CAmbigNQ), a dataset consisting of 5,653 AQs with relevant passages, possible answers, and clarification questions (CQ). Unlike previous methods that resolved ambiguities by **directly** generating disambiguated questions (DQs), the authors present a novel method that first ask CQ before generating an answer. The pipeline consists of three tasks: ambiguity detection, clarification question generation, and clarification-based QA. Experimental results are presented, emphasizing the need for further enhancements.\n\nreasons_to_accept: **-- Novel Approach:** The authors introduce a new method by shifting from conventional disambiguated question generation to a clarification questions-based method, aligning more closely with real-world applications.\n**-- New Dataset:** The creation of the CAmbigNQ dataset to support research in ambiguity in QA represents a valuable asset for the community.\n**-- Writing Quality:** The paper is clear, well-written, and effectively uses examples to explain the concepts, enhancing readability and understanding.\n\nreasons_to_reject: **-- Doubtful Motivation:** While CQ improves user preference (as shown in Experiment 1), 33% of people preferred not to generate a CQ, which raises questions about the necessity and effectiveness of producing these CQs.\n**-- Insufficient Comparison with SoTA:** The paper lacks comparison with state-of-the-art methods, which is essential to gauge the true effectiveness of the proposed approach.\n**-- Questions about Methodology:** Several questions arise concerning the methodology, including the choice of evaluation metrics, the contradiction in observations, and the real necessity of CQ generation for end-to-end performance. These need to be clearly addressed to strengthen the paper (see questions for more details).\n\nquestions_for_the_authors: 1. While CQ improves user preference (as shown in experiment 1), 33% of people preferred not to generate a CQ. Could this indicate the generated CQs could be misleading or incorrect? Why do human annotators do not prefer those CQs? It would be beneficial to understand the underlying reasons for this preference.\nAnd at the very least, generating CQs should ideally (at least) not alter human preference, as more information is provided compared to only AQ and DQ provided. However, the fact that human preference decreases by 33% when generating CQs raises questions about the necessity and effectiveness of producing these CQs.\n2. The paper relies on large language models such as ChatGPT to generate CQs. I am curious about the ability of a model like ChatGPT to **directly** solve AQ, instead of using generated CQs to train small language model? Can ChatGPT solve AQ in a form similar to chain-of-thought, for example, a chain consisting of (1) ask CQ (2) generate DQ (3) answer the AQ. To summarize, what is the ChatGPT performance on such task? Could ChatGPT perform better than this complex pipeline?\n3. line 394 stated *\u201csince predicted answers for AQ have been shown to be helpful for DQ based approaches\u201d*, however, as shown in Table  3 and Table 6, it seems *No Answers for AQ* makes better performance than *Predicted Answers for AQ* on the ambiguity detection and end-QA task. Does it mean your observation contradict to the observation from existing literature? Can you explain this discrepancy? Or please correct me if I misunderstand this.  4. Since *category* name are mostly short, so, does the use of BLEU or EM score as evaluation metrics reflect the correctness of generated CQs accurately? How does it align with human evaluations? And also same question as the evaluation of CQs, did authors perform human evaluation or semantic-based evaluation on CQs, since the CQs is much longer than *category*?\n5. As shown in Table 5, it seems adding this CQ generation, as an intermediate process, does not help much on end-to-end QA performance, i.e., predicting answers for AQ. So, why do authors think generating CQ is necessary if people still mostly care about the end-to-end QA performance?  6. The proposed method makes good ablation study on using or not using CQ in the question answering generation process. However, the paper does not compare with SoTA performance on AmbigQA.  Leaderboard: https://nlp.cs.washington.edu/ambigqa/leaderboard.html Minor:  7. For CQ generation evaluation, how many references are provided? Do authors construct multi-reference for evaluation to improve the robustness?  8. Do authors try different prompts (as shown in line 239 and line 289) to generate CQ using ChatGPT?\n\nmissing_references: n/a\n\ntypos_grammar_style_and_presentation_improvements: n/a\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "bitOc3EZcv",
        "length": 489,
        "human_text": "paper_topic_and_main_contributions: When confronted with an ambiguous question, rather than have the alternatives spelled out as a list of separate questions, it is more natural to produce a clarification question with concisely expressed options. In this paper the authors describe the addition to an existing dataset of ambiguous questions, relevant passages, and possible answers, a clarification question of the preferred type. This was done using InstructGPT with some few-shot examples to generate possible candidates which were then manually checked and revised.\nThey distinguish two subtasks relevant to the creation of this dataset: ambiguity detection and clarification question generation, and one which uses the results of these subtasks, namely finding answers to the clarification question.\nAnnotators were asked to state a preference between a number of the original verbose individual questions and the more compact alternative version. This latter was usually preferred, and was invariably preferred when more than three interpretations were possible.\nThe paper presents preliminary results for ambiguity detection: under one condition, a BERT-based classifier is trained to distinguish ambiguous from non-ambiguous questions (trained and tested on the original dataset) and under another a BART-based model is used to predict answers, with the input classified as ambiguous if more than one is predicted. The first classifier achieved 61.3 F1, the second 34.3.\nThere are also preliminary results for clarification question generation. Various BART models are trained to predict the clarification question given (1) the ambiguous one and relevant passages, or (2) the ambiguous question, answers as predicted by one of the models just described as well as the relevant passages. For comparison, in one condition (3) the predicted answers were replaced by the actual answers, as a kind of best case. Naturally, the latter case produced the best results, with no significant variation between 1 and 2.\nThe final set of experiments tests QA performance on clarification questions, testing four conditions: ambiguity detection with and without predicted answers and clarification questions created with or without the predicted answers. Perhaps surprisingly the best results were obtained with no answers for either stage.\nThe main contributions of the paper are (1) a clear description of the problem (2) provision (public?) of a dataset and (3) initial results for the accuracy of automation of some of the stages, and utility in a QA setting.\n\nreasons_to_accept: This is an interesting set of problems, which the paper presents one type of solution to.\n\nreasons_to_reject: I can't see any reason to reject this paper.\n\ntypos_grammar_style_and_presentation_improvements: I could not see any typos. The paper is clear and well written.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "83_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_83_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7059000000000001,
      "max_similarity": 0.7403,
      "avg_coverage": 0.4545333333333333,
      "max_coverage": 0.5152
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 476,
      "avg_human_length": 620.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 7,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "clbWUYlaw3",
        "similarity": 0.7183,
        "coverage": 0.5152,
        "human_length": 845,
        "human_text": "paper_topic_and_main_contributions: The paper looks at stereotypes in Stable Diffusion (text-to-image), with a focus on gender and nationality/continental identity. The research revealed that when generating images of a 'person', the model disproportionately skews towards males and individuals from Europe/North America, presenting an implicit bias. A troubling pattern of sexualization of women, especially Latin American, Mexican, Egyptian, and Indian women, was also identified. The authors establish these patterns using pairwise CLIP-cosine similarity and manual examination across 138 prompts.\n\nreasons_to_accept: Years ago there was important work in linguistics and related fields to establish that, in English, \"generic man\" and \"generic he\" (standing in for 'humanity') was not neutral, this paper is in a similar vein but works on images/representations. What does \"a person\" look like? I think many researcher will be interested in their use of CLIP-cosine similarity and disagreement with this method feels likely to be fruitful for everyone involved.\n\nreasons_to_reject: I guess if I'm looking for risks, I could say \"Oh no, the authors are going to flash a lot of overly sexualized pictures of women at the audience\" but uh, the care the authors have around this make me feel like that's not a real risk here.  I think the authors could probably be clearer about the drawbacks of picking cosine similarity. They do mention that all sorts of stuff might be present in the image that humans would see but that aren't picked up by CLIP. They also mention the problem of bias already known in CLIP but I think this could be slowed down and built out to show readers that the logic isn't circular.  The obvious alternative or addition would seem to be a big annotation project...that would be great to verify the method but to be clear I don't see that as being required to shore this up (though it should be someone's follow-on research).  Something more within the authors' power/scope could be to demonstrate the images/descriptions of pairs that are especially high. The aggregate stats are meant to zoom out but it can be useful to give more granular examples. In particular, showing cross-category highly similar pairs and within-category highly dissimilar pairs may help readers understand what the embeddings are and aren't doing.\n\nquestions_for_the_authors: 1) If you could direct the engineering team behind Stable Diffusion to address these problems, where would you have them put their effort?\nFor example, I assume one part of the problem is the training data, LAION-5B likely includes plenty of sexualized images of women but even the labeling of non-offensive images is likely to have a subtle POV that reflects social structures. That is, many white Americans giving a caption wouldn't think to write \"a white person waving goodbye\".\nYou may think of that as a morass and prefer to direct the engineers to sampling\u2014note when users have typed text that seems to be fairly generic and make sure that the model doesn't reproduce biases by, say, adding in other terms at random (\"a person waving goodbye\" will be given some probability of x gender being selected, some probability of y ethnicity/country).  2) Relatedly, what are the uses of Stable Diffusion that you are most worried about? For example, is it marketing folks generating images for  their websites/emails/ads? Individuals generating memes? Something else? If you could solve the bias you're detecting for one and only one use case, which would it be and why?\n--- Whether or not your answers to these questions make it into the paper (I hope they do), I think this discussion among yourselves will clarify how you see the situation and judge interventions to deal with the problems you're detecting.  You have written \"Our findings have worrisome implications on exacerbating societal tendencies of the Western stereotype, and designers should consider how their datasets and design choices lead up to such results.\" I think that is well put but to say \"think carefully\" without saying \"we recommend X\" is to put a lot more onus on those designers and if you have a way of helping them, that feels like it is a helpful part of harm reduction. And if other researchers disagree, that seems like a valuable thing for the field to wrestle with.\n\ntypos_grammar_style_and_presentation_improvements: I feel torn about this part of your Ethics Statement: \"Though our finding of the stereotypical definition of personhood being a Western, light-skinned man can amplify societal problems where people of nonbinary gender, especially transgender individuals, are considered inhuman abominations by conservative peoples (Roen, 2002), we do not claim this as a finding.\"\nPart of me feels like you are helping readers see a very stark and dehumanizing reality. But more of me feels like you could probably convey this without saying \"inhuman abominations\".\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "KuF6HlGd2a",
        "similarity": 0.6591,
        "coverage": 0.4706,
        "human_length": 205,
        "human_text": "paper_topic_and_main_contributions: The paper conducts a diagnostic study to uncover stereotypes in Stable Diffusion. The stereotypical definition of personhood corresponds closely to Western, light-skinned men. Sexualization of women, mostly Latin Amerin was also common.\n\nreasons_to_accept: 1. A comprehensive diagnostic study was conducted to reveal stereotypes in Stable Diffusion. \n2. The paper is well written.\n\nreasons_to_reject: 1. The findings are not novel compared to [1], which finds that \"diffusion models over-represent the portion of their latent space associated with whiteness and masculinity across target attributes.\"\n[1] Luccioni, A. S., Akiki, C., Mitchell, M., & Jernite, Y. (2023). Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "yo74GpioI8",
        "similarity": 0.7403,
        "coverage": 0.3778,
        "human_length": 811,
        "human_text": "paper_topic_and_main_contributions: This paper analyzes the stereotypical definitions of personhood in Stable Diffusion, a text-to-image generator. They tested 138 prompts for front-facing photos of people of different genders and continental/national identities and compared the results to reveal two stereotypes - gender and continental/national identities. They found that Stable Diffusion's definition of personhood corresponded closely to Western, light-skinned men, threatening to erase historically marginalized groups. Additionally, they found a pattern of sexualization of women, perpetuating the Western stereotype of fetishizing women of color. These findings suggest the need for more careful use of these tools and improvements in developing fair generators.\n\nreasons_to_accept: This paper provides several exciting findings in text-to-image generations: (1) there exists stereotypes of 'person' for Stable Diffusion when no other information about gender is provided, skews male and ignores nonbinary genders; (2) based on my understanding, this paper is the pioneer to discuss the stereotypes in contexts of national/continental identities; (3) they uncovered the patterns of sexualization of woman, which extends to the findings from previous work. These results can help researchers build new methods to resolve fairness issues and alarm the models' reliability when practitioners want to apply them to real-world applications.\n\nreasons_to_reject: 1. As mentioned in line 360, CLIP-embeddings are to be biased; I would question whether the higher cosine similarity between \u2018person\u2019 and \u2018man\u2019 may come from the biased evaluation metric. The paper will be sound if experiments evaluate the inherent biases in CLIP-embeddings and confirm that the biases will not have a dominant effect on the results are done. Moreover, I would like to see human evaluation beyond the cosine similarity to support the findings in the paper. \n2. As mentioned in line 289, the countries chosen in the paper are the top five most populated countries; I would question the selection here to be unfair. I would recommend selecting more countries regarding population size: large, medium, and small, to conduct more comprehensive experiments. \n3. As mentioned in line 655, models like Fair Diffusion/Safe Latent Diffusion have been developed to improve the quality of Stable Diffusion generation in terms of social stereotypes. It would be interesting and important to see similar experiments conducted in this paper applied to debiasing text-to-image generators. I believe these experimental results will provide strong evidence to support the arguments in this paper since the debiasing method has been already developed and it is necessary to see whether the arguments mentioned in this paper have been resolved entirely or partially or not by the methods. \n4. The overall reason is although this paper provides many qualitative discussions, it lacks quantitative explanations to support the arguments. It is crucial to include human evaluation to balance the drawback of biased evaluation metrics (CLIP-embeddings.) Moreover, it is impressive to learn the findings from this paper, but thinking about how to reduce/mitigate the stereotypes might also be noteworthy.\n\nquestions_for_the_authors: 1. The images presented in Figure 1 seem cherry-picked to some extent. I would like to see more statistics on these generated images. I think it is possible to randomly sample the number of pictures and apply human evaluation to label the details. \n2. Would it be possible to provide images of examples of nonbinary gender? \n3. In Table 5, for Bangladesh and Ghana, cosine similarity scores of nonbinary gender are higher than \u2018man\u2019 or \u2018woman\u2019. Could you provide more explanations on these results? Similarly, in Table 4, many cosine similarity scores are very close among different columns. These results make me question whether the huge difference in Table 3 comes from pre-existing bias in CLIP-embeddings. \n4. I would like to see the statistical significance of these quantitative results.\n\ntypos_grammar_style_and_presentation_improvements: 1. The paper could be much appreciated if proofread by an English native speaker. Many repetitions exist in several sections, making the overall paper look wordy. \n2. Captions are not self-contained, which makes the reader hard to understand without referring to the main context. Besides, it would be appreciated if the authors could improve the presentation of Tables 3, 4, and 5 since the current format can easily confuse the reader and hinder understanding.\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: In section 4.3, which discusses the sexualization of non-western women, I think the authors need to revise the content, for example, avoiding using 'sexy' in Table 2 and other places since this word usage might be inappropriate for specific groups.\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "clbWUYlaw3",
        "length": 845,
        "human_text": "paper_topic_and_main_contributions: The paper looks at stereotypes in Stable Diffusion (text-to-image), with a focus on gender and nationality/continental identity. The research revealed that when generating images of a 'person', the model disproportionately skews towards males and individuals from Europe/North America, presenting an implicit bias. A troubling pattern of sexualization of women, especially Latin American, Mexican, Egyptian, and Indian women, was also identified. The authors establish these patterns using pairwise CLIP-cosine similarity and manual examination across 138 prompts.\n\nreasons_to_accept: Years ago there was important work in linguistics and related fields to establish that, in English, \"generic man\" and \"generic he\" (standing in for 'humanity') was not neutral, this paper is in a similar vein but works on images/representations. What does \"a person\" look like? I think many researcher will be interested in their use of CLIP-cosine similarity and disagreement with this method feels likely to be fruitful for everyone involved.\n\nreasons_to_reject: I guess if I'm looking for risks, I could say \"Oh no, the authors are going to flash a lot of overly sexualized pictures of women at the audience\" but uh, the care the authors have around this make me feel like that's not a real risk here.  I think the authors could probably be clearer about the drawbacks of picking cosine similarity. They do mention that all sorts of stuff might be present in the image that humans would see but that aren't picked up by CLIP. They also mention the problem of bias already known in CLIP but I think this could be slowed down and built out to show readers that the logic isn't circular.  The obvious alternative or addition would seem to be a big annotation project...that would be great to verify the method but to be clear I don't see that as being required to shore this up (though it should be someone's follow-on research).  Something more within the authors' power/scope could be to demonstrate the images/descriptions of pairs that are especially high. The aggregate stats are meant to zoom out but it can be useful to give more granular examples. In particular, showing cross-category highly similar pairs and within-category highly dissimilar pairs may help readers understand what the embeddings are and aren't doing.\n\nquestions_for_the_authors: 1) If you could direct the engineering team behind Stable Diffusion to address these problems, where would you have them put their effort?\nFor example, I assume one part of the problem is the training data, LAION-5B likely includes plenty of sexualized images of women but even the labeling of non-offensive images is likely to have a subtle POV that reflects social structures. That is, many white Americans giving a caption wouldn't think to write \"a white person waving goodbye\".\nYou may think of that as a morass and prefer to direct the engineers to sampling\u2014note when users have typed text that seems to be fairly generic and make sure that the model doesn't reproduce biases by, say, adding in other terms at random (\"a person waving goodbye\" will be given some probability of x gender being selected, some probability of y ethnicity/country).  2) Relatedly, what are the uses of Stable Diffusion that you are most worried about? For example, is it marketing folks generating images for  their websites/emails/ads? Individuals generating memes? Something else? If you could solve the bias you're detecting for one and only one use case, which would it be and why?\n--- Whether or not your answers to these questions make it into the paper (I hope they do), I think this discussion among yourselves will clarify how you see the situation and judge interventions to deal with the problems you're detecting.  You have written \"Our findings have worrisome implications on exacerbating societal tendencies of the Western stereotype, and designers should consider how their datasets and design choices lead up to such results.\" I think that is well put but to say \"think carefully\" without saying \"we recommend X\" is to put a lot more onus on those designers and if you have a way of helping them, that feels like it is a helpful part of harm reduction. And if other researchers disagree, that seems like a valuable thing for the field to wrestle with.\n\ntypos_grammar_style_and_presentation_improvements: I feel torn about this part of your Ethics Statement: \"Though our finding of the stereotypical definition of personhood being a Western, light-skinned man can amplify societal problems where people of nonbinary gender, especially transgender individuals, are considered inhuman abominations by conservative peoples (Roen, 2002), we do not claim this as a finding.\"\nPart of me feels like you are helping readers see a very stark and dehumanizing reality. But more of me feels like you could probably convey this without saying \"inhuman abominations\".\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "KuF6HlGd2a",
        "length": 205,
        "human_text": "paper_topic_and_main_contributions: The paper conducts a diagnostic study to uncover stereotypes in Stable Diffusion. The stereotypical definition of personhood corresponds closely to Western, light-skinned men. Sexualization of women, mostly Latin Amerin was also common.\n\nreasons_to_accept: 1. A comprehensive diagnostic study was conducted to reveal stereotypes in Stable Diffusion. \n2. The paper is well written.\n\nreasons_to_reject: 1. The findings are not novel compared to [1], which finds that \"diffusion models over-represent the portion of their latent space associated with whiteness and masculinity across target attributes.\"\n[1] Luccioni, A. S., Akiki, C., Mitchell, M., & Jernite, Y. (2023). Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "yo74GpioI8",
        "length": 811,
        "human_text": "paper_topic_and_main_contributions: This paper analyzes the stereotypical definitions of personhood in Stable Diffusion, a text-to-image generator. They tested 138 prompts for front-facing photos of people of different genders and continental/national identities and compared the results to reveal two stereotypes - gender and continental/national identities. They found that Stable Diffusion's definition of personhood corresponded closely to Western, light-skinned men, threatening to erase historically marginalized groups. Additionally, they found a pattern of sexualization of women, perpetuating the Western stereotype of fetishizing women of color. These findings suggest the need for more careful use of these tools and improvements in developing fair generators.\n\nreasons_to_accept: This paper provides several exciting findings in text-to-image generations: (1) there exists stereotypes of 'person' for Stable Diffusion when no other information about gender is provided, skews male and ignores nonbinary genders; (2) based on my understanding, this paper is the pioneer to discuss the stereotypes in contexts of national/continental identities; (3) they uncovered the patterns of sexualization of woman, which extends to the findings from previous work. These results can help researchers build new methods to resolve fairness issues and alarm the models' reliability when practitioners want to apply them to real-world applications.\n\nreasons_to_reject: 1. As mentioned in line 360, CLIP-embeddings are to be biased; I would question whether the higher cosine similarity between \u2018person\u2019 and \u2018man\u2019 may come from the biased evaluation metric. The paper will be sound if experiments evaluate the inherent biases in CLIP-embeddings and confirm that the biases will not have a dominant effect on the results are done. Moreover, I would like to see human evaluation beyond the cosine similarity to support the findings in the paper. \n2. As mentioned in line 289, the countries chosen in the paper are the top five most populated countries; I would question the selection here to be unfair. I would recommend selecting more countries regarding population size: large, medium, and small, to conduct more comprehensive experiments. \n3. As mentioned in line 655, models like Fair Diffusion/Safe Latent Diffusion have been developed to improve the quality of Stable Diffusion generation in terms of social stereotypes. It would be interesting and important to see similar experiments conducted in this paper applied to debiasing text-to-image generators. I believe these experimental results will provide strong evidence to support the arguments in this paper since the debiasing method has been already developed and it is necessary to see whether the arguments mentioned in this paper have been resolved entirely or partially or not by the methods. \n4. The overall reason is although this paper provides many qualitative discussions, it lacks quantitative explanations to support the arguments. It is crucial to include human evaluation to balance the drawback of biased evaluation metrics (CLIP-embeddings.) Moreover, it is impressive to learn the findings from this paper, but thinking about how to reduce/mitigate the stereotypes might also be noteworthy.\n\nquestions_for_the_authors: 1. The images presented in Figure 1 seem cherry-picked to some extent. I would like to see more statistics on these generated images. I think it is possible to randomly sample the number of pictures and apply human evaluation to label the details. \n2. Would it be possible to provide images of examples of nonbinary gender? \n3. In Table 5, for Bangladesh and Ghana, cosine similarity scores of nonbinary gender are higher than \u2018man\u2019 or \u2018woman\u2019. Could you provide more explanations on these results? Similarly, in Table 4, many cosine similarity scores are very close among different columns. These results make me question whether the huge difference in Table 3 comes from pre-existing bias in CLIP-embeddings. \n4. I would like to see the statistical significance of these quantitative results.\n\ntypos_grammar_style_and_presentation_improvements: 1. The paper could be much appreciated if proofread by an English native speaker. Many repetitions exist in several sections, making the overall paper look wordy. \n2. Captions are not self-contained, which makes the reader hard to understand without referring to the main context. Besides, it would be appreciated if the authors could improve the presentation of Tables 3, 4, and 5 since the current format can easily confuse the reader and hinder understanding.\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: In section 4.3, which discusses the sexualization of non-western women, I think the authors need to revise the content, for example, avoiding using 'sexy' in Table 2 and other places since this word usage might be inappropriate for specific groups.\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "83_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_83_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7075999999999999,
      "max_similarity": 0.7377,
      "avg_coverage": 0.45713333333333334,
      "max_coverage": 0.5152
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 528,
      "avg_human_length": 620.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 7,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "clbWUYlaw3",
        "similarity": 0.7181,
        "coverage": 0.5152,
        "human_length": 845,
        "human_text": "paper_topic_and_main_contributions: The paper looks at stereotypes in Stable Diffusion (text-to-image), with a focus on gender and nationality/continental identity. The research revealed that when generating images of a 'person', the model disproportionately skews towards males and individuals from Europe/North America, presenting an implicit bias. A troubling pattern of sexualization of women, especially Latin American, Mexican, Egyptian, and Indian women, was also identified. The authors establish these patterns using pairwise CLIP-cosine similarity and manual examination across 138 prompts.\n\nreasons_to_accept: Years ago there was important work in linguistics and related fields to establish that, in English, \"generic man\" and \"generic he\" (standing in for 'humanity') was not neutral, this paper is in a similar vein but works on images/representations. What does \"a person\" look like? I think many researcher will be interested in their use of CLIP-cosine similarity and disagreement with this method feels likely to be fruitful for everyone involved.\n\nreasons_to_reject: I guess if I'm looking for risks, I could say \"Oh no, the authors are going to flash a lot of overly sexualized pictures of women at the audience\" but uh, the care the authors have around this make me feel like that's not a real risk here.  I think the authors could probably be clearer about the drawbacks of picking cosine similarity. They do mention that all sorts of stuff might be present in the image that humans would see but that aren't picked up by CLIP. They also mention the problem of bias already known in CLIP but I think this could be slowed down and built out to show readers that the logic isn't circular.  The obvious alternative or addition would seem to be a big annotation project...that would be great to verify the method but to be clear I don't see that as being required to shore this up (though it should be someone's follow-on research).  Something more within the authors' power/scope could be to demonstrate the images/descriptions of pairs that are especially high. The aggregate stats are meant to zoom out but it can be useful to give more granular examples. In particular, showing cross-category highly similar pairs and within-category highly dissimilar pairs may help readers understand what the embeddings are and aren't doing.\n\nquestions_for_the_authors: 1) If you could direct the engineering team behind Stable Diffusion to address these problems, where would you have them put their effort?\nFor example, I assume one part of the problem is the training data, LAION-5B likely includes plenty of sexualized images of women but even the labeling of non-offensive images is likely to have a subtle POV that reflects social structures. That is, many white Americans giving a caption wouldn't think to write \"a white person waving goodbye\".\nYou may think of that as a morass and prefer to direct the engineers to sampling\u2014note when users have typed text that seems to be fairly generic and make sure that the model doesn't reproduce biases by, say, adding in other terms at random (\"a person waving goodbye\" will be given some probability of x gender being selected, some probability of y ethnicity/country).  2) Relatedly, what are the uses of Stable Diffusion that you are most worried about? For example, is it marketing folks generating images for  their websites/emails/ads? Individuals generating memes? Something else? If you could solve the bias you're detecting for one and only one use case, which would it be and why?\n--- Whether or not your answers to these questions make it into the paper (I hope they do), I think this discussion among yourselves will clarify how you see the situation and judge interventions to deal with the problems you're detecting.  You have written \"Our findings have worrisome implications on exacerbating societal tendencies of the Western stereotype, and designers should consider how their datasets and design choices lead up to such results.\" I think that is well put but to say \"think carefully\" without saying \"we recommend X\" is to put a lot more onus on those designers and if you have a way of helping them, that feels like it is a helpful part of harm reduction. And if other researchers disagree, that seems like a valuable thing for the field to wrestle with.\n\ntypos_grammar_style_and_presentation_improvements: I feel torn about this part of your Ethics Statement: \"Though our finding of the stereotypical definition of personhood being a Western, light-skinned man can amplify societal problems where people of nonbinary gender, especially transgender individuals, are considered inhuman abominations by conservative peoples (Roen, 2002), we do not claim this as a finding.\"\nPart of me feels like you are helping readers see a very stark and dehumanizing reality. But more of me feels like you could probably convey this without saying \"inhuman abominations\".\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "KuF6HlGd2a",
        "similarity": 0.667,
        "coverage": 0.4118,
        "human_length": 205,
        "human_text": "paper_topic_and_main_contributions: The paper conducts a diagnostic study to uncover stereotypes in Stable Diffusion. The stereotypical definition of personhood corresponds closely to Western, light-skinned men. Sexualization of women, mostly Latin Amerin was also common.\n\nreasons_to_accept: 1. A comprehensive diagnostic study was conducted to reveal stereotypes in Stable Diffusion. \n2. The paper is well written.\n\nreasons_to_reject: 1. The findings are not novel compared to [1], which finds that \"diffusion models over-represent the portion of their latent space associated with whiteness and masculinity across target attributes.\"\n[1] Luccioni, A. S., Akiki, C., Mitchell, M., & Jernite, Y. (2023). Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "yo74GpioI8",
        "similarity": 0.7377,
        "coverage": 0.4444,
        "human_length": 811,
        "human_text": "paper_topic_and_main_contributions: This paper analyzes the stereotypical definitions of personhood in Stable Diffusion, a text-to-image generator. They tested 138 prompts for front-facing photos of people of different genders and continental/national identities and compared the results to reveal two stereotypes - gender and continental/national identities. They found that Stable Diffusion's definition of personhood corresponded closely to Western, light-skinned men, threatening to erase historically marginalized groups. Additionally, they found a pattern of sexualization of women, perpetuating the Western stereotype of fetishizing women of color. These findings suggest the need for more careful use of these tools and improvements in developing fair generators.\n\nreasons_to_accept: This paper provides several exciting findings in text-to-image generations: (1) there exists stereotypes of 'person' for Stable Diffusion when no other information about gender is provided, skews male and ignores nonbinary genders; (2) based on my understanding, this paper is the pioneer to discuss the stereotypes in contexts of national/continental identities; (3) they uncovered the patterns of sexualization of woman, which extends to the findings from previous work. These results can help researchers build new methods to resolve fairness issues and alarm the models' reliability when practitioners want to apply them to real-world applications.\n\nreasons_to_reject: 1. As mentioned in line 360, CLIP-embeddings are to be biased; I would question whether the higher cosine similarity between \u2018person\u2019 and \u2018man\u2019 may come from the biased evaluation metric. The paper will be sound if experiments evaluate the inherent biases in CLIP-embeddings and confirm that the biases will not have a dominant effect on the results are done. Moreover, I would like to see human evaluation beyond the cosine similarity to support the findings in the paper. \n2. As mentioned in line 289, the countries chosen in the paper are the top five most populated countries; I would question the selection here to be unfair. I would recommend selecting more countries regarding population size: large, medium, and small, to conduct more comprehensive experiments. \n3. As mentioned in line 655, models like Fair Diffusion/Safe Latent Diffusion have been developed to improve the quality of Stable Diffusion generation in terms of social stereotypes. It would be interesting and important to see similar experiments conducted in this paper applied to debiasing text-to-image generators. I believe these experimental results will provide strong evidence to support the arguments in this paper since the debiasing method has been already developed and it is necessary to see whether the arguments mentioned in this paper have been resolved entirely or partially or not by the methods. \n4. The overall reason is although this paper provides many qualitative discussions, it lacks quantitative explanations to support the arguments. It is crucial to include human evaluation to balance the drawback of biased evaluation metrics (CLIP-embeddings.) Moreover, it is impressive to learn the findings from this paper, but thinking about how to reduce/mitigate the stereotypes might also be noteworthy.\n\nquestions_for_the_authors: 1. The images presented in Figure 1 seem cherry-picked to some extent. I would like to see more statistics on these generated images. I think it is possible to randomly sample the number of pictures and apply human evaluation to label the details. \n2. Would it be possible to provide images of examples of nonbinary gender? \n3. In Table 5, for Bangladesh and Ghana, cosine similarity scores of nonbinary gender are higher than \u2018man\u2019 or \u2018woman\u2019. Could you provide more explanations on these results? Similarly, in Table 4, many cosine similarity scores are very close among different columns. These results make me question whether the huge difference in Table 3 comes from pre-existing bias in CLIP-embeddings. \n4. I would like to see the statistical significance of these quantitative results.\n\ntypos_grammar_style_and_presentation_improvements: 1. The paper could be much appreciated if proofread by an English native speaker. Many repetitions exist in several sections, making the overall paper look wordy. \n2. Captions are not self-contained, which makes the reader hard to understand without referring to the main context. Besides, it would be appreciated if the authors could improve the presentation of Tables 3, 4, and 5 since the current format can easily confuse the reader and hinder understanding.\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: In section 4.3, which discusses the sexualization of non-western women, I think the authors need to revise the content, for example, avoiding using 'sexy' in Table 2 and other places since this word usage might be inappropriate for specific groups.\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "clbWUYlaw3",
        "length": 845,
        "human_text": "paper_topic_and_main_contributions: The paper looks at stereotypes in Stable Diffusion (text-to-image), with a focus on gender and nationality/continental identity. The research revealed that when generating images of a 'person', the model disproportionately skews towards males and individuals from Europe/North America, presenting an implicit bias. A troubling pattern of sexualization of women, especially Latin American, Mexican, Egyptian, and Indian women, was also identified. The authors establish these patterns using pairwise CLIP-cosine similarity and manual examination across 138 prompts.\n\nreasons_to_accept: Years ago there was important work in linguistics and related fields to establish that, in English, \"generic man\" and \"generic he\" (standing in for 'humanity') was not neutral, this paper is in a similar vein but works on images/representations. What does \"a person\" look like? I think many researcher will be interested in their use of CLIP-cosine similarity and disagreement with this method feels likely to be fruitful for everyone involved.\n\nreasons_to_reject: I guess if I'm looking for risks, I could say \"Oh no, the authors are going to flash a lot of overly sexualized pictures of women at the audience\" but uh, the care the authors have around this make me feel like that's not a real risk here.  I think the authors could probably be clearer about the drawbacks of picking cosine similarity. They do mention that all sorts of stuff might be present in the image that humans would see but that aren't picked up by CLIP. They also mention the problem of bias already known in CLIP but I think this could be slowed down and built out to show readers that the logic isn't circular.  The obvious alternative or addition would seem to be a big annotation project...that would be great to verify the method but to be clear I don't see that as being required to shore this up (though it should be someone's follow-on research).  Something more within the authors' power/scope could be to demonstrate the images/descriptions of pairs that are especially high. The aggregate stats are meant to zoom out but it can be useful to give more granular examples. In particular, showing cross-category highly similar pairs and within-category highly dissimilar pairs may help readers understand what the embeddings are and aren't doing.\n\nquestions_for_the_authors: 1) If you could direct the engineering team behind Stable Diffusion to address these problems, where would you have them put their effort?\nFor example, I assume one part of the problem is the training data, LAION-5B likely includes plenty of sexualized images of women but even the labeling of non-offensive images is likely to have a subtle POV that reflects social structures. That is, many white Americans giving a caption wouldn't think to write \"a white person waving goodbye\".\nYou may think of that as a morass and prefer to direct the engineers to sampling\u2014note when users have typed text that seems to be fairly generic and make sure that the model doesn't reproduce biases by, say, adding in other terms at random (\"a person waving goodbye\" will be given some probability of x gender being selected, some probability of y ethnicity/country).  2) Relatedly, what are the uses of Stable Diffusion that you are most worried about? For example, is it marketing folks generating images for  their websites/emails/ads? Individuals generating memes? Something else? If you could solve the bias you're detecting for one and only one use case, which would it be and why?\n--- Whether or not your answers to these questions make it into the paper (I hope they do), I think this discussion among yourselves will clarify how you see the situation and judge interventions to deal with the problems you're detecting.  You have written \"Our findings have worrisome implications on exacerbating societal tendencies of the Western stereotype, and designers should consider how their datasets and design choices lead up to such results.\" I think that is well put but to say \"think carefully\" without saying \"we recommend X\" is to put a lot more onus on those designers and if you have a way of helping them, that feels like it is a helpful part of harm reduction. And if other researchers disagree, that seems like a valuable thing for the field to wrestle with.\n\ntypos_grammar_style_and_presentation_improvements: I feel torn about this part of your Ethics Statement: \"Though our finding of the stereotypical definition of personhood being a Western, light-skinned man can amplify societal problems where people of nonbinary gender, especially transgender individuals, are considered inhuman abominations by conservative peoples (Roen, 2002), we do not claim this as a finding.\"\nPart of me feels like you are helping readers see a very stark and dehumanizing reality. But more of me feels like you could probably convey this without saying \"inhuman abominations\".\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "KuF6HlGd2a",
        "length": 205,
        "human_text": "paper_topic_and_main_contributions: The paper conducts a diagnostic study to uncover stereotypes in Stable Diffusion. The stereotypical definition of personhood corresponds closely to Western, light-skinned men. Sexualization of women, mostly Latin Amerin was also common.\n\nreasons_to_accept: 1. A comprehensive diagnostic study was conducted to reveal stereotypes in Stable Diffusion. \n2. The paper is well written.\n\nreasons_to_reject: 1. The findings are not novel compared to [1], which finds that \"diffusion models over-represent the portion of their latent space associated with whiteness and masculinity across target attributes.\"\n[1] Luccioni, A. S., Akiki, C., Mitchell, M., & Jernite, Y. (2023). Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "yo74GpioI8",
        "length": 811,
        "human_text": "paper_topic_and_main_contributions: This paper analyzes the stereotypical definitions of personhood in Stable Diffusion, a text-to-image generator. They tested 138 prompts for front-facing photos of people of different genders and continental/national identities and compared the results to reveal two stereotypes - gender and continental/national identities. They found that Stable Diffusion's definition of personhood corresponded closely to Western, light-skinned men, threatening to erase historically marginalized groups. Additionally, they found a pattern of sexualization of women, perpetuating the Western stereotype of fetishizing women of color. These findings suggest the need for more careful use of these tools and improvements in developing fair generators.\n\nreasons_to_accept: This paper provides several exciting findings in text-to-image generations: (1) there exists stereotypes of 'person' for Stable Diffusion when no other information about gender is provided, skews male and ignores nonbinary genders; (2) based on my understanding, this paper is the pioneer to discuss the stereotypes in contexts of national/continental identities; (3) they uncovered the patterns of sexualization of woman, which extends to the findings from previous work. These results can help researchers build new methods to resolve fairness issues and alarm the models' reliability when practitioners want to apply them to real-world applications.\n\nreasons_to_reject: 1. As mentioned in line 360, CLIP-embeddings are to be biased; I would question whether the higher cosine similarity between \u2018person\u2019 and \u2018man\u2019 may come from the biased evaluation metric. The paper will be sound if experiments evaluate the inherent biases in CLIP-embeddings and confirm that the biases will not have a dominant effect on the results are done. Moreover, I would like to see human evaluation beyond the cosine similarity to support the findings in the paper. \n2. As mentioned in line 289, the countries chosen in the paper are the top five most populated countries; I would question the selection here to be unfair. I would recommend selecting more countries regarding population size: large, medium, and small, to conduct more comprehensive experiments. \n3. As mentioned in line 655, models like Fair Diffusion/Safe Latent Diffusion have been developed to improve the quality of Stable Diffusion generation in terms of social stereotypes. It would be interesting and important to see similar experiments conducted in this paper applied to debiasing text-to-image generators. I believe these experimental results will provide strong evidence to support the arguments in this paper since the debiasing method has been already developed and it is necessary to see whether the arguments mentioned in this paper have been resolved entirely or partially or not by the methods. \n4. The overall reason is although this paper provides many qualitative discussions, it lacks quantitative explanations to support the arguments. It is crucial to include human evaluation to balance the drawback of biased evaluation metrics (CLIP-embeddings.) Moreover, it is impressive to learn the findings from this paper, but thinking about how to reduce/mitigate the stereotypes might also be noteworthy.\n\nquestions_for_the_authors: 1. The images presented in Figure 1 seem cherry-picked to some extent. I would like to see more statistics on these generated images. I think it is possible to randomly sample the number of pictures and apply human evaluation to label the details. \n2. Would it be possible to provide images of examples of nonbinary gender? \n3. In Table 5, for Bangladesh and Ghana, cosine similarity scores of nonbinary gender are higher than \u2018man\u2019 or \u2018woman\u2019. Could you provide more explanations on these results? Similarly, in Table 4, many cosine similarity scores are very close among different columns. These results make me question whether the huge difference in Table 3 comes from pre-existing bias in CLIP-embeddings. \n4. I would like to see the statistical significance of these quantitative results.\n\ntypos_grammar_style_and_presentation_improvements: 1. The paper could be much appreciated if proofread by an English native speaker. Many repetitions exist in several sections, making the overall paper look wordy. \n2. Captions are not self-contained, which makes the reader hard to understand without referring to the main context. Besides, it would be appreciated if the authors could improve the presentation of Tables 3, 4, and 5 since the current format can easily confuse the reader and hinder understanding.\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: In section 4.3, which discusses the sexualization of non-western women, I think the authors need to revise the content, for example, avoiding using 'sexy' in Table 2 and other places since this word usage might be inappropriate for specific groups.\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "210_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_210_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7159000000000001,
      "max_similarity": 0.7306,
      "avg_coverage": 0.49639999999999995,
      "max_coverage": 0.7692
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 394,
      "avg_human_length": 366.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 5,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "S4vOnRTkdK",
        "similarity": 0.687,
        "coverage": 0.36,
        "human_length": 458,
        "human_text": "paper_topic_and_main_contributions: Authors study an important direction of use of mixture of experts (MoE) / switch layers at the FF layers and try to optimize the use of top-2 experts (MoE) vs only top-1 expert (switch). As expected, using top-2 experts leads to better performance than top-1 at the expense of twice as many FLOPs and longer runtime. In this work authors, propose to use top-2 experts only when the difference between the highest expert probability and the second highest expert probability is smaller than a pre-decided threshold saving them compute and retaining the performance of top-2 experts. Unfortunately, to gain full computational speedups they also require a sophisticated curriculum learning schedule.\n\nreasons_to_accept: 1. MoE/ switch layers is a useful technique and optimizing it is important - the proposed solution is simple and intuitive. \n2. I liked the analysis of complexity of various tokens. \n3. The speedups over top-2 are decent, albeit with the use of curriculum.\n\nreasons_to_reject: 1. Require sophisticated curriculum making the training pipeline complex and not plug-n-play. \n2. The decision of training top-1 baseline for longer is not convincing given that all results are in same ballpark.\n\nquestions_for_the_authors: 1) line 365: it was not clear how you initialize the experts. As you say use runs use same parameters, I am assuming the effective FF dimension of each expert is original FF dimension/ 16. If yes, this raises the question how do you split the parameters of original FF layer among these 16 experts. Do you initialize them based by splitting the original init across this in order or do you init the params of new expert FFs randomly. I'd be surprised that you match the performance of original dense model even after throwing away most of the original weights.\n2) Table 3: it is not convincing that you train top-1 for longer as the numbers are so close that maybe, if you would have kept a long enough wallclock time for all runs, top-1 would have been comparable to the rest irrespective of the loss.\n\ntypos_grammar_style_and_presentation_improvements: line 124-125  table1 : top-1 missing x line 216: what do you mean 55% - is this w.r.t. to a threshhold\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "YD3RBw1kfI",
        "similarity": 0.7306,
        "coverage": 0.36,
        "human_length": 415,
        "human_text": "paper_topic_and_main_contributions: The paper proposed an adaptive gating mechanism in a Mixture of Experts (MoE) training scheme, which enables tokens to be processed by a flexible number of experts. The proposed training strategy allows tokens to be processed by a variable number of experts based on expert probability distribution. Overall the results show that the method often achieves improved training efficiency without compromising inference performance, with the integration of curriculum learning to be beneficial for reducing training costs. The authors present an interesting analysis where they explain how the gating mechanism adjusts to each NLP task. For instance, for sentiment analysis the tokens expressing neutral opinion are routed to two experts, while this happens in QA for both questions and context text snippets.\n\nreasons_to_accept: - The paper is clear and easy to follow. The experimental setup seems solid, with multiple NLP tasks evaluated.\n- The proposed approach looks promising, as it usually outperforms baselines in terms of training time, inference FLOPs and performance. The analysis and ablation study nicely explains model design choices and provides insights on how the method works in each NLP task.\n\nreasons_to_reject: - I am skeptical about the novelty of this paper. Despite its potential impact and value, it looks like a combination of existing techniques.\n- The performance improvement (training, inference and test performance) over the baseline is quite small.\n\nquestions_for_the_authors: - How are findings in Section 3.1 / Figure 1 drawn? How are these experiments performed in detail?\n\ntypos_grammar_style_and_presentation_improvements: - The authors could use 1-2 sentences in the intro to briefly explain what top-1 and top-2 gating is (L67).\n- Section 2 should be changed structure-wise. It has only a single subsection (2.1), while Related Work is just a \\paragraph{}. - Section 4 is also lacking structure. 4.1 is only one sentence.\n- The words 'significantly' should be used more carefully (e.g. L 583), only if a significance test is performed.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "mb6Q475JV7",
        "similarity": 0.7301,
        "coverage": 0.7692,
        "human_length": 225,
        "human_text": "paper_topic_and_main_contributions: This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution.  Extensive experiments on diverse NLP tasks show that adaptive gating reduces at most 22.5% training time while maintaining inference quality.\n\nreasons_to_accept: The paper is well-written and easy to follow. The illustration of motivation and method is clear. \nThe design of adaptive gating in MoE is intuitive and technically novel.\n\nreasons_to_reject: This paper uses an existing method and does not mention or lack description of its own. \nInsufficient experiments, as many figures as possible should be used to illustrate the author's point of view. \nMore baseline models are encouraged to be used to evaluate Training Time, FLOPs and Inference Performance.\n\ntypos_grammar_style_and_presentation_improvements: \"Figures 1 depict the normalized activation values of four sampled tokens across 16 experts.\" See line 209, page3. \nFigure 2 is not clear.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "S4vOnRTkdK",
        "length": 458,
        "human_text": "paper_topic_and_main_contributions: Authors study an important direction of use of mixture of experts (MoE) / switch layers at the FF layers and try to optimize the use of top-2 experts (MoE) vs only top-1 expert (switch). As expected, using top-2 experts leads to better performance than top-1 at the expense of twice as many FLOPs and longer runtime. In this work authors, propose to use top-2 experts only when the difference between the highest expert probability and the second highest expert probability is smaller than a pre-decided threshold saving them compute and retaining the performance of top-2 experts. Unfortunately, to gain full computational speedups they also require a sophisticated curriculum learning schedule.\n\nreasons_to_accept: 1. MoE/ switch layers is a useful technique and optimizing it is important - the proposed solution is simple and intuitive. \n2. I liked the analysis of complexity of various tokens. \n3. The speedups over top-2 are decent, albeit with the use of curriculum.\n\nreasons_to_reject: 1. Require sophisticated curriculum making the training pipeline complex and not plug-n-play. \n2. The decision of training top-1 baseline for longer is not convincing given that all results are in same ballpark.\n\nquestions_for_the_authors: 1) line 365: it was not clear how you initialize the experts. As you say use runs use same parameters, I am assuming the effective FF dimension of each expert is original FF dimension/ 16. If yes, this raises the question how do you split the parameters of original FF layer among these 16 experts. Do you initialize them based by splitting the original init across this in order or do you init the params of new expert FFs randomly. I'd be surprised that you match the performance of original dense model even after throwing away most of the original weights.\n2) Table 3: it is not convincing that you train top-1 for longer as the numbers are so close that maybe, if you would have kept a long enough wallclock time for all runs, top-1 would have been comparable to the rest irrespective of the loss.\n\ntypos_grammar_style_and_presentation_improvements: line 124-125  table1 : top-1 missing x line 216: what do you mean 55% - is this w.r.t. to a threshhold\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "YD3RBw1kfI",
        "length": 415,
        "human_text": "paper_topic_and_main_contributions: The paper proposed an adaptive gating mechanism in a Mixture of Experts (MoE) training scheme, which enables tokens to be processed by a flexible number of experts. The proposed training strategy allows tokens to be processed by a variable number of experts based on expert probability distribution. Overall the results show that the method often achieves improved training efficiency without compromising inference performance, with the integration of curriculum learning to be beneficial for reducing training costs. The authors present an interesting analysis where they explain how the gating mechanism adjusts to each NLP task. For instance, for sentiment analysis the tokens expressing neutral opinion are routed to two experts, while this happens in QA for both questions and context text snippets.\n\nreasons_to_accept: - The paper is clear and easy to follow. The experimental setup seems solid, with multiple NLP tasks evaluated.\n- The proposed approach looks promising, as it usually outperforms baselines in terms of training time, inference FLOPs and performance. The analysis and ablation study nicely explains model design choices and provides insights on how the method works in each NLP task.\n\nreasons_to_reject: - I am skeptical about the novelty of this paper. Despite its potential impact and value, it looks like a combination of existing techniques.\n- The performance improvement (training, inference and test performance) over the baseline is quite small.\n\nquestions_for_the_authors: - How are findings in Section 3.1 / Figure 1 drawn? How are these experiments performed in detail?\n\ntypos_grammar_style_and_presentation_improvements: - The authors could use 1-2 sentences in the intro to briefly explain what top-1 and top-2 gating is (L67).\n- Section 2 should be changed structure-wise. It has only a single subsection (2.1), while Related Work is just a \\paragraph{}. - Section 4 is also lacking structure. 4.1 is only one sentence.\n- The words 'significantly' should be used more carefully (e.g. L 583), only if a significance test is performed.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "mb6Q475JV7",
        "length": 225,
        "human_text": "paper_topic_and_main_contributions: This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution.  Extensive experiments on diverse NLP tasks show that adaptive gating reduces at most 22.5% training time while maintaining inference quality.\n\nreasons_to_accept: The paper is well-written and easy to follow. The illustration of motivation and method is clear. \nThe design of adaptive gating in MoE is intuitive and technically novel.\n\nreasons_to_reject: This paper uses an existing method and does not mention or lack description of its own. \nInsufficient experiments, as many figures as possible should be used to illustrate the author's point of view. \nMore baseline models are encouraged to be used to evaluate Training Time, FLOPs and Inference Performance.\n\ntypos_grammar_style_and_presentation_improvements: \"Figures 1 depict the normalized activation values of four sampled tokens across 16 experts.\" See line 209, page3. \nFigure 2 is not clear.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "210_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_210_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7193999999999999,
      "max_similarity": 0.7367,
      "avg_coverage": 0.4441,
      "max_coverage": 0.6923
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 333,
      "avg_human_length": 366.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 4,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "S4vOnRTkdK",
        "similarity": 0.69,
        "coverage": 0.32,
        "human_length": 458,
        "human_text": "paper_topic_and_main_contributions: Authors study an important direction of use of mixture of experts (MoE) / switch layers at the FF layers and try to optimize the use of top-2 experts (MoE) vs only top-1 expert (switch). As expected, using top-2 experts leads to better performance than top-1 at the expense of twice as many FLOPs and longer runtime. In this work authors, propose to use top-2 experts only when the difference between the highest expert probability and the second highest expert probability is smaller than a pre-decided threshold saving them compute and retaining the performance of top-2 experts. Unfortunately, to gain full computational speedups they also require a sophisticated curriculum learning schedule.\n\nreasons_to_accept: 1. MoE/ switch layers is a useful technique and optimizing it is important - the proposed solution is simple and intuitive. \n2. I liked the analysis of complexity of various tokens. \n3. The speedups over top-2 are decent, albeit with the use of curriculum.\n\nreasons_to_reject: 1. Require sophisticated curriculum making the training pipeline complex and not plug-n-play. \n2. The decision of training top-1 baseline for longer is not convincing given that all results are in same ballpark.\n\nquestions_for_the_authors: 1) line 365: it was not clear how you initialize the experts. As you say use runs use same parameters, I am assuming the effective FF dimension of each expert is original FF dimension/ 16. If yes, this raises the question how do you split the parameters of original FF layer among these 16 experts. Do you initialize them based by splitting the original init across this in order or do you init the params of new expert FFs randomly. I'd be surprised that you match the performance of original dense model even after throwing away most of the original weights.\n2) Table 3: it is not convincing that you train top-1 for longer as the numbers are so close that maybe, if you would have kept a long enough wallclock time for all runs, top-1 would have been comparable to the rest irrespective of the loss.\n\ntypos_grammar_style_and_presentation_improvements: line 124-125  table1 : top-1 missing x line 216: what do you mean 55% - is this w.r.t. to a threshhold\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "YD3RBw1kfI",
        "similarity": 0.7315,
        "coverage": 0.32,
        "human_length": 415,
        "human_text": "paper_topic_and_main_contributions: The paper proposed an adaptive gating mechanism in a Mixture of Experts (MoE) training scheme, which enables tokens to be processed by a flexible number of experts. The proposed training strategy allows tokens to be processed by a variable number of experts based on expert probability distribution. Overall the results show that the method often achieves improved training efficiency without compromising inference performance, with the integration of curriculum learning to be beneficial for reducing training costs. The authors present an interesting analysis where they explain how the gating mechanism adjusts to each NLP task. For instance, for sentiment analysis the tokens expressing neutral opinion are routed to two experts, while this happens in QA for both questions and context text snippets.\n\nreasons_to_accept: - The paper is clear and easy to follow. The experimental setup seems solid, with multiple NLP tasks evaluated.\n- The proposed approach looks promising, as it usually outperforms baselines in terms of training time, inference FLOPs and performance. The analysis and ablation study nicely explains model design choices and provides insights on how the method works in each NLP task.\n\nreasons_to_reject: - I am skeptical about the novelty of this paper. Despite its potential impact and value, it looks like a combination of existing techniques.\n- The performance improvement (training, inference and test performance) over the baseline is quite small.\n\nquestions_for_the_authors: - How are findings in Section 3.1 / Figure 1 drawn? How are these experiments performed in detail?\n\ntypos_grammar_style_and_presentation_improvements: - The authors could use 1-2 sentences in the intro to briefly explain what top-1 and top-2 gating is (L67).\n- Section 2 should be changed structure-wise. It has only a single subsection (2.1), while Related Work is just a \\paragraph{}. - Section 4 is also lacking structure. 4.1 is only one sentence.\n- The words 'significantly' should be used more carefully (e.g. L 583), only if a significance test is performed.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "mb6Q475JV7",
        "similarity": 0.7367,
        "coverage": 0.6923,
        "human_length": 225,
        "human_text": "paper_topic_and_main_contributions: This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution.  Extensive experiments on diverse NLP tasks show that adaptive gating reduces at most 22.5% training time while maintaining inference quality.\n\nreasons_to_accept: The paper is well-written and easy to follow. The illustration of motivation and method is clear. \nThe design of adaptive gating in MoE is intuitive and technically novel.\n\nreasons_to_reject: This paper uses an existing method and does not mention or lack description of its own. \nInsufficient experiments, as many figures as possible should be used to illustrate the author's point of view. \nMore baseline models are encouraged to be used to evaluate Training Time, FLOPs and Inference Performance.\n\ntypos_grammar_style_and_presentation_improvements: \"Figures 1 depict the normalized activation values of four sampled tokens across 16 experts.\" See line 209, page3. \nFigure 2 is not clear.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "S4vOnRTkdK",
        "length": 458,
        "human_text": "paper_topic_and_main_contributions: Authors study an important direction of use of mixture of experts (MoE) / switch layers at the FF layers and try to optimize the use of top-2 experts (MoE) vs only top-1 expert (switch). As expected, using top-2 experts leads to better performance than top-1 at the expense of twice as many FLOPs and longer runtime. In this work authors, propose to use top-2 experts only when the difference between the highest expert probability and the second highest expert probability is smaller than a pre-decided threshold saving them compute and retaining the performance of top-2 experts. Unfortunately, to gain full computational speedups they also require a sophisticated curriculum learning schedule.\n\nreasons_to_accept: 1. MoE/ switch layers is a useful technique and optimizing it is important - the proposed solution is simple and intuitive. \n2. I liked the analysis of complexity of various tokens. \n3. The speedups over top-2 are decent, albeit with the use of curriculum.\n\nreasons_to_reject: 1. Require sophisticated curriculum making the training pipeline complex and not plug-n-play. \n2. The decision of training top-1 baseline for longer is not convincing given that all results are in same ballpark.\n\nquestions_for_the_authors: 1) line 365: it was not clear how you initialize the experts. As you say use runs use same parameters, I am assuming the effective FF dimension of each expert is original FF dimension/ 16. If yes, this raises the question how do you split the parameters of original FF layer among these 16 experts. Do you initialize them based by splitting the original init across this in order or do you init the params of new expert FFs randomly. I'd be surprised that you match the performance of original dense model even after throwing away most of the original weights.\n2) Table 3: it is not convincing that you train top-1 for longer as the numbers are so close that maybe, if you would have kept a long enough wallclock time for all runs, top-1 would have been comparable to the rest irrespective of the loss.\n\ntypos_grammar_style_and_presentation_improvements: line 124-125  table1 : top-1 missing x line 216: what do you mean 55% - is this w.r.t. to a threshhold\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "YD3RBw1kfI",
        "length": 415,
        "human_text": "paper_topic_and_main_contributions: The paper proposed an adaptive gating mechanism in a Mixture of Experts (MoE) training scheme, which enables tokens to be processed by a flexible number of experts. The proposed training strategy allows tokens to be processed by a variable number of experts based on expert probability distribution. Overall the results show that the method often achieves improved training efficiency without compromising inference performance, with the integration of curriculum learning to be beneficial for reducing training costs. The authors present an interesting analysis where they explain how the gating mechanism adjusts to each NLP task. For instance, for sentiment analysis the tokens expressing neutral opinion are routed to two experts, while this happens in QA for both questions and context text snippets.\n\nreasons_to_accept: - The paper is clear and easy to follow. The experimental setup seems solid, with multiple NLP tasks evaluated.\n- The proposed approach looks promising, as it usually outperforms baselines in terms of training time, inference FLOPs and performance. The analysis and ablation study nicely explains model design choices and provides insights on how the method works in each NLP task.\n\nreasons_to_reject: - I am skeptical about the novelty of this paper. Despite its potential impact and value, it looks like a combination of existing techniques.\n- The performance improvement (training, inference and test performance) over the baseline is quite small.\n\nquestions_for_the_authors: - How are findings in Section 3.1 / Figure 1 drawn? How are these experiments performed in detail?\n\ntypos_grammar_style_and_presentation_improvements: - The authors could use 1-2 sentences in the intro to briefly explain what top-1 and top-2 gating is (L67).\n- Section 2 should be changed structure-wise. It has only a single subsection (2.1), while Related Work is just a \\paragraph{}. - Section 4 is also lacking structure. 4.1 is only one sentence.\n- The words 'significantly' should be used more carefully (e.g. L 583), only if a significance test is performed.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "mb6Q475JV7",
        "length": 225,
        "human_text": "paper_topic_and_main_contributions: This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution.  Extensive experiments on diverse NLP tasks show that adaptive gating reduces at most 22.5% training time while maintaining inference quality.\n\nreasons_to_accept: The paper is well-written and easy to follow. The illustration of motivation and method is clear. \nThe design of adaptive gating in MoE is intuitive and technically novel.\n\nreasons_to_reject: This paper uses an existing method and does not mention or lack description of its own. \nInsufficient experiments, as many figures as possible should be used to illustrate the author's point of view. \nMore baseline models are encouraged to be used to evaluate Training Time, FLOPs and Inference Performance.\n\ntypos_grammar_style_and_presentation_improvements: \"Figures 1 depict the normalized activation values of four sampled tokens across 16 experts.\" See line 209, page3. \nFigure 2 is not clear.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "144_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_144_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7581000000000001,
      "max_similarity": 0.7774,
      "avg_coverage": 0.47619999999999996,
      "max_coverage": 0.5
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 731,
      "avg_human_length": 504.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 12,
      "suggestions_count": 13
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "du7kQQQCpK",
        "similarity": 0.7639,
        "coverage": 0.5,
        "human_length": 495,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the problem of evaluating the factuality of long-form text that is generated by large language models.\nThe authors propose FACTSCORE, a new scoring mechanism that takes into consideration each of atomic facts in the generated text. \nFACTSCORE enables more fine-grained evaluation and comparison of the models.\nSince a straightforward way of annotating data by human annotators is time-consuming and costly, the authors also propose an automatic approach of approximately estimating the value of FACTSCORE by using other models for retrieval and prediction of whether or not each atomic fact is supported by the pre-defined knowledge source.\nThorough analysis of the estimation for 12 different LMs indicates some interesting findings on correlation between factual accuracy and the model size, training data and instruction tuning, etc.\n\nreasons_to_accept: - FACTSCORE is a novel formulation of evaluation of factuality and addresses the problem of existing sentence-level evaluation methods.\n- Though most of the details are in the Appendix, the paper is very well organized and each section is clearly explained.\n\nreasons_to_reject: - It is not explicitly stated how the findings discussed in Section 4 are utilized for deeper understanding of a model for factuality.\n- Since analysis was done over only People biographies and Wikipedia for prompts and a knowledge source respectively, it is not clear how universal the findings are, which is also pointed out by the authors.\n\nquestions_for_the_authors: - In Page 3, line 484, \"and ChatGPT for PerplexityAI.\"  In Table 3, LLAMA is the best for PerplexityAI with ER=0.1, so my understanding is that the authors choose ChatGPT (with ER=0.8) because it preserves the ranking while LLAMA does not. Is it correct?\n- I wonder if the model size is dominant in factual precision because ChatGPT (GPT-3.5 or InstructGPT) and GPT-4 are known to be much larger than the other models discussed in the paper though no exact information on model sizes is available for them. Could you elaborate more about this if you hit upon anything? I read through the paper including Appendix and could find relevant discussions only in 4.3.2, but may have missed something important about the model size.\n\ntypos_grammar_style_and_presentation_improvements: - Page 3, line 217: I think that [a is supported by C] can be written by the indicator function https://en.wikipedia.org/wiki/Indicator_function  . Or simply f(y) can be written as |{a \\in A_y | a is supported by C}| / |A_y| .\n- Page 6, line 406: across five variants -> \"four\" variants?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "1ccBpvtcUy",
        "similarity": 0.7774,
        "coverage": 0.5,
        "human_length": 441,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an evaluation method of factual precision called FACTSCORE, the percentage of pieces of information as atomic facts supported by a given knowledge source.  Since it is expensive to calculate this evaluation metric manually, this paper also proposes a method for automatic evaluation.\nThe evaluations of several commercial language models showed that   1. infrequent entities tend to have lower  FACTSCORE,   1. FACTSCORE of facts mentioned later in generated text tends to be lower, and   1. even PerplexityAI, which uses search results, has a low FACTSCORE.\nThe experiment results also showed that the proposed automatic evaluation method can estimate FACTSCORE with a relatively low error rate for human evaluation. \nIn addition, the automatic evaluation of FACTSCORE for 12 large language models showed several interesting tendencies such that   1.  none of the models is very high,   1.  GPT-4 and ChatGPT are comparable, and   1. GPT-4 and ChatGPT are better than other published models.\n\nreasons_to_accept: - This paper proposes a new score for evaluating factual precision and a method for automatically estimating the score without manual labor.\n- The paper reports some interesting results by evaluating the scores of various large language models.\n- There are plans to release it as open source.\n\nreasons_to_reject: - The generality needs to be clarified because the experiment was conducted only on Biographies.\n- The method's validity needs to be clarified because the error compared to the case where the correct facts are considered, which is not written in knowledge sources, has not been evaluated.\n- Since it is difficult for LLMs to generate facts without referring to any sources in principle, evaluating it in a setting that generates text including facts from given relevant sources may be more valuable.\n\nquestions_for_the_authors: - A: What kind and size of the corpus were the NP models trained?\n- B: Is there any reason there are no NP results for ChatGPT in Table 3?\n- C: The reviewer thinks it is difficult for LLMs to generate facts without referring to any sources. \nConsidering that, what do the authors think is more valuable to evaluate it in a setting that generates text including facts from given relevant sources? \n  - This setting would also have the advantage of evaluating \"factual recall\" from the facts mentioned in given relevant sources.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "kITYZq5iG6",
        "similarity": 0.733,
        "coverage": 0.4286,
        "human_length": 577,
        "human_text": "paper_topic_and_main_contributions: The paper investigates hallucinations in generated biographies. The main contribution is in the in depth analysis of hallucinations in the generated biographies, with the added benefit of considering SOTA language models. FACTScore, the metric proposed by the authors, is simply the average truth score of a sentence composed of several facts, where each fact gets 0 (not true) or 1 (true) score. This score is computed by humans in a large scale evaluation, but the authors also propose a few models that can achieve good results.\n\nreasons_to_accept: I found the paper well written and easy to follow. In my opinion, the work is sound and the experimental evaluation rigorous. In addition, the problem studied is interesting. The authors invested a lot of money and effort in the experimental evaluation of hallucinations, which is the main strength.  In particular, they do a human evaluation of biography generation for three models: InstructGPT, ChatGPT, PerplexityAI on 183 generations (biographies) from each model. The humans split the generations in smaller units of content that can be easily verified by checking the Wikipedia page of the person.\n\nreasons_to_reject: The FactScore metric is a very simple metric that cannot be counted as a contribution, as it hides a complex discussion of what constitutes a fact and when a fact is truly supported by an underlying KB/document. For example, if a sentences is not correctly split in the underlying parts, evaluating its truth value is non trivial, for example: President Trump no longer lives in the White House. Here we have 2 facts (Trump, is, President), (Trump, no longer lives in, White House). Also negations are an added layer of difficulty as often KBs or documents do not have all possible negations if any. Hence, the true contribution of the paper is more in the human analysis of the hallucinations of current models. \nThe human analysis of hallucinations has also the drawback that it does not considers that some facts might be true, but not present in the Wikipedia page. This can be more serious for entities with shorter Wikipedia pages, for which the authors find that models tend to produce less accurate results. \nFinally, models might produce much more accurate results if given in input the Wikipedia page - an hypothesis which is very interesting to test.\n\nquestions_for_the_authors: A. In which measure Figure 2 (rare entities has less correct biography) could be explained through the fact that the human annotators did not find those particularly facts on the Wikipedia page? Did you look if there is a correlation between the type of unsupported fact (page level contradiction or annotation level) and the popularity/completeness of the page of entities? Is it possible that for the rare entities the fact is true, just not present on Wikipedia?\nB. Did you consider the setting in which the LM receives in the prompt the Wikipedia page of the entity and has the task of creating a biography? The model is very likely to produce more accurate results, which will be consistent with the findings you have when you create the automatic measure.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "du7kQQQCpK",
        "length": 495,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the problem of evaluating the factuality of long-form text that is generated by large language models.\nThe authors propose FACTSCORE, a new scoring mechanism that takes into consideration each of atomic facts in the generated text. \nFACTSCORE enables more fine-grained evaluation and comparison of the models.\nSince a straightforward way of annotating data by human annotators is time-consuming and costly, the authors also propose an automatic approach of approximately estimating the value of FACTSCORE by using other models for retrieval and prediction of whether or not each atomic fact is supported by the pre-defined knowledge source.\nThorough analysis of the estimation for 12 different LMs indicates some interesting findings on correlation between factual accuracy and the model size, training data and instruction tuning, etc.\n\nreasons_to_accept: - FACTSCORE is a novel formulation of evaluation of factuality and addresses the problem of existing sentence-level evaluation methods.\n- Though most of the details are in the Appendix, the paper is very well organized and each section is clearly explained.\n\nreasons_to_reject: - It is not explicitly stated how the findings discussed in Section 4 are utilized for deeper understanding of a model for factuality.\n- Since analysis was done over only People biographies and Wikipedia for prompts and a knowledge source respectively, it is not clear how universal the findings are, which is also pointed out by the authors.\n\nquestions_for_the_authors: - In Page 3, line 484, \"and ChatGPT for PerplexityAI.\"  In Table 3, LLAMA is the best for PerplexityAI with ER=0.1, so my understanding is that the authors choose ChatGPT (with ER=0.8) because it preserves the ranking while LLAMA does not. Is it correct?\n- I wonder if the model size is dominant in factual precision because ChatGPT (GPT-3.5 or InstructGPT) and GPT-4 are known to be much larger than the other models discussed in the paper though no exact information on model sizes is available for them. Could you elaborate more about this if you hit upon anything? I read through the paper including Appendix and could find relevant discussions only in 4.3.2, but may have missed something important about the model size.\n\ntypos_grammar_style_and_presentation_improvements: - Page 3, line 217: I think that [a is supported by C] can be written by the indicator function https://en.wikipedia.org/wiki/Indicator_function  . Or simply f(y) can be written as |{a \\in A_y | a is supported by C}| / |A_y| .\n- Page 6, line 406: across five variants -> \"four\" variants?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "1ccBpvtcUy",
        "length": 441,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an evaluation method of factual precision called FACTSCORE, the percentage of pieces of information as atomic facts supported by a given knowledge source.  Since it is expensive to calculate this evaluation metric manually, this paper also proposes a method for automatic evaluation.\nThe evaluations of several commercial language models showed that   1. infrequent entities tend to have lower  FACTSCORE,   1. FACTSCORE of facts mentioned later in generated text tends to be lower, and   1. even PerplexityAI, which uses search results, has a low FACTSCORE.\nThe experiment results also showed that the proposed automatic evaluation method can estimate FACTSCORE with a relatively low error rate for human evaluation. \nIn addition, the automatic evaluation of FACTSCORE for 12 large language models showed several interesting tendencies such that   1.  none of the models is very high,   1.  GPT-4 and ChatGPT are comparable, and   1. GPT-4 and ChatGPT are better than other published models.\n\nreasons_to_accept: - This paper proposes a new score for evaluating factual precision and a method for automatically estimating the score without manual labor.\n- The paper reports some interesting results by evaluating the scores of various large language models.\n- There are plans to release it as open source.\n\nreasons_to_reject: - The generality needs to be clarified because the experiment was conducted only on Biographies.\n- The method's validity needs to be clarified because the error compared to the case where the correct facts are considered, which is not written in knowledge sources, has not been evaluated.\n- Since it is difficult for LLMs to generate facts without referring to any sources in principle, evaluating it in a setting that generates text including facts from given relevant sources may be more valuable.\n\nquestions_for_the_authors: - A: What kind and size of the corpus were the NP models trained?\n- B: Is there any reason there are no NP results for ChatGPT in Table 3?\n- C: The reviewer thinks it is difficult for LLMs to generate facts without referring to any sources. \nConsidering that, what do the authors think is more valuable to evaluate it in a setting that generates text including facts from given relevant sources? \n  - This setting would also have the advantage of evaluating \"factual recall\" from the facts mentioned in given relevant sources.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "kITYZq5iG6",
        "length": 577,
        "human_text": "paper_topic_and_main_contributions: The paper investigates hallucinations in generated biographies. The main contribution is in the in depth analysis of hallucinations in the generated biographies, with the added benefit of considering SOTA language models. FACTScore, the metric proposed by the authors, is simply the average truth score of a sentence composed of several facts, where each fact gets 0 (not true) or 1 (true) score. This score is computed by humans in a large scale evaluation, but the authors also propose a few models that can achieve good results.\n\nreasons_to_accept: I found the paper well written and easy to follow. In my opinion, the work is sound and the experimental evaluation rigorous. In addition, the problem studied is interesting. The authors invested a lot of money and effort in the experimental evaluation of hallucinations, which is the main strength.  In particular, they do a human evaluation of biography generation for three models: InstructGPT, ChatGPT, PerplexityAI on 183 generations (biographies) from each model. The humans split the generations in smaller units of content that can be easily verified by checking the Wikipedia page of the person.\n\nreasons_to_reject: The FactScore metric is a very simple metric that cannot be counted as a contribution, as it hides a complex discussion of what constitutes a fact and when a fact is truly supported by an underlying KB/document. For example, if a sentences is not correctly split in the underlying parts, evaluating its truth value is non trivial, for example: President Trump no longer lives in the White House. Here we have 2 facts (Trump, is, President), (Trump, no longer lives in, White House). Also negations are an added layer of difficulty as often KBs or documents do not have all possible negations if any. Hence, the true contribution of the paper is more in the human analysis of the hallucinations of current models. \nThe human analysis of hallucinations has also the drawback that it does not considers that some facts might be true, but not present in the Wikipedia page. This can be more serious for entities with shorter Wikipedia pages, for which the authors find that models tend to produce less accurate results. \nFinally, models might produce much more accurate results if given in input the Wikipedia page - an hypothesis which is very interesting to test.\n\nquestions_for_the_authors: A. In which measure Figure 2 (rare entities has less correct biography) could be explained through the fact that the human annotators did not find those particularly facts on the Wikipedia page? Did you look if there is a correlation between the type of unsupported fact (page level contradiction or annotation level) and the popularity/completeness of the page of entities? Is it possible that for the rare entities the fact is true, just not present on Wikipedia?\nB. Did you consider the setting in which the LM receives in the prompt the Wikipedia page of the entity and has the task of creating a biography? The model is very likely to produce more accurate results, which will be consistent with the findings you have when you create the automatic measure.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "144_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_144_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7562666666666668,
      "max_similarity": 0.7724,
      "avg_coverage": 0.5687666666666666,
      "max_coverage": 0.6154
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 806,
      "avg_human_length": 504.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 12,
      "suggestions_count": 13
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "du7kQQQCpK",
        "similarity": 0.7613,
        "coverage": 0.5909,
        "human_length": 495,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the problem of evaluating the factuality of long-form text that is generated by large language models.\nThe authors propose FACTSCORE, a new scoring mechanism that takes into consideration each of atomic facts in the generated text. \nFACTSCORE enables more fine-grained evaluation and comparison of the models.\nSince a straightforward way of annotating data by human annotators is time-consuming and costly, the authors also propose an automatic approach of approximately estimating the value of FACTSCORE by using other models for retrieval and prediction of whether or not each atomic fact is supported by the pre-defined knowledge source.\nThorough analysis of the estimation for 12 different LMs indicates some interesting findings on correlation between factual accuracy and the model size, training data and instruction tuning, etc.\n\nreasons_to_accept: - FACTSCORE is a novel formulation of evaluation of factuality and addresses the problem of existing sentence-level evaluation methods.\n- Though most of the details are in the Appendix, the paper is very well organized and each section is clearly explained.\n\nreasons_to_reject: - It is not explicitly stated how the findings discussed in Section 4 are utilized for deeper understanding of a model for factuality.\n- Since analysis was done over only People biographies and Wikipedia for prompts and a knowledge source respectively, it is not clear how universal the findings are, which is also pointed out by the authors.\n\nquestions_for_the_authors: - In Page 3, line 484, \"and ChatGPT for PerplexityAI.\"  In Table 3, LLAMA is the best for PerplexityAI with ER=0.1, so my understanding is that the authors choose ChatGPT (with ER=0.8) because it preserves the ranking while LLAMA does not. Is it correct?\n- I wonder if the model size is dominant in factual precision because ChatGPT (GPT-3.5 or InstructGPT) and GPT-4 are known to be much larger than the other models discussed in the paper though no exact information on model sizes is available for them. Could you elaborate more about this if you hit upon anything? I read through the paper including Appendix and could find relevant discussions only in 4.3.2, but may have missed something important about the model size.\n\ntypos_grammar_style_and_presentation_improvements: - Page 3, line 217: I think that [a is supported by C] can be written by the indicator function https://en.wikipedia.org/wiki/Indicator_function  . Or simply f(y) can be written as |{a \\in A_y | a is supported by C}| / |A_y| .\n- Page 6, line 406: across five variants -> \"four\" variants?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "1ccBpvtcUy",
        "similarity": 0.7724,
        "coverage": 0.6154,
        "human_length": 441,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an evaluation method of factual precision called FACTSCORE, the percentage of pieces of information as atomic facts supported by a given knowledge source.  Since it is expensive to calculate this evaluation metric manually, this paper also proposes a method for automatic evaluation.\nThe evaluations of several commercial language models showed that   1. infrequent entities tend to have lower  FACTSCORE,   1. FACTSCORE of facts mentioned later in generated text tends to be lower, and   1. even PerplexityAI, which uses search results, has a low FACTSCORE.\nThe experiment results also showed that the proposed automatic evaluation method can estimate FACTSCORE with a relatively low error rate for human evaluation. \nIn addition, the automatic evaluation of FACTSCORE for 12 large language models showed several interesting tendencies such that   1.  none of the models is very high,   1.  GPT-4 and ChatGPT are comparable, and   1. GPT-4 and ChatGPT are better than other published models.\n\nreasons_to_accept: - This paper proposes a new score for evaluating factual precision and a method for automatically estimating the score without manual labor.\n- The paper reports some interesting results by evaluating the scores of various large language models.\n- There are plans to release it as open source.\n\nreasons_to_reject: - The generality needs to be clarified because the experiment was conducted only on Biographies.\n- The method's validity needs to be clarified because the error compared to the case where the correct facts are considered, which is not written in knowledge sources, has not been evaluated.\n- Since it is difficult for LLMs to generate facts without referring to any sources in principle, evaluating it in a setting that generates text including facts from given relevant sources may be more valuable.\n\nquestions_for_the_authors: - A: What kind and size of the corpus were the NP models trained?\n- B: Is there any reason there are no NP results for ChatGPT in Table 3?\n- C: The reviewer thinks it is difficult for LLMs to generate facts without referring to any sources. \nConsidering that, what do the authors think is more valuable to evaluate it in a setting that generates text including facts from given relevant sources? \n  - This setting would also have the advantage of evaluating \"factual recall\" from the facts mentioned in given relevant sources.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "kITYZq5iG6",
        "similarity": 0.7351,
        "coverage": 0.5,
        "human_length": 577,
        "human_text": "paper_topic_and_main_contributions: The paper investigates hallucinations in generated biographies. The main contribution is in the in depth analysis of hallucinations in the generated biographies, with the added benefit of considering SOTA language models. FACTScore, the metric proposed by the authors, is simply the average truth score of a sentence composed of several facts, where each fact gets 0 (not true) or 1 (true) score. This score is computed by humans in a large scale evaluation, but the authors also propose a few models that can achieve good results.\n\nreasons_to_accept: I found the paper well written and easy to follow. In my opinion, the work is sound and the experimental evaluation rigorous. In addition, the problem studied is interesting. The authors invested a lot of money and effort in the experimental evaluation of hallucinations, which is the main strength.  In particular, they do a human evaluation of biography generation for three models: InstructGPT, ChatGPT, PerplexityAI on 183 generations (biographies) from each model. The humans split the generations in smaller units of content that can be easily verified by checking the Wikipedia page of the person.\n\nreasons_to_reject: The FactScore metric is a very simple metric that cannot be counted as a contribution, as it hides a complex discussion of what constitutes a fact and when a fact is truly supported by an underlying KB/document. For example, if a sentences is not correctly split in the underlying parts, evaluating its truth value is non trivial, for example: President Trump no longer lives in the White House. Here we have 2 facts (Trump, is, President), (Trump, no longer lives in, White House). Also negations are an added layer of difficulty as often KBs or documents do not have all possible negations if any. Hence, the true contribution of the paper is more in the human analysis of the hallucinations of current models. \nThe human analysis of hallucinations has also the drawback that it does not considers that some facts might be true, but not present in the Wikipedia page. This can be more serious for entities with shorter Wikipedia pages, for which the authors find that models tend to produce less accurate results. \nFinally, models might produce much more accurate results if given in input the Wikipedia page - an hypothesis which is very interesting to test.\n\nquestions_for_the_authors: A. In which measure Figure 2 (rare entities has less correct biography) could be explained through the fact that the human annotators did not find those particularly facts on the Wikipedia page? Did you look if there is a correlation between the type of unsupported fact (page level contradiction or annotation level) and the popularity/completeness of the page of entities? Is it possible that for the rare entities the fact is true, just not present on Wikipedia?\nB. Did you consider the setting in which the LM receives in the prompt the Wikipedia page of the entity and has the task of creating a biography? The model is very likely to produce more accurate results, which will be consistent with the findings you have when you create the automatic measure.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "du7kQQQCpK",
        "length": 495,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the problem of evaluating the factuality of long-form text that is generated by large language models.\nThe authors propose FACTSCORE, a new scoring mechanism that takes into consideration each of atomic facts in the generated text. \nFACTSCORE enables more fine-grained evaluation and comparison of the models.\nSince a straightforward way of annotating data by human annotators is time-consuming and costly, the authors also propose an automatic approach of approximately estimating the value of FACTSCORE by using other models for retrieval and prediction of whether or not each atomic fact is supported by the pre-defined knowledge source.\nThorough analysis of the estimation for 12 different LMs indicates some interesting findings on correlation between factual accuracy and the model size, training data and instruction tuning, etc.\n\nreasons_to_accept: - FACTSCORE is a novel formulation of evaluation of factuality and addresses the problem of existing sentence-level evaluation methods.\n- Though most of the details are in the Appendix, the paper is very well organized and each section is clearly explained.\n\nreasons_to_reject: - It is not explicitly stated how the findings discussed in Section 4 are utilized for deeper understanding of a model for factuality.\n- Since analysis was done over only People biographies and Wikipedia for prompts and a knowledge source respectively, it is not clear how universal the findings are, which is also pointed out by the authors.\n\nquestions_for_the_authors: - In Page 3, line 484, \"and ChatGPT for PerplexityAI.\"  In Table 3, LLAMA is the best for PerplexityAI with ER=0.1, so my understanding is that the authors choose ChatGPT (with ER=0.8) because it preserves the ranking while LLAMA does not. Is it correct?\n- I wonder if the model size is dominant in factual precision because ChatGPT (GPT-3.5 or InstructGPT) and GPT-4 are known to be much larger than the other models discussed in the paper though no exact information on model sizes is available for them. Could you elaborate more about this if you hit upon anything? I read through the paper including Appendix and could find relevant discussions only in 4.3.2, but may have missed something important about the model size.\n\ntypos_grammar_style_and_presentation_improvements: - Page 3, line 217: I think that [a is supported by C] can be written by the indicator function https://en.wikipedia.org/wiki/Indicator_function  . Or simply f(y) can be written as |{a \\in A_y | a is supported by C}| / |A_y| .\n- Page 6, line 406: across five variants -> \"four\" variants?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "1ccBpvtcUy",
        "length": 441,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an evaluation method of factual precision called FACTSCORE, the percentage of pieces of information as atomic facts supported by a given knowledge source.  Since it is expensive to calculate this evaluation metric manually, this paper also proposes a method for automatic evaluation.\nThe evaluations of several commercial language models showed that   1. infrequent entities tend to have lower  FACTSCORE,   1. FACTSCORE of facts mentioned later in generated text tends to be lower, and   1. even PerplexityAI, which uses search results, has a low FACTSCORE.\nThe experiment results also showed that the proposed automatic evaluation method can estimate FACTSCORE with a relatively low error rate for human evaluation. \nIn addition, the automatic evaluation of FACTSCORE for 12 large language models showed several interesting tendencies such that   1.  none of the models is very high,   1.  GPT-4 and ChatGPT are comparable, and   1. GPT-4 and ChatGPT are better than other published models.\n\nreasons_to_accept: - This paper proposes a new score for evaluating factual precision and a method for automatically estimating the score without manual labor.\n- The paper reports some interesting results by evaluating the scores of various large language models.\n- There are plans to release it as open source.\n\nreasons_to_reject: - The generality needs to be clarified because the experiment was conducted only on Biographies.\n- The method's validity needs to be clarified because the error compared to the case where the correct facts are considered, which is not written in knowledge sources, has not been evaluated.\n- Since it is difficult for LLMs to generate facts without referring to any sources in principle, evaluating it in a setting that generates text including facts from given relevant sources may be more valuable.\n\nquestions_for_the_authors: - A: What kind and size of the corpus were the NP models trained?\n- B: Is there any reason there are no NP results for ChatGPT in Table 3?\n- C: The reviewer thinks it is difficult for LLMs to generate facts without referring to any sources. \nConsidering that, what do the authors think is more valuable to evaluate it in a setting that generates text including facts from given relevant sources? \n  - This setting would also have the advantage of evaluating \"factual recall\" from the facts mentioned in given relevant sources.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "kITYZq5iG6",
        "length": 577,
        "human_text": "paper_topic_and_main_contributions: The paper investigates hallucinations in generated biographies. The main contribution is in the in depth analysis of hallucinations in the generated biographies, with the added benefit of considering SOTA language models. FACTScore, the metric proposed by the authors, is simply the average truth score of a sentence composed of several facts, where each fact gets 0 (not true) or 1 (true) score. This score is computed by humans in a large scale evaluation, but the authors also propose a few models that can achieve good results.\n\nreasons_to_accept: I found the paper well written and easy to follow. In my opinion, the work is sound and the experimental evaluation rigorous. In addition, the problem studied is interesting. The authors invested a lot of money and effort in the experimental evaluation of hallucinations, which is the main strength.  In particular, they do a human evaluation of biography generation for three models: InstructGPT, ChatGPT, PerplexityAI on 183 generations (biographies) from each model. The humans split the generations in smaller units of content that can be easily verified by checking the Wikipedia page of the person.\n\nreasons_to_reject: The FactScore metric is a very simple metric that cannot be counted as a contribution, as it hides a complex discussion of what constitutes a fact and when a fact is truly supported by an underlying KB/document. For example, if a sentences is not correctly split in the underlying parts, evaluating its truth value is non trivial, for example: President Trump no longer lives in the White House. Here we have 2 facts (Trump, is, President), (Trump, no longer lives in, White House). Also negations are an added layer of difficulty as often KBs or documents do not have all possible negations if any. Hence, the true contribution of the paper is more in the human analysis of the hallucinations of current models. \nThe human analysis of hallucinations has also the drawback that it does not considers that some facts might be true, but not present in the Wikipedia page. This can be more serious for entities with shorter Wikipedia pages, for which the authors find that models tend to produce less accurate results. \nFinally, models might produce much more accurate results if given in input the Wikipedia page - an hypothesis which is very interesting to test.\n\nquestions_for_the_authors: A. In which measure Figure 2 (rare entities has less correct biography) could be explained through the fact that the human annotators did not find those particularly facts on the Wikipedia page? Did you look if there is a correlation between the type of unsupported fact (page level contradiction or annotation level) and the popularity/completeness of the page of entities? Is it possible that for the rare entities the fact is true, just not present on Wikipedia?\nB. Did you consider the setting in which the LM receives in the prompt the Wikipedia page of the entity and has the task of creating a biography? The model is very likely to produce more accurate results, which will be consistent with the findings you have when you create the automatic measure.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "71_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_71_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7080333333333334,
      "max_similarity": 0.7139,
      "avg_coverage": 0.5461666666666667,
      "max_coverage": 0.7143
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 770,
      "avg_human_length": 531.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 11,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "WxqqTDM8lo",
        "similarity": 0.6984,
        "coverage": 0.5,
        "human_length": 541,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method to evaluate natural language debates based on a hybrid approach of combining argumentation theory and NLP techniques. The approach taken seems to be an interesting one, with the potential of leveraging the improvements made in both fields to perform the challenging task of debate outcome prediction. The model used is, to my knowledge, an original one and comparisons are made to reasonable baselines although some details are missing. The two phase approach taken to argument scoring is another good contribution.\n\nreasons_to_accept: 1. Interesting two phase approach to argument quality detection. \n2. Innovative model proposed to handle debate outcome prediction,\n\nreasons_to_reject: There are some decisions taken by the author/s that I would like some clarifications on, as to my understanding, they impact the outcome of the paper in a major way. \n1. The removal of arguments that have an attack relation on them prior to them being scored strikes me as being quite extreme. There may be a good argument that has a substandard attack on it. Removal of that argument from the set of admissible arguments is likely to make the model non representative, as human adjudicators may still credit the argument. It may also be the case that an argument is too bad to be attacked. Most debates have a defined time for speakers to rebut their opposition and often the arguments that are focused on are the ones most fundamental to the case that the opposition is running. It makes little sense at that point to spend valuable time on rebutting throwaway statements. In the case of the method proposed by the authors, a good argument that is rebutted badly would be inadmissible and a bad argument that is deemed not worth rebutting would be admissible. This may cause issues in the subsequent steps. \n2. I am unclear on how arguments impact the probability of winning the debate. While section 4.2 is titled argument scoring, I am not clear where that process is actually happening.\n\nquestions_for_the_authors: A. How is the removal of arguments having an attack relation on them justified? \nB. Question on extension of work - often debates are not neatly categorized into phases of speech and rebuttal - how does this architecture handle that?\n\nmissing_references: In section 5.2 the author/s discuss Argument Theory Baselines and subsequently utilize them in table 1. I was not able to find a citation for this and the author/s do not go into details of what these baselines entail. Either a citation or some details on the baselines used would be appreciated here.\n\ntypos_grammar_style_and_presentation_improvements: Spelling error on line 654 - miss-classified -> misclassified\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Xyp1UCTOKv",
        "similarity": 0.7139,
        "coverage": 0.7143,
        "human_length": 411,
        "human_text": "paper_topic_and_main_contributions: The article presents an original method for evaluating the winning stance in complete debates, under real-world circumstances involving professional debaters. The proposed method is a hybrid approach that combines argumentation frameworks and semantics on one side, and Graph-Network and Transformer-based architectures on the other.\nThe argumentation frameworks module categorizes arguments as either Conflict-free or Admissible. Subsequently, the arguments are processed by a hybrid model that integrates a Graph-Network architecture with embeddings generated by a Transformer model. The winner is estimated as the participant with the highest count of acceptable arguments.\n\nreasons_to_accept: - This work achieved results surpass those previously reported in the literature for this task.\n- Detailed analyses present original and interesting outcomes of argumentation processing. The realization that \"learning representations from graph-structured data is a better idea than just using the whole text\" holds significant importance within the current context of NLP research, wherein generative LLMs are garnering widespread attention.\n- The incorporation of error analysis enriched the discussions.\n- The text provides a concise and well-structured literature review on automatic debate processing.\n\nreasons_to_reject: - Despite the strengths of the proposed work, the attained F1 score by the best model still appears relatively low. As a matter of fact, this is not a poor outcome when considering the conventional models employed for the task, but it's possible that the proposed model might be outperformed by newer Generative LLMs. It would be important to discuss the influence of these novel models on the task's execution.\n\nquestions_for_the_authors: - You wrote that \"both baselines relying exclusively on NLP algorithms and techniques performed worse than the random baseline. The main cause of this problem can be probably attributed to the lack of data in our domain\". But did you consider testing simpler probabilistic models for NLP, such as a Bayesian classifier, which performs well with small datasets?\n- Did you consider comparing the results of your method with those of a generative LLM, which have been widely used for complex NLP tasks?\n\ntypos_grammar_style_and_presentation_improvements: - Line 452: has been carried OUR using Pandas - Line 624-5: learning representations from A graph-structured data\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "2EKmTcKDe1",
        "similarity": 0.7118,
        "coverage": 0.4242,
        "human_length": 643,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method for automatically evaluating and predicting the winning stance in professional argumentative debates. The key problem it addresses is performing debate analysis and evaluation on complex natural language arguments, which have been relatively unexplored.  The main contributions are: - Introduces a hybrid technique combining concepts from argumentation theory and NLP to model complete reasoning lines in debates and determine a winning stance.\n- Demonstrates superior performance of the hybrid technique over pure NLP or argumentation theory baselines on the debate evaluation task.\n- Provides detailed experimental analysis and error analysis lending insight into the benefits of combining logical and linguistic knowledge for this problem.\nOverall, this engineering-focused paper makes contributions in methods, and analysis towards advancing NLP for a complex argumentation problem.\n\nreasons_to_accept: - Tackles a complex and underexplored NLP problem - evaluation of professional debates with complete logical argumentation - using a hybrid approach.\n- The details of the hybrid technique combining concepts from argumentation theory seem sound and applicable to debates.  - Well-written with a clear explanation of all concepts for audiences unfamiliar with argumentation theory.\n- Provides compelling evidence that bridging logical and neural techniques is beneficial for argument-mining problems.\n- Could inspire more creative hybrid modelling and use of structured knowledge in NLP systems.\n\nreasons_to_reject: My biggest issue with this paper is the lack of comparison to other techniques for the debate evaluation task, which makes it difficult to situate the novelty of the proposed approach. The authors only compare against baselines they define, without discussing prior proposals from the literature. Given the lack of comparisons, it is unclear whether this approach substantially advances upon the state-of-the-art for this task.   Additionally, the literature review overlooks highly relevant prior work. The authors claim in line 110 that \"Most of this research has been focused on performing an individual evaluation of arguments or argumentative lines of reasoning (Wachsmuth et al., 2017) instead of a global, interactive viewpoint where complete debates consisting of multiple, conflicting lines of reasoning are analysed\". However, Wachsmuth et al. (2017) proposed a Reasonableness dimension that evaluates the whole argument, considering counter-arguments and rebuttals. In a debate scenario, counter-arguments and rebuttals are what constitute the bare bones of it so one would imagine that a dimension like Reasonableness would at least be discussed. Furthermore, Marro et al. (2022) also introduced a technique to assess the reasonableness of full argument graphs, combining text and graph structure. Yet neither of these directly related works are acknowledged or compared to. The lack of citations and comparisons to existing techniques that also evaluate full argument graphs raises concerns. The paper does not sufficiently situate its contributions among prior methods or demonstrate advancement over closely related approaches. A more rigorous review and comparison to previous debate evaluation and reasonableness assessment research is needed.\n\nquestions_for_the_authors: A) Your proposed approach is evaluated only on a single dataset of Catalan debates. Can you discuss how you expect the hybrid technique to transfer to other debate domains and languages? Do you plan to test generalization capability on other debate datasets?\n\nmissing_references: - [Graph Embeddings for Argumentation Quality Assessment](https://aclanthology.org/2022.findings-emnlp.306) (Marro et al., Findings 2022) - [Yes, we can! Mining Arguments in 50 Years of US Presidential Campaign Debates](https://aclanthology.org/P19-1463) (Haddadan et al., ACL 2019)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "WxqqTDM8lo",
        "length": 541,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method to evaluate natural language debates based on a hybrid approach of combining argumentation theory and NLP techniques. The approach taken seems to be an interesting one, with the potential of leveraging the improvements made in both fields to perform the challenging task of debate outcome prediction. The model used is, to my knowledge, an original one and comparisons are made to reasonable baselines although some details are missing. The two phase approach taken to argument scoring is another good contribution.\n\nreasons_to_accept: 1. Interesting two phase approach to argument quality detection. \n2. Innovative model proposed to handle debate outcome prediction,\n\nreasons_to_reject: There are some decisions taken by the author/s that I would like some clarifications on, as to my understanding, they impact the outcome of the paper in a major way. \n1. The removal of arguments that have an attack relation on them prior to them being scored strikes me as being quite extreme. There may be a good argument that has a substandard attack on it. Removal of that argument from the set of admissible arguments is likely to make the model non representative, as human adjudicators may still credit the argument. It may also be the case that an argument is too bad to be attacked. Most debates have a defined time for speakers to rebut their opposition and often the arguments that are focused on are the ones most fundamental to the case that the opposition is running. It makes little sense at that point to spend valuable time on rebutting throwaway statements. In the case of the method proposed by the authors, a good argument that is rebutted badly would be inadmissible and a bad argument that is deemed not worth rebutting would be admissible. This may cause issues in the subsequent steps. \n2. I am unclear on how arguments impact the probability of winning the debate. While section 4.2 is titled argument scoring, I am not clear where that process is actually happening.\n\nquestions_for_the_authors: A. How is the removal of arguments having an attack relation on them justified? \nB. Question on extension of work - often debates are not neatly categorized into phases of speech and rebuttal - how does this architecture handle that?\n\nmissing_references: In section 5.2 the author/s discuss Argument Theory Baselines and subsequently utilize them in table 1. I was not able to find a citation for this and the author/s do not go into details of what these baselines entail. Either a citation or some details on the baselines used would be appreciated here.\n\ntypos_grammar_style_and_presentation_improvements: Spelling error on line 654 - miss-classified -> misclassified\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "Xyp1UCTOKv",
        "length": 411,
        "human_text": "paper_topic_and_main_contributions: The article presents an original method for evaluating the winning stance in complete debates, under real-world circumstances involving professional debaters. The proposed method is a hybrid approach that combines argumentation frameworks and semantics on one side, and Graph-Network and Transformer-based architectures on the other.\nThe argumentation frameworks module categorizes arguments as either Conflict-free or Admissible. Subsequently, the arguments are processed by a hybrid model that integrates a Graph-Network architecture with embeddings generated by a Transformer model. The winner is estimated as the participant with the highest count of acceptable arguments.\n\nreasons_to_accept: - This work achieved results surpass those previously reported in the literature for this task.\n- Detailed analyses present original and interesting outcomes of argumentation processing. The realization that \"learning representations from graph-structured data is a better idea than just using the whole text\" holds significant importance within the current context of NLP research, wherein generative LLMs are garnering widespread attention.\n- The incorporation of error analysis enriched the discussions.\n- The text provides a concise and well-structured literature review on automatic debate processing.\n\nreasons_to_reject: - Despite the strengths of the proposed work, the attained F1 score by the best model still appears relatively low. As a matter of fact, this is not a poor outcome when considering the conventional models employed for the task, but it's possible that the proposed model might be outperformed by newer Generative LLMs. It would be important to discuss the influence of these novel models on the task's execution.\n\nquestions_for_the_authors: - You wrote that \"both baselines relying exclusively on NLP algorithms and techniques performed worse than the random baseline. The main cause of this problem can be probably attributed to the lack of data in our domain\". But did you consider testing simpler probabilistic models for NLP, such as a Bayesian classifier, which performs well with small datasets?\n- Did you consider comparing the results of your method with those of a generative LLM, which have been widely used for complex NLP tasks?\n\ntypos_grammar_style_and_presentation_improvements: - Line 452: has been carried OUR using Pandas - Line 624-5: learning representations from A graph-structured data\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "2EKmTcKDe1",
        "length": 643,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method for automatically evaluating and predicting the winning stance in professional argumentative debates. The key problem it addresses is performing debate analysis and evaluation on complex natural language arguments, which have been relatively unexplored.  The main contributions are: - Introduces a hybrid technique combining concepts from argumentation theory and NLP to model complete reasoning lines in debates and determine a winning stance.\n- Demonstrates superior performance of the hybrid technique over pure NLP or argumentation theory baselines on the debate evaluation task.\n- Provides detailed experimental analysis and error analysis lending insight into the benefits of combining logical and linguistic knowledge for this problem.\nOverall, this engineering-focused paper makes contributions in methods, and analysis towards advancing NLP for a complex argumentation problem.\n\nreasons_to_accept: - Tackles a complex and underexplored NLP problem - evaluation of professional debates with complete logical argumentation - using a hybrid approach.\n- The details of the hybrid technique combining concepts from argumentation theory seem sound and applicable to debates.  - Well-written with a clear explanation of all concepts for audiences unfamiliar with argumentation theory.\n- Provides compelling evidence that bridging logical and neural techniques is beneficial for argument-mining problems.\n- Could inspire more creative hybrid modelling and use of structured knowledge in NLP systems.\n\nreasons_to_reject: My biggest issue with this paper is the lack of comparison to other techniques for the debate evaluation task, which makes it difficult to situate the novelty of the proposed approach. The authors only compare against baselines they define, without discussing prior proposals from the literature. Given the lack of comparisons, it is unclear whether this approach substantially advances upon the state-of-the-art for this task.   Additionally, the literature review overlooks highly relevant prior work. The authors claim in line 110 that \"Most of this research has been focused on performing an individual evaluation of arguments or argumentative lines of reasoning (Wachsmuth et al., 2017) instead of a global, interactive viewpoint where complete debates consisting of multiple, conflicting lines of reasoning are analysed\". However, Wachsmuth et al. (2017) proposed a Reasonableness dimension that evaluates the whole argument, considering counter-arguments and rebuttals. In a debate scenario, counter-arguments and rebuttals are what constitute the bare bones of it so one would imagine that a dimension like Reasonableness would at least be discussed. Furthermore, Marro et al. (2022) also introduced a technique to assess the reasonableness of full argument graphs, combining text and graph structure. Yet neither of these directly related works are acknowledged or compared to. The lack of citations and comparisons to existing techniques that also evaluate full argument graphs raises concerns. The paper does not sufficiently situate its contributions among prior methods or demonstrate advancement over closely related approaches. A more rigorous review and comparison to previous debate evaluation and reasonableness assessment research is needed.\n\nquestions_for_the_authors: A) Your proposed approach is evaluated only on a single dataset of Catalan debates. Can you discuss how you expect the hybrid technique to transfer to other debate domains and languages? Do you plan to test generalization capability on other debate datasets?\n\nmissing_references: - [Graph Embeddings for Argumentation Quality Assessment](https://aclanthology.org/2022.findings-emnlp.306) (Marro et al., Findings 2022) - [Yes, we can! Mining Arguments in 50 Years of US Presidential Campaign Debates](https://aclanthology.org/P19-1463) (Haddadan et al., ACL 2019)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "71_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_71_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7098,
      "max_similarity": 0.716,
      "avg_coverage": 0.5357666666666666,
      "max_coverage": 0.7143
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 700,
      "avg_human_length": 531.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 11,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "WxqqTDM8lo",
        "similarity": 0.7029,
        "coverage": 0.4688,
        "human_length": 541,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method to evaluate natural language debates based on a hybrid approach of combining argumentation theory and NLP techniques. The approach taken seems to be an interesting one, with the potential of leveraging the improvements made in both fields to perform the challenging task of debate outcome prediction. The model used is, to my knowledge, an original one and comparisons are made to reasonable baselines although some details are missing. The two phase approach taken to argument scoring is another good contribution.\n\nreasons_to_accept: 1. Interesting two phase approach to argument quality detection. \n2. Innovative model proposed to handle debate outcome prediction,\n\nreasons_to_reject: There are some decisions taken by the author/s that I would like some clarifications on, as to my understanding, they impact the outcome of the paper in a major way. \n1. The removal of arguments that have an attack relation on them prior to them being scored strikes me as being quite extreme. There may be a good argument that has a substandard attack on it. Removal of that argument from the set of admissible arguments is likely to make the model non representative, as human adjudicators may still credit the argument. It may also be the case that an argument is too bad to be attacked. Most debates have a defined time for speakers to rebut their opposition and often the arguments that are focused on are the ones most fundamental to the case that the opposition is running. It makes little sense at that point to spend valuable time on rebutting throwaway statements. In the case of the method proposed by the authors, a good argument that is rebutted badly would be inadmissible and a bad argument that is deemed not worth rebutting would be admissible. This may cause issues in the subsequent steps. \n2. I am unclear on how arguments impact the probability of winning the debate. While section 4.2 is titled argument scoring, I am not clear where that process is actually happening.\n\nquestions_for_the_authors: A. How is the removal of arguments having an attack relation on them justified? \nB. Question on extension of work - often debates are not neatly categorized into phases of speech and rebuttal - how does this architecture handle that?\n\nmissing_references: In section 5.2 the author/s discuss Argument Theory Baselines and subsequently utilize them in table 1. I was not able to find a citation for this and the author/s do not go into details of what these baselines entail. Either a citation or some details on the baselines used would be appreciated here.\n\ntypos_grammar_style_and_presentation_improvements: Spelling error on line 654 - miss-classified -> misclassified\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "Xyp1UCTOKv",
        "similarity": 0.716,
        "coverage": 0.7143,
        "human_length": 411,
        "human_text": "paper_topic_and_main_contributions: The article presents an original method for evaluating the winning stance in complete debates, under real-world circumstances involving professional debaters. The proposed method is a hybrid approach that combines argumentation frameworks and semantics on one side, and Graph-Network and Transformer-based architectures on the other.\nThe argumentation frameworks module categorizes arguments as either Conflict-free or Admissible. Subsequently, the arguments are processed by a hybrid model that integrates a Graph-Network architecture with embeddings generated by a Transformer model. The winner is estimated as the participant with the highest count of acceptable arguments.\n\nreasons_to_accept: - This work achieved results surpass those previously reported in the literature for this task.\n- Detailed analyses present original and interesting outcomes of argumentation processing. The realization that \"learning representations from graph-structured data is a better idea than just using the whole text\" holds significant importance within the current context of NLP research, wherein generative LLMs are garnering widespread attention.\n- The incorporation of error analysis enriched the discussions.\n- The text provides a concise and well-structured literature review on automatic debate processing.\n\nreasons_to_reject: - Despite the strengths of the proposed work, the attained F1 score by the best model still appears relatively low. As a matter of fact, this is not a poor outcome when considering the conventional models employed for the task, but it's possible that the proposed model might be outperformed by newer Generative LLMs. It would be important to discuss the influence of these novel models on the task's execution.\n\nquestions_for_the_authors: - You wrote that \"both baselines relying exclusively on NLP algorithms and techniques performed worse than the random baseline. The main cause of this problem can be probably attributed to the lack of data in our domain\". But did you consider testing simpler probabilistic models for NLP, such as a Bayesian classifier, which performs well with small datasets?\n- Did you consider comparing the results of your method with those of a generative LLM, which have been widely used for complex NLP tasks?\n\ntypos_grammar_style_and_presentation_improvements: - Line 452: has been carried OUR using Pandas - Line 624-5: learning representations from A graph-structured data\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "2EKmTcKDe1",
        "similarity": 0.7105,
        "coverage": 0.4242,
        "human_length": 643,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method for automatically evaluating and predicting the winning stance in professional argumentative debates. The key problem it addresses is performing debate analysis and evaluation on complex natural language arguments, which have been relatively unexplored.  The main contributions are: - Introduces a hybrid technique combining concepts from argumentation theory and NLP to model complete reasoning lines in debates and determine a winning stance.\n- Demonstrates superior performance of the hybrid technique over pure NLP or argumentation theory baselines on the debate evaluation task.\n- Provides detailed experimental analysis and error analysis lending insight into the benefits of combining logical and linguistic knowledge for this problem.\nOverall, this engineering-focused paper makes contributions in methods, and analysis towards advancing NLP for a complex argumentation problem.\n\nreasons_to_accept: - Tackles a complex and underexplored NLP problem - evaluation of professional debates with complete logical argumentation - using a hybrid approach.\n- The details of the hybrid technique combining concepts from argumentation theory seem sound and applicable to debates.  - Well-written with a clear explanation of all concepts for audiences unfamiliar with argumentation theory.\n- Provides compelling evidence that bridging logical and neural techniques is beneficial for argument-mining problems.\n- Could inspire more creative hybrid modelling and use of structured knowledge in NLP systems.\n\nreasons_to_reject: My biggest issue with this paper is the lack of comparison to other techniques for the debate evaluation task, which makes it difficult to situate the novelty of the proposed approach. The authors only compare against baselines they define, without discussing prior proposals from the literature. Given the lack of comparisons, it is unclear whether this approach substantially advances upon the state-of-the-art for this task.   Additionally, the literature review overlooks highly relevant prior work. The authors claim in line 110 that \"Most of this research has been focused on performing an individual evaluation of arguments or argumentative lines of reasoning (Wachsmuth et al., 2017) instead of a global, interactive viewpoint where complete debates consisting of multiple, conflicting lines of reasoning are analysed\". However, Wachsmuth et al. (2017) proposed a Reasonableness dimension that evaluates the whole argument, considering counter-arguments and rebuttals. In a debate scenario, counter-arguments and rebuttals are what constitute the bare bones of it so one would imagine that a dimension like Reasonableness would at least be discussed. Furthermore, Marro et al. (2022) also introduced a technique to assess the reasonableness of full argument graphs, combining text and graph structure. Yet neither of these directly related works are acknowledged or compared to. The lack of citations and comparisons to existing techniques that also evaluate full argument graphs raises concerns. The paper does not sufficiently situate its contributions among prior methods or demonstrate advancement over closely related approaches. A more rigorous review and comparison to previous debate evaluation and reasonableness assessment research is needed.\n\nquestions_for_the_authors: A) Your proposed approach is evaluated only on a single dataset of Catalan debates. Can you discuss how you expect the hybrid technique to transfer to other debate domains and languages? Do you plan to test generalization capability on other debate datasets?\n\nmissing_references: - [Graph Embeddings for Argumentation Quality Assessment](https://aclanthology.org/2022.findings-emnlp.306) (Marro et al., Findings 2022) - [Yes, we can! Mining Arguments in 50 Years of US Presidential Campaign Debates](https://aclanthology.org/P19-1463) (Haddadan et al., ACL 2019)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "WxqqTDM8lo",
        "length": 541,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method to evaluate natural language debates based on a hybrid approach of combining argumentation theory and NLP techniques. The approach taken seems to be an interesting one, with the potential of leveraging the improvements made in both fields to perform the challenging task of debate outcome prediction. The model used is, to my knowledge, an original one and comparisons are made to reasonable baselines although some details are missing. The two phase approach taken to argument scoring is another good contribution.\n\nreasons_to_accept: 1. Interesting two phase approach to argument quality detection. \n2. Innovative model proposed to handle debate outcome prediction,\n\nreasons_to_reject: There are some decisions taken by the author/s that I would like some clarifications on, as to my understanding, they impact the outcome of the paper in a major way. \n1. The removal of arguments that have an attack relation on them prior to them being scored strikes me as being quite extreme. There may be a good argument that has a substandard attack on it. Removal of that argument from the set of admissible arguments is likely to make the model non representative, as human adjudicators may still credit the argument. It may also be the case that an argument is too bad to be attacked. Most debates have a defined time for speakers to rebut their opposition and often the arguments that are focused on are the ones most fundamental to the case that the opposition is running. It makes little sense at that point to spend valuable time on rebutting throwaway statements. In the case of the method proposed by the authors, a good argument that is rebutted badly would be inadmissible and a bad argument that is deemed not worth rebutting would be admissible. This may cause issues in the subsequent steps. \n2. I am unclear on how arguments impact the probability of winning the debate. While section 4.2 is titled argument scoring, I am not clear where that process is actually happening.\n\nquestions_for_the_authors: A. How is the removal of arguments having an attack relation on them justified? \nB. Question on extension of work - often debates are not neatly categorized into phases of speech and rebuttal - how does this architecture handle that?\n\nmissing_references: In section 5.2 the author/s discuss Argument Theory Baselines and subsequently utilize them in table 1. I was not able to find a citation for this and the author/s do not go into details of what these baselines entail. Either a citation or some details on the baselines used would be appreciated here.\n\ntypos_grammar_style_and_presentation_improvements: Spelling error on line 654 - miss-classified -> misclassified\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "Xyp1UCTOKv",
        "length": 411,
        "human_text": "paper_topic_and_main_contributions: The article presents an original method for evaluating the winning stance in complete debates, under real-world circumstances involving professional debaters. The proposed method is a hybrid approach that combines argumentation frameworks and semantics on one side, and Graph-Network and Transformer-based architectures on the other.\nThe argumentation frameworks module categorizes arguments as either Conflict-free or Admissible. Subsequently, the arguments are processed by a hybrid model that integrates a Graph-Network architecture with embeddings generated by a Transformer model. The winner is estimated as the participant with the highest count of acceptable arguments.\n\nreasons_to_accept: - This work achieved results surpass those previously reported in the literature for this task.\n- Detailed analyses present original and interesting outcomes of argumentation processing. The realization that \"learning representations from graph-structured data is a better idea than just using the whole text\" holds significant importance within the current context of NLP research, wherein generative LLMs are garnering widespread attention.\n- The incorporation of error analysis enriched the discussions.\n- The text provides a concise and well-structured literature review on automatic debate processing.\n\nreasons_to_reject: - Despite the strengths of the proposed work, the attained F1 score by the best model still appears relatively low. As a matter of fact, this is not a poor outcome when considering the conventional models employed for the task, but it's possible that the proposed model might be outperformed by newer Generative LLMs. It would be important to discuss the influence of these novel models on the task's execution.\n\nquestions_for_the_authors: - You wrote that \"both baselines relying exclusively on NLP algorithms and techniques performed worse than the random baseline. The main cause of this problem can be probably attributed to the lack of data in our domain\". But did you consider testing simpler probabilistic models for NLP, such as a Bayesian classifier, which performs well with small datasets?\n- Did you consider comparing the results of your method with those of a generative LLM, which have been widely used for complex NLP tasks?\n\ntypos_grammar_style_and_presentation_improvements: - Line 452: has been carried OUR using Pandas - Line 624-5: learning representations from A graph-structured data\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "2EKmTcKDe1",
        "length": 643,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a new method for automatically evaluating and predicting the winning stance in professional argumentative debates. The key problem it addresses is performing debate analysis and evaluation on complex natural language arguments, which have been relatively unexplored.  The main contributions are: - Introduces a hybrid technique combining concepts from argumentation theory and NLP to model complete reasoning lines in debates and determine a winning stance.\n- Demonstrates superior performance of the hybrid technique over pure NLP or argumentation theory baselines on the debate evaluation task.\n- Provides detailed experimental analysis and error analysis lending insight into the benefits of combining logical and linguistic knowledge for this problem.\nOverall, this engineering-focused paper makes contributions in methods, and analysis towards advancing NLP for a complex argumentation problem.\n\nreasons_to_accept: - Tackles a complex and underexplored NLP problem - evaluation of professional debates with complete logical argumentation - using a hybrid approach.\n- The details of the hybrid technique combining concepts from argumentation theory seem sound and applicable to debates.  - Well-written with a clear explanation of all concepts for audiences unfamiliar with argumentation theory.\n- Provides compelling evidence that bridging logical and neural techniques is beneficial for argument-mining problems.\n- Could inspire more creative hybrid modelling and use of structured knowledge in NLP systems.\n\nreasons_to_reject: My biggest issue with this paper is the lack of comparison to other techniques for the debate evaluation task, which makes it difficult to situate the novelty of the proposed approach. The authors only compare against baselines they define, without discussing prior proposals from the literature. Given the lack of comparisons, it is unclear whether this approach substantially advances upon the state-of-the-art for this task.   Additionally, the literature review overlooks highly relevant prior work. The authors claim in line 110 that \"Most of this research has been focused on performing an individual evaluation of arguments or argumentative lines of reasoning (Wachsmuth et al., 2017) instead of a global, interactive viewpoint where complete debates consisting of multiple, conflicting lines of reasoning are analysed\". However, Wachsmuth et al. (2017) proposed a Reasonableness dimension that evaluates the whole argument, considering counter-arguments and rebuttals. In a debate scenario, counter-arguments and rebuttals are what constitute the bare bones of it so one would imagine that a dimension like Reasonableness would at least be discussed. Furthermore, Marro et al. (2022) also introduced a technique to assess the reasonableness of full argument graphs, combining text and graph structure. Yet neither of these directly related works are acknowledged or compared to. The lack of citations and comparisons to existing techniques that also evaluate full argument graphs raises concerns. The paper does not sufficiently situate its contributions among prior methods or demonstrate advancement over closely related approaches. A more rigorous review and comparison to previous debate evaluation and reasonableness assessment research is needed.\n\nquestions_for_the_authors: A) Your proposed approach is evaluated only on a single dataset of Catalan debates. Can you discuss how you expect the hybrid technique to transfer to other debate domains and languages? Do you plan to test generalization capability on other debate datasets?\n\nmissing_references: - [Graph Embeddings for Argumentation Quality Assessment](https://aclanthology.org/2022.findings-emnlp.306) (Marro et al., Findings 2022) - [Yes, we can! Mining Arguments in 50 Years of US Presidential Campaign Debates](https://aclanthology.org/P19-1463) (Haddadan et al., ACL 2019)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "9_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_9_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.68265,
      "max_similarity": 0.7046,
      "avg_coverage": 0.092575,
      "max_coverage": 0.125
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 24,
      "avg_human_length": 426.75
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": false,
      "has_weaknesses": false,
      "has_suggestions": false
    },
    "content_count_metrics": {
      "strengths_count": 0,
      "weaknesses_count": 0,
      "suggestions_count": 0
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "SkuYMZFEYl",
        "similarity": 0.6367,
        "coverage": 0.125,
        "human_length": 310,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the task of multi-intent detection for dialogue systems applications. The authors expand upon prior work that utilizes two BERT networks to capture the semantics of utterances and intent labels to calculate the similarity between them for multi-intent detection. The authors argue that prior work ignores the complexity of the intent label embedding space thus failing to capture the semantic relation between individual utterance tokens and the intent labels. To address, this limitation the authors propose Multi-Intent Detection through Supervised Prototypical Contrastive Learning (PCMID) which utilizes a single BERT Encoder for constructing utterance and label embeddings. Additionally, PCMID utilizes conservative learning to optimize the representation of both intent labels and the utterance samples in the embedding space.\n\nreasons_to_accept: - PCMID outperforms existing approaches - The topic area is interesting and relevant to the dialogue systems community - Paper is easy to read and follow. Experiment settings are clearly defined and analysis is sufficient.\n\nreasons_to_reject: - Contribution is incremental and relies heavily upon the initialized base encoder. BERT out-of-the-box is terrible at producing sentence embeddings and as such needs to be pre-trained.\n\nquestions_for_the_authors: - Are utterances and labels encoded jointly or separately? If jointly then this means that label representations cannot be re-used for new utterances.\n- Are MAB and the utterance encoder the same model?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "ZX90wMzgy3",
        "similarity": 0.7046,
        "coverage": 0.037,
        "human_length": 556,
        "human_text": "paper_topic_and_main_contributions: The paper proposes PCMID, a novel Multi-Intent Detection framework enabled by Prototypical Contrastive Learning under a supervised setting. The PCMID model can learn multiple semantic representations of a given user utterance under the context of different intent labels in an optimized semantic space.\n\nreasons_to_accept: 1. The paper conducts experiments on a real-word dataset. \n2. The paper is easy to follow. \n3. The paper provides the url of datasets. \n4. The paper provides introduction of baselines.\n\nreasons_to_reject: 1. You should compare your model with more recent models [1-5]. \n2. Contrastive learning has been widely used in Intent Detection [6-9], although the tasks are not identical. I think the novelty of this simple modification is not suitable for EMNLP. \n3. You should provide more details about the formula in the text, e.g. $\\ell_{BCE}$ ,even if it is simple, give specific details. \n4. You don't provide the value of some hyper-parameters, such as \u03c4. \n5. The Figure 1 is blurry, which affects reading.\n[1] Qin L, Wei F, Xie T, et al. GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling[C]//Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021: 178-188.\n[2] Xing B, Tsang I. Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 159-169.\n[3] Xing B, Tsang I. Group is better than individual: Exploiting Label Topologies and Label Relations for Joint Multiple Intent Detection and Slot Filling[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 3964-3975.  [4] Song M, Yu B, Quangang L, et al. Enhancing Joint Multiple Intent Detection and Slot Filling with Global Intent-Slot Co-occurrence[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 7967-7977.\n[5] Cheng L, Yang W, Jia W. A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken Language Understanding[J]. arXiv e-prints, 2022: arXiv: 2211.12220.\n[6] Liu H, Zhang F, Zhang X, et al. An Explicit-Joint and Supervised-Contrastive Learning Framework for Few-Shot Intent Classification and Slot Filling[C]//Findings of the Association for Computational Linguistics: EMNLP 2021. 2021: 1945-1955.\n[7] Qin L, Chen Q, Xie T, et al. GL-CLeF: A Global\u2013Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding[C]//Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022: 2677-2686.\n[8] Liang S, Shou L, Pei J, et al. Label-aware Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 9903-9918.\n[9] Chang Y H, Chen Y N. Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding[J]. arXiv preprint arXiv:2205.00693, 2022.\n\nquestions_for_the_authors: Can you provide more analysis about figure 2? \nYou should compare your work with more recent SOTA to improve Soundness.\n\nmissing_references: -\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "60E8WXFHcE",
        "similarity": 0.695,
        "coverage": 0.125,
        "human_length": 390,
        "human_text": "paper_topic_and_main_contributions: Multi-intent detection is closer to the reality of complex situations and is more challenging than single-intent detection. This work proposes PCMID to model the semantics between individual utterance tokens and the intent label words, and achieves state-of-the-art performance on four datasets. In addition, this work constructs a multi-intent detection dataset\n\nreasons_to_accept: 1.The structure of the paper is clear and easy to read; 2.The results of the experiment are detailed and the results have been analyzed in detail in various ways.\n\nreasons_to_reject: 1. Failed to write about the shortcomings of previous multi-intent detection work in the abstract, simply stating that multi-intent detection missions are closer to the real world, the abstract failed to excite me; 2. The introduction does not summarize the work of the thesis well enough to indicate the innovative nature of the thesis; 3. The paper is redundant, with large parts of previous work (e.g. 3.3, etc.) in the introduction to the methodology, which should focus on the problem to be solved and how it was solved; 4. The introduction mentions that PCMID is lighter than previous frameworks, which should be reflected in the later paper by reporting the number of different model parameters and highlighting the advantages of PCMID; 5. There are some errors in detail, such as: spelling of words, formulas without punctuation, and full names that appeared in the previous text do not need to be repeated and explained when abbreviations are used in the later text. It is recommended that you read the entire text carefully and check it thoroughly.\n\nethical_concerns: No\n\nquestions_for_the_authors: When selecting the baseline, you chose a number of models that do joint training for intent detection and slot filling, so why not a few more models that only do multiple intent detection?\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "PDhXnojEYM",
        "similarity": 0.6943,
        "coverage": 0.0833,
        "human_length": 451,
        "human_text": "paper_topic_and_main_contributions: This paper talks about an important issue of real-world dialog systems, where the utterances can be crafted with multi-intent which may or may not be nested. This paper showcases a new way of doing multi-intent detection \u2013 PCMID that is a novel Multi-Intent Detection framework enabled by Prototypical Contrastive Learning under a supervised setting. The authors have clearly showcased their approach and explained the architecture. This work has been tested on 3 public datasets \u2013 MixATIS, MixSNIPS, and FSPS; and one private dataset CREDIT16. The authors have demonstrated slight to moderate improvement across all the datasets using this approach.\n\nreasons_to_accept: \u2022\tThe authors did a good job explaining the approach in detail, and the paper is written cleanly with very few minor typos \u2013 such as different types of double quotations.\n\u2022\tThis is a very important problem statement and the conversational agents getting more complex day by day and the utterances becoming trickier.  \u2022\tThe method is novel.\n\nreasons_to_reject: \u2022\tIt\u2019s better to have a dataflow diagram with examples from any of the datasets used, which helps to follow the paper better.\n\u2022\tIt\u2019s a supervised method, which is very specific to the tasks. It will be very hard to use it in any other tasks, as it requires an extensive amount of data-collection and annotation. No comparison is given with LLMs performance on these datasets or even zeroshot comparisons across the datasets (e.g., trained on mixATIS and tested on MixSNIPS). It helps to understand the robustness of this approach.   \u2022\tPerformance improvements are very slight on all the datasets except MixATIS.\n\nquestions_for_the_authors: Please address the points under weakness along with the following questions: \u2022\tCould you please explain how the slots are processed? as I found most of the discussions are based on intents. The examples from table-3 are also only intents. So curious to know, how slots are extracted.\n\u2022\tQualitative studies over predicted outputs with examples (not only on numbers) will be better to understand\n\ntypos_grammar_style_and_presentation_improvements: - double quotation marks are not consistent - Many concepts can be moved from section 3 to a separate Background section, such as SCL.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "SkuYMZFEYl",
        "length": 310,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the task of multi-intent detection for dialogue systems applications. The authors expand upon prior work that utilizes two BERT networks to capture the semantics of utterances and intent labels to calculate the similarity between them for multi-intent detection. The authors argue that prior work ignores the complexity of the intent label embedding space thus failing to capture the semantic relation between individual utterance tokens and the intent labels. To address, this limitation the authors propose Multi-Intent Detection through Supervised Prototypical Contrastive Learning (PCMID) which utilizes a single BERT Encoder for constructing utterance and label embeddings. Additionally, PCMID utilizes conservative learning to optimize the representation of both intent labels and the utterance samples in the embedding space.\n\nreasons_to_accept: - PCMID outperforms existing approaches - The topic area is interesting and relevant to the dialogue systems community - Paper is easy to read and follow. Experiment settings are clearly defined and analysis is sufficient.\n\nreasons_to_reject: - Contribution is incremental and relies heavily upon the initialized base encoder. BERT out-of-the-box is terrible at producing sentence embeddings and as such needs to be pre-trained.\n\nquestions_for_the_authors: - Are utterances and labels encoded jointly or separately? If jointly then this means that label representations cannot be re-used for new utterances.\n- Are MAB and the utterance encoder the same model?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ZX90wMzgy3",
        "length": 556,
        "human_text": "paper_topic_and_main_contributions: The paper proposes PCMID, a novel Multi-Intent Detection framework enabled by Prototypical Contrastive Learning under a supervised setting. The PCMID model can learn multiple semantic representations of a given user utterance under the context of different intent labels in an optimized semantic space.\n\nreasons_to_accept: 1. The paper conducts experiments on a real-word dataset. \n2. The paper is easy to follow. \n3. The paper provides the url of datasets. \n4. The paper provides introduction of baselines.\n\nreasons_to_reject: 1. You should compare your model with more recent models [1-5]. \n2. Contrastive learning has been widely used in Intent Detection [6-9], although the tasks are not identical. I think the novelty of this simple modification is not suitable for EMNLP. \n3. You should provide more details about the formula in the text, e.g. $\\ell_{BCE}$ ,even if it is simple, give specific details. \n4. You don't provide the value of some hyper-parameters, such as \u03c4. \n5. The Figure 1 is blurry, which affects reading.\n[1] Qin L, Wei F, Xie T, et al. GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling[C]//Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021: 178-188.\n[2] Xing B, Tsang I. Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 159-169.\n[3] Xing B, Tsang I. Group is better than individual: Exploiting Label Topologies and Label Relations for Joint Multiple Intent Detection and Slot Filling[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 3964-3975.  [4] Song M, Yu B, Quangang L, et al. Enhancing Joint Multiple Intent Detection and Slot Filling with Global Intent-Slot Co-occurrence[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 7967-7977.\n[5] Cheng L, Yang W, Jia W. A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken Language Understanding[J]. arXiv e-prints, 2022: arXiv: 2211.12220.\n[6] Liu H, Zhang F, Zhang X, et al. An Explicit-Joint and Supervised-Contrastive Learning Framework for Few-Shot Intent Classification and Slot Filling[C]//Findings of the Association for Computational Linguistics: EMNLP 2021. 2021: 1945-1955.\n[7] Qin L, Chen Q, Xie T, et al. GL-CLeF: A Global\u2013Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding[C]//Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022: 2677-2686.\n[8] Liang S, Shou L, Pei J, et al. Label-aware Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 9903-9918.\n[9] Chang Y H, Chen Y N. Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding[J]. arXiv preprint arXiv:2205.00693, 2022.\n\nquestions_for_the_authors: Can you provide more analysis about figure 2? \nYou should compare your work with more recent SOTA to improve Soundness.\n\nmissing_references: -\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "60E8WXFHcE",
        "length": 390,
        "human_text": "paper_topic_and_main_contributions: Multi-intent detection is closer to the reality of complex situations and is more challenging than single-intent detection. This work proposes PCMID to model the semantics between individual utterance tokens and the intent label words, and achieves state-of-the-art performance on four datasets. In addition, this work constructs a multi-intent detection dataset\n\nreasons_to_accept: 1.The structure of the paper is clear and easy to read; 2.The results of the experiment are detailed and the results have been analyzed in detail in various ways.\n\nreasons_to_reject: 1. Failed to write about the shortcomings of previous multi-intent detection work in the abstract, simply stating that multi-intent detection missions are closer to the real world, the abstract failed to excite me; 2. The introduction does not summarize the work of the thesis well enough to indicate the innovative nature of the thesis; 3. The paper is redundant, with large parts of previous work (e.g. 3.3, etc.) in the introduction to the methodology, which should focus on the problem to be solved and how it was solved; 4. The introduction mentions that PCMID is lighter than previous frameworks, which should be reflected in the later paper by reporting the number of different model parameters and highlighting the advantages of PCMID; 5. There are some errors in detail, such as: spelling of words, formulas without punctuation, and full names that appeared in the previous text do not need to be repeated and explained when abbreviations are used in the later text. It is recommended that you read the entire text carefully and check it thoroughly.\n\nethical_concerns: No\n\nquestions_for_the_authors: When selecting the baseline, you chose a number of models that do joint training for intent detection and slot filling, so why not a few more models that only do multiple intent detection?\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "PDhXnojEYM",
        "length": 451,
        "human_text": "paper_topic_and_main_contributions: This paper talks about an important issue of real-world dialog systems, where the utterances can be crafted with multi-intent which may or may not be nested. This paper showcases a new way of doing multi-intent detection \u2013 PCMID that is a novel Multi-Intent Detection framework enabled by Prototypical Contrastive Learning under a supervised setting. The authors have clearly showcased their approach and explained the architecture. This work has been tested on 3 public datasets \u2013 MixATIS, MixSNIPS, and FSPS; and one private dataset CREDIT16. The authors have demonstrated slight to moderate improvement across all the datasets using this approach.\n\nreasons_to_accept: \u2022\tThe authors did a good job explaining the approach in detail, and the paper is written cleanly with very few minor typos \u2013 such as different types of double quotations.\n\u2022\tThis is a very important problem statement and the conversational agents getting more complex day by day and the utterances becoming trickier.  \u2022\tThe method is novel.\n\nreasons_to_reject: \u2022\tIt\u2019s better to have a dataflow diagram with examples from any of the datasets used, which helps to follow the paper better.\n\u2022\tIt\u2019s a supervised method, which is very specific to the tasks. It will be very hard to use it in any other tasks, as it requires an extensive amount of data-collection and annotation. No comparison is given with LLMs performance on these datasets or even zeroshot comparisons across the datasets (e.g., trained on mixATIS and tested on MixSNIPS). It helps to understand the robustness of this approach.   \u2022\tPerformance improvements are very slight on all the datasets except MixATIS.\n\nquestions_for_the_authors: Please address the points under weakness along with the following questions: \u2022\tCould you please explain how the slots are processed? as I found most of the discussions are based on intents. The examples from table-3 are also only intents. So curious to know, how slots are extracted.\n\u2022\tQualitative studies over predicted outputs with examples (not only on numbers) will be better to understand\n\ntypos_grammar_style_and_presentation_improvements: - double quotation marks are not consistent - Many concepts can be moved from section 3 to a separate Background section, such as SCL.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "9_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_9_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.690475,
      "max_similarity": 0.698,
      "avg_coverage": 0.092575,
      "max_coverage": 0.125
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 29,
      "avg_human_length": 426.75
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": false,
      "has_weaknesses": false,
      "has_suggestions": false
    },
    "content_count_metrics": {
      "strengths_count": 0,
      "weaknesses_count": 0,
      "suggestions_count": 0
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "SkuYMZFEYl",
        "similarity": 0.6732,
        "coverage": 0.125,
        "human_length": 310,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the task of multi-intent detection for dialogue systems applications. The authors expand upon prior work that utilizes two BERT networks to capture the semantics of utterances and intent labels to calculate the similarity between them for multi-intent detection. The authors argue that prior work ignores the complexity of the intent label embedding space thus failing to capture the semantic relation between individual utterance tokens and the intent labels. To address, this limitation the authors propose Multi-Intent Detection through Supervised Prototypical Contrastive Learning (PCMID) which utilizes a single BERT Encoder for constructing utterance and label embeddings. Additionally, PCMID utilizes conservative learning to optimize the representation of both intent labels and the utterance samples in the embedding space.\n\nreasons_to_accept: - PCMID outperforms existing approaches - The topic area is interesting and relevant to the dialogue systems community - Paper is easy to read and follow. Experiment settings are clearly defined and analysis is sufficient.\n\nreasons_to_reject: - Contribution is incremental and relies heavily upon the initialized base encoder. BERT out-of-the-box is terrible at producing sentence embeddings and as such needs to be pre-trained.\n\nquestions_for_the_authors: - Are utterances and labels encoded jointly or separately? If jointly then this means that label representations cannot be re-used for new utterances.\n- Are MAB and the utterance encoder the same model?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "ZX90wMzgy3",
        "similarity": 0.698,
        "coverage": 0.037,
        "human_length": 556,
        "human_text": "paper_topic_and_main_contributions: The paper proposes PCMID, a novel Multi-Intent Detection framework enabled by Prototypical Contrastive Learning under a supervised setting. The PCMID model can learn multiple semantic representations of a given user utterance under the context of different intent labels in an optimized semantic space.\n\nreasons_to_accept: 1. The paper conducts experiments on a real-word dataset. \n2. The paper is easy to follow. \n3. The paper provides the url of datasets. \n4. The paper provides introduction of baselines.\n\nreasons_to_reject: 1. You should compare your model with more recent models [1-5]. \n2. Contrastive learning has been widely used in Intent Detection [6-9], although the tasks are not identical. I think the novelty of this simple modification is not suitable for EMNLP. \n3. You should provide more details about the formula in the text, e.g. $\\ell_{BCE}$ ,even if it is simple, give specific details. \n4. You don't provide the value of some hyper-parameters, such as \u03c4. \n5. The Figure 1 is blurry, which affects reading.\n[1] Qin L, Wei F, Xie T, et al. GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling[C]//Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021: 178-188.\n[2] Xing B, Tsang I. Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 159-169.\n[3] Xing B, Tsang I. Group is better than individual: Exploiting Label Topologies and Label Relations for Joint Multiple Intent Detection and Slot Filling[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 3964-3975.  [4] Song M, Yu B, Quangang L, et al. Enhancing Joint Multiple Intent Detection and Slot Filling with Global Intent-Slot Co-occurrence[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 7967-7977.\n[5] Cheng L, Yang W, Jia W. A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken Language Understanding[J]. arXiv e-prints, 2022: arXiv: 2211.12220.\n[6] Liu H, Zhang F, Zhang X, et al. An Explicit-Joint and Supervised-Contrastive Learning Framework for Few-Shot Intent Classification and Slot Filling[C]//Findings of the Association for Computational Linguistics: EMNLP 2021. 2021: 1945-1955.\n[7] Qin L, Chen Q, Xie T, et al. GL-CLeF: A Global\u2013Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding[C]//Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022: 2677-2686.\n[8] Liang S, Shou L, Pei J, et al. Label-aware Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 9903-9918.\n[9] Chang Y H, Chen Y N. Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding[J]. arXiv preprint arXiv:2205.00693, 2022.\n\nquestions_for_the_authors: Can you provide more analysis about figure 2? \nYou should compare your work with more recent SOTA to improve Soundness.\n\nmissing_references: -\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "60E8WXFHcE",
        "similarity": 0.6963,
        "coverage": 0.125,
        "human_length": 390,
        "human_text": "paper_topic_and_main_contributions: Multi-intent detection is closer to the reality of complex situations and is more challenging than single-intent detection. This work proposes PCMID to model the semantics between individual utterance tokens and the intent label words, and achieves state-of-the-art performance on four datasets. In addition, this work constructs a multi-intent detection dataset\n\nreasons_to_accept: 1.The structure of the paper is clear and easy to read; 2.The results of the experiment are detailed and the results have been analyzed in detail in various ways.\n\nreasons_to_reject: 1. Failed to write about the shortcomings of previous multi-intent detection work in the abstract, simply stating that multi-intent detection missions are closer to the real world, the abstract failed to excite me; 2. The introduction does not summarize the work of the thesis well enough to indicate the innovative nature of the thesis; 3. The paper is redundant, with large parts of previous work (e.g. 3.3, etc.) in the introduction to the methodology, which should focus on the problem to be solved and how it was solved; 4. The introduction mentions that PCMID is lighter than previous frameworks, which should be reflected in the later paper by reporting the number of different model parameters and highlighting the advantages of PCMID; 5. There are some errors in detail, such as: spelling of words, formulas without punctuation, and full names that appeared in the previous text do not need to be repeated and explained when abbreviations are used in the later text. It is recommended that you read the entire text carefully and check it thoroughly.\n\nethical_concerns: No\n\nquestions_for_the_authors: When selecting the baseline, you chose a number of models that do joint training for intent detection and slot filling, so why not a few more models that only do multiple intent detection?\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "PDhXnojEYM",
        "similarity": 0.6944,
        "coverage": 0.0833,
        "human_length": 451,
        "human_text": "paper_topic_and_main_contributions: This paper talks about an important issue of real-world dialog systems, where the utterances can be crafted with multi-intent which may or may not be nested. This paper showcases a new way of doing multi-intent detection \u2013 PCMID that is a novel Multi-Intent Detection framework enabled by Prototypical Contrastive Learning under a supervised setting. The authors have clearly showcased their approach and explained the architecture. This work has been tested on 3 public datasets \u2013 MixATIS, MixSNIPS, and FSPS; and one private dataset CREDIT16. The authors have demonstrated slight to moderate improvement across all the datasets using this approach.\n\nreasons_to_accept: \u2022\tThe authors did a good job explaining the approach in detail, and the paper is written cleanly with very few minor typos \u2013 such as different types of double quotations.\n\u2022\tThis is a very important problem statement and the conversational agents getting more complex day by day and the utterances becoming trickier.  \u2022\tThe method is novel.\n\nreasons_to_reject: \u2022\tIt\u2019s better to have a dataflow diagram with examples from any of the datasets used, which helps to follow the paper better.\n\u2022\tIt\u2019s a supervised method, which is very specific to the tasks. It will be very hard to use it in any other tasks, as it requires an extensive amount of data-collection and annotation. No comparison is given with LLMs performance on these datasets or even zeroshot comparisons across the datasets (e.g., trained on mixATIS and tested on MixSNIPS). It helps to understand the robustness of this approach.   \u2022\tPerformance improvements are very slight on all the datasets except MixATIS.\n\nquestions_for_the_authors: Please address the points under weakness along with the following questions: \u2022\tCould you please explain how the slots are processed? as I found most of the discussions are based on intents. The examples from table-3 are also only intents. So curious to know, how slots are extracted.\n\u2022\tQualitative studies over predicted outputs with examples (not only on numbers) will be better to understand\n\ntypos_grammar_style_and_presentation_improvements: - double quotation marks are not consistent - Many concepts can be moved from section 3 to a separate Background section, such as SCL.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "SkuYMZFEYl",
        "length": 310,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the task of multi-intent detection for dialogue systems applications. The authors expand upon prior work that utilizes two BERT networks to capture the semantics of utterances and intent labels to calculate the similarity between them for multi-intent detection. The authors argue that prior work ignores the complexity of the intent label embedding space thus failing to capture the semantic relation between individual utterance tokens and the intent labels. To address, this limitation the authors propose Multi-Intent Detection through Supervised Prototypical Contrastive Learning (PCMID) which utilizes a single BERT Encoder for constructing utterance and label embeddings. Additionally, PCMID utilizes conservative learning to optimize the representation of both intent labels and the utterance samples in the embedding space.\n\nreasons_to_accept: - PCMID outperforms existing approaches - The topic area is interesting and relevant to the dialogue systems community - Paper is easy to read and follow. Experiment settings are clearly defined and analysis is sufficient.\n\nreasons_to_reject: - Contribution is incremental and relies heavily upon the initialized base encoder. BERT out-of-the-box is terrible at producing sentence embeddings and as such needs to be pre-trained.\n\nquestions_for_the_authors: - Are utterances and labels encoded jointly or separately? If jointly then this means that label representations cannot be re-used for new utterances.\n- Are MAB and the utterance encoder the same model?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "ZX90wMzgy3",
        "length": 556,
        "human_text": "paper_topic_and_main_contributions: The paper proposes PCMID, a novel Multi-Intent Detection framework enabled by Prototypical Contrastive Learning under a supervised setting. The PCMID model can learn multiple semantic representations of a given user utterance under the context of different intent labels in an optimized semantic space.\n\nreasons_to_accept: 1. The paper conducts experiments on a real-word dataset. \n2. The paper is easy to follow. \n3. The paper provides the url of datasets. \n4. The paper provides introduction of baselines.\n\nreasons_to_reject: 1. You should compare your model with more recent models [1-5]. \n2. Contrastive learning has been widely used in Intent Detection [6-9], although the tasks are not identical. I think the novelty of this simple modification is not suitable for EMNLP. \n3. You should provide more details about the formula in the text, e.g. $\\ell_{BCE}$ ,even if it is simple, give specific details. \n4. You don't provide the value of some hyper-parameters, such as \u03c4. \n5. The Figure 1 is blurry, which affects reading.\n[1] Qin L, Wei F, Xie T, et al. GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling[C]//Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021: 178-188.\n[2] Xing B, Tsang I. Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 159-169.\n[3] Xing B, Tsang I. Group is better than individual: Exploiting Label Topologies and Label Relations for Joint Multiple Intent Detection and Slot Filling[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 3964-3975.  [4] Song M, Yu B, Quangang L, et al. Enhancing Joint Multiple Intent Detection and Slot Filling with Global Intent-Slot Co-occurrence[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 7967-7977.\n[5] Cheng L, Yang W, Jia W. A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken Language Understanding[J]. arXiv e-prints, 2022: arXiv: 2211.12220.\n[6] Liu H, Zhang F, Zhang X, et al. An Explicit-Joint and Supervised-Contrastive Learning Framework for Few-Shot Intent Classification and Slot Filling[C]//Findings of the Association for Computational Linguistics: EMNLP 2021. 2021: 1945-1955.\n[7] Qin L, Chen Q, Xie T, et al. GL-CLeF: A Global\u2013Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding[C]//Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022: 2677-2686.\n[8] Liang S, Shou L, Pei J, et al. Label-aware Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 9903-9918.\n[9] Chang Y H, Chen Y N. Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding[J]. arXiv preprint arXiv:2205.00693, 2022.\n\nquestions_for_the_authors: Can you provide more analysis about figure 2? \nYou should compare your work with more recent SOTA to improve Soundness.\n\nmissing_references: -\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "60E8WXFHcE",
        "length": 390,
        "human_text": "paper_topic_and_main_contributions: Multi-intent detection is closer to the reality of complex situations and is more challenging than single-intent detection. This work proposes PCMID to model the semantics between individual utterance tokens and the intent label words, and achieves state-of-the-art performance on four datasets. In addition, this work constructs a multi-intent detection dataset\n\nreasons_to_accept: 1.The structure of the paper is clear and easy to read; 2.The results of the experiment are detailed and the results have been analyzed in detail in various ways.\n\nreasons_to_reject: 1. Failed to write about the shortcomings of previous multi-intent detection work in the abstract, simply stating that multi-intent detection missions are closer to the real world, the abstract failed to excite me; 2. The introduction does not summarize the work of the thesis well enough to indicate the innovative nature of the thesis; 3. The paper is redundant, with large parts of previous work (e.g. 3.3, etc.) in the introduction to the methodology, which should focus on the problem to be solved and how it was solved; 4. The introduction mentions that PCMID is lighter than previous frameworks, which should be reflected in the later paper by reporting the number of different model parameters and highlighting the advantages of PCMID; 5. There are some errors in detail, such as: spelling of words, formulas without punctuation, and full names that appeared in the previous text do not need to be repeated and explained when abbreviations are used in the later text. It is recommended that you read the entire text carefully and check it thoroughly.\n\nethical_concerns: No\n\nquestions_for_the_authors: When selecting the baseline, you chose a number of models that do joint training for intent detection and slot filling, so why not a few more models that only do multiple intent detection?\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "PDhXnojEYM",
        "length": 451,
        "human_text": "paper_topic_and_main_contributions: This paper talks about an important issue of real-world dialog systems, where the utterances can be crafted with multi-intent which may or may not be nested. This paper showcases a new way of doing multi-intent detection \u2013 PCMID that is a novel Multi-Intent Detection framework enabled by Prototypical Contrastive Learning under a supervised setting. The authors have clearly showcased their approach and explained the architecture. This work has been tested on 3 public datasets \u2013 MixATIS, MixSNIPS, and FSPS; and one private dataset CREDIT16. The authors have demonstrated slight to moderate improvement across all the datasets using this approach.\n\nreasons_to_accept: \u2022\tThe authors did a good job explaining the approach in detail, and the paper is written cleanly with very few minor typos \u2013 such as different types of double quotations.\n\u2022\tThis is a very important problem statement and the conversational agents getting more complex day by day and the utterances becoming trickier.  \u2022\tThe method is novel.\n\nreasons_to_reject: \u2022\tIt\u2019s better to have a dataflow diagram with examples from any of the datasets used, which helps to follow the paper better.\n\u2022\tIt\u2019s a supervised method, which is very specific to the tasks. It will be very hard to use it in any other tasks, as it requires an extensive amount of data-collection and annotation. No comparison is given with LLMs performance on these datasets or even zeroshot comparisons across the datasets (e.g., trained on mixATIS and tested on MixSNIPS). It helps to understand the robustness of this approach.   \u2022\tPerformance improvements are very slight on all the datasets except MixATIS.\n\nquestions_for_the_authors: Please address the points under weakness along with the following questions: \u2022\tCould you please explain how the slots are processed? as I found most of the discussions are based on intents. The examples from table-3 are also only intents. So curious to know, how slots are extracted.\n\u2022\tQualitative studies over predicted outputs with examples (not only on numbers) will be better to understand\n\ntypos_grammar_style_and_presentation_improvements: - double quotation marks are not consistent - Many concepts can be moved from section 3 to a separate Background section, such as SCL.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "124_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_124_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7298666666666667,
      "max_similarity": 0.7471,
      "avg_coverage": 0.3782666666666667,
      "max_coverage": 0.4348
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 378,
      "avg_human_length": 425.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 5,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "wmNTaI1J1h",
        "similarity": 0.7471,
        "coverage": 0.3429,
        "human_length": 485,
        "human_text": "paper_topic_and_main_contributions: Human preference judgments act as training data for reward models, and as such are key ingredients for training more aligned language models with RLHF. The authors analyse which concrete factors (e.g. length and factuality) influence one-dimensional human preference judgments over pairs of summaries. For this purpose, the authors use a Bradley-Terry-Luce model. For Reddit and News summaries from a dataset collected by OpenAI, they find that the most favoured factors vary by task and genre, whereas the least-favoured factors such as excessive off-focus content and hallucinated facts, tend to be consistent.\n\nreasons_to_accept: 1. Explaining human preference judgments is an open challenge that is extremely relevant to the current LLM development paradigm. \n2. The use of the BTL model is a clever and novel idea that seems to work well. Since it is an ex-post method it can be applied to many existing datasets. \n3. The chosen factors appear well-motivated and comprehensive.\n\nreasons_to_reject: I do not see any major issues with this paper. However, there are some things that I would like to see addressed in the final version.\n1. The framing (human preference judgments) is perhaps a bit more general than the analysis (human preference judgments over summaries in data from one particular paper). It would be nice to be more clear and open about this. \n2. I would also appreciate more discussion about whose preferences are analysed here: for example, what is known about the evaluators who made these judgments? I know that for other preference datasets, a very small number of US-based crowdworkers was responsible for the vast majority of preference judgments. This is important to delineate the generalisabilty of the findings. \n3. Lastly, it would be great to see some more discussion of the emerging literature on more fine-grained preference feedback like https://arxiv.org/abs/2306.01693 or natural language feedback like https://arxiv.org/abs/2204.14146, since they offer an alternative to having to explain one-dimensional feedback.\n\nquestions_for_the_authors: 1. Have you considered including nonsensical factors to evaluate the robustness of your approach? For example, something like \u201ccount of letter t\u2019s in the summary\u201d. This should not be a most- or least-favoured factor if the method is robust. \n2. Is there any way for you to quantify the goodness-of-fit of your model, akin to an R-squared that would quantify the amount of variance explained by a regression model? \n3. Relatedly, can you say anything about the significance of the factors estimates you obtain?\n\ntypos_grammar_style_and_presentation_improvements: Since you are using GPT API models throughout the article, please indicate somewhere when exactly and what version exactly you used.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "FV4WJSCQAJ",
        "similarity": 0.7325,
        "coverage": 0.4348,
        "human_length": 580,
        "human_text": "paper_topic_and_main_contributions: The paper does an in depth analysis of the factors that influence human preference judgment. A lot of LLM work recently has been using human preferences but there is limited research probing the preferences in detail. \nThe main contributions are: - Using gpt4 the authors asses system outputs based on a set of predefined factors - fluency, clarity, coverage, alignment etc - Using the factors above, they analyze how the factors influence human preferences and show which factors are most favored vs which are least favored - this can possible help in data curation for preference modeling going forward  - Dataset with an analysis of the factors and estimates\n\nreasons_to_accept: There is a lot of work recently that builds on human preferences, if humans prefer A over B but unfortunately generally datasets do not extend into knowing *why* a human prefers A to B. This paper is a step into that direction and they do an in depth analysis of which factors play an important role and how to get an estimate of their importance from the preferences already marked by humans. \nHaving a more fine grained analysis like this can help the community come up with better datasets/guidelines/think more about how to better collect preference data.\n\nreasons_to_reject: - Human evaluation of the factors is missing - it would be nice to see if the BTL weighting of the factors is similar to how the humans weigh these factors/if humans agree with the estimates - An example, maybe in the appendix of what the values look like for a pair of examples would have been nice to visualize this in practice - In section 3, authors say that they aim to establish a robust sample collection practise for reward models but don\u2019t explicitly address this point in the paper again\n\nquestions_for_the_authors: a. In the BTL modeling step, the authors say they do not consider factors if the same factor is present in the outputs being compared. \nFor a pair of output, with the same factor, is it possible for the two to still have noticeable difference in the factor? Mostly because it is possible that the two lie on the opposite ends of that quartile range? Eg: for output a and b if they both have \u201csrc-cov-medium\u201d it is possible that output a lies to the lower end of the range and output b lies to the upper end of the range.  b. For pairs with a gap of only one or two splits in the axis-evals-reddit dataset, do you see a lot of factors being canceled out? I'm wondering if the estimates of factors are primarily because of the pairs with a higher gap between them while the lower gap samples [which are similar to each other] contribute less.\nc. What is the range of some of the factors that get divided into quantiles?\nd. why do you think gpt-4 doesn't do well on \"similar\" summaries? do you think this can be improved by incorporating the factors in some way?\ne. what was the coverage of ACU extraction by GPT4?\n\nmissing_references: NA\n\ntypos_grammar_style_and_presentation_improvements: NA\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "tQN315Xh2C",
        "similarity": 0.71,
        "coverage": 0.3571,
        "human_length": 212,
        "human_text": "paper_topic_and_main_contributions: The contributions in this paper are:  - the authors conducted comprehensive analyses of a collection of human comparisons to identify key factors that may influence human judgment.  - They used GPT models assess system outputs both qualitatively and quantitatively.  - They examined  fluency, clarity, coverage, alignment with the original text\u2019s intent and style, and detect hallucinations based on atomic facts. \n-The study could enhance the reliability of human evaluations.\n\nreasons_to_accept: This paper provides a comprehensive framework to study how different key factors may influence human judgment that was used to assess and sometime train generative models. The framework could contribute to other area as well given the rising interesting in generative models\n\nreasons_to_reject: This study is more on the qualitative analysis side and not very technical. But it should be fine for the EMNLP community.\n\nquestions_for_the_authors: This reviewer actually would like to see how the human judger themselves impact the results, for example, female judger vs. male judger, low-educated judger vs. high-educated judger. Authors could discuss it probably.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "wmNTaI1J1h",
        "length": 485,
        "human_text": "paper_topic_and_main_contributions: Human preference judgments act as training data for reward models, and as such are key ingredients for training more aligned language models with RLHF. The authors analyse which concrete factors (e.g. length and factuality) influence one-dimensional human preference judgments over pairs of summaries. For this purpose, the authors use a Bradley-Terry-Luce model. For Reddit and News summaries from a dataset collected by OpenAI, they find that the most favoured factors vary by task and genre, whereas the least-favoured factors such as excessive off-focus content and hallucinated facts, tend to be consistent.\n\nreasons_to_accept: 1. Explaining human preference judgments is an open challenge that is extremely relevant to the current LLM development paradigm. \n2. The use of the BTL model is a clever and novel idea that seems to work well. Since it is an ex-post method it can be applied to many existing datasets. \n3. The chosen factors appear well-motivated and comprehensive.\n\nreasons_to_reject: I do not see any major issues with this paper. However, there are some things that I would like to see addressed in the final version.\n1. The framing (human preference judgments) is perhaps a bit more general than the analysis (human preference judgments over summaries in data from one particular paper). It would be nice to be more clear and open about this. \n2. I would also appreciate more discussion about whose preferences are analysed here: for example, what is known about the evaluators who made these judgments? I know that for other preference datasets, a very small number of US-based crowdworkers was responsible for the vast majority of preference judgments. This is important to delineate the generalisabilty of the findings. \n3. Lastly, it would be great to see some more discussion of the emerging literature on more fine-grained preference feedback like https://arxiv.org/abs/2306.01693 or natural language feedback like https://arxiv.org/abs/2204.14146, since they offer an alternative to having to explain one-dimensional feedback.\n\nquestions_for_the_authors: 1. Have you considered including nonsensical factors to evaluate the robustness of your approach? For example, something like \u201ccount of letter t\u2019s in the summary\u201d. This should not be a most- or least-favoured factor if the method is robust. \n2. Is there any way for you to quantify the goodness-of-fit of your model, akin to an R-squared that would quantify the amount of variance explained by a regression model? \n3. Relatedly, can you say anything about the significance of the factors estimates you obtain?\n\ntypos_grammar_style_and_presentation_improvements: Since you are using GPT API models throughout the article, please indicate somewhere when exactly and what version exactly you used.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "FV4WJSCQAJ",
        "length": 580,
        "human_text": "paper_topic_and_main_contributions: The paper does an in depth analysis of the factors that influence human preference judgment. A lot of LLM work recently has been using human preferences but there is limited research probing the preferences in detail. \nThe main contributions are: - Using gpt4 the authors asses system outputs based on a set of predefined factors - fluency, clarity, coverage, alignment etc - Using the factors above, they analyze how the factors influence human preferences and show which factors are most favored vs which are least favored - this can possible help in data curation for preference modeling going forward  - Dataset with an analysis of the factors and estimates\n\nreasons_to_accept: There is a lot of work recently that builds on human preferences, if humans prefer A over B but unfortunately generally datasets do not extend into knowing *why* a human prefers A to B. This paper is a step into that direction and they do an in depth analysis of which factors play an important role and how to get an estimate of their importance from the preferences already marked by humans. \nHaving a more fine grained analysis like this can help the community come up with better datasets/guidelines/think more about how to better collect preference data.\n\nreasons_to_reject: - Human evaluation of the factors is missing - it would be nice to see if the BTL weighting of the factors is similar to how the humans weigh these factors/if humans agree with the estimates - An example, maybe in the appendix of what the values look like for a pair of examples would have been nice to visualize this in practice - In section 3, authors say that they aim to establish a robust sample collection practise for reward models but don\u2019t explicitly address this point in the paper again\n\nquestions_for_the_authors: a. In the BTL modeling step, the authors say they do not consider factors if the same factor is present in the outputs being compared. \nFor a pair of output, with the same factor, is it possible for the two to still have noticeable difference in the factor? Mostly because it is possible that the two lie on the opposite ends of that quartile range? Eg: for output a and b if they both have \u201csrc-cov-medium\u201d it is possible that output a lies to the lower end of the range and output b lies to the upper end of the range.  b. For pairs with a gap of only one or two splits in the axis-evals-reddit dataset, do you see a lot of factors being canceled out? I'm wondering if the estimates of factors are primarily because of the pairs with a higher gap between them while the lower gap samples [which are similar to each other] contribute less.\nc. What is the range of some of the factors that get divided into quantiles?\nd. why do you think gpt-4 doesn't do well on \"similar\" summaries? do you think this can be improved by incorporating the factors in some way?\ne. what was the coverage of ACU extraction by GPT4?\n\nmissing_references: NA\n\ntypos_grammar_style_and_presentation_improvements: NA\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "tQN315Xh2C",
        "length": 212,
        "human_text": "paper_topic_and_main_contributions: The contributions in this paper are:  - the authors conducted comprehensive analyses of a collection of human comparisons to identify key factors that may influence human judgment.  - They used GPT models assess system outputs both qualitatively and quantitatively.  - They examined  fluency, clarity, coverage, alignment with the original text\u2019s intent and style, and detect hallucinations based on atomic facts. \n-The study could enhance the reliability of human evaluations.\n\nreasons_to_accept: This paper provides a comprehensive framework to study how different key factors may influence human judgment that was used to assess and sometime train generative models. The framework could contribute to other area as well given the rising interesting in generative models\n\nreasons_to_reject: This study is more on the qualitative analysis side and not very technical. But it should be fine for the EMNLP community.\n\nquestions_for_the_authors: This reviewer actually would like to see how the human judger themselves impact the results, for example, female judger vs. male judger, low-educated judger vs. high-educated judger. Authors could discuss it probably.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "124_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_124_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7315333333333335,
      "max_similarity": 0.7457,
      "avg_coverage": 0.3782666666666667,
      "max_coverage": 0.4348
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 432,
      "avg_human_length": 425.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 5,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "wmNTaI1J1h",
        "similarity": 0.7457,
        "coverage": 0.3429,
        "human_length": 485,
        "human_text": "paper_topic_and_main_contributions: Human preference judgments act as training data for reward models, and as such are key ingredients for training more aligned language models with RLHF. The authors analyse which concrete factors (e.g. length and factuality) influence one-dimensional human preference judgments over pairs of summaries. For this purpose, the authors use a Bradley-Terry-Luce model. For Reddit and News summaries from a dataset collected by OpenAI, they find that the most favoured factors vary by task and genre, whereas the least-favoured factors such as excessive off-focus content and hallucinated facts, tend to be consistent.\n\nreasons_to_accept: 1. Explaining human preference judgments is an open challenge that is extremely relevant to the current LLM development paradigm. \n2. The use of the BTL model is a clever and novel idea that seems to work well. Since it is an ex-post method it can be applied to many existing datasets. \n3. The chosen factors appear well-motivated and comprehensive.\n\nreasons_to_reject: I do not see any major issues with this paper. However, there are some things that I would like to see addressed in the final version.\n1. The framing (human preference judgments) is perhaps a bit more general than the analysis (human preference judgments over summaries in data from one particular paper). It would be nice to be more clear and open about this. \n2. I would also appreciate more discussion about whose preferences are analysed here: for example, what is known about the evaluators who made these judgments? I know that for other preference datasets, a very small number of US-based crowdworkers was responsible for the vast majority of preference judgments. This is important to delineate the generalisabilty of the findings. \n3. Lastly, it would be great to see some more discussion of the emerging literature on more fine-grained preference feedback like https://arxiv.org/abs/2306.01693 or natural language feedback like https://arxiv.org/abs/2204.14146, since they offer an alternative to having to explain one-dimensional feedback.\n\nquestions_for_the_authors: 1. Have you considered including nonsensical factors to evaluate the robustness of your approach? For example, something like \u201ccount of letter t\u2019s in the summary\u201d. This should not be a most- or least-favoured factor if the method is robust. \n2. Is there any way for you to quantify the goodness-of-fit of your model, akin to an R-squared that would quantify the amount of variance explained by a regression model? \n3. Relatedly, can you say anything about the significance of the factors estimates you obtain?\n\ntypos_grammar_style_and_presentation_improvements: Since you are using GPT API models throughout the article, please indicate somewhere when exactly and what version exactly you used.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "FV4WJSCQAJ",
        "similarity": 0.7333,
        "coverage": 0.4348,
        "human_length": 580,
        "human_text": "paper_topic_and_main_contributions: The paper does an in depth analysis of the factors that influence human preference judgment. A lot of LLM work recently has been using human preferences but there is limited research probing the preferences in detail. \nThe main contributions are: - Using gpt4 the authors asses system outputs based on a set of predefined factors - fluency, clarity, coverage, alignment etc - Using the factors above, they analyze how the factors influence human preferences and show which factors are most favored vs which are least favored - this can possible help in data curation for preference modeling going forward  - Dataset with an analysis of the factors and estimates\n\nreasons_to_accept: There is a lot of work recently that builds on human preferences, if humans prefer A over B but unfortunately generally datasets do not extend into knowing *why* a human prefers A to B. This paper is a step into that direction and they do an in depth analysis of which factors play an important role and how to get an estimate of their importance from the preferences already marked by humans. \nHaving a more fine grained analysis like this can help the community come up with better datasets/guidelines/think more about how to better collect preference data.\n\nreasons_to_reject: - Human evaluation of the factors is missing - it would be nice to see if the BTL weighting of the factors is similar to how the humans weigh these factors/if humans agree with the estimates - An example, maybe in the appendix of what the values look like for a pair of examples would have been nice to visualize this in practice - In section 3, authors say that they aim to establish a robust sample collection practise for reward models but don\u2019t explicitly address this point in the paper again\n\nquestions_for_the_authors: a. In the BTL modeling step, the authors say they do not consider factors if the same factor is present in the outputs being compared. \nFor a pair of output, with the same factor, is it possible for the two to still have noticeable difference in the factor? Mostly because it is possible that the two lie on the opposite ends of that quartile range? Eg: for output a and b if they both have \u201csrc-cov-medium\u201d it is possible that output a lies to the lower end of the range and output b lies to the upper end of the range.  b. For pairs with a gap of only one or two splits in the axis-evals-reddit dataset, do you see a lot of factors being canceled out? I'm wondering if the estimates of factors are primarily because of the pairs with a higher gap between them while the lower gap samples [which are similar to each other] contribute less.\nc. What is the range of some of the factors that get divided into quantiles?\nd. why do you think gpt-4 doesn't do well on \"similar\" summaries? do you think this can be improved by incorporating the factors in some way?\ne. what was the coverage of ACU extraction by GPT4?\n\nmissing_references: NA\n\ntypos_grammar_style_and_presentation_improvements: NA\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "tQN315Xh2C",
        "similarity": 0.7156,
        "coverage": 0.3571,
        "human_length": 212,
        "human_text": "paper_topic_and_main_contributions: The contributions in this paper are:  - the authors conducted comprehensive analyses of a collection of human comparisons to identify key factors that may influence human judgment.  - They used GPT models assess system outputs both qualitatively and quantitatively.  - They examined  fluency, clarity, coverage, alignment with the original text\u2019s intent and style, and detect hallucinations based on atomic facts. \n-The study could enhance the reliability of human evaluations.\n\nreasons_to_accept: This paper provides a comprehensive framework to study how different key factors may influence human judgment that was used to assess and sometime train generative models. The framework could contribute to other area as well given the rising interesting in generative models\n\nreasons_to_reject: This study is more on the qualitative analysis side and not very technical. But it should be fine for the EMNLP community.\n\nquestions_for_the_authors: This reviewer actually would like to see how the human judger themselves impact the results, for example, female judger vs. male judger, low-educated judger vs. high-educated judger. Authors could discuss it probably.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "wmNTaI1J1h",
        "length": 485,
        "human_text": "paper_topic_and_main_contributions: Human preference judgments act as training data for reward models, and as such are key ingredients for training more aligned language models with RLHF. The authors analyse which concrete factors (e.g. length and factuality) influence one-dimensional human preference judgments over pairs of summaries. For this purpose, the authors use a Bradley-Terry-Luce model. For Reddit and News summaries from a dataset collected by OpenAI, they find that the most favoured factors vary by task and genre, whereas the least-favoured factors such as excessive off-focus content and hallucinated facts, tend to be consistent.\n\nreasons_to_accept: 1. Explaining human preference judgments is an open challenge that is extremely relevant to the current LLM development paradigm. \n2. The use of the BTL model is a clever and novel idea that seems to work well. Since it is an ex-post method it can be applied to many existing datasets. \n3. The chosen factors appear well-motivated and comprehensive.\n\nreasons_to_reject: I do not see any major issues with this paper. However, there are some things that I would like to see addressed in the final version.\n1. The framing (human preference judgments) is perhaps a bit more general than the analysis (human preference judgments over summaries in data from one particular paper). It would be nice to be more clear and open about this. \n2. I would also appreciate more discussion about whose preferences are analysed here: for example, what is known about the evaluators who made these judgments? I know that for other preference datasets, a very small number of US-based crowdworkers was responsible for the vast majority of preference judgments. This is important to delineate the generalisabilty of the findings. \n3. Lastly, it would be great to see some more discussion of the emerging literature on more fine-grained preference feedback like https://arxiv.org/abs/2306.01693 or natural language feedback like https://arxiv.org/abs/2204.14146, since they offer an alternative to having to explain one-dimensional feedback.\n\nquestions_for_the_authors: 1. Have you considered including nonsensical factors to evaluate the robustness of your approach? For example, something like \u201ccount of letter t\u2019s in the summary\u201d. This should not be a most- or least-favoured factor if the method is robust. \n2. Is there any way for you to quantify the goodness-of-fit of your model, akin to an R-squared that would quantify the amount of variance explained by a regression model? \n3. Relatedly, can you say anything about the significance of the factors estimates you obtain?\n\ntypos_grammar_style_and_presentation_improvements: Since you are using GPT API models throughout the article, please indicate somewhere when exactly and what version exactly you used.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "FV4WJSCQAJ",
        "length": 580,
        "human_text": "paper_topic_and_main_contributions: The paper does an in depth analysis of the factors that influence human preference judgment. A lot of LLM work recently has been using human preferences but there is limited research probing the preferences in detail. \nThe main contributions are: - Using gpt4 the authors asses system outputs based on a set of predefined factors - fluency, clarity, coverage, alignment etc - Using the factors above, they analyze how the factors influence human preferences and show which factors are most favored vs which are least favored - this can possible help in data curation for preference modeling going forward  - Dataset with an analysis of the factors and estimates\n\nreasons_to_accept: There is a lot of work recently that builds on human preferences, if humans prefer A over B but unfortunately generally datasets do not extend into knowing *why* a human prefers A to B. This paper is a step into that direction and they do an in depth analysis of which factors play an important role and how to get an estimate of their importance from the preferences already marked by humans. \nHaving a more fine grained analysis like this can help the community come up with better datasets/guidelines/think more about how to better collect preference data.\n\nreasons_to_reject: - Human evaluation of the factors is missing - it would be nice to see if the BTL weighting of the factors is similar to how the humans weigh these factors/if humans agree with the estimates - An example, maybe in the appendix of what the values look like for a pair of examples would have been nice to visualize this in practice - In section 3, authors say that they aim to establish a robust sample collection practise for reward models but don\u2019t explicitly address this point in the paper again\n\nquestions_for_the_authors: a. In the BTL modeling step, the authors say they do not consider factors if the same factor is present in the outputs being compared. \nFor a pair of output, with the same factor, is it possible for the two to still have noticeable difference in the factor? Mostly because it is possible that the two lie on the opposite ends of that quartile range? Eg: for output a and b if they both have \u201csrc-cov-medium\u201d it is possible that output a lies to the lower end of the range and output b lies to the upper end of the range.  b. For pairs with a gap of only one or two splits in the axis-evals-reddit dataset, do you see a lot of factors being canceled out? I'm wondering if the estimates of factors are primarily because of the pairs with a higher gap between them while the lower gap samples [which are similar to each other] contribute less.\nc. What is the range of some of the factors that get divided into quantiles?\nd. why do you think gpt-4 doesn't do well on \"similar\" summaries? do you think this can be improved by incorporating the factors in some way?\ne. what was the coverage of ACU extraction by GPT4?\n\nmissing_references: NA\n\ntypos_grammar_style_and_presentation_improvements: NA\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "tQN315Xh2C",
        "length": 212,
        "human_text": "paper_topic_and_main_contributions: The contributions in this paper are:  - the authors conducted comprehensive analyses of a collection of human comparisons to identify key factors that may influence human judgment.  - They used GPT models assess system outputs both qualitatively and quantitatively.  - They examined  fluency, clarity, coverage, alignment with the original text\u2019s intent and style, and detect hallucinations based on atomic facts. \n-The study could enhance the reliability of human evaluations.\n\nreasons_to_accept: This paper provides a comprehensive framework to study how different key factors may influence human judgment that was used to assess and sometime train generative models. The framework could contribute to other area as well given the rising interesting in generative models\n\nreasons_to_reject: This study is more on the qualitative analysis side and not very technical. But it should be fine for the EMNLP community.\n\nquestions_for_the_authors: This reviewer actually would like to see how the human judger themselves impact the results, for example, female judger vs. male judger, low-educated judger vs. high-educated judger. Authors could discuss it probably.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "161_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_161_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7151666666666667,
      "max_similarity": 0.7339,
      "avg_coverage": 0.6690333333333333,
      "max_coverage": 0.75
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 540,
      "avg_human_length": 277.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "qkwc9rewRI",
        "similarity": 0.7339,
        "coverage": 0.5238,
        "human_length": 324,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a large dataset, WikiWeb2M, for studying multimodal webpage understanding. Additionally, the authors propose a novel attention mechanism, Prefix Global, to enhance performance and efficiency. Extensive experiments and analyses demonstrate the superiority of both the dataset and the proposed method.\n\nreasons_to_accept: (1) The paper is very well-written and easy to follow. The organization of the manuscript effectively follows their contributions.\n(2) This paper's contributions, including both the dataset and the new attention mechanism, will significantly benefit the development of multimodal webpage understanding.\n(3) The experiments and analyses are conducted comprehensively, encompassing ablation studies and efficiency analysis.\n(4) Figure 4 demonstrates that the proposed method does not encounter performance degradation as the input sequence length increases, which is a promising solution for handling long texts.\n\nreasons_to_reject: (1) Other evaluation metrics, especially those [1,2] that reflect human preference and those designed for multimodal evaluation [3,4], should be reported (see missing references section).\n\nquestions_for_the_authors: (1) Is there any noise present in the dataset parsing process? If so, including those noises and discussing their types will be beneficial.\n(2) As mentioned in line 236, the method utilizes the earlier content in a body of text. Have the authors explored other parts, such as the last content in a body of text (which is sometimes the conclusion)?\n\nmissing_references: [1] Bertscore: Evaluating text generation with bert, ICLR 2018.\n[2] BLEURT: Learning Robust Metrics for Text Generation, ACL 2020.\n[3] CLIPScore: A Reference-free Evaluation Metric for Image Captioning, EMNLP 2021.\n[4] EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching, CVPR 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "ixVVHzIEhA",
        "similarity": 0.6889,
        "coverage": 0.75,
        "human_length": 288,
        "human_text": "paper_topic_and_main_contributions: The paper introduces the Wikipedia Webpage suite (WikiWeb2M) dataset, which consists of 2 million webpages, to study multimodal webpage understanding. The key feature of WikiWeb2M is that it retains the complete web content, instead of information filtered for a particular use case. WikiWeb2M enables a systematic study of several generative modeling tasks with text as the output: page description generation, section summarization, and contextual image captioning. To address the long context problem, the authors also propose a novel method called Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context. The experimental results demonstrate that the new annotations from WikiWeb2M improve task performance compared to prior data. Comprehensive analyses of sequence length, input features, and model size were also conducted.\n\nreasons_to_accept: 1. WikiWeb2M is a useful resource for multimodal webpage understanding. The inclusion of both image-caption pairs and long text articles in one place makes it a comprehensive dataset for various generative tasks. \n2. The proposed Prefix Global method is well-motivated. It helps lower computational complexity and retain performance. \n3. The experimental results demonstrate the utility of WikiWeb2M annotations. Comprehensive analyses were also conducted.\n\nreasons_to_reject: 1. It would be interesting to see the performance of LLMs with vision input (e.g., llava) on the proposed dataset.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "zeTKXLVeEZ",
        "similarity": 0.7227,
        "coverage": 0.7333,
        "human_length": 219,
        "human_text": "paper_topic_and_main_contributions: The submission introduces three new generative tasks: page description generation, section summarization, and contextual image caption to verify multimodel webpage understanding. A dataset of 2M pages named WikiWeb2M also be introduced. The authors also design a novel attention mechanism Prefix GLobal to select the most relevant image and text content.\n\nreasons_to_accept: It is well written with detailed analysis. \nIt provides a large dedicated dataset  of MultimodalWebpage with a trained model on this dataset.\n\nreasons_to_reject: The tasks they proposed are either multi-modal summarization or captioning and don't have much novelty.\nThe contribution of structural metadata signaling is not clear.\n\nquestions_for_the_authors: What are the contributions of structural data work in different tasks?  As the first sentence can be a reasonable summary, how do you know if the model will directly borrow the first sentence to make the task trivial?\n\nmissing_references: Pauwels, Luc. \" A multimodal framework for analyzing websites as cultural expressions.\" Journal of Computer-Mediated Communication 17.3 (2012): 247-265.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "qkwc9rewRI",
        "length": 324,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a large dataset, WikiWeb2M, for studying multimodal webpage understanding. Additionally, the authors propose a novel attention mechanism, Prefix Global, to enhance performance and efficiency. Extensive experiments and analyses demonstrate the superiority of both the dataset and the proposed method.\n\nreasons_to_accept: (1) The paper is very well-written and easy to follow. The organization of the manuscript effectively follows their contributions.\n(2) This paper's contributions, including both the dataset and the new attention mechanism, will significantly benefit the development of multimodal webpage understanding.\n(3) The experiments and analyses are conducted comprehensively, encompassing ablation studies and efficiency analysis.\n(4) Figure 4 demonstrates that the proposed method does not encounter performance degradation as the input sequence length increases, which is a promising solution for handling long texts.\n\nreasons_to_reject: (1) Other evaluation metrics, especially those [1,2] that reflect human preference and those designed for multimodal evaluation [3,4], should be reported (see missing references section).\n\nquestions_for_the_authors: (1) Is there any noise present in the dataset parsing process? If so, including those noises and discussing their types will be beneficial.\n(2) As mentioned in line 236, the method utilizes the earlier content in a body of text. Have the authors explored other parts, such as the last content in a body of text (which is sometimes the conclusion)?\n\nmissing_references: [1] Bertscore: Evaluating text generation with bert, ICLR 2018.\n[2] BLEURT: Learning Robust Metrics for Text Generation, ACL 2020.\n[3] CLIPScore: A Reference-free Evaluation Metric for Image Captioning, EMNLP 2021.\n[4] EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching, CVPR 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "ixVVHzIEhA",
        "length": 288,
        "human_text": "paper_topic_and_main_contributions: The paper introduces the Wikipedia Webpage suite (WikiWeb2M) dataset, which consists of 2 million webpages, to study multimodal webpage understanding. The key feature of WikiWeb2M is that it retains the complete web content, instead of information filtered for a particular use case. WikiWeb2M enables a systematic study of several generative modeling tasks with text as the output: page description generation, section summarization, and contextual image captioning. To address the long context problem, the authors also propose a novel method called Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context. The experimental results demonstrate that the new annotations from WikiWeb2M improve task performance compared to prior data. Comprehensive analyses of sequence length, input features, and model size were also conducted.\n\nreasons_to_accept: 1. WikiWeb2M is a useful resource for multimodal webpage understanding. The inclusion of both image-caption pairs and long text articles in one place makes it a comprehensive dataset for various generative tasks. \n2. The proposed Prefix Global method is well-motivated. It helps lower computational complexity and retain performance. \n3. The experimental results demonstrate the utility of WikiWeb2M annotations. Comprehensive analyses were also conducted.\n\nreasons_to_reject: 1. It would be interesting to see the performance of LLMs with vision input (e.g., llava) on the proposed dataset.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "zeTKXLVeEZ",
        "length": 219,
        "human_text": "paper_topic_and_main_contributions: The submission introduces three new generative tasks: page description generation, section summarization, and contextual image caption to verify multimodel webpage understanding. A dataset of 2M pages named WikiWeb2M also be introduced. The authors also design a novel attention mechanism Prefix GLobal to select the most relevant image and text content.\n\nreasons_to_accept: It is well written with detailed analysis. \nIt provides a large dedicated dataset  of MultimodalWebpage with a trained model on this dataset.\n\nreasons_to_reject: The tasks they proposed are either multi-modal summarization or captioning and don't have much novelty.\nThe contribution of structural metadata signaling is not clear.\n\nquestions_for_the_authors: What are the contributions of structural data work in different tasks?  As the first sentence can be a reasonable summary, how do you know if the model will directly borrow the first sentence to make the task trivial?\n\nmissing_references: Pauwels, Luc. \" A multimodal framework for analyzing websites as cultural expressions.\" Journal of Computer-Mediated Communication 17.3 (2012): 247-265.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "161_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_161_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7372333333333335,
      "max_similarity": 0.7569,
      "avg_coverage": 0.5095333333333333,
      "max_coverage": 0.6
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 335,
      "avg_human_length": 277.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 4,
      "suggestions_count": 4
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "qkwc9rewRI",
        "similarity": 0.7569,
        "coverage": 0.4286,
        "human_length": 324,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a large dataset, WikiWeb2M, for studying multimodal webpage understanding. Additionally, the authors propose a novel attention mechanism, Prefix Global, to enhance performance and efficiency. Extensive experiments and analyses demonstrate the superiority of both the dataset and the proposed method.\n\nreasons_to_accept: (1) The paper is very well-written and easy to follow. The organization of the manuscript effectively follows their contributions.\n(2) This paper's contributions, including both the dataset and the new attention mechanism, will significantly benefit the development of multimodal webpage understanding.\n(3) The experiments and analyses are conducted comprehensively, encompassing ablation studies and efficiency analysis.\n(4) Figure 4 demonstrates that the proposed method does not encounter performance degradation as the input sequence length increases, which is a promising solution for handling long texts.\n\nreasons_to_reject: (1) Other evaluation metrics, especially those [1,2] that reflect human preference and those designed for multimodal evaluation [3,4], should be reported (see missing references section).\n\nquestions_for_the_authors: (1) Is there any noise present in the dataset parsing process? If so, including those noises and discussing their types will be beneficial.\n(2) As mentioned in line 236, the method utilizes the earlier content in a body of text. Have the authors explored other parts, such as the last content in a body of text (which is sometimes the conclusion)?\n\nmissing_references: [1] Bertscore: Evaluating text generation with bert, ICLR 2018.\n[2] BLEURT: Learning Robust Metrics for Text Generation, ACL 2020.\n[3] CLIPScore: A Reference-free Evaluation Metric for Image Captioning, EMNLP 2021.\n[4] EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching, CVPR 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "ixVVHzIEhA",
        "similarity": 0.7133,
        "coverage": 0.5,
        "human_length": 288,
        "human_text": "paper_topic_and_main_contributions: The paper introduces the Wikipedia Webpage suite (WikiWeb2M) dataset, which consists of 2 million webpages, to study multimodal webpage understanding. The key feature of WikiWeb2M is that it retains the complete web content, instead of information filtered for a particular use case. WikiWeb2M enables a systematic study of several generative modeling tasks with text as the output: page description generation, section summarization, and contextual image captioning. To address the long context problem, the authors also propose a novel method called Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context. The experimental results demonstrate that the new annotations from WikiWeb2M improve task performance compared to prior data. Comprehensive analyses of sequence length, input features, and model size were also conducted.\n\nreasons_to_accept: 1. WikiWeb2M is a useful resource for multimodal webpage understanding. The inclusion of both image-caption pairs and long text articles in one place makes it a comprehensive dataset for various generative tasks. \n2. The proposed Prefix Global method is well-motivated. It helps lower computational complexity and retain performance. \n3. The experimental results demonstrate the utility of WikiWeb2M annotations. Comprehensive analyses were also conducted.\n\nreasons_to_reject: 1. It would be interesting to see the performance of LLMs with vision input (e.g., llava) on the proposed dataset.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "zeTKXLVeEZ",
        "similarity": 0.7415,
        "coverage": 0.6,
        "human_length": 219,
        "human_text": "paper_topic_and_main_contributions: The submission introduces three new generative tasks: page description generation, section summarization, and contextual image caption to verify multimodel webpage understanding. A dataset of 2M pages named WikiWeb2M also be introduced. The authors also design a novel attention mechanism Prefix GLobal to select the most relevant image and text content.\n\nreasons_to_accept: It is well written with detailed analysis. \nIt provides a large dedicated dataset  of MultimodalWebpage with a trained model on this dataset.\n\nreasons_to_reject: The tasks they proposed are either multi-modal summarization or captioning and don't have much novelty.\nThe contribution of structural metadata signaling is not clear.\n\nquestions_for_the_authors: What are the contributions of structural data work in different tasks?  As the first sentence can be a reasonable summary, how do you know if the model will directly borrow the first sentence to make the task trivial?\n\nmissing_references: Pauwels, Luc. \" A multimodal framework for analyzing websites as cultural expressions.\" Journal of Computer-Mediated Communication 17.3 (2012): 247-265.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "qkwc9rewRI",
        "length": 324,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a large dataset, WikiWeb2M, for studying multimodal webpage understanding. Additionally, the authors propose a novel attention mechanism, Prefix Global, to enhance performance and efficiency. Extensive experiments and analyses demonstrate the superiority of both the dataset and the proposed method.\n\nreasons_to_accept: (1) The paper is very well-written and easy to follow. The organization of the manuscript effectively follows their contributions.\n(2) This paper's contributions, including both the dataset and the new attention mechanism, will significantly benefit the development of multimodal webpage understanding.\n(3) The experiments and analyses are conducted comprehensively, encompassing ablation studies and efficiency analysis.\n(4) Figure 4 demonstrates that the proposed method does not encounter performance degradation as the input sequence length increases, which is a promising solution for handling long texts.\n\nreasons_to_reject: (1) Other evaluation metrics, especially those [1,2] that reflect human preference and those designed for multimodal evaluation [3,4], should be reported (see missing references section).\n\nquestions_for_the_authors: (1) Is there any noise present in the dataset parsing process? If so, including those noises and discussing their types will be beneficial.\n(2) As mentioned in line 236, the method utilizes the earlier content in a body of text. Have the authors explored other parts, such as the last content in a body of text (which is sometimes the conclusion)?\n\nmissing_references: [1] Bertscore: Evaluating text generation with bert, ICLR 2018.\n[2] BLEURT: Learning Robust Metrics for Text Generation, ACL 2020.\n[3] CLIPScore: A Reference-free Evaluation Metric for Image Captioning, EMNLP 2021.\n[4] EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching, CVPR 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "ixVVHzIEhA",
        "length": 288,
        "human_text": "paper_topic_and_main_contributions: The paper introduces the Wikipedia Webpage suite (WikiWeb2M) dataset, which consists of 2 million webpages, to study multimodal webpage understanding. The key feature of WikiWeb2M is that it retains the complete web content, instead of information filtered for a particular use case. WikiWeb2M enables a systematic study of several generative modeling tasks with text as the output: page description generation, section summarization, and contextual image captioning. To address the long context problem, the authors also propose a novel method called Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context. The experimental results demonstrate that the new annotations from WikiWeb2M improve task performance compared to prior data. Comprehensive analyses of sequence length, input features, and model size were also conducted.\n\nreasons_to_accept: 1. WikiWeb2M is a useful resource for multimodal webpage understanding. The inclusion of both image-caption pairs and long text articles in one place makes it a comprehensive dataset for various generative tasks. \n2. The proposed Prefix Global method is well-motivated. It helps lower computational complexity and retain performance. \n3. The experimental results demonstrate the utility of WikiWeb2M annotations. Comprehensive analyses were also conducted.\n\nreasons_to_reject: 1. It would be interesting to see the performance of LLMs with vision input (e.g., llava) on the proposed dataset.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "zeTKXLVeEZ",
        "length": 219,
        "human_text": "paper_topic_and_main_contributions: The submission introduces three new generative tasks: page description generation, section summarization, and contextual image caption to verify multimodel webpage understanding. A dataset of 2M pages named WikiWeb2M also be introduced. The authors also design a novel attention mechanism Prefix GLobal to select the most relevant image and text content.\n\nreasons_to_accept: It is well written with detailed analysis. \nIt provides a large dedicated dataset  of MultimodalWebpage with a trained model on this dataset.\n\nreasons_to_reject: The tasks they proposed are either multi-modal summarization or captioning and don't have much novelty.\nThe contribution of structural metadata signaling is not clear.\n\nquestions_for_the_authors: What are the contributions of structural data work in different tasks?  As the first sentence can be a reasonable summary, how do you know if the model will directly borrow the first sentence to make the task trivial?\n\nmissing_references: Pauwels, Luc. \" A multimodal framework for analyzing websites as cultural expressions.\" Journal of Computer-Mediated Communication 17.3 (2012): 247-265.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "208_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_208_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7102666666666666,
      "max_similarity": 0.7131,
      "avg_coverage": 0.5473333333333333,
      "max_coverage": 0.7333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 424,
      "avg_human_length": 342.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "8RJ4kyanAD",
        "similarity": 0.7123,
        "coverage": 0.3462,
        "human_length": 564,
        "human_text": "paper_topic_and_main_contributions: I'm not sure whether my understanding is correct because the methodology section of this study is a little bit hard to follow. It seems this study proposed a two-phase model. In the first phase, the author adopted a large language model (LLM, Flan-T5-XXL) to extract a series of preset features in a zero-shot manner. Then,  they trained a simple linear model to generate labels via the extracted features in a supervised manner. As the simple linear model is intrinsically interpretable, the classification result can be regarded as interpretable.\n\nreasons_to_accept: From the point of my view, the main contribution of this study is that the author and their clinical collaborator derived a detailed, hand-crafted prompt set to extract features from Chest X-ray Dataset (Supplementary A, B). I believe these prompts can significantly help the community to establish a structured dataset from unstructured medical text.\n\nreasons_to_reject: The main drawback of this study is that the authors did not prove their model is better than the TF-IDF-based BoW model or other traditional models.\nOf note, Medical text classification basically does not require a model to understand the language. The task performance basically relies on the existence of several key-word. Therefore, we can observe that the BERT-based model did not obtain significantly better performance than TF-IDF (BoW) model, and the performance of proposed model even obtained worse performance than the TF-IDF model (Figure 4, 8). Although the author claimed performance is not their primary objective, it is disappointing that such a computationally expensive model only obtained a worse performance than a model proposed several decades ago. Meanwhile, there is also a topic model (i.e., LDA) or neural network-based topic model that can extract interpretable features and be applied to downstream tasks. It will be better if the author can include them as baselines.\nMeanwhile, the author does not prove the proposed model is more interpretable. In Figure 5, the author claimed that the proposed model is interpretable because coefficient magnitude mass is concentrated on the very top of features, while the TF-IDF masses are distributed uniformly and are hard to interpret. However, we can find that the most positive high-level feature is hypertensive chronic kidney disease, and this finding is also obvious in the TF-IDF model. We can find that ESRD (end stage renal disease), dialysis, and hemodialysis are also on the top of the TF-IDF mass distribution. If we directly use TF-IDF to analyze the data, we can also obtain the conclusion that chronic kidney disease is the most important factor in readmission prediction. Meanwhile, the proposed model claims that the coronary atherosclerotic is a protective factor of readmission, which is unintuitive, but the author does not explain it. Therefore, it seems not inappropriate to claim that the TF-IDF is hard to interpret and the proposed model is more interpretable.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "hZx9fe2s0f",
        "similarity": 0.7131,
        "coverage": 0.7333,
        "human_length": 214,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors utilize the power of LLM for feature extraction in the medical domain. Plus, they investigate how the alignment of simple models' weights affects the prediction. They conduct experiments on medical datasets and provide discussion based on the results.\n\nreasons_to_accept: 1, The task is an interesting task, especially in the medical domain. \n2, They provide comprehensive experiment results and discussion. \n3, This paper is well-written and easy to follow.\n\nreasons_to_reject: 1, I do concern about the novelty of this work. To my best knowledge, most of the methods in this work are existing techniques. \n2, I am wondering if this approach would raise the privacy issue. In the real-world setting, sensitive data would be strictly constrained to be fed into LLMs.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "gqbm67DNLE",
        "similarity": 0.7054,
        "coverage": 0.5625,
        "human_length": 250,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to use LLM to extract information from the raw patient notes, which could serve as the interpretable features for training a simple classifier. This kind of method could achieve compatible results with other methods, while maintaining the interpretability which might be useful for real applications.\n\nreasons_to_accept: 1. The idea to use LLM to generate examples for training is somewhat interesting;  2. The experiments are comprehensive and illustrate the effectiveness of this idea.\n\nreasons_to_reject: 1. I may doubt the necessity of the linear classifier. What if we just let the LLM give the prediction according to the classified results from the templates? Maybe we could give the model several examples to see the few-shot classification performance;  2. From Table2, it seems that the large language models are not so good at prediction these diseases. AUC > 0.5 does not seem to be a good guarantee that the returned features are reasonable.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "8RJ4kyanAD",
        "length": 564,
        "human_text": "paper_topic_and_main_contributions: I'm not sure whether my understanding is correct because the methodology section of this study is a little bit hard to follow. It seems this study proposed a two-phase model. In the first phase, the author adopted a large language model (LLM, Flan-T5-XXL) to extract a series of preset features in a zero-shot manner. Then,  they trained a simple linear model to generate labels via the extracted features in a supervised manner. As the simple linear model is intrinsically interpretable, the classification result can be regarded as interpretable.\n\nreasons_to_accept: From the point of my view, the main contribution of this study is that the author and their clinical collaborator derived a detailed, hand-crafted prompt set to extract features from Chest X-ray Dataset (Supplementary A, B). I believe these prompts can significantly help the community to establish a structured dataset from unstructured medical text.\n\nreasons_to_reject: The main drawback of this study is that the authors did not prove their model is better than the TF-IDF-based BoW model or other traditional models.\nOf note, Medical text classification basically does not require a model to understand the language. The task performance basically relies on the existence of several key-word. Therefore, we can observe that the BERT-based model did not obtain significantly better performance than TF-IDF (BoW) model, and the performance of proposed model even obtained worse performance than the TF-IDF model (Figure 4, 8). Although the author claimed performance is not their primary objective, it is disappointing that such a computationally expensive model only obtained a worse performance than a model proposed several decades ago. Meanwhile, there is also a topic model (i.e., LDA) or neural network-based topic model that can extract interpretable features and be applied to downstream tasks. It will be better if the author can include them as baselines.\nMeanwhile, the author does not prove the proposed model is more interpretable. In Figure 5, the author claimed that the proposed model is interpretable because coefficient magnitude mass is concentrated on the very top of features, while the TF-IDF masses are distributed uniformly and are hard to interpret. However, we can find that the most positive high-level feature is hypertensive chronic kidney disease, and this finding is also obvious in the TF-IDF model. We can find that ESRD (end stage renal disease), dialysis, and hemodialysis are also on the top of the TF-IDF mass distribution. If we directly use TF-IDF to analyze the data, we can also obtain the conclusion that chronic kidney disease is the most important factor in readmission prediction. Meanwhile, the proposed model claims that the coronary atherosclerotic is a protective factor of readmission, which is unintuitive, but the author does not explain it. Therefore, it seems not inappropriate to claim that the TF-IDF is hard to interpret and the proposed model is more interpretable.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "hZx9fe2s0f",
        "length": 214,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors utilize the power of LLM for feature extraction in the medical domain. Plus, they investigate how the alignment of simple models' weights affects the prediction. They conduct experiments on medical datasets and provide discussion based on the results.\n\nreasons_to_accept: 1, The task is an interesting task, especially in the medical domain. \n2, They provide comprehensive experiment results and discussion. \n3, This paper is well-written and easy to follow.\n\nreasons_to_reject: 1, I do concern about the novelty of this work. To my best knowledge, most of the methods in this work are existing techniques. \n2, I am wondering if this approach would raise the privacy issue. In the real-world setting, sensitive data would be strictly constrained to be fed into LLMs.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "gqbm67DNLE",
        "length": 250,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to use LLM to extract information from the raw patient notes, which could serve as the interpretable features for training a simple classifier. This kind of method could achieve compatible results with other methods, while maintaining the interpretability which might be useful for real applications.\n\nreasons_to_accept: 1. The idea to use LLM to generate examples for training is somewhat interesting;  2. The experiments are comprehensive and illustrate the effectiveness of this idea.\n\nreasons_to_reject: 1. I may doubt the necessity of the linear classifier. What if we just let the LLM give the prediction according to the classified results from the templates? Maybe we could give the model several examples to see the few-shot classification performance;  2. From Table2, it seems that the large language models are not so good at prediction these diseases. AUC > 0.5 does not seem to be a good guarantee that the returned features are reasonable.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "208_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_208_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7193999999999999,
      "max_similarity": 0.7278,
      "avg_coverage": 0.447,
      "max_coverage": 0.5333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 318,
      "avg_human_length": 342.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 1
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "8RJ4kyanAD",
        "similarity": 0.7278,
        "coverage": 0.3077,
        "human_length": 564,
        "human_text": "paper_topic_and_main_contributions: I'm not sure whether my understanding is correct because the methodology section of this study is a little bit hard to follow. It seems this study proposed a two-phase model. In the first phase, the author adopted a large language model (LLM, Flan-T5-XXL) to extract a series of preset features in a zero-shot manner. Then,  they trained a simple linear model to generate labels via the extracted features in a supervised manner. As the simple linear model is intrinsically interpretable, the classification result can be regarded as interpretable.\n\nreasons_to_accept: From the point of my view, the main contribution of this study is that the author and their clinical collaborator derived a detailed, hand-crafted prompt set to extract features from Chest X-ray Dataset (Supplementary A, B). I believe these prompts can significantly help the community to establish a structured dataset from unstructured medical text.\n\nreasons_to_reject: The main drawback of this study is that the authors did not prove their model is better than the TF-IDF-based BoW model or other traditional models.\nOf note, Medical text classification basically does not require a model to understand the language. The task performance basically relies on the existence of several key-word. Therefore, we can observe that the BERT-based model did not obtain significantly better performance than TF-IDF (BoW) model, and the performance of proposed model even obtained worse performance than the TF-IDF model (Figure 4, 8). Although the author claimed performance is not their primary objective, it is disappointing that such a computationally expensive model only obtained a worse performance than a model proposed several decades ago. Meanwhile, there is also a topic model (i.e., LDA) or neural network-based topic model that can extract interpretable features and be applied to downstream tasks. It will be better if the author can include them as baselines.\nMeanwhile, the author does not prove the proposed model is more interpretable. In Figure 5, the author claimed that the proposed model is interpretable because coefficient magnitude mass is concentrated on the very top of features, while the TF-IDF masses are distributed uniformly and are hard to interpret. However, we can find that the most positive high-level feature is hypertensive chronic kidney disease, and this finding is also obvious in the TF-IDF model. We can find that ESRD (end stage renal disease), dialysis, and hemodialysis are also on the top of the TF-IDF mass distribution. If we directly use TF-IDF to analyze the data, we can also obtain the conclusion that chronic kidney disease is the most important factor in readmission prediction. Meanwhile, the proposed model claims that the coronary atherosclerotic is a protective factor of readmission, which is unintuitive, but the author does not explain it. Therefore, it seems not inappropriate to claim that the TF-IDF is hard to interpret and the proposed model is more interpretable.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "hZx9fe2s0f",
        "similarity": 0.7196,
        "coverage": 0.5333,
        "human_length": 214,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors utilize the power of LLM for feature extraction in the medical domain. Plus, they investigate how the alignment of simple models' weights affects the prediction. They conduct experiments on medical datasets and provide discussion based on the results.\n\nreasons_to_accept: 1, The task is an interesting task, especially in the medical domain. \n2, They provide comprehensive experiment results and discussion. \n3, This paper is well-written and easy to follow.\n\nreasons_to_reject: 1, I do concern about the novelty of this work. To my best knowledge, most of the methods in this work are existing techniques. \n2, I am wondering if this approach would raise the privacy issue. In the real-world setting, sensitive data would be strictly constrained to be fed into LLMs.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "gqbm67DNLE",
        "similarity": 0.7108,
        "coverage": 0.5,
        "human_length": 250,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to use LLM to extract information from the raw patient notes, which could serve as the interpretable features for training a simple classifier. This kind of method could achieve compatible results with other methods, while maintaining the interpretability which might be useful for real applications.\n\nreasons_to_accept: 1. The idea to use LLM to generate examples for training is somewhat interesting;  2. The experiments are comprehensive and illustrate the effectiveness of this idea.\n\nreasons_to_reject: 1. I may doubt the necessity of the linear classifier. What if we just let the LLM give the prediction according to the classified results from the templates? Maybe we could give the model several examples to see the few-shot classification performance;  2. From Table2, it seems that the large language models are not so good at prediction these diseases. AUC > 0.5 does not seem to be a good guarantee that the returned features are reasonable.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "8RJ4kyanAD",
        "length": 564,
        "human_text": "paper_topic_and_main_contributions: I'm not sure whether my understanding is correct because the methodology section of this study is a little bit hard to follow. It seems this study proposed a two-phase model. In the first phase, the author adopted a large language model (LLM, Flan-T5-XXL) to extract a series of preset features in a zero-shot manner. Then,  they trained a simple linear model to generate labels via the extracted features in a supervised manner. As the simple linear model is intrinsically interpretable, the classification result can be regarded as interpretable.\n\nreasons_to_accept: From the point of my view, the main contribution of this study is that the author and their clinical collaborator derived a detailed, hand-crafted prompt set to extract features from Chest X-ray Dataset (Supplementary A, B). I believe these prompts can significantly help the community to establish a structured dataset from unstructured medical text.\n\nreasons_to_reject: The main drawback of this study is that the authors did not prove their model is better than the TF-IDF-based BoW model or other traditional models.\nOf note, Medical text classification basically does not require a model to understand the language. The task performance basically relies on the existence of several key-word. Therefore, we can observe that the BERT-based model did not obtain significantly better performance than TF-IDF (BoW) model, and the performance of proposed model even obtained worse performance than the TF-IDF model (Figure 4, 8). Although the author claimed performance is not their primary objective, it is disappointing that such a computationally expensive model only obtained a worse performance than a model proposed several decades ago. Meanwhile, there is also a topic model (i.e., LDA) or neural network-based topic model that can extract interpretable features and be applied to downstream tasks. It will be better if the author can include them as baselines.\nMeanwhile, the author does not prove the proposed model is more interpretable. In Figure 5, the author claimed that the proposed model is interpretable because coefficient magnitude mass is concentrated on the very top of features, while the TF-IDF masses are distributed uniformly and are hard to interpret. However, we can find that the most positive high-level feature is hypertensive chronic kidney disease, and this finding is also obvious in the TF-IDF model. We can find that ESRD (end stage renal disease), dialysis, and hemodialysis are also on the top of the TF-IDF mass distribution. If we directly use TF-IDF to analyze the data, we can also obtain the conclusion that chronic kidney disease is the most important factor in readmission prediction. Meanwhile, the proposed model claims that the coronary atherosclerotic is a protective factor of readmission, which is unintuitive, but the author does not explain it. Therefore, it seems not inappropriate to claim that the TF-IDF is hard to interpret and the proposed model is more interpretable.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "hZx9fe2s0f",
        "length": 214,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors utilize the power of LLM for feature extraction in the medical domain. Plus, they investigate how the alignment of simple models' weights affects the prediction. They conduct experiments on medical datasets and provide discussion based on the results.\n\nreasons_to_accept: 1, The task is an interesting task, especially in the medical domain. \n2, They provide comprehensive experiment results and discussion. \n3, This paper is well-written and easy to follow.\n\nreasons_to_reject: 1, I do concern about the novelty of this work. To my best knowledge, most of the methods in this work are existing techniques. \n2, I am wondering if this approach would raise the privacy issue. In the real-world setting, sensitive data would be strictly constrained to be fed into LLMs.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "gqbm67DNLE",
        "length": 250,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to use LLM to extract information from the raw patient notes, which could serve as the interpretable features for training a simple classifier. This kind of method could achieve compatible results with other methods, while maintaining the interpretability which might be useful for real applications.\n\nreasons_to_accept: 1. The idea to use LLM to generate examples for training is somewhat interesting;  2. The experiments are comprehensive and illustrate the effectiveness of this idea.\n\nreasons_to_reject: 1. I may doubt the necessity of the linear classifier. What if we just let the LLM give the prediction according to the classified results from the templates? Maybe we could give the model several examples to see the few-shot classification performance;  2. From Table2, it seems that the large language models are not so good at prediction these diseases. AUC > 0.5 does not seem to be a good guarantee that the returned features are reasonable.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "85_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_85_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.736325,
      "max_similarity": 0.767,
      "avg_coverage": 0.36557500000000004,
      "max_coverage": 0.5
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 378,
      "avg_human_length": 404.75
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 4,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "groniG3mHd",
        "similarity": 0.7536,
        "coverage": 0.4348,
        "human_length": 383,
        "human_text": "paper_topic_and_main_contributions: This paper addresses empathic resonance between people, by utilizing their personalized narratives.  They compare empathy similarity through people's stories by using three components of the stories: main event, emotional reaction, and moral. The authors introduce a new dataset, which contains empathic similarity annotations between pairs of stories. Moreover, they introduce two tasks: empathic similarity prediction and empathy reasoning. The main models that they utilize are SBERT, BART (with fine-tuning), and GPT3, ChatGPT (with few-shot learning). In addition, to the automatic evaluation, the authors also conduct a user study where the main goal is to evaluate the models for story retrieval.\nThe main contributions of the paper are as follows: - They introduce a framework to model empathic similarity, which focuses on the relationship between the narratives of a pair of people.  - They release \u201cEmpathaticStories\u201d  a corpus of 1500 stories with data from three different sources.\n- A user study that involves personalized human evaluation of the models predictions\n\nreasons_to_accept: 1. The collected dataset, is a resource that might prove valuable for future work on empathic analysis by utilizing personalized narratives of the users 2. A user study that conducts a personalized evaluation of the model's ability to retrieve similar stories that the user can empathize. I found this user study particularly interesting.\n\nreasons_to_reject: 1. The paper mentions that it summarizes stories before presenting the pairs to the annotators. However, valuable information might be lost in the process. For example, the individual annotations of the first part of the annotation, like main events, emotional reaction, or moral, which are an important part of the framework defined for empathy similarity 2. In addition, there is a very low agreement between the annotators, and it is not stated how the annotations are aggregated, or how the disagreements are solved 3. Analysis for the empathy reasoning task is vague. It does not include any human evaluation and no further analysis except reporting automated metrics.\n\ntypos_grammar_style_and_presentation_improvements: I would suggest that in the introduction, you include a paragraph where you clearly state your contributions.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "3PrODBjAKV",
        "similarity": 0.7262,
        "coverage": 0.5,
        "human_length": 215,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a new task of similarity detection in personal stories based on the extend of empathy people have to each other stories instead of using raw semantic similarity.\n\nreasons_to_accept: The paper introduces a new and interesting task in conjunction of NLP and Social Psychology to identify the similarity of two texts, specifically two personal stories, based on the extend of their narrator empathy to the other story.  The paper introduces a new dataset for this task., which is carefully analyzed and they also provide the first benchmark on this dataset.\nThe author(s) formulated the task with mathematical notations which helps in understanding the task and further development of the task by new research teams.\nThey did human evaluations to asses their model performance.\n\nreasons_to_reject: My concerns have been addressed in the rebuttal.\n\ntypos_grammar_style_and_presentation_improvements: In section 7, further relation work, in paragraph 2, line 3, you used two usings.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "s632myiXtI",
        "similarity": 0.767,
        "coverage": 0.1525,
        "human_length": 806,
        "human_text": "paper_topic_and_main_contributions: This paper presents an empathic similarity dataset with pairwise annotations for 2000 pairs of stories from a pool of 1500 stories. The authors show that their finetuned model is better aligned with human interpretations of empathic similarity than previous models.\n\nreasons_to_accept: The methodology is overall sound and some of the concepts introduced in this paper are fairly novel as well useful and interesting. The authors also manage to discuss the matter from an interdisciplinary perspective bringing in psychology and social studies work.\nThe dataset is also likely to be useful to other researchers specifically in empathy and intimacy-related projects, but also sentiment analysis and summarization projects.\nAdditionally, to the delight of CS people everywhere, the authors evaluate their model using BLEU, ROUGE, and METEOR.\n\nreasons_to_reject: This paper is a little confused about what it wants to be and is in some ways two papers merged into one as exemplified by section 6 which is a standalone study with its own hypothesis, previous work, data, and method as well as results section and is followed by section 7 \"Further related work\" just before the \"Conclusions\" section. Although these issues are structural they confound the core message of the paper which should be the pairwise empathic dataset.\nI am also not entirely convinced of the results and their significance. At the very least, the improvements of the model presented here should be discussed more in-depth in the text. As it currently stands the link between the results in table 3-5 and figures 3 and 4 are confusing at best.\n\nquestions_for_the_authors: Will you make the code available online? \nWill the dataset be available online? \nAlthough you list many different similarity scores and model performance metrics (tables 3, 4, 5) I'm unconvinced these are very significant. Of course, fine-tuning will make the model more domain-specific and this is where I think you should bring the results from section 6 to show that these are not just generic domain-adaptation improvements but actually useful from a real-world perspective where your model is the best at detecting empathic similarity (which I do believe it probably is). Could you present this disconnect better somehow to improve the narrative of your paper?\n\nmissing_references: One glaring omission in the reference list is work done by David Jurgens et al. on sympathy and empathy in online fora.\nZhou, N. and Jurgens, D., 2020, November. Condolence and empathy in online communities. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 609-626).\nLahnala, A., Welch, C., Jurgens, D. and Flek, L., 2022. A critical reflection and forward perspective on empathy and natural language processing. arXiv preprint arXiv:2210.16604.\nBao, J., Wu, J., Zhang, Y., Chandrasekharan, E. and Jurgens, D., 2021, April. Conversations gone alright: Quantifying and predicting prosocial outcomes in online conversations. In Proceedings of the Web Conference 2021 (pp. 1134-1145).\nOther more NLP-focused work could also be added to strengthen the arguments made by the authors: Lahnala, A., Welch, C. and Flek, L., 2022, May. CAISA at WASSA 2022: Adapter-tuning for empathy prediction. In Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (pp. 280-285).\nKumano, S., Ishii, R. and Otsuka, K., 2017, October. Comparing empathy perceived by interlocutors in multiparty conversation and external observers. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 50-57). IEEE.\nBoukricha, H., Wachsmuth, I., Carminati, M.N. and Knoeferle, P., 2013, September. A computational model of empathy: Empirical evaluation. In 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction (pp. 1-6). IEEE.\n\ntypos_grammar_style_and_presentation_improvements: I'm not entirely certain that the flow of the paper is the best. I understand the choice to place section 6 where it is, but it could very well be merged into previous sections as another hypothesis. Currently, section 6 reads like its own short paper so perhaps the authors could consider redistributing the information of section 6 into the other sections (intro, previous work, data & method, results etc).\nSimilarly, it is strange to see section 7 \"Further related work\" as the next to last chapter just before \"conclusions\". I suggest that baking section 6 into the whole paper includes moving section 7 as a subsection to section 2. Otherwise, the paper is just too disjointed and reads as if trying to cram two papers into one. If your paper is not accepted, you could consider resubmitting it elsewhere as two short papers rather than one long paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "qTOgV9vA0R",
        "similarity": 0.6985,
        "coverage": 0.375,
        "human_length": 215,
        "human_text": "paper_topic_and_main_contributions: The paper is a good attempt to enable social-emotional correlation between people based on their stories.  The dataset of 2000 stories is constructed. 150 participants feedback on the story is also created and compared with the model. The stories are classified into main, empathy and moral events.\n\nreasons_to_accept: The paper is a good attempt to enable social-emotional correlation between people based on their stories.  The dataset of 2000 stories is constructed. 150 participants feedback on the story is also created and compared with the model. The stories are classified into main, empathy and moral events.\n\nreasons_to_reject: The concerns are addressed in the revision.\n\nquestions_for_the_authors: The impact of a personal story and its similarity is useful to get some valid conclusion ?? though it is not in the scope, the impact needs to be calculated in order to find the success of the method.  Please write a paragraph about this point in a paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "groniG3mHd",
        "length": 383,
        "human_text": "paper_topic_and_main_contributions: This paper addresses empathic resonance between people, by utilizing their personalized narratives.  They compare empathy similarity through people's stories by using three components of the stories: main event, emotional reaction, and moral. The authors introduce a new dataset, which contains empathic similarity annotations between pairs of stories. Moreover, they introduce two tasks: empathic similarity prediction and empathy reasoning. The main models that they utilize are SBERT, BART (with fine-tuning), and GPT3, ChatGPT (with few-shot learning). In addition, to the automatic evaluation, the authors also conduct a user study where the main goal is to evaluate the models for story retrieval.\nThe main contributions of the paper are as follows: - They introduce a framework to model empathic similarity, which focuses on the relationship between the narratives of a pair of people.  - They release \u201cEmpathaticStories\u201d  a corpus of 1500 stories with data from three different sources.\n- A user study that involves personalized human evaluation of the models predictions\n\nreasons_to_accept: 1. The collected dataset, is a resource that might prove valuable for future work on empathic analysis by utilizing personalized narratives of the users 2. A user study that conducts a personalized evaluation of the model's ability to retrieve similar stories that the user can empathize. I found this user study particularly interesting.\n\nreasons_to_reject: 1. The paper mentions that it summarizes stories before presenting the pairs to the annotators. However, valuable information might be lost in the process. For example, the individual annotations of the first part of the annotation, like main events, emotional reaction, or moral, which are an important part of the framework defined for empathy similarity 2. In addition, there is a very low agreement between the annotators, and it is not stated how the annotations are aggregated, or how the disagreements are solved 3. Analysis for the empathy reasoning task is vague. It does not include any human evaluation and no further analysis except reporting automated metrics.\n\ntypos_grammar_style_and_presentation_improvements: I would suggest that in the introduction, you include a paragraph where you clearly state your contributions.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "3PrODBjAKV",
        "length": 215,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a new task of similarity detection in personal stories based on the extend of empathy people have to each other stories instead of using raw semantic similarity.\n\nreasons_to_accept: The paper introduces a new and interesting task in conjunction of NLP and Social Psychology to identify the similarity of two texts, specifically two personal stories, based on the extend of their narrator empathy to the other story.  The paper introduces a new dataset for this task., which is carefully analyzed and they also provide the first benchmark on this dataset.\nThe author(s) formulated the task with mathematical notations which helps in understanding the task and further development of the task by new research teams.\nThey did human evaluations to asses their model performance.\n\nreasons_to_reject: My concerns have been addressed in the rebuttal.\n\ntypos_grammar_style_and_presentation_improvements: In section 7, further relation work, in paragraph 2, line 3, you used two usings.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "s632myiXtI",
        "length": 806,
        "human_text": "paper_topic_and_main_contributions: This paper presents an empathic similarity dataset with pairwise annotations for 2000 pairs of stories from a pool of 1500 stories. The authors show that their finetuned model is better aligned with human interpretations of empathic similarity than previous models.\n\nreasons_to_accept: The methodology is overall sound and some of the concepts introduced in this paper are fairly novel as well useful and interesting. The authors also manage to discuss the matter from an interdisciplinary perspective bringing in psychology and social studies work.\nThe dataset is also likely to be useful to other researchers specifically in empathy and intimacy-related projects, but also sentiment analysis and summarization projects.\nAdditionally, to the delight of CS people everywhere, the authors evaluate their model using BLEU, ROUGE, and METEOR.\n\nreasons_to_reject: This paper is a little confused about what it wants to be and is in some ways two papers merged into one as exemplified by section 6 which is a standalone study with its own hypothesis, previous work, data, and method as well as results section and is followed by section 7 \"Further related work\" just before the \"Conclusions\" section. Although these issues are structural they confound the core message of the paper which should be the pairwise empathic dataset.\nI am also not entirely convinced of the results and their significance. At the very least, the improvements of the model presented here should be discussed more in-depth in the text. As it currently stands the link between the results in table 3-5 and figures 3 and 4 are confusing at best.\n\nquestions_for_the_authors: Will you make the code available online? \nWill the dataset be available online? \nAlthough you list many different similarity scores and model performance metrics (tables 3, 4, 5) I'm unconvinced these are very significant. Of course, fine-tuning will make the model more domain-specific and this is where I think you should bring the results from section 6 to show that these are not just generic domain-adaptation improvements but actually useful from a real-world perspective where your model is the best at detecting empathic similarity (which I do believe it probably is). Could you present this disconnect better somehow to improve the narrative of your paper?\n\nmissing_references: One glaring omission in the reference list is work done by David Jurgens et al. on sympathy and empathy in online fora.\nZhou, N. and Jurgens, D., 2020, November. Condolence and empathy in online communities. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 609-626).\nLahnala, A., Welch, C., Jurgens, D. and Flek, L., 2022. A critical reflection and forward perspective on empathy and natural language processing. arXiv preprint arXiv:2210.16604.\nBao, J., Wu, J., Zhang, Y., Chandrasekharan, E. and Jurgens, D., 2021, April. Conversations gone alright: Quantifying and predicting prosocial outcomes in online conversations. In Proceedings of the Web Conference 2021 (pp. 1134-1145).\nOther more NLP-focused work could also be added to strengthen the arguments made by the authors: Lahnala, A., Welch, C. and Flek, L., 2022, May. CAISA at WASSA 2022: Adapter-tuning for empathy prediction. In Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (pp. 280-285).\nKumano, S., Ishii, R. and Otsuka, K., 2017, October. Comparing empathy perceived by interlocutors in multiparty conversation and external observers. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 50-57). IEEE.\nBoukricha, H., Wachsmuth, I., Carminati, M.N. and Knoeferle, P., 2013, September. A computational model of empathy: Empirical evaluation. In 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction (pp. 1-6). IEEE.\n\ntypos_grammar_style_and_presentation_improvements: I'm not entirely certain that the flow of the paper is the best. I understand the choice to place section 6 where it is, but it could very well be merged into previous sections as another hypothesis. Currently, section 6 reads like its own short paper so perhaps the authors could consider redistributing the information of section 6 into the other sections (intro, previous work, data & method, results etc).\nSimilarly, it is strange to see section 7 \"Further related work\" as the next to last chapter just before \"conclusions\". I suggest that baking section 6 into the whole paper includes moving section 7 as a subsection to section 2. Otherwise, the paper is just too disjointed and reads as if trying to cram two papers into one. If your paper is not accepted, you could consider resubmitting it elsewhere as two short papers rather than one long paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "qTOgV9vA0R",
        "length": 215,
        "human_text": "paper_topic_and_main_contributions: The paper is a good attempt to enable social-emotional correlation between people based on their stories.  The dataset of 2000 stories is constructed. 150 participants feedback on the story is also created and compared with the model. The stories are classified into main, empathy and moral events.\n\nreasons_to_accept: The paper is a good attempt to enable social-emotional correlation between people based on their stories.  The dataset of 2000 stories is constructed. 150 participants feedback on the story is also created and compared with the model. The stories are classified into main, empathy and moral events.\n\nreasons_to_reject: The concerns are addressed in the revision.\n\nquestions_for_the_authors: The impact of a personal story and its similarity is useful to get some valid conclusion ?? though it is not in the scope, the impact needs to be calculated in order to find the success of the method.  Please write a paragraph about this point in a paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "85_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_85_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.731775,
      "max_similarity": 0.7644,
      "avg_coverage": 0.433675,
      "max_coverage": 0.5652
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 318,
      "avg_human_length": 404.75
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 4,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "groniG3mHd",
        "similarity": 0.7465,
        "coverage": 0.5652,
        "human_length": 383,
        "human_text": "paper_topic_and_main_contributions: This paper addresses empathic resonance between people, by utilizing their personalized narratives.  They compare empathy similarity through people's stories by using three components of the stories: main event, emotional reaction, and moral. The authors introduce a new dataset, which contains empathic similarity annotations between pairs of stories. Moreover, they introduce two tasks: empathic similarity prediction and empathy reasoning. The main models that they utilize are SBERT, BART (with fine-tuning), and GPT3, ChatGPT (with few-shot learning). In addition, to the automatic evaluation, the authors also conduct a user study where the main goal is to evaluate the models for story retrieval.\nThe main contributions of the paper are as follows: - They introduce a framework to model empathic similarity, which focuses on the relationship between the narratives of a pair of people.  - They release \u201cEmpathaticStories\u201d  a corpus of 1500 stories with data from three different sources.\n- A user study that involves personalized human evaluation of the models predictions\n\nreasons_to_accept: 1. The collected dataset, is a resource that might prove valuable for future work on empathic analysis by utilizing personalized narratives of the users 2. A user study that conducts a personalized evaluation of the model's ability to retrieve similar stories that the user can empathize. I found this user study particularly interesting.\n\nreasons_to_reject: 1. The paper mentions that it summarizes stories before presenting the pairs to the annotators. However, valuable information might be lost in the process. For example, the individual annotations of the first part of the annotation, like main events, emotional reaction, or moral, which are an important part of the framework defined for empathy similarity 2. In addition, there is a very low agreement between the annotators, and it is not stated how the annotations are aggregated, or how the disagreements are solved 3. Analysis for the empathy reasoning task is vague. It does not include any human evaluation and no further analysis except reporting automated metrics.\n\ntypos_grammar_style_and_presentation_improvements: I would suggest that in the introduction, you include a paragraph where you clearly state your contributions.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "3PrODBjAKV",
        "similarity": 0.7174,
        "coverage": 0.5,
        "human_length": 215,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a new task of similarity detection in personal stories based on the extend of empathy people have to each other stories instead of using raw semantic similarity.\n\nreasons_to_accept: The paper introduces a new and interesting task in conjunction of NLP and Social Psychology to identify the similarity of two texts, specifically two personal stories, based on the extend of their narrator empathy to the other story.  The paper introduces a new dataset for this task., which is carefully analyzed and they also provide the first benchmark on this dataset.\nThe author(s) formulated the task with mathematical notations which helps in understanding the task and further development of the task by new research teams.\nThey did human evaluations to asses their model performance.\n\nreasons_to_reject: My concerns have been addressed in the rebuttal.\n\ntypos_grammar_style_and_presentation_improvements: In section 7, further relation work, in paragraph 2, line 3, you used two usings.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "s632myiXtI",
        "similarity": 0.7644,
        "coverage": 0.1695,
        "human_length": 806,
        "human_text": "paper_topic_and_main_contributions: This paper presents an empathic similarity dataset with pairwise annotations for 2000 pairs of stories from a pool of 1500 stories. The authors show that their finetuned model is better aligned with human interpretations of empathic similarity than previous models.\n\nreasons_to_accept: The methodology is overall sound and some of the concepts introduced in this paper are fairly novel as well useful and interesting. The authors also manage to discuss the matter from an interdisciplinary perspective bringing in psychology and social studies work.\nThe dataset is also likely to be useful to other researchers specifically in empathy and intimacy-related projects, but also sentiment analysis and summarization projects.\nAdditionally, to the delight of CS people everywhere, the authors evaluate their model using BLEU, ROUGE, and METEOR.\n\nreasons_to_reject: This paper is a little confused about what it wants to be and is in some ways two papers merged into one as exemplified by section 6 which is a standalone study with its own hypothesis, previous work, data, and method as well as results section and is followed by section 7 \"Further related work\" just before the \"Conclusions\" section. Although these issues are structural they confound the core message of the paper which should be the pairwise empathic dataset.\nI am also not entirely convinced of the results and their significance. At the very least, the improvements of the model presented here should be discussed more in-depth in the text. As it currently stands the link between the results in table 3-5 and figures 3 and 4 are confusing at best.\n\nquestions_for_the_authors: Will you make the code available online? \nWill the dataset be available online? \nAlthough you list many different similarity scores and model performance metrics (tables 3, 4, 5) I'm unconvinced these are very significant. Of course, fine-tuning will make the model more domain-specific and this is where I think you should bring the results from section 6 to show that these are not just generic domain-adaptation improvements but actually useful from a real-world perspective where your model is the best at detecting empathic similarity (which I do believe it probably is). Could you present this disconnect better somehow to improve the narrative of your paper?\n\nmissing_references: One glaring omission in the reference list is work done by David Jurgens et al. on sympathy and empathy in online fora.\nZhou, N. and Jurgens, D., 2020, November. Condolence and empathy in online communities. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 609-626).\nLahnala, A., Welch, C., Jurgens, D. and Flek, L., 2022. A critical reflection and forward perspective on empathy and natural language processing. arXiv preprint arXiv:2210.16604.\nBao, J., Wu, J., Zhang, Y., Chandrasekharan, E. and Jurgens, D., 2021, April. Conversations gone alright: Quantifying and predicting prosocial outcomes in online conversations. In Proceedings of the Web Conference 2021 (pp. 1134-1145).\nOther more NLP-focused work could also be added to strengthen the arguments made by the authors: Lahnala, A., Welch, C. and Flek, L., 2022, May. CAISA at WASSA 2022: Adapter-tuning for empathy prediction. In Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (pp. 280-285).\nKumano, S., Ishii, R. and Otsuka, K., 2017, October. Comparing empathy perceived by interlocutors in multiparty conversation and external observers. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 50-57). IEEE.\nBoukricha, H., Wachsmuth, I., Carminati, M.N. and Knoeferle, P., 2013, September. A computational model of empathy: Empirical evaluation. In 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction (pp. 1-6). IEEE.\n\ntypos_grammar_style_and_presentation_improvements: I'm not entirely certain that the flow of the paper is the best. I understand the choice to place section 6 where it is, but it could very well be merged into previous sections as another hypothesis. Currently, section 6 reads like its own short paper so perhaps the authors could consider redistributing the information of section 6 into the other sections (intro, previous work, data & method, results etc).\nSimilarly, it is strange to see section 7 \"Further related work\" as the next to last chapter just before \"conclusions\". I suggest that baking section 6 into the whole paper includes moving section 7 as a subsection to section 2. Otherwise, the paper is just too disjointed and reads as if trying to cram two papers into one. If your paper is not accepted, you could consider resubmitting it elsewhere as two short papers rather than one long paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "qTOgV9vA0R",
        "similarity": 0.6988,
        "coverage": 0.5,
        "human_length": 215,
        "human_text": "paper_topic_and_main_contributions: The paper is a good attempt to enable social-emotional correlation between people based on their stories.  The dataset of 2000 stories is constructed. 150 participants feedback on the story is also created and compared with the model. The stories are classified into main, empathy and moral events.\n\nreasons_to_accept: The paper is a good attempt to enable social-emotional correlation between people based on their stories.  The dataset of 2000 stories is constructed. 150 participants feedback on the story is also created and compared with the model. The stories are classified into main, empathy and moral events.\n\nreasons_to_reject: The concerns are addressed in the revision.\n\nquestions_for_the_authors: The impact of a personal story and its similarity is useful to get some valid conclusion ?? though it is not in the scope, the impact needs to be calculated in order to find the success of the method.  Please write a paragraph about this point in a paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "groniG3mHd",
        "length": 383,
        "human_text": "paper_topic_and_main_contributions: This paper addresses empathic resonance between people, by utilizing their personalized narratives.  They compare empathy similarity through people's stories by using three components of the stories: main event, emotional reaction, and moral. The authors introduce a new dataset, which contains empathic similarity annotations between pairs of stories. Moreover, they introduce two tasks: empathic similarity prediction and empathy reasoning. The main models that they utilize are SBERT, BART (with fine-tuning), and GPT3, ChatGPT (with few-shot learning). In addition, to the automatic evaluation, the authors also conduct a user study where the main goal is to evaluate the models for story retrieval.\nThe main contributions of the paper are as follows: - They introduce a framework to model empathic similarity, which focuses on the relationship between the narratives of a pair of people.  - They release \u201cEmpathaticStories\u201d  a corpus of 1500 stories with data from three different sources.\n- A user study that involves personalized human evaluation of the models predictions\n\nreasons_to_accept: 1. The collected dataset, is a resource that might prove valuable for future work on empathic analysis by utilizing personalized narratives of the users 2. A user study that conducts a personalized evaluation of the model's ability to retrieve similar stories that the user can empathize. I found this user study particularly interesting.\n\nreasons_to_reject: 1. The paper mentions that it summarizes stories before presenting the pairs to the annotators. However, valuable information might be lost in the process. For example, the individual annotations of the first part of the annotation, like main events, emotional reaction, or moral, which are an important part of the framework defined for empathy similarity 2. In addition, there is a very low agreement between the annotators, and it is not stated how the annotations are aggregated, or how the disagreements are solved 3. Analysis for the empathy reasoning task is vague. It does not include any human evaluation and no further analysis except reporting automated metrics.\n\ntypos_grammar_style_and_presentation_improvements: I would suggest that in the introduction, you include a paragraph where you clearly state your contributions.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "3PrODBjAKV",
        "length": 215,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a new task of similarity detection in personal stories based on the extend of empathy people have to each other stories instead of using raw semantic similarity.\n\nreasons_to_accept: The paper introduces a new and interesting task in conjunction of NLP and Social Psychology to identify the similarity of two texts, specifically two personal stories, based on the extend of their narrator empathy to the other story.  The paper introduces a new dataset for this task., which is carefully analyzed and they also provide the first benchmark on this dataset.\nThe author(s) formulated the task with mathematical notations which helps in understanding the task and further development of the task by new research teams.\nThey did human evaluations to asses their model performance.\n\nreasons_to_reject: My concerns have been addressed in the rebuttal.\n\ntypos_grammar_style_and_presentation_improvements: In section 7, further relation work, in paragraph 2, line 3, you used two usings.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "s632myiXtI",
        "length": 806,
        "human_text": "paper_topic_and_main_contributions: This paper presents an empathic similarity dataset with pairwise annotations for 2000 pairs of stories from a pool of 1500 stories. The authors show that their finetuned model is better aligned with human interpretations of empathic similarity than previous models.\n\nreasons_to_accept: The methodology is overall sound and some of the concepts introduced in this paper are fairly novel as well useful and interesting. The authors also manage to discuss the matter from an interdisciplinary perspective bringing in psychology and social studies work.\nThe dataset is also likely to be useful to other researchers specifically in empathy and intimacy-related projects, but also sentiment analysis and summarization projects.\nAdditionally, to the delight of CS people everywhere, the authors evaluate their model using BLEU, ROUGE, and METEOR.\n\nreasons_to_reject: This paper is a little confused about what it wants to be and is in some ways two papers merged into one as exemplified by section 6 which is a standalone study with its own hypothesis, previous work, data, and method as well as results section and is followed by section 7 \"Further related work\" just before the \"Conclusions\" section. Although these issues are structural they confound the core message of the paper which should be the pairwise empathic dataset.\nI am also not entirely convinced of the results and their significance. At the very least, the improvements of the model presented here should be discussed more in-depth in the text. As it currently stands the link between the results in table 3-5 and figures 3 and 4 are confusing at best.\n\nquestions_for_the_authors: Will you make the code available online? \nWill the dataset be available online? \nAlthough you list many different similarity scores and model performance metrics (tables 3, 4, 5) I'm unconvinced these are very significant. Of course, fine-tuning will make the model more domain-specific and this is where I think you should bring the results from section 6 to show that these are not just generic domain-adaptation improvements but actually useful from a real-world perspective where your model is the best at detecting empathic similarity (which I do believe it probably is). Could you present this disconnect better somehow to improve the narrative of your paper?\n\nmissing_references: One glaring omission in the reference list is work done by David Jurgens et al. on sympathy and empathy in online fora.\nZhou, N. and Jurgens, D., 2020, November. Condolence and empathy in online communities. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 609-626).\nLahnala, A., Welch, C., Jurgens, D. and Flek, L., 2022. A critical reflection and forward perspective on empathy and natural language processing. arXiv preprint arXiv:2210.16604.\nBao, J., Wu, J., Zhang, Y., Chandrasekharan, E. and Jurgens, D., 2021, April. Conversations gone alright: Quantifying and predicting prosocial outcomes in online conversations. In Proceedings of the Web Conference 2021 (pp. 1134-1145).\nOther more NLP-focused work could also be added to strengthen the arguments made by the authors: Lahnala, A., Welch, C. and Flek, L., 2022, May. CAISA at WASSA 2022: Adapter-tuning for empathy prediction. In Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (pp. 280-285).\nKumano, S., Ishii, R. and Otsuka, K., 2017, October. Comparing empathy perceived by interlocutors in multiparty conversation and external observers. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII) (pp. 50-57). IEEE.\nBoukricha, H., Wachsmuth, I., Carminati, M.N. and Knoeferle, P., 2013, September. A computational model of empathy: Empirical evaluation. In 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction (pp. 1-6). IEEE.\n\ntypos_grammar_style_and_presentation_improvements: I'm not entirely certain that the flow of the paper is the best. I understand the choice to place section 6 where it is, but it could very well be merged into previous sections as another hypothesis. Currently, section 6 reads like its own short paper so perhaps the authors could consider redistributing the information of section 6 into the other sections (intro, previous work, data & method, results etc).\nSimilarly, it is strange to see section 7 \"Further related work\" as the next to last chapter just before \"conclusions\". I suggest that baking section 6 into the whole paper includes moving section 7 as a subsection to section 2. Otherwise, the paper is just too disjointed and reads as if trying to cram two papers into one. If your paper is not accepted, you could consider resubmitting it elsewhere as two short papers rather than one long paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "qTOgV9vA0R",
        "length": 215,
        "human_text": "paper_topic_and_main_contributions: The paper is a good attempt to enable social-emotional correlation between people based on their stories.  The dataset of 2000 stories is constructed. 150 participants feedback on the story is also created and compared with the model. The stories are classified into main, empathy and moral events.\n\nreasons_to_accept: The paper is a good attempt to enable social-emotional correlation between people based on their stories.  The dataset of 2000 stories is constructed. 150 participants feedback on the story is also created and compared with the model. The stories are classified into main, empathy and moral events.\n\nreasons_to_reject: The concerns are addressed in the revision.\n\nquestions_for_the_authors: The impact of a personal story and its similarity is useful to get some valid conclusion ?? though it is not in the scope, the impact needs to be calculated in order to find the success of the method.  Please write a paragraph about this point in a paper.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "11_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_11_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6970666666666667,
      "max_similarity": 0.7102,
      "avg_coverage": 0.5277333333333333,
      "max_coverage": 0.8571
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 746,
      "avg_human_length": 606.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 8,
      "weaknesses_count": 10,
      "suggestions_count": 13
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "nDCeASmceL",
        "similarity": 0.6914,
        "coverage": 0.8571,
        "human_length": 227,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a new Twitter benchmark for sentiment analysis in african languages, and provide experimental results with few baseline models.\n\nreasons_to_accept: - A new useful dataset that can be used for sentiment analysis in Afriacan languages.\n- Extensive experimental results on the benchmark.\n\nreasons_to_reject: - Limited explanation of imbalanced data distribution in Table 6. Why there is no training samples in 'orm' and 'tir'? I see that there was a difficulty in collecting data from annotators, but it does not explain why we don't have training samples. For example, we can have 1500 training samples and 600 test samples in 'orm' language.\n- More detailed experimental results are required. In Table 8, only F1 score is reported, but it is better to give per-label precision and recall as well.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "pKSBS3XZlG",
        "similarity": 0.7102,
        "coverage": 0.2778,
        "human_length": 1321,
        "human_text": "paper_topic_and_main_contributions: The authors present a dataset of 12 African languages, annotated for sentiment by native (and usually local) speakers. They describe challenges in collection and annotation. They also present some results from pre-trained language models, discussing which models do best with which languages and which source languages do the best in zero-shot learning for Oromo and Tigrinya.\n\nreasons_to_accept: A very good reason to accept this paper is the dataset itself and the ability to talk about collection and annotation outside the parts of the world NLP researchers typically focus on. The authors share specific problems and their solutions that should also be good for discussion and help researchers who are interested in working on other lower resource languages.  They are also releasing the annotations, not just the majority-vote label, which should be useful for researchers interested in methodologies and inter-annotator agreement topics.\n\nreasons_to_reject: There are no risks for having this paper presented at the conference. In the next section, you'll see me ask a bunch of questions of the authors but I don't really consider these weaknesses, certainly not enough to jump up to a reason to reject.\n\nquestions_for_the_authors: I'm going to ask my questions in the order they pop up in the text, but I'll try to give a sense of which ones I think are more important to address: 1) You have picked sentiment and in lines 47-56, you mention a number of reasons that sentiment may be useful (literary analysis, culturonomics, commerce, psychology, social science). One that I don't think those quite capture would be \"linguistic\". Here, I'm thinking about how, for example, there's a malefactive particle in at least one Ethiopian language that lets a speaker/writer give the sense that someone did something AGAINST someone else, in Dutch there's \"tet\" which marks surprise/irritation, in Czech diphthongs can mean affection (or pejoration), in Cantonese, throwing a -k at the end of particles can intensify the emotion being expressed, in Tongan there are determiners that express sympathy, in Manambu there is a \"frustrative\" case marking for when things are done in vain, etc.  In my heart of hearts, my hope is that your dataset could help people find affective markers/word-order/morphemes but...is that plausible, do you think? Or is your expectation that what your annotators picked up on were louder words like adjectives (\"awful\", \"wonderful\", etc). I don't think this particularly needs to be added to your paper, but I am curious about how you'd look at the dataset being used for more linguistic/sociolinguistic/pragmatics research questions.\n2) You mentioned using Saif Mohammad's sentiment guidelines, I think you mean the \"base\" questions he has (not the ones that try to get at what the sentiment is direct towards). It probably would be good to share the explicit instructions for annotators, probably in an appendix. This I do think is probably a minor weakness and worth addressing, btw. It also connects to some of your comments on code-switching texts\u2014you have some annotators who don't know parts of those mixed language tweets and it feels like they should have a \"I don't understand this\" option (which would also be relevant for everyone else since I am certain as a native English speaker you could give me completely English tweets that I could not understand to give a sentiment to).  2b) Part of this question comes from looking at your examples in Table 1. The Amharic example kicks off with the very powerful bigram \"\u1328\u12ab\u129d \u12a0\u1228\u1218\u1294\" (~brutal barbarian, pitiless pagan) but the rest of it has a much lighter tone as far as I can tell. I don't know the referent but it strikes me that this is fairly sarcastic and has an intent to be humorous which is \"negative\" in a way that is less clear than \"I hate X\". You mention sarcasm a number of times\u2014as does Mohammad 2016\u2014I'm wondering what you might say about that topic more generally.  This also feels related to the annotation problem you describe in Twi with \u201cTweaa\u201d being potentially offensive depending on context\u2014many annotators have a hard time distinguishing a speaker/writer\u2019s sentiment when they use taboo terms (the terms that one community of speakers might think is unexceptional or just expressive may be no-go\u2019s for other speakers who ignore the speaker intention). Do you have any thoughts about your annotators\u2019 predilections on these things?  If I gave you a magic button to press that would replace positive/negative/neutral with any categories of your choosing with high accuracy, which categories would you pick? That is, is valence the real thing you most want or is it a stand-in/approximation/first-step for something you actually would rather collect but is harder?\n3) You briefly mention some of the ways African languages present challenges for sentiment analysis that English and other languages may not. You mention tone, code-switching, and digraphia in lines 127-128 but I don't think you actually talk about tone. Here, I assume you mean phonemic tone since lots of African languages are tonal in nature. You discuss code-switching and digraphia but do you have anything to say about phonemic tone and your dataset/annotations? About half your languages use tone but I think only Yor\u00f9b\u00e1 marks them in writing...but maybe the lack of marking them in tweets causes some extra homographs but...I'm not sure how that would be different than other languages which are replete with homographs (English lead=metal, lead=guide).  4) You have to do a bunch of things to get a meaningful (not-all-neutral) set of tweets. I think you do a pretty good job but I wonder if you could address what you see as the consequences of these choices for possible uses of the dataset. This is a suggestion I think would help the paper.  4b) A minor thing but why are the Nigerian language test sets lower-cased (you reference Muhammad et al 2022, but it\u2019d be nice if you give the reason so readers don\u2019t have to track that reference down).  4c) You discard \u201cfull disagreement\u201d tweets\u2014are those going to be in the dataset? I think that may help with the use of your dataset, provided it\u2019s easy for people to see that these are more for agreement/annotator kinds of questions. I think your mention of Prabhakaran et al (2021) means that you probably do release these but it\u2019s not completely clear to me.  4d) What do you think are the consequences of removing offensive tweets in Algerian Arabic? And was your intention more to produce a non-offensive dataset or to not disturb your annotators? I think it is relevant to share your intentions for this.  5) The idea that Yor\u00f9b\u00e1 is a generally good source language strikes me as very strange. I mean, you are empirically showing it to be the case and mention Adelani et al\u2019s similar finding in NER but...this strikes me as \u201csurely a coincidence\u201d or perhaps because there are some proper names with similar sentiment in Ethiopia and Nigeria\u2026what do you think the most reasonable explanation of this is?\n6) Your plan is to keep releasing more languages (and enable others to follow your lead), which is great. I wonder which languages are at the top of your list. Do you want coverage in terms of speaker populations? Coverage over countries that yet covered? Languages that are outside the language families you\u2019ve already covered? I\u2019m interested in your actual \u201cTop Five\u201d but even moreso for what leads you to that Top Five instead of another Top Five. I think being explicit about this helps readers understand where you're headed/why and what they should keep an eye out for or help with.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "xSHspiDcCx",
        "similarity": 0.6896,
        "coverage": 0.4483,
        "human_length": 270,
        "human_text": "paper_topic_and_main_contributions: This paper describes a sentiment dataset creation for 14 African languages and performed several experiments to classify the tweets into sentiment categories.\n\nreasons_to_accept: 1. This dataset with 14 African languages benefits the NLP community to develop sentiment classification tools. \n2. The data collection challenges were described in detail which is useful for NLP researchers working for low-resource languages.\n\nreasons_to_reject: 1. There is no innovation in implemetation for NLP development.\n\nquestions_for_the_authors: 1. Sentiment categories are not explicitly mentioned in the text. I can guess from Table 1 that there are three categories (positive, negative and neutral). The authors should provide the categories in the text.  2. More details on the models will be helpful. How the fine tunning was performed? What are the tunning parameters? The architecture of the system will be helpful.  3. As the F1-score for the system are no great, what can be done to improve the system performances?  4. What was the reason to test the cross-lingual system only on orm and tir languages?  5. Is there any statistics avaialble on percentage of code-switching and code-mixing? It will be great to have these information in the paper.  6. Are these trained models will also be avialble for the researchers?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "nDCeASmceL",
        "length": 227,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a new Twitter benchmark for sentiment analysis in african languages, and provide experimental results with few baseline models.\n\nreasons_to_accept: - A new useful dataset that can be used for sentiment analysis in Afriacan languages.\n- Extensive experimental results on the benchmark.\n\nreasons_to_reject: - Limited explanation of imbalanced data distribution in Table 6. Why there is no training samples in 'orm' and 'tir'? I see that there was a difficulty in collecting data from annotators, but it does not explain why we don't have training samples. For example, we can have 1500 training samples and 600 test samples in 'orm' language.\n- More detailed experimental results are required. In Table 8, only F1 score is reported, but it is better to give per-label precision and recall as well.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "pKSBS3XZlG",
        "length": 1321,
        "human_text": "paper_topic_and_main_contributions: The authors present a dataset of 12 African languages, annotated for sentiment by native (and usually local) speakers. They describe challenges in collection and annotation. They also present some results from pre-trained language models, discussing which models do best with which languages and which source languages do the best in zero-shot learning for Oromo and Tigrinya.\n\nreasons_to_accept: A very good reason to accept this paper is the dataset itself and the ability to talk about collection and annotation outside the parts of the world NLP researchers typically focus on. The authors share specific problems and their solutions that should also be good for discussion and help researchers who are interested in working on other lower resource languages.  They are also releasing the annotations, not just the majority-vote label, which should be useful for researchers interested in methodologies and inter-annotator agreement topics.\n\nreasons_to_reject: There are no risks for having this paper presented at the conference. In the next section, you'll see me ask a bunch of questions of the authors but I don't really consider these weaknesses, certainly not enough to jump up to a reason to reject.\n\nquestions_for_the_authors: I'm going to ask my questions in the order they pop up in the text, but I'll try to give a sense of which ones I think are more important to address: 1) You have picked sentiment and in lines 47-56, you mention a number of reasons that sentiment may be useful (literary analysis, culturonomics, commerce, psychology, social science). One that I don't think those quite capture would be \"linguistic\". Here, I'm thinking about how, for example, there's a malefactive particle in at least one Ethiopian language that lets a speaker/writer give the sense that someone did something AGAINST someone else, in Dutch there's \"tet\" which marks surprise/irritation, in Czech diphthongs can mean affection (or pejoration), in Cantonese, throwing a -k at the end of particles can intensify the emotion being expressed, in Tongan there are determiners that express sympathy, in Manambu there is a \"frustrative\" case marking for when things are done in vain, etc.  In my heart of hearts, my hope is that your dataset could help people find affective markers/word-order/morphemes but...is that plausible, do you think? Or is your expectation that what your annotators picked up on were louder words like adjectives (\"awful\", \"wonderful\", etc). I don't think this particularly needs to be added to your paper, but I am curious about how you'd look at the dataset being used for more linguistic/sociolinguistic/pragmatics research questions.\n2) You mentioned using Saif Mohammad's sentiment guidelines, I think you mean the \"base\" questions he has (not the ones that try to get at what the sentiment is direct towards). It probably would be good to share the explicit instructions for annotators, probably in an appendix. This I do think is probably a minor weakness and worth addressing, btw. It also connects to some of your comments on code-switching texts\u2014you have some annotators who don't know parts of those mixed language tweets and it feels like they should have a \"I don't understand this\" option (which would also be relevant for everyone else since I am certain as a native English speaker you could give me completely English tweets that I could not understand to give a sentiment to).  2b) Part of this question comes from looking at your examples in Table 1. The Amharic example kicks off with the very powerful bigram \"\u1328\u12ab\u129d \u12a0\u1228\u1218\u1294\" (~brutal barbarian, pitiless pagan) but the rest of it has a much lighter tone as far as I can tell. I don't know the referent but it strikes me that this is fairly sarcastic and has an intent to be humorous which is \"negative\" in a way that is less clear than \"I hate X\". You mention sarcasm a number of times\u2014as does Mohammad 2016\u2014I'm wondering what you might say about that topic more generally.  This also feels related to the annotation problem you describe in Twi with \u201cTweaa\u201d being potentially offensive depending on context\u2014many annotators have a hard time distinguishing a speaker/writer\u2019s sentiment when they use taboo terms (the terms that one community of speakers might think is unexceptional or just expressive may be no-go\u2019s for other speakers who ignore the speaker intention). Do you have any thoughts about your annotators\u2019 predilections on these things?  If I gave you a magic button to press that would replace positive/negative/neutral with any categories of your choosing with high accuracy, which categories would you pick? That is, is valence the real thing you most want or is it a stand-in/approximation/first-step for something you actually would rather collect but is harder?\n3) You briefly mention some of the ways African languages present challenges for sentiment analysis that English and other languages may not. You mention tone, code-switching, and digraphia in lines 127-128 but I don't think you actually talk about tone. Here, I assume you mean phonemic tone since lots of African languages are tonal in nature. You discuss code-switching and digraphia but do you have anything to say about phonemic tone and your dataset/annotations? About half your languages use tone but I think only Yor\u00f9b\u00e1 marks them in writing...but maybe the lack of marking them in tweets causes some extra homographs but...I'm not sure how that would be different than other languages which are replete with homographs (English lead=metal, lead=guide).  4) You have to do a bunch of things to get a meaningful (not-all-neutral) set of tweets. I think you do a pretty good job but I wonder if you could address what you see as the consequences of these choices for possible uses of the dataset. This is a suggestion I think would help the paper.  4b) A minor thing but why are the Nigerian language test sets lower-cased (you reference Muhammad et al 2022, but it\u2019d be nice if you give the reason so readers don\u2019t have to track that reference down).  4c) You discard \u201cfull disagreement\u201d tweets\u2014are those going to be in the dataset? I think that may help with the use of your dataset, provided it\u2019s easy for people to see that these are more for agreement/annotator kinds of questions. I think your mention of Prabhakaran et al (2021) means that you probably do release these but it\u2019s not completely clear to me.  4d) What do you think are the consequences of removing offensive tweets in Algerian Arabic? And was your intention more to produce a non-offensive dataset or to not disturb your annotators? I think it is relevant to share your intentions for this.  5) The idea that Yor\u00f9b\u00e1 is a generally good source language strikes me as very strange. I mean, you are empirically showing it to be the case and mention Adelani et al\u2019s similar finding in NER but...this strikes me as \u201csurely a coincidence\u201d or perhaps because there are some proper names with similar sentiment in Ethiopia and Nigeria\u2026what do you think the most reasonable explanation of this is?\n6) Your plan is to keep releasing more languages (and enable others to follow your lead), which is great. I wonder which languages are at the top of your list. Do you want coverage in terms of speaker populations? Coverage over countries that yet covered? Languages that are outside the language families you\u2019ve already covered? I\u2019m interested in your actual \u201cTop Five\u201d but even moreso for what leads you to that Top Five instead of another Top Five. I think being explicit about this helps readers understand where you're headed/why and what they should keep an eye out for or help with.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "xSHspiDcCx",
        "length": 270,
        "human_text": "paper_topic_and_main_contributions: This paper describes a sentiment dataset creation for 14 African languages and performed several experiments to classify the tweets into sentiment categories.\n\nreasons_to_accept: 1. This dataset with 14 African languages benefits the NLP community to develop sentiment classification tools. \n2. The data collection challenges were described in detail which is useful for NLP researchers working for low-resource languages.\n\nreasons_to_reject: 1. There is no innovation in implemetation for NLP development.\n\nquestions_for_the_authors: 1. Sentiment categories are not explicitly mentioned in the text. I can guess from Table 1 that there are three categories (positive, negative and neutral). The authors should provide the categories in the text.  2. More details on the models will be helpful. How the fine tunning was performed? What are the tunning parameters? The architecture of the system will be helpful.  3. As the F1-score for the system are no great, what can be done to improve the system performances?  4. What was the reason to test the cross-lingual system only on orm and tir languages?  5. Is there any statistics avaialble on percentage of code-switching and code-mixing? It will be great to have these information in the paper.  6. Are these trained models will also be avialble for the researchers?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "11_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_11_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7053666666666666,
      "max_similarity": 0.7186,
      "avg_coverage": 0.48619999999999997,
      "max_coverage": 0.8571
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 599,
      "avg_human_length": 606.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 8,
      "weaknesses_count": 10,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "nDCeASmceL",
        "similarity": 0.701,
        "coverage": 0.8571,
        "human_length": 227,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a new Twitter benchmark for sentiment analysis in african languages, and provide experimental results with few baseline models.\n\nreasons_to_accept: - A new useful dataset that can be used for sentiment analysis in Afriacan languages.\n- Extensive experimental results on the benchmark.\n\nreasons_to_reject: - Limited explanation of imbalanced data distribution in Table 6. Why there is no training samples in 'orm' and 'tir'? I see that there was a difficulty in collecting data from annotators, but it does not explain why we don't have training samples. For example, we can have 1500 training samples and 600 test samples in 'orm' language.\n- More detailed experimental results are required. In Table 8, only F1 score is reported, but it is better to give per-label precision and recall as well.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "pKSBS3XZlG",
        "similarity": 0.7186,
        "coverage": 0.2222,
        "human_length": 1321,
        "human_text": "paper_topic_and_main_contributions: The authors present a dataset of 12 African languages, annotated for sentiment by native (and usually local) speakers. They describe challenges in collection and annotation. They also present some results from pre-trained language models, discussing which models do best with which languages and which source languages do the best in zero-shot learning for Oromo and Tigrinya.\n\nreasons_to_accept: A very good reason to accept this paper is the dataset itself and the ability to talk about collection and annotation outside the parts of the world NLP researchers typically focus on. The authors share specific problems and their solutions that should also be good for discussion and help researchers who are interested in working on other lower resource languages.  They are also releasing the annotations, not just the majority-vote label, which should be useful for researchers interested in methodologies and inter-annotator agreement topics.\n\nreasons_to_reject: There are no risks for having this paper presented at the conference. In the next section, you'll see me ask a bunch of questions of the authors but I don't really consider these weaknesses, certainly not enough to jump up to a reason to reject.\n\nquestions_for_the_authors: I'm going to ask my questions in the order they pop up in the text, but I'll try to give a sense of which ones I think are more important to address: 1) You have picked sentiment and in lines 47-56, you mention a number of reasons that sentiment may be useful (literary analysis, culturonomics, commerce, psychology, social science). One that I don't think those quite capture would be \"linguistic\". Here, I'm thinking about how, for example, there's a malefactive particle in at least one Ethiopian language that lets a speaker/writer give the sense that someone did something AGAINST someone else, in Dutch there's \"tet\" which marks surprise/irritation, in Czech diphthongs can mean affection (or pejoration), in Cantonese, throwing a -k at the end of particles can intensify the emotion being expressed, in Tongan there are determiners that express sympathy, in Manambu there is a \"frustrative\" case marking for when things are done in vain, etc.  In my heart of hearts, my hope is that your dataset could help people find affective markers/word-order/morphemes but...is that plausible, do you think? Or is your expectation that what your annotators picked up on were louder words like adjectives (\"awful\", \"wonderful\", etc). I don't think this particularly needs to be added to your paper, but I am curious about how you'd look at the dataset being used for more linguistic/sociolinguistic/pragmatics research questions.\n2) You mentioned using Saif Mohammad's sentiment guidelines, I think you mean the \"base\" questions he has (not the ones that try to get at what the sentiment is direct towards). It probably would be good to share the explicit instructions for annotators, probably in an appendix. This I do think is probably a minor weakness and worth addressing, btw. It also connects to some of your comments on code-switching texts\u2014you have some annotators who don't know parts of those mixed language tweets and it feels like they should have a \"I don't understand this\" option (which would also be relevant for everyone else since I am certain as a native English speaker you could give me completely English tweets that I could not understand to give a sentiment to).  2b) Part of this question comes from looking at your examples in Table 1. The Amharic example kicks off with the very powerful bigram \"\u1328\u12ab\u129d \u12a0\u1228\u1218\u1294\" (~brutal barbarian, pitiless pagan) but the rest of it has a much lighter tone as far as I can tell. I don't know the referent but it strikes me that this is fairly sarcastic and has an intent to be humorous which is \"negative\" in a way that is less clear than \"I hate X\". You mention sarcasm a number of times\u2014as does Mohammad 2016\u2014I'm wondering what you might say about that topic more generally.  This also feels related to the annotation problem you describe in Twi with \u201cTweaa\u201d being potentially offensive depending on context\u2014many annotators have a hard time distinguishing a speaker/writer\u2019s sentiment when they use taboo terms (the terms that one community of speakers might think is unexceptional or just expressive may be no-go\u2019s for other speakers who ignore the speaker intention). Do you have any thoughts about your annotators\u2019 predilections on these things?  If I gave you a magic button to press that would replace positive/negative/neutral with any categories of your choosing with high accuracy, which categories would you pick? That is, is valence the real thing you most want or is it a stand-in/approximation/first-step for something you actually would rather collect but is harder?\n3) You briefly mention some of the ways African languages present challenges for sentiment analysis that English and other languages may not. You mention tone, code-switching, and digraphia in lines 127-128 but I don't think you actually talk about tone. Here, I assume you mean phonemic tone since lots of African languages are tonal in nature. You discuss code-switching and digraphia but do you have anything to say about phonemic tone and your dataset/annotations? About half your languages use tone but I think only Yor\u00f9b\u00e1 marks them in writing...but maybe the lack of marking them in tweets causes some extra homographs but...I'm not sure how that would be different than other languages which are replete with homographs (English lead=metal, lead=guide).  4) You have to do a bunch of things to get a meaningful (not-all-neutral) set of tweets. I think you do a pretty good job but I wonder if you could address what you see as the consequences of these choices for possible uses of the dataset. This is a suggestion I think would help the paper.  4b) A minor thing but why are the Nigerian language test sets lower-cased (you reference Muhammad et al 2022, but it\u2019d be nice if you give the reason so readers don\u2019t have to track that reference down).  4c) You discard \u201cfull disagreement\u201d tweets\u2014are those going to be in the dataset? I think that may help with the use of your dataset, provided it\u2019s easy for people to see that these are more for agreement/annotator kinds of questions. I think your mention of Prabhakaran et al (2021) means that you probably do release these but it\u2019s not completely clear to me.  4d) What do you think are the consequences of removing offensive tweets in Algerian Arabic? And was your intention more to produce a non-offensive dataset or to not disturb your annotators? I think it is relevant to share your intentions for this.  5) The idea that Yor\u00f9b\u00e1 is a generally good source language strikes me as very strange. I mean, you are empirically showing it to be the case and mention Adelani et al\u2019s similar finding in NER but...this strikes me as \u201csurely a coincidence\u201d or perhaps because there are some proper names with similar sentiment in Ethiopia and Nigeria\u2026what do you think the most reasonable explanation of this is?\n6) Your plan is to keep releasing more languages (and enable others to follow your lead), which is great. I wonder which languages are at the top of your list. Do you want coverage in terms of speaker populations? Coverage over countries that yet covered? Languages that are outside the language families you\u2019ve already covered? I\u2019m interested in your actual \u201cTop Five\u201d but even moreso for what leads you to that Top Five instead of another Top Five. I think being explicit about this helps readers understand where you're headed/why and what they should keep an eye out for or help with.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "xSHspiDcCx",
        "similarity": 0.6965,
        "coverage": 0.3793,
        "human_length": 270,
        "human_text": "paper_topic_and_main_contributions: This paper describes a sentiment dataset creation for 14 African languages and performed several experiments to classify the tweets into sentiment categories.\n\nreasons_to_accept: 1. This dataset with 14 African languages benefits the NLP community to develop sentiment classification tools. \n2. The data collection challenges were described in detail which is useful for NLP researchers working for low-resource languages.\n\nreasons_to_reject: 1. There is no innovation in implemetation for NLP development.\n\nquestions_for_the_authors: 1. Sentiment categories are not explicitly mentioned in the text. I can guess from Table 1 that there are three categories (positive, negative and neutral). The authors should provide the categories in the text.  2. More details on the models will be helpful. How the fine tunning was performed? What are the tunning parameters? The architecture of the system will be helpful.  3. As the F1-score for the system are no great, what can be done to improve the system performances?  4. What was the reason to test the cross-lingual system only on orm and tir languages?  5. Is there any statistics avaialble on percentage of code-switching and code-mixing? It will be great to have these information in the paper.  6. Are these trained models will also be avialble for the researchers?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "nDCeASmceL",
        "length": 227,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a new Twitter benchmark for sentiment analysis in african languages, and provide experimental results with few baseline models.\n\nreasons_to_accept: - A new useful dataset that can be used for sentiment analysis in Afriacan languages.\n- Extensive experimental results on the benchmark.\n\nreasons_to_reject: - Limited explanation of imbalanced data distribution in Table 6. Why there is no training samples in 'orm' and 'tir'? I see that there was a difficulty in collecting data from annotators, but it does not explain why we don't have training samples. For example, we can have 1500 training samples and 600 test samples in 'orm' language.\n- More detailed experimental results are required. In Table 8, only F1 score is reported, but it is better to give per-label precision and recall as well.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "pKSBS3XZlG",
        "length": 1321,
        "human_text": "paper_topic_and_main_contributions: The authors present a dataset of 12 African languages, annotated for sentiment by native (and usually local) speakers. They describe challenges in collection and annotation. They also present some results from pre-trained language models, discussing which models do best with which languages and which source languages do the best in zero-shot learning for Oromo and Tigrinya.\n\nreasons_to_accept: A very good reason to accept this paper is the dataset itself and the ability to talk about collection and annotation outside the parts of the world NLP researchers typically focus on. The authors share specific problems and their solutions that should also be good for discussion and help researchers who are interested in working on other lower resource languages.  They are also releasing the annotations, not just the majority-vote label, which should be useful for researchers interested in methodologies and inter-annotator agreement topics.\n\nreasons_to_reject: There are no risks for having this paper presented at the conference. In the next section, you'll see me ask a bunch of questions of the authors but I don't really consider these weaknesses, certainly not enough to jump up to a reason to reject.\n\nquestions_for_the_authors: I'm going to ask my questions in the order they pop up in the text, but I'll try to give a sense of which ones I think are more important to address: 1) You have picked sentiment and in lines 47-56, you mention a number of reasons that sentiment may be useful (literary analysis, culturonomics, commerce, psychology, social science). One that I don't think those quite capture would be \"linguistic\". Here, I'm thinking about how, for example, there's a malefactive particle in at least one Ethiopian language that lets a speaker/writer give the sense that someone did something AGAINST someone else, in Dutch there's \"tet\" which marks surprise/irritation, in Czech diphthongs can mean affection (or pejoration), in Cantonese, throwing a -k at the end of particles can intensify the emotion being expressed, in Tongan there are determiners that express sympathy, in Manambu there is a \"frustrative\" case marking for when things are done in vain, etc.  In my heart of hearts, my hope is that your dataset could help people find affective markers/word-order/morphemes but...is that plausible, do you think? Or is your expectation that what your annotators picked up on were louder words like adjectives (\"awful\", \"wonderful\", etc). I don't think this particularly needs to be added to your paper, but I am curious about how you'd look at the dataset being used for more linguistic/sociolinguistic/pragmatics research questions.\n2) You mentioned using Saif Mohammad's sentiment guidelines, I think you mean the \"base\" questions he has (not the ones that try to get at what the sentiment is direct towards). It probably would be good to share the explicit instructions for annotators, probably in an appendix. This I do think is probably a minor weakness and worth addressing, btw. It also connects to some of your comments on code-switching texts\u2014you have some annotators who don't know parts of those mixed language tweets and it feels like they should have a \"I don't understand this\" option (which would also be relevant for everyone else since I am certain as a native English speaker you could give me completely English tweets that I could not understand to give a sentiment to).  2b) Part of this question comes from looking at your examples in Table 1. The Amharic example kicks off with the very powerful bigram \"\u1328\u12ab\u129d \u12a0\u1228\u1218\u1294\" (~brutal barbarian, pitiless pagan) but the rest of it has a much lighter tone as far as I can tell. I don't know the referent but it strikes me that this is fairly sarcastic and has an intent to be humorous which is \"negative\" in a way that is less clear than \"I hate X\". You mention sarcasm a number of times\u2014as does Mohammad 2016\u2014I'm wondering what you might say about that topic more generally.  This also feels related to the annotation problem you describe in Twi with \u201cTweaa\u201d being potentially offensive depending on context\u2014many annotators have a hard time distinguishing a speaker/writer\u2019s sentiment when they use taboo terms (the terms that one community of speakers might think is unexceptional or just expressive may be no-go\u2019s for other speakers who ignore the speaker intention). Do you have any thoughts about your annotators\u2019 predilections on these things?  If I gave you a magic button to press that would replace positive/negative/neutral with any categories of your choosing with high accuracy, which categories would you pick? That is, is valence the real thing you most want or is it a stand-in/approximation/first-step for something you actually would rather collect but is harder?\n3) You briefly mention some of the ways African languages present challenges for sentiment analysis that English and other languages may not. You mention tone, code-switching, and digraphia in lines 127-128 but I don't think you actually talk about tone. Here, I assume you mean phonemic tone since lots of African languages are tonal in nature. You discuss code-switching and digraphia but do you have anything to say about phonemic tone and your dataset/annotations? About half your languages use tone but I think only Yor\u00f9b\u00e1 marks them in writing...but maybe the lack of marking them in tweets causes some extra homographs but...I'm not sure how that would be different than other languages which are replete with homographs (English lead=metal, lead=guide).  4) You have to do a bunch of things to get a meaningful (not-all-neutral) set of tweets. I think you do a pretty good job but I wonder if you could address what you see as the consequences of these choices for possible uses of the dataset. This is a suggestion I think would help the paper.  4b) A minor thing but why are the Nigerian language test sets lower-cased (you reference Muhammad et al 2022, but it\u2019d be nice if you give the reason so readers don\u2019t have to track that reference down).  4c) You discard \u201cfull disagreement\u201d tweets\u2014are those going to be in the dataset? I think that may help with the use of your dataset, provided it\u2019s easy for people to see that these are more for agreement/annotator kinds of questions. I think your mention of Prabhakaran et al (2021) means that you probably do release these but it\u2019s not completely clear to me.  4d) What do you think are the consequences of removing offensive tweets in Algerian Arabic? And was your intention more to produce a non-offensive dataset or to not disturb your annotators? I think it is relevant to share your intentions for this.  5) The idea that Yor\u00f9b\u00e1 is a generally good source language strikes me as very strange. I mean, you are empirically showing it to be the case and mention Adelani et al\u2019s similar finding in NER but...this strikes me as \u201csurely a coincidence\u201d or perhaps because there are some proper names with similar sentiment in Ethiopia and Nigeria\u2026what do you think the most reasonable explanation of this is?\n6) Your plan is to keep releasing more languages (and enable others to follow your lead), which is great. I wonder which languages are at the top of your list. Do you want coverage in terms of speaker populations? Coverage over countries that yet covered? Languages that are outside the language families you\u2019ve already covered? I\u2019m interested in your actual \u201cTop Five\u201d but even moreso for what leads you to that Top Five instead of another Top Five. I think being explicit about this helps readers understand where you're headed/why and what they should keep an eye out for or help with.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "xSHspiDcCx",
        "length": 270,
        "human_text": "paper_topic_and_main_contributions: This paper describes a sentiment dataset creation for 14 African languages and performed several experiments to classify the tweets into sentiment categories.\n\nreasons_to_accept: 1. This dataset with 14 African languages benefits the NLP community to develop sentiment classification tools. \n2. The data collection challenges were described in detail which is useful for NLP researchers working for low-resource languages.\n\nreasons_to_reject: 1. There is no innovation in implemetation for NLP development.\n\nquestions_for_the_authors: 1. Sentiment categories are not explicitly mentioned in the text. I can guess from Table 1 that there are three categories (positive, negative and neutral). The authors should provide the categories in the text.  2. More details on the models will be helpful. How the fine tunning was performed? What are the tunning parameters? The architecture of the system will be helpful.  3. As the F1-score for the system are no great, what can be done to improve the system performances?  4. What was the reason to test the cross-lingual system only on orm and tir languages?  5. Is there any statistics avaialble on percentage of code-switching and code-mixing? It will be great to have these information in the paper.  6. Are these trained models will also be avialble for the researchers?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "216_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_216_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6958666666666667,
      "max_similarity": 0.7317,
      "avg_coverage": 0.49760000000000004,
      "max_coverage": 0.6667
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 382,
      "avg_human_length": 568.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 5,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "bhgJFUIOjq",
        "similarity": 0.6753,
        "coverage": 0.6667,
        "human_length": 343,
        "human_text": "paper_topic_and_main_contributions: The authors present a series of improvements to the T5 language model architecture, focusing on both increased input context and improvements to speed for both training and inference. Specifically, they extend the LongT5 model with a routing mechanism that examines only a portion of the input sequence deemed importance by a learnable score with a 'heavy' component with more network resources and a lower-dimensional 'light' component that attends to the entire input sequence. Multi-query cross-attention is used instead of standard multi-head attention to improve inference speed and the UL2  (span corruption, prefix, causal LM) training objective is used in place of LongT5's PEGASUS sentence reconstruction objective.\n\nreasons_to_accept: The authors carefully set up experiments to show how their approaches both improve on training and inference time and how each of the algorithmic improvements compares to the LongT5 model presented as the baseline. Training speed increases of 35-75% and inference speed increases of 50-100% are shown as well as improvements in output quality when increasing input token sizes from 16k to 64k.\n\nreasons_to_reject: The improvements to the model architecture do increase speed and performance by using computational 'tricks', but do not address overall algorithm complexity (e.g. the transformer algorithm used still has overall quadric complexity.) However, the authors architecture choices do have concrete runtime improvements when considering potential real-world use cases.\nThe authors state this type of model needs to be trained from scratch, which may be prohibitive for some compute-constrained research organizations. This can be mitigated by providing a pretrained base model to serve as a starting point.\n\ntypos_grammar_style_and_presentation_improvements: It would be nice to see the \"best\" scores for each column highlighted in Table 6 (similar to Table 3.)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "PY6rAyg2TB",
        "similarity": 0.6806,
        "coverage": 0.5,
        "human_length": 456,
        "human_text": "paper_topic_and_main_contributions: The authors present a transformer model designed for handling long-input texts. They split the computational process into two branches: a light one that processes all tokens and a heavy one that focuses on important tokens selected by a trainable routing network. By incorporating multi-query cross-attention and pretraining with UL2, they showcase the model's performance and efficiency in both few-shot and fine-tuning scenarios.\n\nreasons_to_accept: - The paper has a clear and well-organized structure, with a concise statement of objectives and clear description of the methodology.\n- The idea of integrating both a light route and a heavy route using a dynamic routing mechanism is thoughtfully designed and appears to be an innovative approach, particularly for long-range sequence-to-sequence models.\n- The extensive range of experiments conducted across various NLP tasks convincingly demonstrates the model's superior performance.\n- The inclusion of a thorough ablation study and analysis on the routed tokens provides valuable insights into the design decisions and inner workings of the model.\n\nreasons_to_reject: My primary concern is that the selected important tokens ratio (m) in this mechanism remains static after pretraining. In some cases, particularly when training a multilingual version of this model, the ratio of important tokens could be dependent on the task/language. I wonder if it is possible to adjust this ratio during finetuning?\n\nquestions_for_the_authors: A. In the computation of FLOPs for the FFds, it's mentioned in lines 231 and 231 that r_L and r_H represent the dimension ratios relative to the standard T5. Considering this, it appears that the formula on line 235 should include squares for both r_H and r_L since they are applied to \"d.\" If that adjustment is made, the entire FLOPs formulation and final FLOPs formulas would likely change, although this change should not affect the experimentation results.\nB: Is it possible for this architecture to accommodate a more customized setting where each layer has its own ratio? For instance, the model could have larger local attention windows \"w\" and lower ratios \"m\" for the early layers, while the higher layers could have smaller local attention windows and higher important token ratios, considering their increased contextualization.\n\ntypos_grammar_style_and_presentation_improvements: A. It would be an improvement if some concepts have a brief description, like the UL2 objective and MQA.\nB. If PEGASUS is weaker in a few-shot setup (according to lines 474-475) better demonstrate in some experimental results.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "DdP7Ozq5lF",
        "similarity": 0.7317,
        "coverage": 0.3261,
        "human_length": 907,
        "human_text": "paper_topic_and_main_contributions: - This paper deals with the problem of handling longer inputs with Transformers. \u00a0 - Problem arises not only due to quadratic attention complexity but also because of\u00a0 feedforward and attention layer projection to every token - Authors build on top of LongT5 and suggest to divide each feedforward and attention layer into light branch and heavy branch and perform routing of tokens.\n- Consists of Light branch and heavy branch     * Light branch : caters to all tokens, has lower hidden dimension, performs local attention, has fewer heads than LongT5     * Heavy branch : caters to important tokens, has higher hidden dimension, performs global attention, has more heads than LongT5 - Authors suggest learning routing function using learned embeddings.\n- Apart from architectural change, authors also use a different objective for pre-training and add multi-query cross-attention to speed up inference - CoLT5 is faster when it comes to inference or fine-tuning as compared to LongT5. Does better on almost all the datasets as compared to LongT5 and achieves SOTA results on SCROLLS benchmark - As input length scales up, the model performance improves and it achieves faster speed than LongT5 (model scales well)\n\nreasons_to_accept: - Motivation and intuition behind the paper are explained really well.\n- Changes to the existing architecture of LongT5 are quite simple and novel.\n- CoLT5 is not just doing well when it comes to numbers but also efficient as compared to the baseline mentioned.\n- Conditional computing section is explained thoroughly except for Conditional Attention sub-section - Utilizes SCROLLS benchmark precisely made for the task at hand covering summarization, question answering, and natural language inference tasks.\n- Promising performance with extremely long inputs (~64k tokens). This is one of the main highlights of the paper for me.\n- Ablation study is done quite well serving as a foundation for upcoming models dealing with large input length\n\nreasons_to_reject: - Results look great, however, SCROLLS is limited to English only. Testing on benchmarks focusing on other languages as well could have been helpful.\n- Selection of light and heavy hidden dimensions (rL, rH) looks a bit hacky especially for calculating FLOPS for the CoLT5 attention layer\u00a0 (rL=\u00bc, rH=\u00be). An ideal scenario would have been to show trade off with different hidden dimensions, corresponding parameter count and show its effect on downstream tasks (apologies if it\u2019s already mentioned anywhere)\n\nquestions_for_the_authors: A. Apologies if I\u2019ve missed that part but after going through the pre-training section (page 6) I realized the dataset on which CoLT5 is pre-trained isn\u2019t mentioned.\nB. Reproducibility - Numbers of CoLT5 -B in Table 6 and Table 3 are different. I wonder why that is happening. QAS F1 score is 42.1 v/s 38.3 in Table 3 and Table 6 respectively. Same for QMS and GovR Rgm scores have a difference of almost 5% which is a bit high. Let me know if I\u2019m missing something that\u2019s different in both these tables. \u00a0 C. Are all these experiments done on multiple seeds because I couldn\u2019t see that anywhere.\nD. In Table 5, when you say the number of Natural Questions or TriviaQA, the number stated is in decimals like 0.1. Is it % or in hundreds, thousands? It will be great if you can clear that.\nE. Why didn\u2019t you pre-train CoLT5 on input length equivalent to that of the dataset with the highest median input length in Table 4? Was it because there were no significant gains after a certain input length or did it take too long?\nF. I can\u2019t see the number of parameters for different versions of CoLT5. The paper only mentions that it has more overall parameters as compared to LongT5 but uses less compute (Update: The concerned numbers are mentioned in Appendix, I\u2019d recommend moving them in the main paper. Not the whole table but the parameter count)\n\ntypos_grammar_style_and_presentation_improvements: - One minor correction: Page 6, \u201cSCROLLS contains question-answering datasets: Narrative QA \u2026, NLI dataset: ContractNLI.., and summarization datasets: SummScreenFD.. \u201d. Minor punctuation adjustments that can improve clarity and readability.\n- \u201cFeedforward and projection layer\u201d and \u201cfeedforward and attention layer\u201d have been used throughout the paper. I\u2019d recommend sticking to one of those preferably feedforward and attention projection layers.\n- Figure 2 doesn\u2019t mention the metric of average perf which is F1 score. Also, the caption should be: CoLT5 achieves faster performance than LongT5 for the same number of parameters instead of saying CoLT5 achieves stronger performance than LongT5 at any speed.\n- In Table 3 you have computed the average score under the Avg column, however, these metrics may not always be directly comparable, so averaging them might not accurately represent the overall performance. You should have averaged similar metrics like TQA, NQA, QAS under avg F1, QuAL, CNLI under avg EM, and arXiv, SumS, QMS, GovR under avg Rgm.\n- I feel the equation for calculating FLOPs (attn), the global projection part, can be explained a bit. It took me some time to understand that (this one is totally up to you, if you think readers can figure this out easily then don\u2019t)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "bhgJFUIOjq",
        "length": 343,
        "human_text": "paper_topic_and_main_contributions: The authors present a series of improvements to the T5 language model architecture, focusing on both increased input context and improvements to speed for both training and inference. Specifically, they extend the LongT5 model with a routing mechanism that examines only a portion of the input sequence deemed importance by a learnable score with a 'heavy' component with more network resources and a lower-dimensional 'light' component that attends to the entire input sequence. Multi-query cross-attention is used instead of standard multi-head attention to improve inference speed and the UL2  (span corruption, prefix, causal LM) training objective is used in place of LongT5's PEGASUS sentence reconstruction objective.\n\nreasons_to_accept: The authors carefully set up experiments to show how their approaches both improve on training and inference time and how each of the algorithmic improvements compares to the LongT5 model presented as the baseline. Training speed increases of 35-75% and inference speed increases of 50-100% are shown as well as improvements in output quality when increasing input token sizes from 16k to 64k.\n\nreasons_to_reject: The improvements to the model architecture do increase speed and performance by using computational 'tricks', but do not address overall algorithm complexity (e.g. the transformer algorithm used still has overall quadric complexity.) However, the authors architecture choices do have concrete runtime improvements when considering potential real-world use cases.\nThe authors state this type of model needs to be trained from scratch, which may be prohibitive for some compute-constrained research organizations. This can be mitigated by providing a pretrained base model to serve as a starting point.\n\ntypos_grammar_style_and_presentation_improvements: It would be nice to see the \"best\" scores for each column highlighted in Table 6 (similar to Table 3.)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "PY6rAyg2TB",
        "length": 456,
        "human_text": "paper_topic_and_main_contributions: The authors present a transformer model designed for handling long-input texts. They split the computational process into two branches: a light one that processes all tokens and a heavy one that focuses on important tokens selected by a trainable routing network. By incorporating multi-query cross-attention and pretraining with UL2, they showcase the model's performance and efficiency in both few-shot and fine-tuning scenarios.\n\nreasons_to_accept: - The paper has a clear and well-organized structure, with a concise statement of objectives and clear description of the methodology.\n- The idea of integrating both a light route and a heavy route using a dynamic routing mechanism is thoughtfully designed and appears to be an innovative approach, particularly for long-range sequence-to-sequence models.\n- The extensive range of experiments conducted across various NLP tasks convincingly demonstrates the model's superior performance.\n- The inclusion of a thorough ablation study and analysis on the routed tokens provides valuable insights into the design decisions and inner workings of the model.\n\nreasons_to_reject: My primary concern is that the selected important tokens ratio (m) in this mechanism remains static after pretraining. In some cases, particularly when training a multilingual version of this model, the ratio of important tokens could be dependent on the task/language. I wonder if it is possible to adjust this ratio during finetuning?\n\nquestions_for_the_authors: A. In the computation of FLOPs for the FFds, it's mentioned in lines 231 and 231 that r_L and r_H represent the dimension ratios relative to the standard T5. Considering this, it appears that the formula on line 235 should include squares for both r_H and r_L since they are applied to \"d.\" If that adjustment is made, the entire FLOPs formulation and final FLOPs formulas would likely change, although this change should not affect the experimentation results.\nB: Is it possible for this architecture to accommodate a more customized setting where each layer has its own ratio? For instance, the model could have larger local attention windows \"w\" and lower ratios \"m\" for the early layers, while the higher layers could have smaller local attention windows and higher important token ratios, considering their increased contextualization.\n\ntypos_grammar_style_and_presentation_improvements: A. It would be an improvement if some concepts have a brief description, like the UL2 objective and MQA.\nB. If PEGASUS is weaker in a few-shot setup (according to lines 474-475) better demonstrate in some experimental results.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "DdP7Ozq5lF",
        "length": 907,
        "human_text": "paper_topic_and_main_contributions: - This paper deals with the problem of handling longer inputs with Transformers. \u00a0 - Problem arises not only due to quadratic attention complexity but also because of\u00a0 feedforward and attention layer projection to every token - Authors build on top of LongT5 and suggest to divide each feedforward and attention layer into light branch and heavy branch and perform routing of tokens.\n- Consists of Light branch and heavy branch     * Light branch : caters to all tokens, has lower hidden dimension, performs local attention, has fewer heads than LongT5     * Heavy branch : caters to important tokens, has higher hidden dimension, performs global attention, has more heads than LongT5 - Authors suggest learning routing function using learned embeddings.\n- Apart from architectural change, authors also use a different objective for pre-training and add multi-query cross-attention to speed up inference - CoLT5 is faster when it comes to inference or fine-tuning as compared to LongT5. Does better on almost all the datasets as compared to LongT5 and achieves SOTA results on SCROLLS benchmark - As input length scales up, the model performance improves and it achieves faster speed than LongT5 (model scales well)\n\nreasons_to_accept: - Motivation and intuition behind the paper are explained really well.\n- Changes to the existing architecture of LongT5 are quite simple and novel.\n- CoLT5 is not just doing well when it comes to numbers but also efficient as compared to the baseline mentioned.\n- Conditional computing section is explained thoroughly except for Conditional Attention sub-section - Utilizes SCROLLS benchmark precisely made for the task at hand covering summarization, question answering, and natural language inference tasks.\n- Promising performance with extremely long inputs (~64k tokens). This is one of the main highlights of the paper for me.\n- Ablation study is done quite well serving as a foundation for upcoming models dealing with large input length\n\nreasons_to_reject: - Results look great, however, SCROLLS is limited to English only. Testing on benchmarks focusing on other languages as well could have been helpful.\n- Selection of light and heavy hidden dimensions (rL, rH) looks a bit hacky especially for calculating FLOPS for the CoLT5 attention layer\u00a0 (rL=\u00bc, rH=\u00be). An ideal scenario would have been to show trade off with different hidden dimensions, corresponding parameter count and show its effect on downstream tasks (apologies if it\u2019s already mentioned anywhere)\n\nquestions_for_the_authors: A. Apologies if I\u2019ve missed that part but after going through the pre-training section (page 6) I realized the dataset on which CoLT5 is pre-trained isn\u2019t mentioned.\nB. Reproducibility - Numbers of CoLT5 -B in Table 6 and Table 3 are different. I wonder why that is happening. QAS F1 score is 42.1 v/s 38.3 in Table 3 and Table 6 respectively. Same for QMS and GovR Rgm scores have a difference of almost 5% which is a bit high. Let me know if I\u2019m missing something that\u2019s different in both these tables. \u00a0 C. Are all these experiments done on multiple seeds because I couldn\u2019t see that anywhere.\nD. In Table 5, when you say the number of Natural Questions or TriviaQA, the number stated is in decimals like 0.1. Is it % or in hundreds, thousands? It will be great if you can clear that.\nE. Why didn\u2019t you pre-train CoLT5 on input length equivalent to that of the dataset with the highest median input length in Table 4? Was it because there were no significant gains after a certain input length or did it take too long?\nF. I can\u2019t see the number of parameters for different versions of CoLT5. The paper only mentions that it has more overall parameters as compared to LongT5 but uses less compute (Update: The concerned numbers are mentioned in Appendix, I\u2019d recommend moving them in the main paper. Not the whole table but the parameter count)\n\ntypos_grammar_style_and_presentation_improvements: - One minor correction: Page 6, \u201cSCROLLS contains question-answering datasets: Narrative QA \u2026, NLI dataset: ContractNLI.., and summarization datasets: SummScreenFD.. \u201d. Minor punctuation adjustments that can improve clarity and readability.\n- \u201cFeedforward and projection layer\u201d and \u201cfeedforward and attention layer\u201d have been used throughout the paper. I\u2019d recommend sticking to one of those preferably feedforward and attention projection layers.\n- Figure 2 doesn\u2019t mention the metric of average perf which is F1 score. Also, the caption should be: CoLT5 achieves faster performance than LongT5 for the same number of parameters instead of saying CoLT5 achieves stronger performance than LongT5 at any speed.\n- In Table 3 you have computed the average score under the Avg column, however, these metrics may not always be directly comparable, so averaging them might not accurately represent the overall performance. You should have averaged similar metrics like TQA, NQA, QAS under avg F1, QuAL, CNLI under avg EM, and arXiv, SumS, QMS, GovR under avg Rgm.\n- I feel the equation for calculating FLOPs (attn), the global projection part, can be explained a bit. It took me some time to understand that (this one is totally up to you, if you think readers can figure this out easily then don\u2019t)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "216_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_216_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6974666666666667,
      "max_similarity": 0.7327,
      "avg_coverage": 0.4322333333333333,
      "max_coverage": 0.5833
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 365,
      "avg_human_length": 568.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 4,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "bhgJFUIOjq",
        "similarity": 0.6747,
        "coverage": 0.5833,
        "human_length": 343,
        "human_text": "paper_topic_and_main_contributions: The authors present a series of improvements to the T5 language model architecture, focusing on both increased input context and improvements to speed for both training and inference. Specifically, they extend the LongT5 model with a routing mechanism that examines only a portion of the input sequence deemed importance by a learnable score with a 'heavy' component with more network resources and a lower-dimensional 'light' component that attends to the entire input sequence. Multi-query cross-attention is used instead of standard multi-head attention to improve inference speed and the UL2  (span corruption, prefix, causal LM) training objective is used in place of LongT5's PEGASUS sentence reconstruction objective.\n\nreasons_to_accept: The authors carefully set up experiments to show how their approaches both improve on training and inference time and how each of the algorithmic improvements compares to the LongT5 model presented as the baseline. Training speed increases of 35-75% and inference speed increases of 50-100% are shown as well as improvements in output quality when increasing input token sizes from 16k to 64k.\n\nreasons_to_reject: The improvements to the model architecture do increase speed and performance by using computational 'tricks', but do not address overall algorithm complexity (e.g. the transformer algorithm used still has overall quadric complexity.) However, the authors architecture choices do have concrete runtime improvements when considering potential real-world use cases.\nThe authors state this type of model needs to be trained from scratch, which may be prohibitive for some compute-constrained research organizations. This can be mitigated by providing a pretrained base model to serve as a starting point.\n\ntypos_grammar_style_and_presentation_improvements: It would be nice to see the \"best\" scores for each column highlighted in Table 6 (similar to Table 3.)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "PY6rAyg2TB",
        "similarity": 0.685,
        "coverage": 0.4091,
        "human_length": 456,
        "human_text": "paper_topic_and_main_contributions: The authors present a transformer model designed for handling long-input texts. They split the computational process into two branches: a light one that processes all tokens and a heavy one that focuses on important tokens selected by a trainable routing network. By incorporating multi-query cross-attention and pretraining with UL2, they showcase the model's performance and efficiency in both few-shot and fine-tuning scenarios.\n\nreasons_to_accept: - The paper has a clear and well-organized structure, with a concise statement of objectives and clear description of the methodology.\n- The idea of integrating both a light route and a heavy route using a dynamic routing mechanism is thoughtfully designed and appears to be an innovative approach, particularly for long-range sequence-to-sequence models.\n- The extensive range of experiments conducted across various NLP tasks convincingly demonstrates the model's superior performance.\n- The inclusion of a thorough ablation study and analysis on the routed tokens provides valuable insights into the design decisions and inner workings of the model.\n\nreasons_to_reject: My primary concern is that the selected important tokens ratio (m) in this mechanism remains static after pretraining. In some cases, particularly when training a multilingual version of this model, the ratio of important tokens could be dependent on the task/language. I wonder if it is possible to adjust this ratio during finetuning?\n\nquestions_for_the_authors: A. In the computation of FLOPs for the FFds, it's mentioned in lines 231 and 231 that r_L and r_H represent the dimension ratios relative to the standard T5. Considering this, it appears that the formula on line 235 should include squares for both r_H and r_L since they are applied to \"d.\" If that adjustment is made, the entire FLOPs formulation and final FLOPs formulas would likely change, although this change should not affect the experimentation results.\nB: Is it possible for this architecture to accommodate a more customized setting where each layer has its own ratio? For instance, the model could have larger local attention windows \"w\" and lower ratios \"m\" for the early layers, while the higher layers could have smaller local attention windows and higher important token ratios, considering their increased contextualization.\n\ntypos_grammar_style_and_presentation_improvements: A. It would be an improvement if some concepts have a brief description, like the UL2 objective and MQA.\nB. If PEGASUS is weaker in a few-shot setup (according to lines 474-475) better demonstrate in some experimental results.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "DdP7Ozq5lF",
        "similarity": 0.7327,
        "coverage": 0.3043,
        "human_length": 907,
        "human_text": "paper_topic_and_main_contributions: - This paper deals with the problem of handling longer inputs with Transformers. \u00a0 - Problem arises not only due to quadratic attention complexity but also because of\u00a0 feedforward and attention layer projection to every token - Authors build on top of LongT5 and suggest to divide each feedforward and attention layer into light branch and heavy branch and perform routing of tokens.\n- Consists of Light branch and heavy branch     * Light branch : caters to all tokens, has lower hidden dimension, performs local attention, has fewer heads than LongT5     * Heavy branch : caters to important tokens, has higher hidden dimension, performs global attention, has more heads than LongT5 - Authors suggest learning routing function using learned embeddings.\n- Apart from architectural change, authors also use a different objective for pre-training and add multi-query cross-attention to speed up inference - CoLT5 is faster when it comes to inference or fine-tuning as compared to LongT5. Does better on almost all the datasets as compared to LongT5 and achieves SOTA results on SCROLLS benchmark - As input length scales up, the model performance improves and it achieves faster speed than LongT5 (model scales well)\n\nreasons_to_accept: - Motivation and intuition behind the paper are explained really well.\n- Changes to the existing architecture of LongT5 are quite simple and novel.\n- CoLT5 is not just doing well when it comes to numbers but also efficient as compared to the baseline mentioned.\n- Conditional computing section is explained thoroughly except for Conditional Attention sub-section - Utilizes SCROLLS benchmark precisely made for the task at hand covering summarization, question answering, and natural language inference tasks.\n- Promising performance with extremely long inputs (~64k tokens). This is one of the main highlights of the paper for me.\n- Ablation study is done quite well serving as a foundation for upcoming models dealing with large input length\n\nreasons_to_reject: - Results look great, however, SCROLLS is limited to English only. Testing on benchmarks focusing on other languages as well could have been helpful.\n- Selection of light and heavy hidden dimensions (rL, rH) looks a bit hacky especially for calculating FLOPS for the CoLT5 attention layer\u00a0 (rL=\u00bc, rH=\u00be). An ideal scenario would have been to show trade off with different hidden dimensions, corresponding parameter count and show its effect on downstream tasks (apologies if it\u2019s already mentioned anywhere)\n\nquestions_for_the_authors: A. Apologies if I\u2019ve missed that part but after going through the pre-training section (page 6) I realized the dataset on which CoLT5 is pre-trained isn\u2019t mentioned.\nB. Reproducibility - Numbers of CoLT5 -B in Table 6 and Table 3 are different. I wonder why that is happening. QAS F1 score is 42.1 v/s 38.3 in Table 3 and Table 6 respectively. Same for QMS and GovR Rgm scores have a difference of almost 5% which is a bit high. Let me know if I\u2019m missing something that\u2019s different in both these tables. \u00a0 C. Are all these experiments done on multiple seeds because I couldn\u2019t see that anywhere.\nD. In Table 5, when you say the number of Natural Questions or TriviaQA, the number stated is in decimals like 0.1. Is it % or in hundreds, thousands? It will be great if you can clear that.\nE. Why didn\u2019t you pre-train CoLT5 on input length equivalent to that of the dataset with the highest median input length in Table 4? Was it because there were no significant gains after a certain input length or did it take too long?\nF. I can\u2019t see the number of parameters for different versions of CoLT5. The paper only mentions that it has more overall parameters as compared to LongT5 but uses less compute (Update: The concerned numbers are mentioned in Appendix, I\u2019d recommend moving them in the main paper. Not the whole table but the parameter count)\n\ntypos_grammar_style_and_presentation_improvements: - One minor correction: Page 6, \u201cSCROLLS contains question-answering datasets: Narrative QA \u2026, NLI dataset: ContractNLI.., and summarization datasets: SummScreenFD.. \u201d. Minor punctuation adjustments that can improve clarity and readability.\n- \u201cFeedforward and projection layer\u201d and \u201cfeedforward and attention layer\u201d have been used throughout the paper. I\u2019d recommend sticking to one of those preferably feedforward and attention projection layers.\n- Figure 2 doesn\u2019t mention the metric of average perf which is F1 score. Also, the caption should be: CoLT5 achieves faster performance than LongT5 for the same number of parameters instead of saying CoLT5 achieves stronger performance than LongT5 at any speed.\n- In Table 3 you have computed the average score under the Avg column, however, these metrics may not always be directly comparable, so averaging them might not accurately represent the overall performance. You should have averaged similar metrics like TQA, NQA, QAS under avg F1, QuAL, CNLI under avg EM, and arXiv, SumS, QMS, GovR under avg Rgm.\n- I feel the equation for calculating FLOPs (attn), the global projection part, can be explained a bit. It took me some time to understand that (this one is totally up to you, if you think readers can figure this out easily then don\u2019t)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "bhgJFUIOjq",
        "length": 343,
        "human_text": "paper_topic_and_main_contributions: The authors present a series of improvements to the T5 language model architecture, focusing on both increased input context and improvements to speed for both training and inference. Specifically, they extend the LongT5 model with a routing mechanism that examines only a portion of the input sequence deemed importance by a learnable score with a 'heavy' component with more network resources and a lower-dimensional 'light' component that attends to the entire input sequence. Multi-query cross-attention is used instead of standard multi-head attention to improve inference speed and the UL2  (span corruption, prefix, causal LM) training objective is used in place of LongT5's PEGASUS sentence reconstruction objective.\n\nreasons_to_accept: The authors carefully set up experiments to show how their approaches both improve on training and inference time and how each of the algorithmic improvements compares to the LongT5 model presented as the baseline. Training speed increases of 35-75% and inference speed increases of 50-100% are shown as well as improvements in output quality when increasing input token sizes from 16k to 64k.\n\nreasons_to_reject: The improvements to the model architecture do increase speed and performance by using computational 'tricks', but do not address overall algorithm complexity (e.g. the transformer algorithm used still has overall quadric complexity.) However, the authors architecture choices do have concrete runtime improvements when considering potential real-world use cases.\nThe authors state this type of model needs to be trained from scratch, which may be prohibitive for some compute-constrained research organizations. This can be mitigated by providing a pretrained base model to serve as a starting point.\n\ntypos_grammar_style_and_presentation_improvements: It would be nice to see the \"best\" scores for each column highlighted in Table 6 (similar to Table 3.)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "PY6rAyg2TB",
        "length": 456,
        "human_text": "paper_topic_and_main_contributions: The authors present a transformer model designed for handling long-input texts. They split the computational process into two branches: a light one that processes all tokens and a heavy one that focuses on important tokens selected by a trainable routing network. By incorporating multi-query cross-attention and pretraining with UL2, they showcase the model's performance and efficiency in both few-shot and fine-tuning scenarios.\n\nreasons_to_accept: - The paper has a clear and well-organized structure, with a concise statement of objectives and clear description of the methodology.\n- The idea of integrating both a light route and a heavy route using a dynamic routing mechanism is thoughtfully designed and appears to be an innovative approach, particularly for long-range sequence-to-sequence models.\n- The extensive range of experiments conducted across various NLP tasks convincingly demonstrates the model's superior performance.\n- The inclusion of a thorough ablation study and analysis on the routed tokens provides valuable insights into the design decisions and inner workings of the model.\n\nreasons_to_reject: My primary concern is that the selected important tokens ratio (m) in this mechanism remains static after pretraining. In some cases, particularly when training a multilingual version of this model, the ratio of important tokens could be dependent on the task/language. I wonder if it is possible to adjust this ratio during finetuning?\n\nquestions_for_the_authors: A. In the computation of FLOPs for the FFds, it's mentioned in lines 231 and 231 that r_L and r_H represent the dimension ratios relative to the standard T5. Considering this, it appears that the formula on line 235 should include squares for both r_H and r_L since they are applied to \"d.\" If that adjustment is made, the entire FLOPs formulation and final FLOPs formulas would likely change, although this change should not affect the experimentation results.\nB: Is it possible for this architecture to accommodate a more customized setting where each layer has its own ratio? For instance, the model could have larger local attention windows \"w\" and lower ratios \"m\" for the early layers, while the higher layers could have smaller local attention windows and higher important token ratios, considering their increased contextualization.\n\ntypos_grammar_style_and_presentation_improvements: A. It would be an improvement if some concepts have a brief description, like the UL2 objective and MQA.\nB. If PEGASUS is weaker in a few-shot setup (according to lines 474-475) better demonstrate in some experimental results.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "DdP7Ozq5lF",
        "length": 907,
        "human_text": "paper_topic_and_main_contributions: - This paper deals with the problem of handling longer inputs with Transformers. \u00a0 - Problem arises not only due to quadratic attention complexity but also because of\u00a0 feedforward and attention layer projection to every token - Authors build on top of LongT5 and suggest to divide each feedforward and attention layer into light branch and heavy branch and perform routing of tokens.\n- Consists of Light branch and heavy branch     * Light branch : caters to all tokens, has lower hidden dimension, performs local attention, has fewer heads than LongT5     * Heavy branch : caters to important tokens, has higher hidden dimension, performs global attention, has more heads than LongT5 - Authors suggest learning routing function using learned embeddings.\n- Apart from architectural change, authors also use a different objective for pre-training and add multi-query cross-attention to speed up inference - CoLT5 is faster when it comes to inference or fine-tuning as compared to LongT5. Does better on almost all the datasets as compared to LongT5 and achieves SOTA results on SCROLLS benchmark - As input length scales up, the model performance improves and it achieves faster speed than LongT5 (model scales well)\n\nreasons_to_accept: - Motivation and intuition behind the paper are explained really well.\n- Changes to the existing architecture of LongT5 are quite simple and novel.\n- CoLT5 is not just doing well when it comes to numbers but also efficient as compared to the baseline mentioned.\n- Conditional computing section is explained thoroughly except for Conditional Attention sub-section - Utilizes SCROLLS benchmark precisely made for the task at hand covering summarization, question answering, and natural language inference tasks.\n- Promising performance with extremely long inputs (~64k tokens). This is one of the main highlights of the paper for me.\n- Ablation study is done quite well serving as a foundation for upcoming models dealing with large input length\n\nreasons_to_reject: - Results look great, however, SCROLLS is limited to English only. Testing on benchmarks focusing on other languages as well could have been helpful.\n- Selection of light and heavy hidden dimensions (rL, rH) looks a bit hacky especially for calculating FLOPS for the CoLT5 attention layer\u00a0 (rL=\u00bc, rH=\u00be). An ideal scenario would have been to show trade off with different hidden dimensions, corresponding parameter count and show its effect on downstream tasks (apologies if it\u2019s already mentioned anywhere)\n\nquestions_for_the_authors: A. Apologies if I\u2019ve missed that part but after going through the pre-training section (page 6) I realized the dataset on which CoLT5 is pre-trained isn\u2019t mentioned.\nB. Reproducibility - Numbers of CoLT5 -B in Table 6 and Table 3 are different. I wonder why that is happening. QAS F1 score is 42.1 v/s 38.3 in Table 3 and Table 6 respectively. Same for QMS and GovR Rgm scores have a difference of almost 5% which is a bit high. Let me know if I\u2019m missing something that\u2019s different in both these tables. \u00a0 C. Are all these experiments done on multiple seeds because I couldn\u2019t see that anywhere.\nD. In Table 5, when you say the number of Natural Questions or TriviaQA, the number stated is in decimals like 0.1. Is it % or in hundreds, thousands? It will be great if you can clear that.\nE. Why didn\u2019t you pre-train CoLT5 on input length equivalent to that of the dataset with the highest median input length in Table 4? Was it because there were no significant gains after a certain input length or did it take too long?\nF. I can\u2019t see the number of parameters for different versions of CoLT5. The paper only mentions that it has more overall parameters as compared to LongT5 but uses less compute (Update: The concerned numbers are mentioned in Appendix, I\u2019d recommend moving them in the main paper. Not the whole table but the parameter count)\n\ntypos_grammar_style_and_presentation_improvements: - One minor correction: Page 6, \u201cSCROLLS contains question-answering datasets: Narrative QA \u2026, NLI dataset: ContractNLI.., and summarization datasets: SummScreenFD.. \u201d. Minor punctuation adjustments that can improve clarity and readability.\n- \u201cFeedforward and projection layer\u201d and \u201cfeedforward and attention layer\u201d have been used throughout the paper. I\u2019d recommend sticking to one of those preferably feedforward and attention projection layers.\n- Figure 2 doesn\u2019t mention the metric of average perf which is F1 score. Also, the caption should be: CoLT5 achieves faster performance than LongT5 for the same number of parameters instead of saying CoLT5 achieves stronger performance than LongT5 at any speed.\n- In Table 3 you have computed the average score under the Avg column, however, these metrics may not always be directly comparable, so averaging them might not accurately represent the overall performance. You should have averaged similar metrics like TQA, NQA, QAS under avg F1, QuAL, CNLI under avg EM, and arXiv, SumS, QMS, GovR under avg Rgm.\n- I feel the equation for calculating FLOPs (attn), the global projection part, can be explained a bit. It took me some time to understand that (this one is totally up to you, if you think readers can figure this out easily then don\u2019t)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "25_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_25_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7326,
      "max_similarity": 0.7469,
      "avg_coverage": 0.5944333333333334,
      "max_coverage": 0.75
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 727,
      "avg_human_length": 230.66666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 8,
      "weaknesses_count": 9,
      "suggestions_count": 12
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "OT1Ki3DHJg",
        "similarity": 0.7044,
        "coverage": 0.5,
        "human_length": 285,
        "human_text": "paper_topic_and_main_contributions: The study proposes a method for solving the problem of the limited lexicon of multiligual language models by replacing subword embeddings with image embeddings for the individual script.\n\nreasons_to_accept: That's a well designed study which shows how multilingual models can avoid the lexical bottleneck for lesser-resourced languages.\n\nreasons_to_reject: 1. It is suprising that you haven't experimented with different image conversion parameters, such as different fonts, font sizes and resolutions. Google Noto Sans 10pt at 120 dpi is a very specific setting. The question in Lines 218-220 implies that these variations need to be studied. \n 2. Similarly the statement on the source of improvements as in Lines 244-250 should have been studied. Does performance for \"ja\" improves when you add \"zh\"?\n\ntypos_grammar_style_and_presentation_improvements: 1. The claim of \"up to 9 BLEU\" is misleading. Report the mean. And also chrF is more robust for neural models, so it's better to provide your main reporting for it. \n  2. Conneau et al 2020 present a very concise way to describe the contribution of individual properties of their architecture on the overall performance. \n 3. The reasons for choosing the specific languages in Table 2 are not clear. I think the pairing in this table is good as it includes related languages with different scripts  as well as unrelated languages with similar scripts. Please add an explanation.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "l4TWuxfp8v",
        "similarity": 0.7469,
        "coverage": 0.5333,
        "human_length": 207,
        "human_text": "paper_topic_and_main_contributions: In this paper, they proposed a novel approach to train NMT model with pixel representations. They evaluated the model on TED dataset with 7 and 49 languages with a variety of languages and script coverage, and improved the perforamnce compared to previous subword-based studies. Further analysis showed that pixel representation helps better cross-lingual transfer to unseen scripts and is more data efficient.\n\nreasons_to_accept: 1. A novel approach to train NMT model with pixel representation 2. Better performance compared to traditional subword-style tokenization 3. The approach also helps better cross-lingual transfer to unseen scripts and is more data efficient\n\nreasons_to_reject: 1. The technical difference between their approach and Rust et al, 2023 is not clear 2. Experiment with better multilingual model e.g. NLLB (https://arxiv.org/abs/2207.04672), M2M-100 (https://arxiv.org/abs/2010.11125), and SMaLL100 (https://arxiv.org/abs/2210.11621)?\n\nquestions_for_the_authors: QA: Can you please explain the difference of your approach compared to Rust et al, 2023?\nQB: Is it possible to check the approach with better multilingual NMT models?\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: please check section 9\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "HMkqcEYLLT",
        "similarity": 0.7465,
        "coverage": 0.75,
        "human_length": 200,
        "human_text": "paper_topic_and_main_contributions: Evaluating the use of pixel input representations in many-to-one multilingual machine translation. \nWhile pixel representations are not novel, their interaction with multilingual training has not been studied previously. \nText is rendered as an image using a particular font, and then fed into a convolutional network that transforms it into embedding vectors suitable for a transformer network.\n\nreasons_to_accept: The proposed pixel representations improve the crosslingual transfer between languages sharing a script, as well as enabling transfer between historically related but distinct scripts encoded with different unicode codepoints. \nThe proposed method is thus especially beneficial for low-resource languages using non-Latin scripts, a disadvantaged group of languages in NLP.\nModels using pixel representations are more robust when scaling the model size.\n\nreasons_to_reject: None\n\nquestions_for_the_authors: A: 101 On the target side, you decode subword tokens. What is the vocabulary size and construction method for the target subword vocabulary?\n\ntypos_grammar_style_and_presentation_improvements: 121 Please clarify that Table 4 is found in the appendices.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "OT1Ki3DHJg",
        "length": 285,
        "human_text": "paper_topic_and_main_contributions: The study proposes a method for solving the problem of the limited lexicon of multiligual language models by replacing subword embeddings with image embeddings for the individual script.\n\nreasons_to_accept: That's a well designed study which shows how multilingual models can avoid the lexical bottleneck for lesser-resourced languages.\n\nreasons_to_reject: 1. It is suprising that you haven't experimented with different image conversion parameters, such as different fonts, font sizes and resolutions. Google Noto Sans 10pt at 120 dpi is a very specific setting. The question in Lines 218-220 implies that these variations need to be studied. \n 2. Similarly the statement on the source of improvements as in Lines 244-250 should have been studied. Does performance for \"ja\" improves when you add \"zh\"?\n\ntypos_grammar_style_and_presentation_improvements: 1. The claim of \"up to 9 BLEU\" is misleading. Report the mean. And also chrF is more robust for neural models, so it's better to provide your main reporting for it. \n  2. Conneau et al 2020 present a very concise way to describe the contribution of individual properties of their architecture on the overall performance. \n 3. The reasons for choosing the specific languages in Table 2 are not clear. I think the pairing in this table is good as it includes related languages with different scripts  as well as unrelated languages with similar scripts. Please add an explanation.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "l4TWuxfp8v",
        "length": 207,
        "human_text": "paper_topic_and_main_contributions: In this paper, they proposed a novel approach to train NMT model with pixel representations. They evaluated the model on TED dataset with 7 and 49 languages with a variety of languages and script coverage, and improved the perforamnce compared to previous subword-based studies. Further analysis showed that pixel representation helps better cross-lingual transfer to unseen scripts and is more data efficient.\n\nreasons_to_accept: 1. A novel approach to train NMT model with pixel representation 2. Better performance compared to traditional subword-style tokenization 3. The approach also helps better cross-lingual transfer to unseen scripts and is more data efficient\n\nreasons_to_reject: 1. The technical difference between their approach and Rust et al, 2023 is not clear 2. Experiment with better multilingual model e.g. NLLB (https://arxiv.org/abs/2207.04672), M2M-100 (https://arxiv.org/abs/2010.11125), and SMaLL100 (https://arxiv.org/abs/2210.11621)?\n\nquestions_for_the_authors: QA: Can you please explain the difference of your approach compared to Rust et al, 2023?\nQB: Is it possible to check the approach with better multilingual NMT models?\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: please check section 9\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "HMkqcEYLLT",
        "length": 200,
        "human_text": "paper_topic_and_main_contributions: Evaluating the use of pixel input representations in many-to-one multilingual machine translation. \nWhile pixel representations are not novel, their interaction with multilingual training has not been studied previously. \nText is rendered as an image using a particular font, and then fed into a convolutional network that transforms it into embedding vectors suitable for a transformer network.\n\nreasons_to_accept: The proposed pixel representations improve the crosslingual transfer between languages sharing a script, as well as enabling transfer between historically related but distinct scripts encoded with different unicode codepoints. \nThe proposed method is thus especially beneficial for low-resource languages using non-Latin scripts, a disadvantaged group of languages in NLP.\nModels using pixel representations are more robust when scaling the model size.\n\nreasons_to_reject: None\n\nquestions_for_the_authors: A: 101 On the target side, you decode subword tokens. What is the vocabulary size and construction method for the target subword vocabulary?\n\ntypos_grammar_style_and_presentation_improvements: 121 Please clarify that Table 4 is found in the appendices.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "25_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_25_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7333333333333334,
      "max_similarity": 0.751,
      "avg_coverage": 0.5944333333333334,
      "max_coverage": 0.75
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 761,
      "avg_human_length": 230.66666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 9,
      "weaknesses_count": 9,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "OT1Ki3DHJg",
        "similarity": 0.7073,
        "coverage": 0.5,
        "human_length": 285,
        "human_text": "paper_topic_and_main_contributions: The study proposes a method for solving the problem of the limited lexicon of multiligual language models by replacing subword embeddings with image embeddings for the individual script.\n\nreasons_to_accept: That's a well designed study which shows how multilingual models can avoid the lexical bottleneck for lesser-resourced languages.\n\nreasons_to_reject: 1. It is suprising that you haven't experimented with different image conversion parameters, such as different fonts, font sizes and resolutions. Google Noto Sans 10pt at 120 dpi is a very specific setting. The question in Lines 218-220 implies that these variations need to be studied. \n 2. Similarly the statement on the source of improvements as in Lines 244-250 should have been studied. Does performance for \"ja\" improves when you add \"zh\"?\n\ntypos_grammar_style_and_presentation_improvements: 1. The claim of \"up to 9 BLEU\" is misleading. Report the mean. And also chrF is more robust for neural models, so it's better to provide your main reporting for it. \n  2. Conneau et al 2020 present a very concise way to describe the contribution of individual properties of their architecture on the overall performance. \n 3. The reasons for choosing the specific languages in Table 2 are not clear. I think the pairing in this table is good as it includes related languages with different scripts  as well as unrelated languages with similar scripts. Please add an explanation.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "l4TWuxfp8v",
        "similarity": 0.7417,
        "coverage": 0.5333,
        "human_length": 207,
        "human_text": "paper_topic_and_main_contributions: In this paper, they proposed a novel approach to train NMT model with pixel representations. They evaluated the model on TED dataset with 7 and 49 languages with a variety of languages and script coverage, and improved the perforamnce compared to previous subword-based studies. Further analysis showed that pixel representation helps better cross-lingual transfer to unseen scripts and is more data efficient.\n\nreasons_to_accept: 1. A novel approach to train NMT model with pixel representation 2. Better performance compared to traditional subword-style tokenization 3. The approach also helps better cross-lingual transfer to unseen scripts and is more data efficient\n\nreasons_to_reject: 1. The technical difference between their approach and Rust et al, 2023 is not clear 2. Experiment with better multilingual model e.g. NLLB (https://arxiv.org/abs/2207.04672), M2M-100 (https://arxiv.org/abs/2010.11125), and SMaLL100 (https://arxiv.org/abs/2210.11621)?\n\nquestions_for_the_authors: QA: Can you please explain the difference of your approach compared to Rust et al, 2023?\nQB: Is it possible to check the approach with better multilingual NMT models?\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: please check section 9\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "HMkqcEYLLT",
        "similarity": 0.751,
        "coverage": 0.75,
        "human_length": 200,
        "human_text": "paper_topic_and_main_contributions: Evaluating the use of pixel input representations in many-to-one multilingual machine translation. \nWhile pixel representations are not novel, their interaction with multilingual training has not been studied previously. \nText is rendered as an image using a particular font, and then fed into a convolutional network that transforms it into embedding vectors suitable for a transformer network.\n\nreasons_to_accept: The proposed pixel representations improve the crosslingual transfer between languages sharing a script, as well as enabling transfer between historically related but distinct scripts encoded with different unicode codepoints. \nThe proposed method is thus especially beneficial for low-resource languages using non-Latin scripts, a disadvantaged group of languages in NLP.\nModels using pixel representations are more robust when scaling the model size.\n\nreasons_to_reject: None\n\nquestions_for_the_authors: A: 101 On the target side, you decode subword tokens. What is the vocabulary size and construction method for the target subword vocabulary?\n\ntypos_grammar_style_and_presentation_improvements: 121 Please clarify that Table 4 is found in the appendices.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "OT1Ki3DHJg",
        "length": 285,
        "human_text": "paper_topic_and_main_contributions: The study proposes a method for solving the problem of the limited lexicon of multiligual language models by replacing subword embeddings with image embeddings for the individual script.\n\nreasons_to_accept: That's a well designed study which shows how multilingual models can avoid the lexical bottleneck for lesser-resourced languages.\n\nreasons_to_reject: 1. It is suprising that you haven't experimented with different image conversion parameters, such as different fonts, font sizes and resolutions. Google Noto Sans 10pt at 120 dpi is a very specific setting. The question in Lines 218-220 implies that these variations need to be studied. \n 2. Similarly the statement on the source of improvements as in Lines 244-250 should have been studied. Does performance for \"ja\" improves when you add \"zh\"?\n\ntypos_grammar_style_and_presentation_improvements: 1. The claim of \"up to 9 BLEU\" is misleading. Report the mean. And also chrF is more robust for neural models, so it's better to provide your main reporting for it. \n  2. Conneau et al 2020 present a very concise way to describe the contribution of individual properties of their architecture on the overall performance. \n 3. The reasons for choosing the specific languages in Table 2 are not clear. I think the pairing in this table is good as it includes related languages with different scripts  as well as unrelated languages with similar scripts. Please add an explanation.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "l4TWuxfp8v",
        "length": 207,
        "human_text": "paper_topic_and_main_contributions: In this paper, they proposed a novel approach to train NMT model with pixel representations. They evaluated the model on TED dataset with 7 and 49 languages with a variety of languages and script coverage, and improved the perforamnce compared to previous subword-based studies. Further analysis showed that pixel representation helps better cross-lingual transfer to unseen scripts and is more data efficient.\n\nreasons_to_accept: 1. A novel approach to train NMT model with pixel representation 2. Better performance compared to traditional subword-style tokenization 3. The approach also helps better cross-lingual transfer to unseen scripts and is more data efficient\n\nreasons_to_reject: 1. The technical difference between their approach and Rust et al, 2023 is not clear 2. Experiment with better multilingual model e.g. NLLB (https://arxiv.org/abs/2207.04672), M2M-100 (https://arxiv.org/abs/2010.11125), and SMaLL100 (https://arxiv.org/abs/2210.11621)?\n\nquestions_for_the_authors: QA: Can you please explain the difference of your approach compared to Rust et al, 2023?\nQB: Is it possible to check the approach with better multilingual NMT models?\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: please check section 9\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "HMkqcEYLLT",
        "length": 200,
        "human_text": "paper_topic_and_main_contributions: Evaluating the use of pixel input representations in many-to-one multilingual machine translation. \nWhile pixel representations are not novel, their interaction with multilingual training has not been studied previously. \nText is rendered as an image using a particular font, and then fed into a convolutional network that transforms it into embedding vectors suitable for a transformer network.\n\nreasons_to_accept: The proposed pixel representations improve the crosslingual transfer between languages sharing a script, as well as enabling transfer between historically related but distinct scripts encoded with different unicode codepoints. \nThe proposed method is thus especially beneficial for low-resource languages using non-Latin scripts, a disadvantaged group of languages in NLP.\nModels using pixel representations are more robust when scaling the model size.\n\nreasons_to_reject: None\n\nquestions_for_the_authors: A: 101 On the target side, you decode subword tokens. What is the vocabulary size and construction method for the target subword vocabulary?\n\ntypos_grammar_style_and_presentation_improvements: 121 Please clarify that Table 4 is found in the appendices.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "60_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_60_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6996666666666668,
      "max_similarity": 0.7195,
      "avg_coverage": 0.7686333333333334,
      "max_coverage": 0.9231
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 452,
      "avg_human_length": 302.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 8,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "PD7VubKiWD",
        "similarity": 0.6821,
        "coverage": 0.9231,
        "human_length": 272,
        "human_text": "paper_topic_and_main_contributions: The paper studies how transformer-based language models retrieve information to predict for subject-relation queries. The authors designed methods to study the effects on prediction by blocking updates in certain components (e.g., attention edges, MLP layers), and present some interesting and useful results (e.g., information flows through certain critical points to prediction, attribute information was gradually built up in the representation at the last subject position through layers).\n\nreasons_to_accept: Overall the paper is well written and easy to understand. It presents interesting and useful findings for better understanding how a transformer-based LM predicts for subject-relation query. For example, they found that early layers are important in propagating information for predicting the attribute. Another example is the attribute information is aggregated gradually through layers. These findings could not only help understand the models but also inspire new methods that better aggregate information for subject-relation queries.  The experiments are comprehensive and provide answers for questions at different levels.\n\nreasons_to_reject: The study is only on subject-relation query. It might not be easy to apply the proposed methods to understand the information flow in other research tasks, where important words in query are not known beforehand (just as the subject and relation word in the subject-relation query task), such as reading comprehension.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "7wNpNpGd6M",
        "similarity": 0.6974,
        "coverage": 0.9,
        "human_length": 166,
        "human_text": "paper_topic_and_main_contributions: The paper explores the how factual information is retrieved from transformer-based LMs. The authors perform a number of experiments to reveal some unexpected results regarding the location of factual associations. The work suggests further research directions in the form of knowledge localization and model editing.\n\nreasons_to_accept: With the rise of the desire for explainability of LMs works which identify (or go some way towards identifying) important locations are of great potential value.  The paper uses figures to advantage, illustrating complex concepts and results.\n\nreasons_to_reject: This is a fairly technical paper, presumably a stepping stone on an incremental path.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "Qu2g97TNk9",
        "similarity": 0.7195,
        "coverage": 0.4828,
        "human_length": 469,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to use subject-relation queries to investigate how Transformer-based auto-regressive language models (GPT-2 and GPT-J) aggregate information about the subject and relation to predict the correct attribute.  The attention \"knockout\" method is applied to identify critical information flow points in factual predictions. The authors argue that critical information from the subject positions moves directly to the last position at middle-upper layers.\n\"Attributes Rate\" is used to investigate the position of subject representation. The authors find that the model constructs attribute-rich subject representations at the last subject-position. Attribute rates of token embeddings (from the embedding layer?) are also studied, and it reveals that while static subject-token embeddings encode some factual associations, other model components are needed for extraction of subject-related attributes. It is also found that canceling the early MLP sublayers has a destructive effect on the subject representation\u2019s attributes rate, while canceling early MHSA sublayers does not have such a strong effect.\n\"Extraction Rate\" is used to investigate how and where attributes are extracted. The authors suggest that both the MHSA and MLP implement attribute extraction, but MHSA is the prominent mechanism for factual queries. It is also indicated that subject enrichment, through which the model constructs a representation at the last subject-position that encodes many subject-related attributes, has a big impact. Overall, factual associations are encoded in the MHSA parameters, acting as \u201cknowledge hubs\u201d.\n\nreasons_to_accept: 1. The study of internal representations of Transformer-based models is still an evolving domain, which needs more contributions. Particularly, little is known about how factual predictions are built. \n2. This paper proposes an automatic approximation of the subject-attribute relatedness, namely the attribute rate. It is a reasonable and novel idea. \n3. The experiments are well designed and the results are well presented. There are some interesting findings.\n\nreasons_to_reject: 1. Although we should start with simple experiments in order to study a complex system, it is often unclear whether conclusions drawn from simple experiments can extend to general, complicated cases. It is difficult to tell whether other NLP tasks follow the same pattern. \nThe authors suggest that \"these findings open new research directions for knowledge localization and model editing\", but it is unclear whether the localization depends on training tasks, or model size, among other things.\n\ntypos_grammar_style_and_presentation_improvements: L327 Attributes Rate --> Attribute Rate\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "PD7VubKiWD",
        "length": 272,
        "human_text": "paper_topic_and_main_contributions: The paper studies how transformer-based language models retrieve information to predict for subject-relation queries. The authors designed methods to study the effects on prediction by blocking updates in certain components (e.g., attention edges, MLP layers), and present some interesting and useful results (e.g., information flows through certain critical points to prediction, attribute information was gradually built up in the representation at the last subject position through layers).\n\nreasons_to_accept: Overall the paper is well written and easy to understand. It presents interesting and useful findings for better understanding how a transformer-based LM predicts for subject-relation query. For example, they found that early layers are important in propagating information for predicting the attribute. Another example is the attribute information is aggregated gradually through layers. These findings could not only help understand the models but also inspire new methods that better aggregate information for subject-relation queries.  The experiments are comprehensive and provide answers for questions at different levels.\n\nreasons_to_reject: The study is only on subject-relation query. It might not be easy to apply the proposed methods to understand the information flow in other research tasks, where important words in query are not known beforehand (just as the subject and relation word in the subject-relation query task), such as reading comprehension.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "7wNpNpGd6M",
        "length": 166,
        "human_text": "paper_topic_and_main_contributions: The paper explores the how factual information is retrieved from transformer-based LMs. The authors perform a number of experiments to reveal some unexpected results regarding the location of factual associations. The work suggests further research directions in the form of knowledge localization and model editing.\n\nreasons_to_accept: With the rise of the desire for explainability of LMs works which identify (or go some way towards identifying) important locations are of great potential value.  The paper uses figures to advantage, illustrating complex concepts and results.\n\nreasons_to_reject: This is a fairly technical paper, presumably a stepping stone on an incremental path.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "Qu2g97TNk9",
        "length": 469,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to use subject-relation queries to investigate how Transformer-based auto-regressive language models (GPT-2 and GPT-J) aggregate information about the subject and relation to predict the correct attribute.  The attention \"knockout\" method is applied to identify critical information flow points in factual predictions. The authors argue that critical information from the subject positions moves directly to the last position at middle-upper layers.\n\"Attributes Rate\" is used to investigate the position of subject representation. The authors find that the model constructs attribute-rich subject representations at the last subject-position. Attribute rates of token embeddings (from the embedding layer?) are also studied, and it reveals that while static subject-token embeddings encode some factual associations, other model components are needed for extraction of subject-related attributes. It is also found that canceling the early MLP sublayers has a destructive effect on the subject representation\u2019s attributes rate, while canceling early MHSA sublayers does not have such a strong effect.\n\"Extraction Rate\" is used to investigate how and where attributes are extracted. The authors suggest that both the MHSA and MLP implement attribute extraction, but MHSA is the prominent mechanism for factual queries. It is also indicated that subject enrichment, through which the model constructs a representation at the last subject-position that encodes many subject-related attributes, has a big impact. Overall, factual associations are encoded in the MHSA parameters, acting as \u201cknowledge hubs\u201d.\n\nreasons_to_accept: 1. The study of internal representations of Transformer-based models is still an evolving domain, which needs more contributions. Particularly, little is known about how factual predictions are built. \n2. This paper proposes an automatic approximation of the subject-attribute relatedness, namely the attribute rate. It is a reasonable and novel idea. \n3. The experiments are well designed and the results are well presented. There are some interesting findings.\n\nreasons_to_reject: 1. Although we should start with simple experiments in order to study a complex system, it is often unclear whether conclusions drawn from simple experiments can extend to general, complicated cases. It is difficult to tell whether other NLP tasks follow the same pattern. \nThe authors suggest that \"these findings open new research directions for knowledge localization and model editing\", but it is unclear whether the localization depends on training tasks, or model size, among other things.\n\ntypos_grammar_style_and_presentation_improvements: L327 Attributes Rate --> Attribute Rate\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "60_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_60_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7090333333333333,
      "max_similarity": 0.7197,
      "avg_coverage": 0.7211333333333333,
      "max_coverage": 0.8462
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 571,
      "avg_human_length": 302.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 8,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "PD7VubKiWD",
        "similarity": 0.6952,
        "coverage": 0.8462,
        "human_length": 272,
        "human_text": "paper_topic_and_main_contributions: The paper studies how transformer-based language models retrieve information to predict for subject-relation queries. The authors designed methods to study the effects on prediction by blocking updates in certain components (e.g., attention edges, MLP layers), and present some interesting and useful results (e.g., information flows through certain critical points to prediction, attribute information was gradually built up in the representation at the last subject position through layers).\n\nreasons_to_accept: Overall the paper is well written and easy to understand. It presents interesting and useful findings for better understanding how a transformer-based LM predicts for subject-relation query. For example, they found that early layers are important in propagating information for predicting the attribute. Another example is the attribute information is aggregated gradually through layers. These findings could not only help understand the models but also inspire new methods that better aggregate information for subject-relation queries.  The experiments are comprehensive and provide answers for questions at different levels.\n\nreasons_to_reject: The study is only on subject-relation query. It might not be easy to apply the proposed methods to understand the information flow in other research tasks, where important words in query are not known beforehand (just as the subject and relation word in the subject-relation query task), such as reading comprehension.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "7wNpNpGd6M",
        "similarity": 0.7122,
        "coverage": 0.8,
        "human_length": 166,
        "human_text": "paper_topic_and_main_contributions: The paper explores the how factual information is retrieved from transformer-based LMs. The authors perform a number of experiments to reveal some unexpected results regarding the location of factual associations. The work suggests further research directions in the form of knowledge localization and model editing.\n\nreasons_to_accept: With the rise of the desire for explainability of LMs works which identify (or go some way towards identifying) important locations are of great potential value.  The paper uses figures to advantage, illustrating complex concepts and results.\n\nreasons_to_reject: This is a fairly technical paper, presumably a stepping stone on an incremental path.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "Qu2g97TNk9",
        "similarity": 0.7197,
        "coverage": 0.5172,
        "human_length": 469,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to use subject-relation queries to investigate how Transformer-based auto-regressive language models (GPT-2 and GPT-J) aggregate information about the subject and relation to predict the correct attribute.  The attention \"knockout\" method is applied to identify critical information flow points in factual predictions. The authors argue that critical information from the subject positions moves directly to the last position at middle-upper layers.\n\"Attributes Rate\" is used to investigate the position of subject representation. The authors find that the model constructs attribute-rich subject representations at the last subject-position. Attribute rates of token embeddings (from the embedding layer?) are also studied, and it reveals that while static subject-token embeddings encode some factual associations, other model components are needed for extraction of subject-related attributes. It is also found that canceling the early MLP sublayers has a destructive effect on the subject representation\u2019s attributes rate, while canceling early MHSA sublayers does not have such a strong effect.\n\"Extraction Rate\" is used to investigate how and where attributes are extracted. The authors suggest that both the MHSA and MLP implement attribute extraction, but MHSA is the prominent mechanism for factual queries. It is also indicated that subject enrichment, through which the model constructs a representation at the last subject-position that encodes many subject-related attributes, has a big impact. Overall, factual associations are encoded in the MHSA parameters, acting as \u201cknowledge hubs\u201d.\n\nreasons_to_accept: 1. The study of internal representations of Transformer-based models is still an evolving domain, which needs more contributions. Particularly, little is known about how factual predictions are built. \n2. This paper proposes an automatic approximation of the subject-attribute relatedness, namely the attribute rate. It is a reasonable and novel idea. \n3. The experiments are well designed and the results are well presented. There are some interesting findings.\n\nreasons_to_reject: 1. Although we should start with simple experiments in order to study a complex system, it is often unclear whether conclusions drawn from simple experiments can extend to general, complicated cases. It is difficult to tell whether other NLP tasks follow the same pattern. \nThe authors suggest that \"these findings open new research directions for knowledge localization and model editing\", but it is unclear whether the localization depends on training tasks, or model size, among other things.\n\ntypos_grammar_style_and_presentation_improvements: L327 Attributes Rate --> Attribute Rate\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "PD7VubKiWD",
        "length": 272,
        "human_text": "paper_topic_and_main_contributions: The paper studies how transformer-based language models retrieve information to predict for subject-relation queries. The authors designed methods to study the effects on prediction by blocking updates in certain components (e.g., attention edges, MLP layers), and present some interesting and useful results (e.g., information flows through certain critical points to prediction, attribute information was gradually built up in the representation at the last subject position through layers).\n\nreasons_to_accept: Overall the paper is well written and easy to understand. It presents interesting and useful findings for better understanding how a transformer-based LM predicts for subject-relation query. For example, they found that early layers are important in propagating information for predicting the attribute. Another example is the attribute information is aggregated gradually through layers. These findings could not only help understand the models but also inspire new methods that better aggregate information for subject-relation queries.  The experiments are comprehensive and provide answers for questions at different levels.\n\nreasons_to_reject: The study is only on subject-relation query. It might not be easy to apply the proposed methods to understand the information flow in other research tasks, where important words in query are not known beforehand (just as the subject and relation word in the subject-relation query task), such as reading comprehension.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "7wNpNpGd6M",
        "length": 166,
        "human_text": "paper_topic_and_main_contributions: The paper explores the how factual information is retrieved from transformer-based LMs. The authors perform a number of experiments to reveal some unexpected results regarding the location of factual associations. The work suggests further research directions in the form of knowledge localization and model editing.\n\nreasons_to_accept: With the rise of the desire for explainability of LMs works which identify (or go some way towards identifying) important locations are of great potential value.  The paper uses figures to advantage, illustrating complex concepts and results.\n\nreasons_to_reject: This is a fairly technical paper, presumably a stepping stone on an incremental path.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "Qu2g97TNk9",
        "length": 469,
        "human_text": "paper_topic_and_main_contributions: This paper proposes to use subject-relation queries to investigate how Transformer-based auto-regressive language models (GPT-2 and GPT-J) aggregate information about the subject and relation to predict the correct attribute.  The attention \"knockout\" method is applied to identify critical information flow points in factual predictions. The authors argue that critical information from the subject positions moves directly to the last position at middle-upper layers.\n\"Attributes Rate\" is used to investigate the position of subject representation. The authors find that the model constructs attribute-rich subject representations at the last subject-position. Attribute rates of token embeddings (from the embedding layer?) are also studied, and it reveals that while static subject-token embeddings encode some factual associations, other model components are needed for extraction of subject-related attributes. It is also found that canceling the early MLP sublayers has a destructive effect on the subject representation\u2019s attributes rate, while canceling early MHSA sublayers does not have such a strong effect.\n\"Extraction Rate\" is used to investigate how and where attributes are extracted. The authors suggest that both the MHSA and MLP implement attribute extraction, but MHSA is the prominent mechanism for factual queries. It is also indicated that subject enrichment, through which the model constructs a representation at the last subject-position that encodes many subject-related attributes, has a big impact. Overall, factual associations are encoded in the MHSA parameters, acting as \u201cknowledge hubs\u201d.\n\nreasons_to_accept: 1. The study of internal representations of Transformer-based models is still an evolving domain, which needs more contributions. Particularly, little is known about how factual predictions are built. \n2. This paper proposes an automatic approximation of the subject-attribute relatedness, namely the attribute rate. It is a reasonable and novel idea. \n3. The experiments are well designed and the results are well presented. There are some interesting findings.\n\nreasons_to_reject: 1. Although we should start with simple experiments in order to study a complex system, it is often unclear whether conclusions drawn from simple experiments can extend to general, complicated cases. It is difficult to tell whether other NLP tasks follow the same pattern. \nThe authors suggest that \"these findings open new research directions for knowledge localization and model editing\", but it is unclear whether the localization depends on training tasks, or model size, among other things.\n\ntypos_grammar_style_and_presentation_improvements: L327 Attributes Rate --> Attribute Rate\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "155_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_155_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7481333333333334,
      "max_similarity": 0.7653,
      "avg_coverage": 0.41473333333333334,
      "max_coverage": 0.4783
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 491,
      "avg_human_length": 706.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "yDIC18bpq3",
        "similarity": 0.7478,
        "coverage": 0.4,
        "human_length": 595,
        "human_text": "paper_topic_and_main_contributions: This review/position paper identifies common pitfalls in fact checking papers, e.g., vagueness about who should use the proposed systems and how. A comprehensive categorization of such issues is proposed and applied to the analysis of the 100 most highly cited papers in the area, using a content analysis approach conducted by the authors. Finally, recommendations are provided for improving narratives in fact checking contributions.\nWhile I have not worked on fact checking myself, I am familiar with the literature and the overall rationale.\n\nreasons_to_accept: Important examination of the very basic goals of automated fact checking research and the extent to which they are expressed in influencial publications in the area, finding low coherence in many cases.  Clear, well motivated and systematic argumentation that uses just the terminology and reasoning strategies common in the fact checking literature, e.g., lacking evidence, making it well positioned toward this community.\nRobust and thorough annotation and analysis procedure following best practices in content analysis, resulting in both a novel categorization scheme and annotated corpus used to quantitatively back up the arguments in the paper. Exceptionally detailed account of the annotation guidelines and the meaning of each label, including examples for each single case.\nBroad and informative perspective on related work on the goals of fact checking from various disciplines.\n\nreasons_to_reject: Seems to be implicitly making a strong assumption, that (influencial) papers in automated fact checking always contribute an artifact positioned within some (potential) application. Contributions in NLP, on the other hand, often focus on improving or investigating specific aspects of components that can be internal to such a system, leaving out a broader discussion on the overall system, especially given space limitations. It is therefore not entirely clear whether the normative statement the authors are making, that a comprehensive narrative is always warranted, is a subjective preference or if there is evidence that it is necessary in all cases. For example, arguably it is not the researcher's job to discuss what to do with the identified misinformation after it was found by the system.  No information on the papers themselves is given in the paper, such as distribution over publication years or number of citations. It is also not specified how the number was obtained.\n\nquestions_for_the_authors: A: For unsupported ends, the existence of counter evidence is given as a sufficient criterion. Is it, in general, a criterion for unsupported ends? What if there is both evidence and counter evidence? This black and white view of fact checking may be an oversimplification. Have you considered a more nuanced categorization of faithfulness to evidence, e.g. claim strength as in https://aclanthology.org/2021.emnlp-main.845/ ?\n\ntypos_grammar_style_and_presentation_improvements: Feasibility support does not seem to appear in figure 1. This is fine, but may be worth mentioning so the reader does not look for it.\nCiting Spinoza and Heidegger seems somewhat out of context given the subject of technological artifacts, which is implicitly about information technology. Expanding the discourse more explicitly may help.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "W8FuydY6jk",
        "similarity": 0.7653,
        "coverage": 0.3659,
        "human_length": 873,
        "human_text": "paper_topic_and_main_contributions: The authors investigate the usefulness of research papers about automated fact-checking. They argue that their usefulness decreases without considering who will be using the developed fact-checking artifacts, and without specifying how they should be used. Therefore, they identify relevant components of a fact-checking system and document their degree of consideration in 100 popular fact-checking papers. \nThey contribute a holistic perspective on fact-checking systems, critically analyze a high number of existing papers, define influential components for the usefulness of fact-checking systems, and derive recommendations for future research. They document the results of the analyzed papers in the form of annotations, which is provided in a json file for further analysis.\n\nreasons_to_accept: The paper is well written and structured. The research was conducted systematically and logically, and the arguments made are thorough and valid. When accepted, the paper will draw researchers' attention towards the usability of fact-checking systems. It might especially influence those who \"only\" develop a technological solution without considering its practical and societal benefit. The paper would also introduce a new paradigm for evaluating the usefulness of fact-checking papers and systems. It would raise awareness of important aspects to consider for all those who work on fact-checking systems and care about its value for the community.\n\nreasons_to_reject: There are many categories and new terms introduced which are hard to distinguish without concrete examples. The wording of those terms is also somewhat confusing, so that it is hard to read the paper at the first time. Especially when there are terms with subtle differences, like many of the sub-categories for epistemic elements and narrative types in the paper, combining a few of the terms would have been a good idea in order to make those rememberable for the reader. \nOne might argue that many of the analysis results and recommendations are quite simple and intuitive. It might have been more useful to find out why the analyzed papers have those shortcomings of usefulness instead of supporting the fact that they do have them. In my opinion the authors should have addressed the \"why\" and at least come up with ideas for follow-up research.\n\nquestions_for_the_authors: The numbers in \"()\" refer to the page numbers in the paper.\n(1) Where does the term \"artifacts\" come from?\n(3) Put the epistemic elements and narrative types in a table along with short descriptions and simple examples. This can also be part of the appendix if too large for the body.\n(3) There are many percentages given for epistemic elements and the narrative types, and many of them do not make sense to me. E.g., the given categories for \"Model Owners\" total in 8% - what about the other 92%? And the categories for \"Modeling means\" exceed 100% - how can that be? And the \"Vague narrative\" types should make up 56% (as stated in the paper (on page 4), but the percentages add up to 64%? There are more examples like this in the paper - make sure to correct them, or to clarify how the percentages relate.\n(4) What's the discourse level?\n(4)  What is the difference between vague debunking and vague opposition?\n(6)  Chapter \"Evidence is not a silver bullet\" is well researched and the arguments made in it are all valid. However, the paper is about improving the usefulness of fact-checking related research, and stating these general psychological phenomena which cannot be prevented by fact-checking systems is misplaced here, in my opinion. You should at least give suggestions on how researchers can consider these points in their work.\n(7) One point in your recommendations is to include the data actor in fact-checking papers. It totally makes sense to think from the perspective of the target group when developing a tool, but do you have any suggestions of how to involve them? Are you suggesting to actually invite a sample from the target group as evaluators? How are you going to know what they need if they have a special role which you are not familiar with?\n(15) \"Social Media Moderators (0%)\" ??\n\ntypos_grammar_style_and_presentation_improvements: (1) \"Connecting research to potential use allows researchers to shape their work taking into account the expressed needs of key stakeholders, such as professional fact-checkers (Nakov et al., 2021).\" -- > Overcomplicated sentence:  (2) \"In this paper, we investigate narratives about intended use in automated [...]\" --> \"uses\" or \"the intended use\" (2) \"Based on this, we give recommendations to clarify discussions of intended use:\" --> weird style (2) \"[...] time, rendering resulting artefacts diffi- 122 cult to use in real-world scenarios.\" -- > style (8) \"We investigate narratives of intended use in fact-checking papers, finding that the majority describe how this tool will function in vague or unrealistic terms\" --> \"[...] papers, and find that [...] how a tool [...].\"\n(9) \"We chose to understand automated fact-checking artefacts through their intended uses. This is only one way understand technologies.\" -- > \"[...] one way to understand technologies.\"\n(23) Figure 7 should be formatted in a cleaner way.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "kZtCRKktuR",
        "similarity": 0.7313,
        "coverage": 0.4783,
        "human_length": 650,
        "human_text": "paper_topic_and_main_contributions: The paper is an analysis of 100 \"highly cited\" papers on the topic of automated fact-finding regarding the questions of (i) the motivation for the paper? Why was the solution being discussed? ( ii) the means or mechanisms for doing this; the 'how', and (iii) who this proposed solution is intended for -- the participants in the scenarios being looked at.\nThe analysis was conducted by two annotators (presumably the authors) manually going through the abstracts and introductory sections of these 100 papers and marking passages/text-spans with a series of tags based on a set of guidelines that the paper proposes. The paper presents some of the statistics from this analysis.\n\nreasons_to_accept: This is a discussion of a proposed structure for analyzing fact-checker algorithms, along with an overview of how this framework may be used to understand some of the more highly cited recent papers proposing solutions; as such, this is both interesting and important. Especially as we enter a time of increasingly easy generation of realistic looking narratives using AI, fact-checking, whether automated or not, will become increasingly critical. Having a consistent framework and baseline will make it easier to compare these systems against one another. As a starting point, future work in this area can extend the framework in interesting ways, stimulate discussions on how some of these metrics may be automated and how to compare or weight one dimension over another.\n\nreasons_to_reject: I was hoping that this would be an automated scoring algorithm for fact-checkers -- taking as input algorithms and datasets, and outputting a completeness and correctness score for each (essentially an equivalent of Precision and Recall in information retrieval). Perhaps even a machine learning based analysis of language and communication cues that may, in conjunction with background information, may be used to find potential candidates for further checking.  Instead, this is more of a thoughtful discussion or guidelines for how reviewers (or professional fact-checkers) might want to think about reading papers while evaluating them for their claims on fact-checking. The analysis of the 100 papers that the paper presents is somewhat underspecified, leading to ambiguity: for instance, while the authors point out that evidence is often lacking for claims being made in narratives (and missing entirely for 'vague narratives'), they also acknowledge that even though citations are included, they need not always be relevant. Checking a fact, even if there is a citation, requires chasing down the references, understanding them, and making a determination about the validity of the current 'fact'. The paper points out that citations/evidence in itself is not a silver bullet, since there are a number of ways in which the citations or evidence presented can be cause problems for the reader.\nThe paper is an interesting and very timely reminder of the problems that we should be thinking about, but EMNLP is not (in my view) the best forum for this work, given the focus of this work. An ethics or social science conference would be a better venue.\n\nquestions_for_the_authors: - Are there scoring frameworks that could be designed to assign a numerical value to find potential candidates for fact-checking?\n- What are good datasets for training and testing that might be assembled, either real or synthetic? ( Generating synthetic datasets should be an interesting problem since the proposed framework could be inverted) Could some of these be done without doing deep NLP? ( for instance, by doing purely propositional theorem proving, or by solving for evidence graphs, etc)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 1: Poor: This study is not yet sufficiently thorough to warrant publication or is not relevant to EMNLP.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "yDIC18bpq3",
        "length": 595,
        "human_text": "paper_topic_and_main_contributions: This review/position paper identifies common pitfalls in fact checking papers, e.g., vagueness about who should use the proposed systems and how. A comprehensive categorization of such issues is proposed and applied to the analysis of the 100 most highly cited papers in the area, using a content analysis approach conducted by the authors. Finally, recommendations are provided for improving narratives in fact checking contributions.\nWhile I have not worked on fact checking myself, I am familiar with the literature and the overall rationale.\n\nreasons_to_accept: Important examination of the very basic goals of automated fact checking research and the extent to which they are expressed in influencial publications in the area, finding low coherence in many cases.  Clear, well motivated and systematic argumentation that uses just the terminology and reasoning strategies common in the fact checking literature, e.g., lacking evidence, making it well positioned toward this community.\nRobust and thorough annotation and analysis procedure following best practices in content analysis, resulting in both a novel categorization scheme and annotated corpus used to quantitatively back up the arguments in the paper. Exceptionally detailed account of the annotation guidelines and the meaning of each label, including examples for each single case.\nBroad and informative perspective on related work on the goals of fact checking from various disciplines.\n\nreasons_to_reject: Seems to be implicitly making a strong assumption, that (influencial) papers in automated fact checking always contribute an artifact positioned within some (potential) application. Contributions in NLP, on the other hand, often focus on improving or investigating specific aspects of components that can be internal to such a system, leaving out a broader discussion on the overall system, especially given space limitations. It is therefore not entirely clear whether the normative statement the authors are making, that a comprehensive narrative is always warranted, is a subjective preference or if there is evidence that it is necessary in all cases. For example, arguably it is not the researcher's job to discuss what to do with the identified misinformation after it was found by the system.  No information on the papers themselves is given in the paper, such as distribution over publication years or number of citations. It is also not specified how the number was obtained.\n\nquestions_for_the_authors: A: For unsupported ends, the existence of counter evidence is given as a sufficient criterion. Is it, in general, a criterion for unsupported ends? What if there is both evidence and counter evidence? This black and white view of fact checking may be an oversimplification. Have you considered a more nuanced categorization of faithfulness to evidence, e.g. claim strength as in https://aclanthology.org/2021.emnlp-main.845/ ?\n\ntypos_grammar_style_and_presentation_improvements: Feasibility support does not seem to appear in figure 1. This is fine, but may be worth mentioning so the reader does not look for it.\nCiting Spinoza and Heidegger seems somewhat out of context given the subject of technological artifacts, which is implicitly about information technology. Expanding the discourse more explicitly may help.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "W8FuydY6jk",
        "length": 873,
        "human_text": "paper_topic_and_main_contributions: The authors investigate the usefulness of research papers about automated fact-checking. They argue that their usefulness decreases without considering who will be using the developed fact-checking artifacts, and without specifying how they should be used. Therefore, they identify relevant components of a fact-checking system and document their degree of consideration in 100 popular fact-checking papers. \nThey contribute a holistic perspective on fact-checking systems, critically analyze a high number of existing papers, define influential components for the usefulness of fact-checking systems, and derive recommendations for future research. They document the results of the analyzed papers in the form of annotations, which is provided in a json file for further analysis.\n\nreasons_to_accept: The paper is well written and structured. The research was conducted systematically and logically, and the arguments made are thorough and valid. When accepted, the paper will draw researchers' attention towards the usability of fact-checking systems. It might especially influence those who \"only\" develop a technological solution without considering its practical and societal benefit. The paper would also introduce a new paradigm for evaluating the usefulness of fact-checking papers and systems. It would raise awareness of important aspects to consider for all those who work on fact-checking systems and care about its value for the community.\n\nreasons_to_reject: There are many categories and new terms introduced which are hard to distinguish without concrete examples. The wording of those terms is also somewhat confusing, so that it is hard to read the paper at the first time. Especially when there are terms with subtle differences, like many of the sub-categories for epistemic elements and narrative types in the paper, combining a few of the terms would have been a good idea in order to make those rememberable for the reader. \nOne might argue that many of the analysis results and recommendations are quite simple and intuitive. It might have been more useful to find out why the analyzed papers have those shortcomings of usefulness instead of supporting the fact that they do have them. In my opinion the authors should have addressed the \"why\" and at least come up with ideas for follow-up research.\n\nquestions_for_the_authors: The numbers in \"()\" refer to the page numbers in the paper.\n(1) Where does the term \"artifacts\" come from?\n(3) Put the epistemic elements and narrative types in a table along with short descriptions and simple examples. This can also be part of the appendix if too large for the body.\n(3) There are many percentages given for epistemic elements and the narrative types, and many of them do not make sense to me. E.g., the given categories for \"Model Owners\" total in 8% - what about the other 92%? And the categories for \"Modeling means\" exceed 100% - how can that be? And the \"Vague narrative\" types should make up 56% (as stated in the paper (on page 4), but the percentages add up to 64%? There are more examples like this in the paper - make sure to correct them, or to clarify how the percentages relate.\n(4) What's the discourse level?\n(4)  What is the difference between vague debunking and vague opposition?\n(6)  Chapter \"Evidence is not a silver bullet\" is well researched and the arguments made in it are all valid. However, the paper is about improving the usefulness of fact-checking related research, and stating these general psychological phenomena which cannot be prevented by fact-checking systems is misplaced here, in my opinion. You should at least give suggestions on how researchers can consider these points in their work.\n(7) One point in your recommendations is to include the data actor in fact-checking papers. It totally makes sense to think from the perspective of the target group when developing a tool, but do you have any suggestions of how to involve them? Are you suggesting to actually invite a sample from the target group as evaluators? How are you going to know what they need if they have a special role which you are not familiar with?\n(15) \"Social Media Moderators (0%)\" ??\n\ntypos_grammar_style_and_presentation_improvements: (1) \"Connecting research to potential use allows researchers to shape their work taking into account the expressed needs of key stakeholders, such as professional fact-checkers (Nakov et al., 2021).\" -- > Overcomplicated sentence:  (2) \"In this paper, we investigate narratives about intended use in automated [...]\" --> \"uses\" or \"the intended use\" (2) \"Based on this, we give recommendations to clarify discussions of intended use:\" --> weird style (2) \"[...] time, rendering resulting artefacts diffi- 122 cult to use in real-world scenarios.\" -- > style (8) \"We investigate narratives of intended use in fact-checking papers, finding that the majority describe how this tool will function in vague or unrealistic terms\" --> \"[...] papers, and find that [...] how a tool [...].\"\n(9) \"We chose to understand automated fact-checking artefacts through their intended uses. This is only one way understand technologies.\" -- > \"[...] one way to understand technologies.\"\n(23) Figure 7 should be formatted in a cleaner way.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "kZtCRKktuR",
        "length": 650,
        "human_text": "paper_topic_and_main_contributions: The paper is an analysis of 100 \"highly cited\" papers on the topic of automated fact-finding regarding the questions of (i) the motivation for the paper? Why was the solution being discussed? ( ii) the means or mechanisms for doing this; the 'how', and (iii) who this proposed solution is intended for -- the participants in the scenarios being looked at.\nThe analysis was conducted by two annotators (presumably the authors) manually going through the abstracts and introductory sections of these 100 papers and marking passages/text-spans with a series of tags based on a set of guidelines that the paper proposes. The paper presents some of the statistics from this analysis.\n\nreasons_to_accept: This is a discussion of a proposed structure for analyzing fact-checker algorithms, along with an overview of how this framework may be used to understand some of the more highly cited recent papers proposing solutions; as such, this is both interesting and important. Especially as we enter a time of increasingly easy generation of realistic looking narratives using AI, fact-checking, whether automated or not, will become increasingly critical. Having a consistent framework and baseline will make it easier to compare these systems against one another. As a starting point, future work in this area can extend the framework in interesting ways, stimulate discussions on how some of these metrics may be automated and how to compare or weight one dimension over another.\n\nreasons_to_reject: I was hoping that this would be an automated scoring algorithm for fact-checkers -- taking as input algorithms and datasets, and outputting a completeness and correctness score for each (essentially an equivalent of Precision and Recall in information retrieval). Perhaps even a machine learning based analysis of language and communication cues that may, in conjunction with background information, may be used to find potential candidates for further checking.  Instead, this is more of a thoughtful discussion or guidelines for how reviewers (or professional fact-checkers) might want to think about reading papers while evaluating them for their claims on fact-checking. The analysis of the 100 papers that the paper presents is somewhat underspecified, leading to ambiguity: for instance, while the authors point out that evidence is often lacking for claims being made in narratives (and missing entirely for 'vague narratives'), they also acknowledge that even though citations are included, they need not always be relevant. Checking a fact, even if there is a citation, requires chasing down the references, understanding them, and making a determination about the validity of the current 'fact'. The paper points out that citations/evidence in itself is not a silver bullet, since there are a number of ways in which the citations or evidence presented can be cause problems for the reader.\nThe paper is an interesting and very timely reminder of the problems that we should be thinking about, but EMNLP is not (in my view) the best forum for this work, given the focus of this work. An ethics or social science conference would be a better venue.\n\nquestions_for_the_authors: - Are there scoring frameworks that could be designed to assign a numerical value to find potential candidates for fact-checking?\n- What are good datasets for training and testing that might be assembled, either real or synthetic? ( Generating synthetic datasets should be an interesting problem since the proposed framework could be inverted) Could some of these be done without doing deep NLP? ( for instance, by doing purely propositional theorem proving, or by solving for evidence graphs, etc)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 1: Poor: This study is not yet sufficiently thorough to warrant publication or is not relevant to EMNLP.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "155_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_155_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7618,
      "max_similarity": 0.7779,
      "avg_coverage": 0.3164666666666666,
      "max_coverage": 0.3478
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 330,
      "avg_human_length": 706.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": false
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 7,
      "suggestions_count": 0
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "yDIC18bpq3",
        "similarity": 0.7633,
        "coverage": 0.3333,
        "human_length": 595,
        "human_text": "paper_topic_and_main_contributions: This review/position paper identifies common pitfalls in fact checking papers, e.g., vagueness about who should use the proposed systems and how. A comprehensive categorization of such issues is proposed and applied to the analysis of the 100 most highly cited papers in the area, using a content analysis approach conducted by the authors. Finally, recommendations are provided for improving narratives in fact checking contributions.\nWhile I have not worked on fact checking myself, I am familiar with the literature and the overall rationale.\n\nreasons_to_accept: Important examination of the very basic goals of automated fact checking research and the extent to which they are expressed in influencial publications in the area, finding low coherence in many cases.  Clear, well motivated and systematic argumentation that uses just the terminology and reasoning strategies common in the fact checking literature, e.g., lacking evidence, making it well positioned toward this community.\nRobust and thorough annotation and analysis procedure following best practices in content analysis, resulting in both a novel categorization scheme and annotated corpus used to quantitatively back up the arguments in the paper. Exceptionally detailed account of the annotation guidelines and the meaning of each label, including examples for each single case.\nBroad and informative perspective on related work on the goals of fact checking from various disciplines.\n\nreasons_to_reject: Seems to be implicitly making a strong assumption, that (influencial) papers in automated fact checking always contribute an artifact positioned within some (potential) application. Contributions in NLP, on the other hand, often focus on improving or investigating specific aspects of components that can be internal to such a system, leaving out a broader discussion on the overall system, especially given space limitations. It is therefore not entirely clear whether the normative statement the authors are making, that a comprehensive narrative is always warranted, is a subjective preference or if there is evidence that it is necessary in all cases. For example, arguably it is not the researcher's job to discuss what to do with the identified misinformation after it was found by the system.  No information on the papers themselves is given in the paper, such as distribution over publication years or number of citations. It is also not specified how the number was obtained.\n\nquestions_for_the_authors: A: For unsupported ends, the existence of counter evidence is given as a sufficient criterion. Is it, in general, a criterion for unsupported ends? What if there is both evidence and counter evidence? This black and white view of fact checking may be an oversimplification. Have you considered a more nuanced categorization of faithfulness to evidence, e.g. claim strength as in https://aclanthology.org/2021.emnlp-main.845/ ?\n\ntypos_grammar_style_and_presentation_improvements: Feasibility support does not seem to appear in figure 1. This is fine, but may be worth mentioning so the reader does not look for it.\nCiting Spinoza and Heidegger seems somewhat out of context given the subject of technological artifacts, which is implicitly about information technology. Expanding the discourse more explicitly may help.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "W8FuydY6jk",
        "similarity": 0.7779,
        "coverage": 0.2683,
        "human_length": 873,
        "human_text": "paper_topic_and_main_contributions: The authors investigate the usefulness of research papers about automated fact-checking. They argue that their usefulness decreases without considering who will be using the developed fact-checking artifacts, and without specifying how they should be used. Therefore, they identify relevant components of a fact-checking system and document their degree of consideration in 100 popular fact-checking papers. \nThey contribute a holistic perspective on fact-checking systems, critically analyze a high number of existing papers, define influential components for the usefulness of fact-checking systems, and derive recommendations for future research. They document the results of the analyzed papers in the form of annotations, which is provided in a json file for further analysis.\n\nreasons_to_accept: The paper is well written and structured. The research was conducted systematically and logically, and the arguments made are thorough and valid. When accepted, the paper will draw researchers' attention towards the usability of fact-checking systems. It might especially influence those who \"only\" develop a technological solution without considering its practical and societal benefit. The paper would also introduce a new paradigm for evaluating the usefulness of fact-checking papers and systems. It would raise awareness of important aspects to consider for all those who work on fact-checking systems and care about its value for the community.\n\nreasons_to_reject: There are many categories and new terms introduced which are hard to distinguish without concrete examples. The wording of those terms is also somewhat confusing, so that it is hard to read the paper at the first time. Especially when there are terms with subtle differences, like many of the sub-categories for epistemic elements and narrative types in the paper, combining a few of the terms would have been a good idea in order to make those rememberable for the reader. \nOne might argue that many of the analysis results and recommendations are quite simple and intuitive. It might have been more useful to find out why the analyzed papers have those shortcomings of usefulness instead of supporting the fact that they do have them. In my opinion the authors should have addressed the \"why\" and at least come up with ideas for follow-up research.\n\nquestions_for_the_authors: The numbers in \"()\" refer to the page numbers in the paper.\n(1) Where does the term \"artifacts\" come from?\n(3) Put the epistemic elements and narrative types in a table along with short descriptions and simple examples. This can also be part of the appendix if too large for the body.\n(3) There are many percentages given for epistemic elements and the narrative types, and many of them do not make sense to me. E.g., the given categories for \"Model Owners\" total in 8% - what about the other 92%? And the categories for \"Modeling means\" exceed 100% - how can that be? And the \"Vague narrative\" types should make up 56% (as stated in the paper (on page 4), but the percentages add up to 64%? There are more examples like this in the paper - make sure to correct them, or to clarify how the percentages relate.\n(4) What's the discourse level?\n(4)  What is the difference between vague debunking and vague opposition?\n(6)  Chapter \"Evidence is not a silver bullet\" is well researched and the arguments made in it are all valid. However, the paper is about improving the usefulness of fact-checking related research, and stating these general psychological phenomena which cannot be prevented by fact-checking systems is misplaced here, in my opinion. You should at least give suggestions on how researchers can consider these points in their work.\n(7) One point in your recommendations is to include the data actor in fact-checking papers. It totally makes sense to think from the perspective of the target group when developing a tool, but do you have any suggestions of how to involve them? Are you suggesting to actually invite a sample from the target group as evaluators? How are you going to know what they need if they have a special role which you are not familiar with?\n(15) \"Social Media Moderators (0%)\" ??\n\ntypos_grammar_style_and_presentation_improvements: (1) \"Connecting research to potential use allows researchers to shape their work taking into account the expressed needs of key stakeholders, such as professional fact-checkers (Nakov et al., 2021).\" -- > Overcomplicated sentence:  (2) \"In this paper, we investigate narratives about intended use in automated [...]\" --> \"uses\" or \"the intended use\" (2) \"Based on this, we give recommendations to clarify discussions of intended use:\" --> weird style (2) \"[...] time, rendering resulting artefacts diffi- 122 cult to use in real-world scenarios.\" -- > style (8) \"We investigate narratives of intended use in fact-checking papers, finding that the majority describe how this tool will function in vague or unrealistic terms\" --> \"[...] papers, and find that [...] how a tool [...].\"\n(9) \"We chose to understand automated fact-checking artefacts through their intended uses. This is only one way understand technologies.\" -- > \"[...] one way to understand technologies.\"\n(23) Figure 7 should be formatted in a cleaner way.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "kZtCRKktuR",
        "similarity": 0.7442,
        "coverage": 0.3478,
        "human_length": 650,
        "human_text": "paper_topic_and_main_contributions: The paper is an analysis of 100 \"highly cited\" papers on the topic of automated fact-finding regarding the questions of (i) the motivation for the paper? Why was the solution being discussed? ( ii) the means or mechanisms for doing this; the 'how', and (iii) who this proposed solution is intended for -- the participants in the scenarios being looked at.\nThe analysis was conducted by two annotators (presumably the authors) manually going through the abstracts and introductory sections of these 100 papers and marking passages/text-spans with a series of tags based on a set of guidelines that the paper proposes. The paper presents some of the statistics from this analysis.\n\nreasons_to_accept: This is a discussion of a proposed structure for analyzing fact-checker algorithms, along with an overview of how this framework may be used to understand some of the more highly cited recent papers proposing solutions; as such, this is both interesting and important. Especially as we enter a time of increasingly easy generation of realistic looking narratives using AI, fact-checking, whether automated or not, will become increasingly critical. Having a consistent framework and baseline will make it easier to compare these systems against one another. As a starting point, future work in this area can extend the framework in interesting ways, stimulate discussions on how some of these metrics may be automated and how to compare or weight one dimension over another.\n\nreasons_to_reject: I was hoping that this would be an automated scoring algorithm for fact-checkers -- taking as input algorithms and datasets, and outputting a completeness and correctness score for each (essentially an equivalent of Precision and Recall in information retrieval). Perhaps even a machine learning based analysis of language and communication cues that may, in conjunction with background information, may be used to find potential candidates for further checking.  Instead, this is more of a thoughtful discussion or guidelines for how reviewers (or professional fact-checkers) might want to think about reading papers while evaluating them for their claims on fact-checking. The analysis of the 100 papers that the paper presents is somewhat underspecified, leading to ambiguity: for instance, while the authors point out that evidence is often lacking for claims being made in narratives (and missing entirely for 'vague narratives'), they also acknowledge that even though citations are included, they need not always be relevant. Checking a fact, even if there is a citation, requires chasing down the references, understanding them, and making a determination about the validity of the current 'fact'. The paper points out that citations/evidence in itself is not a silver bullet, since there are a number of ways in which the citations or evidence presented can be cause problems for the reader.\nThe paper is an interesting and very timely reminder of the problems that we should be thinking about, but EMNLP is not (in my view) the best forum for this work, given the focus of this work. An ethics or social science conference would be a better venue.\n\nquestions_for_the_authors: - Are there scoring frameworks that could be designed to assign a numerical value to find potential candidates for fact-checking?\n- What are good datasets for training and testing that might be assembled, either real or synthetic? ( Generating synthetic datasets should be an interesting problem since the proposed framework could be inverted) Could some of these be done without doing deep NLP? ( for instance, by doing purely propositional theorem proving, or by solving for evidence graphs, etc)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 1: Poor: This study is not yet sufficiently thorough to warrant publication or is not relevant to EMNLP.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "yDIC18bpq3",
        "length": 595,
        "human_text": "paper_topic_and_main_contributions: This review/position paper identifies common pitfalls in fact checking papers, e.g., vagueness about who should use the proposed systems and how. A comprehensive categorization of such issues is proposed and applied to the analysis of the 100 most highly cited papers in the area, using a content analysis approach conducted by the authors. Finally, recommendations are provided for improving narratives in fact checking contributions.\nWhile I have not worked on fact checking myself, I am familiar with the literature and the overall rationale.\n\nreasons_to_accept: Important examination of the very basic goals of automated fact checking research and the extent to which they are expressed in influencial publications in the area, finding low coherence in many cases.  Clear, well motivated and systematic argumentation that uses just the terminology and reasoning strategies common in the fact checking literature, e.g., lacking evidence, making it well positioned toward this community.\nRobust and thorough annotation and analysis procedure following best practices in content analysis, resulting in both a novel categorization scheme and annotated corpus used to quantitatively back up the arguments in the paper. Exceptionally detailed account of the annotation guidelines and the meaning of each label, including examples for each single case.\nBroad and informative perspective on related work on the goals of fact checking from various disciplines.\n\nreasons_to_reject: Seems to be implicitly making a strong assumption, that (influencial) papers in automated fact checking always contribute an artifact positioned within some (potential) application. Contributions in NLP, on the other hand, often focus on improving or investigating specific aspects of components that can be internal to such a system, leaving out a broader discussion on the overall system, especially given space limitations. It is therefore not entirely clear whether the normative statement the authors are making, that a comprehensive narrative is always warranted, is a subjective preference or if there is evidence that it is necessary in all cases. For example, arguably it is not the researcher's job to discuss what to do with the identified misinformation after it was found by the system.  No information on the papers themselves is given in the paper, such as distribution over publication years or number of citations. It is also not specified how the number was obtained.\n\nquestions_for_the_authors: A: For unsupported ends, the existence of counter evidence is given as a sufficient criterion. Is it, in general, a criterion for unsupported ends? What if there is both evidence and counter evidence? This black and white view of fact checking may be an oversimplification. Have you considered a more nuanced categorization of faithfulness to evidence, e.g. claim strength as in https://aclanthology.org/2021.emnlp-main.845/ ?\n\ntypos_grammar_style_and_presentation_improvements: Feasibility support does not seem to appear in figure 1. This is fine, but may be worth mentioning so the reader does not look for it.\nCiting Spinoza and Heidegger seems somewhat out of context given the subject of technological artifacts, which is implicitly about information technology. Expanding the discourse more explicitly may help.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "W8FuydY6jk",
        "length": 873,
        "human_text": "paper_topic_and_main_contributions: The authors investigate the usefulness of research papers about automated fact-checking. They argue that their usefulness decreases without considering who will be using the developed fact-checking artifacts, and without specifying how they should be used. Therefore, they identify relevant components of a fact-checking system and document their degree of consideration in 100 popular fact-checking papers. \nThey contribute a holistic perspective on fact-checking systems, critically analyze a high number of existing papers, define influential components for the usefulness of fact-checking systems, and derive recommendations for future research. They document the results of the analyzed papers in the form of annotations, which is provided in a json file for further analysis.\n\nreasons_to_accept: The paper is well written and structured. The research was conducted systematically and logically, and the arguments made are thorough and valid. When accepted, the paper will draw researchers' attention towards the usability of fact-checking systems. It might especially influence those who \"only\" develop a technological solution without considering its practical and societal benefit. The paper would also introduce a new paradigm for evaluating the usefulness of fact-checking papers and systems. It would raise awareness of important aspects to consider for all those who work on fact-checking systems and care about its value for the community.\n\nreasons_to_reject: There are many categories and new terms introduced which are hard to distinguish without concrete examples. The wording of those terms is also somewhat confusing, so that it is hard to read the paper at the first time. Especially when there are terms with subtle differences, like many of the sub-categories for epistemic elements and narrative types in the paper, combining a few of the terms would have been a good idea in order to make those rememberable for the reader. \nOne might argue that many of the analysis results and recommendations are quite simple and intuitive. It might have been more useful to find out why the analyzed papers have those shortcomings of usefulness instead of supporting the fact that they do have them. In my opinion the authors should have addressed the \"why\" and at least come up with ideas for follow-up research.\n\nquestions_for_the_authors: The numbers in \"()\" refer to the page numbers in the paper.\n(1) Where does the term \"artifacts\" come from?\n(3) Put the epistemic elements and narrative types in a table along with short descriptions and simple examples. This can also be part of the appendix if too large for the body.\n(3) There are many percentages given for epistemic elements and the narrative types, and many of them do not make sense to me. E.g., the given categories for \"Model Owners\" total in 8% - what about the other 92%? And the categories for \"Modeling means\" exceed 100% - how can that be? And the \"Vague narrative\" types should make up 56% (as stated in the paper (on page 4), but the percentages add up to 64%? There are more examples like this in the paper - make sure to correct them, or to clarify how the percentages relate.\n(4) What's the discourse level?\n(4)  What is the difference between vague debunking and vague opposition?\n(6)  Chapter \"Evidence is not a silver bullet\" is well researched and the arguments made in it are all valid. However, the paper is about improving the usefulness of fact-checking related research, and stating these general psychological phenomena which cannot be prevented by fact-checking systems is misplaced here, in my opinion. You should at least give suggestions on how researchers can consider these points in their work.\n(7) One point in your recommendations is to include the data actor in fact-checking papers. It totally makes sense to think from the perspective of the target group when developing a tool, but do you have any suggestions of how to involve them? Are you suggesting to actually invite a sample from the target group as evaluators? How are you going to know what they need if they have a special role which you are not familiar with?\n(15) \"Social Media Moderators (0%)\" ??\n\ntypos_grammar_style_and_presentation_improvements: (1) \"Connecting research to potential use allows researchers to shape their work taking into account the expressed needs of key stakeholders, such as professional fact-checkers (Nakov et al., 2021).\" -- > Overcomplicated sentence:  (2) \"In this paper, we investigate narratives about intended use in automated [...]\" --> \"uses\" or \"the intended use\" (2) \"Based on this, we give recommendations to clarify discussions of intended use:\" --> weird style (2) \"[...] time, rendering resulting artefacts diffi- 122 cult to use in real-world scenarios.\" -- > style (8) \"We investigate narratives of intended use in fact-checking papers, finding that the majority describe how this tool will function in vague or unrealistic terms\" --> \"[...] papers, and find that [...] how a tool [...].\"\n(9) \"We chose to understand automated fact-checking artefacts through their intended uses. This is only one way understand technologies.\" -- > \"[...] one way to understand technologies.\"\n(23) Figure 7 should be formatted in a cleaner way.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "kZtCRKktuR",
        "length": 650,
        "human_text": "paper_topic_and_main_contributions: The paper is an analysis of 100 \"highly cited\" papers on the topic of automated fact-finding regarding the questions of (i) the motivation for the paper? Why was the solution being discussed? ( ii) the means or mechanisms for doing this; the 'how', and (iii) who this proposed solution is intended for -- the participants in the scenarios being looked at.\nThe analysis was conducted by two annotators (presumably the authors) manually going through the abstracts and introductory sections of these 100 papers and marking passages/text-spans with a series of tags based on a set of guidelines that the paper proposes. The paper presents some of the statistics from this analysis.\n\nreasons_to_accept: This is a discussion of a proposed structure for analyzing fact-checker algorithms, along with an overview of how this framework may be used to understand some of the more highly cited recent papers proposing solutions; as such, this is both interesting and important. Especially as we enter a time of increasingly easy generation of realistic looking narratives using AI, fact-checking, whether automated or not, will become increasingly critical. Having a consistent framework and baseline will make it easier to compare these systems against one another. As a starting point, future work in this area can extend the framework in interesting ways, stimulate discussions on how some of these metrics may be automated and how to compare or weight one dimension over another.\n\nreasons_to_reject: I was hoping that this would be an automated scoring algorithm for fact-checkers -- taking as input algorithms and datasets, and outputting a completeness and correctness score for each (essentially an equivalent of Precision and Recall in information retrieval). Perhaps even a machine learning based analysis of language and communication cues that may, in conjunction with background information, may be used to find potential candidates for further checking.  Instead, this is more of a thoughtful discussion or guidelines for how reviewers (or professional fact-checkers) might want to think about reading papers while evaluating them for their claims on fact-checking. The analysis of the 100 papers that the paper presents is somewhat underspecified, leading to ambiguity: for instance, while the authors point out that evidence is often lacking for claims being made in narratives (and missing entirely for 'vague narratives'), they also acknowledge that even though citations are included, they need not always be relevant. Checking a fact, even if there is a citation, requires chasing down the references, understanding them, and making a determination about the validity of the current 'fact'. The paper points out that citations/evidence in itself is not a silver bullet, since there are a number of ways in which the citations or evidence presented can be cause problems for the reader.\nThe paper is an interesting and very timely reminder of the problems that we should be thinking about, but EMNLP is not (in my view) the best forum for this work, given the focus of this work. An ethics or social science conference would be a better venue.\n\nquestions_for_the_authors: - Are there scoring frameworks that could be designed to assign a numerical value to find potential candidates for fact-checking?\n- What are good datasets for training and testing that might be assembled, either real or synthetic? ( Generating synthetic datasets should be an interesting problem since the proposed framework could be inverted) Could some of these be done without doing deep NLP? ( for instance, by doing purely propositional theorem proving, or by solving for evidence graphs, etc)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 1: Poor: This study is not yet sufficiently thorough to warrant publication or is not relevant to EMNLP.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "110_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_110_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6934999999999999,
      "max_similarity": 0.7144,
      "avg_coverage": 0.4421,
      "max_coverage": 0.6111
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 417,
      "avg_human_length": 670.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 6,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "gzQwTaaDAq",
        "similarity": 0.6867,
        "coverage": 0.6111,
        "human_length": 224,
        "human_text": "paper_topic_and_main_contributions: This paper proposes tGSLM, which is a generative spoken language model with features aligned with lexicon encodings. The advantage of this method is that it operates at a scale similar to phonetic/lexical units so that it achieves high efficiency while maintaining the same performance as the fine-grained GSLM.\n\nreasons_to_accept: 1. The alignment between acoustic encoding and lexical encoding to help GSLM is innovative. \n2. Experiments are thorough and in detail.\n\nreasons_to_reject: 1. As the method involves a lot of specific knowledge and dedicated components, it is a bit hard to follow the description of the system. In particular, I was unclear about how the upsampling to HuBERT units was done. Also, see questions for the authors below.\n\nquestions_for_the_authors: 1. How do you get the corresponding Lex tokens for a corresponding 200ms window exactly? Do the sequence of lexical tokens and the sequence of acoustic tokens have the same lengths? \n2. Have you tried any other lengths rather than 200ms?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "KaRXerKTQV",
        "similarity": 0.6794,
        "coverage": 0.5217,
        "human_length": 397,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a method to transform and concatenate frames extracted by wav2vec 2.0 into 200ms vector-based acoustic tokens. The vector-based acoustic tokens can be applied for training a 200ms-tGSLM speech based language model. Performance of this  200ms-tGSLM is comparable to conventional GSLM based on 240ms frames. The proposed method has the potential to improve speech LLM modelling, in terms of computation efficiency and representation ability.\n\nreasons_to_accept: The significant longer sequence length is a challenge of applying NLP sequence-to-sequence technique to speech. There are a lot of researches trying to shorten speech sequences for effective computation and better performance. This paper provides another perspective by proposing a method to generate longer fixed size acoustic tokens of 200ms which is much close to duration of sub-word units. The length the speech sequence is now comparable to the sequence with sub-word tokens, which has the potential for improving speech LLM training. While the choice of 200ms is somehow heuristic, the duration aligns well with average syllable length of most languages. The work includes experiments of different tasks and the results are encouraging.\n\nreasons_to_reject: The performance of 200ms-tGSLM is not necessary better than conventional 40ms based GSLM, except the MMOS for generated speech. However, I do not think that this is  a strong reason to reject this paper.\n\nquestions_for_the_authors: A. Section 3.2.2: For my understanding, the generated acoustic token vectors are converted into Hubert before applying to Tacotron2.0 vocoder. I understand that the authors try to reuse the available vocoder for comparison. However, directly generating time domain signals from the 200ms acoustic tokens with techniques such as GAN should be feasible, and may achieve better results. Have the authors tried the experiments to generate speech directly from the 200ms tokens?\nB.Section 3.1.2: The quantisation parameter $q$ is pre-computed before training. How to determine the PCA size and the K-mean cluster size? Is it possible to replace this pre-computed parameter with other quantisation such as VQ such that this bottle net layer can be trained end-to-end?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "QvPuua0GZC",
        "similarity": 0.7144,
        "coverage": 0.1935,
        "human_length": 1390,
        "human_text": "paper_topic_and_main_contributions: The paper treats of \"audio\" language modeling, that is language represented as audio spoken modality. This task is tackled by training a language model (LM) on speech data represented as discrete tokens. Such model work similarly to text LMs, with a fixed-size vocabulary, cross-entropy loss during training and multinomial sampling during inference, and can be used jointly with vocoder models to autoregressively generate speech and audio. \nIn previous works, the vocabulary is built from sets of learned speech sequence embeddings (SSE) of sequences of speech. These sequences of speech are often short, between 20 to 40ms, and may however not be representative of any relevant information (i.e. when shorter than a phoneme or syllable). An other drawback is the extensive token sequence length created by that such method, which is a known bottleneck of Transformers / attention mechanism. \nIn this paper, the authors propose to use tokens of longer speech sequences, which would reduce the token sequence length and in turn reduce the overall complexity / memory usage, while having more semantically rich audio embeddings. Doing so will however create a very large number of possible SSEs / tokens, making having a finite and well balanced vocabulary intractable (called clustering problem). The authors propose then to directly work in a continuous space, that is not using a finite vocabulary but directly feed the model with continuous vectors. The model cannot directly be fed the SEEs, as \"as two speech segments may sound the same, i.e. be close in the acoustic space\", so an additional function, learned with the model (\"lexical embedder\") mapping the SEEs to input embeddings (called here \"lexical tokens\"), is designed. And as there is no finite vocabulary, the model cannot be trained with the cross-entropy loss commonly used. The authors resort to use a contrastive learning loss. Another modification is made on the output of the model: the authors used three separate output heads (fully connecter layers) to predict the next three tokens. This modification allows to \"extend\" the scope of prediction of the model, which would be fairly limited by autoregressively sampling one token representing a piece of a word after another. \nCompared to the [GSLM](https://aclanthology.org/2021.tacl-1.79/) baseline, the method proposed by the authors allows to significantly reduce the memory usage (by reducing the input sequence lengths) while having results of better quality (measured by perplexity and zero-shot metrics, along with human preferences) and interpretable embeddings.\n\nreasons_to_accept: - The overall goal is very well motivated. Tackling audio modeling as a sequential problem is a very good direction offering a lot of flexibility in real-condition usages. I believe there is a lot of progress to be made, with significant impact, on the representation / tokenization of audio; - The implementation choices of tGSLM are well made and well justified (e.g. 200ms segmentation). Tackling this problem required a fair amount of model adaptations (LexEmb, using contrastive loss, FAISS for inference...) that the authors cleverly overcame; - The tasks and set of metrics are well chosen; - The interpretation of the \"lexical token\" embeddings is valuable and empirically shows that the method works; - The results show that LMs can generate speech using word-size frames directly fed as continuous embeddings; - The paper is globally well written and structured; - The references are very well documented and completed. Only one or two has missing URLs.\n\nreasons_to_reject: - Although the experimental setup is well made (tasks + metrics), I think the paper can really benefits from one or two more baselines, even though this kind of audio representation is fairly new and that you answer well to the main goal of the paper that you set (i.e. generate speech using word-size audio embeddings). The authors could add tokenizations from [AudioLM](https://arxiv.org/abs/2209.03143) or [BEATs](https://arxiv.org/abs/2212.09058). Eventually a baseline without $LexEmb$, that could highlight its necessity. I think the reader would benefit to see how tGSLM compares to these baselines.\n- There is no information about generation speed at inference. This information is IMO as well important as the memory usage. It as a decisive point when designing a system / product. Here, considering the additional stack (FAISS) compared to models using finite vocabularies, the reader could want to know id it is worth to spend the time needed to implement it.\n- **(no big deal)** The memory gain estimation in A.5 is valuable. But it would be great to also consider the FAISS memory consumption for inference, and give concrete examples with numbers, e.g. how $N$ pairs of SEE / lexical token translate into memory consumption. This $N$ could be a limitation depending on the hardware being used. Also as $N$ is correlated to the lengths of the speech sequences, it could raise questions when using such model in real conditions.\n\nquestions_for_the_authors: **A.** I think the \"lexical tokens\" and \"acoustic tokens\" namings are confusing at first read, and not really accurate. In the literature, a token is a discrete element, from a finite ensemble. I would recommend to rename them to something that might indicate that these elements are actually continuous / vectors from a continuous space. What is your opinion on this?\n**B.** Did you made some benchmarks on the generation speed? I think it could be express in tokens/sec and sec_of_audio/sec. As your model now has additional modules (namely LexEmb, multiple output heads, using FAISS search), I assume that the generation speed gain is not linear to the token sequence reduction; **C.** Maybe a suggestion for future research: what do you think about conditioning the output heads on the results of the previous one? By that I mean that $\\mathbf{y}\\_i = h(\\cdot)$ could be conditioned on $\\mathbf{y}\\_{i-1}$, e.g. $\\mathbf{y}\\_i = h(\\mathbf{x}, \\mathbf{y}\\_{i-1})$. Some models applied the same method of yours for symbolic music modeling ([Compound Word](https://ojs.aaai.org/index.php/AAAI/article/view/16091), [MusicBERT](https://aclanthology.org/2021.findings-acl.70/)). In practice, generating several distinct elements at the same time, unconditionally although they are meant to be complementary / tied, does not work very well for causal generation (as GANS / Diffusion / any non-autoregressive model). Thus conditioning each output on the very last predicted is almost always guaranteed to give more coherent results. [ Compound Word](https://ojs.aaai.org/index.php/AAAI/article/view/16091) does it partially.\n\nmissing_references: In your introduction, you mention \"tokenizer representing word or subword units\" (l60) without mentioning examples of such method. If think the comparison with your method is very relevant. It could then be nice (if you have enough space) to mention and cite the most popular methods currently used to build vocabularies for text, that the reader can relate to: - [BPE](https://www.derczynski.com/papers/archive/BPE_Gage.pdf) and [its application to natural language](https://aclanthology.org/P16-1162/) - [WordPiece](https://arxiv.org/abs/1609.08144v2) - [Unigram](https://aclanthology.org/P18-1007/) NCE loss introduction: [Noise-contrastive estimation: A new estimation principle for unnormalized statistical models](http://proceedings.mlr.press/v9/gutmann10a.html)\n\ntypos_grammar_style_and_presentation_improvements: You should consider converting your figures to vector format, this will provide the reader a better quality of visualization. Assuming Fig. 3 and 4 are generated with pyplot, you just have to save them as pdf.\nRename \"lexical tokens\" and \"acoustic tokens\" (see reasons to reject); Fig. 1: you could change the contour of the \"Lex Emb\" contours in order to better distinguish methods/functions from models, change \"Transformer\" to \"LM\" (more general), add the signification of \"FC\" somewhere in the main text; l244: \"there is often a linear FC layer before the transformer\" --> this is not really accurate, it isn't a fully-connected layer. You should rephrase with something like \"a learned embedding matrix acting as a lookup table\"; l255: \"measured\" --> did you mean studied ?\nl267: precise acoustic tokens; l322: add signification of NCE + cite [Gutmann et al](http://proceedings.mlr.press/v9/gutmann10a.html); l322 footnote (3): It would be good to include in the main text that L2 reconstruction is applicable, but did not get you good results in practice and include them in appendix; l356 / subsection 3.2.1: You could mention that you are using FAISS (I assumed it was when looking at fig 2 but had to look for appendix to find the answer); l1080 footnote: \"those\" --> \"these\" + missing dot; l1355: add the signification of \"BPE\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "gzQwTaaDAq",
        "length": 224,
        "human_text": "paper_topic_and_main_contributions: This paper proposes tGSLM, which is a generative spoken language model with features aligned with lexicon encodings. The advantage of this method is that it operates at a scale similar to phonetic/lexical units so that it achieves high efficiency while maintaining the same performance as the fine-grained GSLM.\n\nreasons_to_accept: 1. The alignment between acoustic encoding and lexical encoding to help GSLM is innovative. \n2. Experiments are thorough and in detail.\n\nreasons_to_reject: 1. As the method involves a lot of specific knowledge and dedicated components, it is a bit hard to follow the description of the system. In particular, I was unclear about how the upsampling to HuBERT units was done. Also, see questions for the authors below.\n\nquestions_for_the_authors: 1. How do you get the corresponding Lex tokens for a corresponding 200ms window exactly? Do the sequence of lexical tokens and the sequence of acoustic tokens have the same lengths? \n2. Have you tried any other lengths rather than 200ms?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "KaRXerKTQV",
        "length": 397,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a method to transform and concatenate frames extracted by wav2vec 2.0 into 200ms vector-based acoustic tokens. The vector-based acoustic tokens can be applied for training a 200ms-tGSLM speech based language model. Performance of this  200ms-tGSLM is comparable to conventional GSLM based on 240ms frames. The proposed method has the potential to improve speech LLM modelling, in terms of computation efficiency and representation ability.\n\nreasons_to_accept: The significant longer sequence length is a challenge of applying NLP sequence-to-sequence technique to speech. There are a lot of researches trying to shorten speech sequences for effective computation and better performance. This paper provides another perspective by proposing a method to generate longer fixed size acoustic tokens of 200ms which is much close to duration of sub-word units. The length the speech sequence is now comparable to the sequence with sub-word tokens, which has the potential for improving speech LLM training. While the choice of 200ms is somehow heuristic, the duration aligns well with average syllable length of most languages. The work includes experiments of different tasks and the results are encouraging.\n\nreasons_to_reject: The performance of 200ms-tGSLM is not necessary better than conventional 40ms based GSLM, except the MMOS for generated speech. However, I do not think that this is  a strong reason to reject this paper.\n\nquestions_for_the_authors: A. Section 3.2.2: For my understanding, the generated acoustic token vectors are converted into Hubert before applying to Tacotron2.0 vocoder. I understand that the authors try to reuse the available vocoder for comparison. However, directly generating time domain signals from the 200ms acoustic tokens with techniques such as GAN should be feasible, and may achieve better results. Have the authors tried the experiments to generate speech directly from the 200ms tokens?\nB.Section 3.1.2: The quantisation parameter $q$ is pre-computed before training. How to determine the PCA size and the K-mean cluster size? Is it possible to replace this pre-computed parameter with other quantisation such as VQ such that this bottle net layer can be trained end-to-end?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "QvPuua0GZC",
        "length": 1390,
        "human_text": "paper_topic_and_main_contributions: The paper treats of \"audio\" language modeling, that is language represented as audio spoken modality. This task is tackled by training a language model (LM) on speech data represented as discrete tokens. Such model work similarly to text LMs, with a fixed-size vocabulary, cross-entropy loss during training and multinomial sampling during inference, and can be used jointly with vocoder models to autoregressively generate speech and audio. \nIn previous works, the vocabulary is built from sets of learned speech sequence embeddings (SSE) of sequences of speech. These sequences of speech are often short, between 20 to 40ms, and may however not be representative of any relevant information (i.e. when shorter than a phoneme or syllable). An other drawback is the extensive token sequence length created by that such method, which is a known bottleneck of Transformers / attention mechanism. \nIn this paper, the authors propose to use tokens of longer speech sequences, which would reduce the token sequence length and in turn reduce the overall complexity / memory usage, while having more semantically rich audio embeddings. Doing so will however create a very large number of possible SSEs / tokens, making having a finite and well balanced vocabulary intractable (called clustering problem). The authors propose then to directly work in a continuous space, that is not using a finite vocabulary but directly feed the model with continuous vectors. The model cannot directly be fed the SEEs, as \"as two speech segments may sound the same, i.e. be close in the acoustic space\", so an additional function, learned with the model (\"lexical embedder\") mapping the SEEs to input embeddings (called here \"lexical tokens\"), is designed. And as there is no finite vocabulary, the model cannot be trained with the cross-entropy loss commonly used. The authors resort to use a contrastive learning loss. Another modification is made on the output of the model: the authors used three separate output heads (fully connecter layers) to predict the next three tokens. This modification allows to \"extend\" the scope of prediction of the model, which would be fairly limited by autoregressively sampling one token representing a piece of a word after another. \nCompared to the [GSLM](https://aclanthology.org/2021.tacl-1.79/) baseline, the method proposed by the authors allows to significantly reduce the memory usage (by reducing the input sequence lengths) while having results of better quality (measured by perplexity and zero-shot metrics, along with human preferences) and interpretable embeddings.\n\nreasons_to_accept: - The overall goal is very well motivated. Tackling audio modeling as a sequential problem is a very good direction offering a lot of flexibility in real-condition usages. I believe there is a lot of progress to be made, with significant impact, on the representation / tokenization of audio; - The implementation choices of tGSLM are well made and well justified (e.g. 200ms segmentation). Tackling this problem required a fair amount of model adaptations (LexEmb, using contrastive loss, FAISS for inference...) that the authors cleverly overcame; - The tasks and set of metrics are well chosen; - The interpretation of the \"lexical token\" embeddings is valuable and empirically shows that the method works; - The results show that LMs can generate speech using word-size frames directly fed as continuous embeddings; - The paper is globally well written and structured; - The references are very well documented and completed. Only one or two has missing URLs.\n\nreasons_to_reject: - Although the experimental setup is well made (tasks + metrics), I think the paper can really benefits from one or two more baselines, even though this kind of audio representation is fairly new and that you answer well to the main goal of the paper that you set (i.e. generate speech using word-size audio embeddings). The authors could add tokenizations from [AudioLM](https://arxiv.org/abs/2209.03143) or [BEATs](https://arxiv.org/abs/2212.09058). Eventually a baseline without $LexEmb$, that could highlight its necessity. I think the reader would benefit to see how tGSLM compares to these baselines.\n- There is no information about generation speed at inference. This information is IMO as well important as the memory usage. It as a decisive point when designing a system / product. Here, considering the additional stack (FAISS) compared to models using finite vocabularies, the reader could want to know id it is worth to spend the time needed to implement it.\n- **(no big deal)** The memory gain estimation in A.5 is valuable. But it would be great to also consider the FAISS memory consumption for inference, and give concrete examples with numbers, e.g. how $N$ pairs of SEE / lexical token translate into memory consumption. This $N$ could be a limitation depending on the hardware being used. Also as $N$ is correlated to the lengths of the speech sequences, it could raise questions when using such model in real conditions.\n\nquestions_for_the_authors: **A.** I think the \"lexical tokens\" and \"acoustic tokens\" namings are confusing at first read, and not really accurate. In the literature, a token is a discrete element, from a finite ensemble. I would recommend to rename them to something that might indicate that these elements are actually continuous / vectors from a continuous space. What is your opinion on this?\n**B.** Did you made some benchmarks on the generation speed? I think it could be express in tokens/sec and sec_of_audio/sec. As your model now has additional modules (namely LexEmb, multiple output heads, using FAISS search), I assume that the generation speed gain is not linear to the token sequence reduction; **C.** Maybe a suggestion for future research: what do you think about conditioning the output heads on the results of the previous one? By that I mean that $\\mathbf{y}\\_i = h(\\cdot)$ could be conditioned on $\\mathbf{y}\\_{i-1}$, e.g. $\\mathbf{y}\\_i = h(\\mathbf{x}, \\mathbf{y}\\_{i-1})$. Some models applied the same method of yours for symbolic music modeling ([Compound Word](https://ojs.aaai.org/index.php/AAAI/article/view/16091), [MusicBERT](https://aclanthology.org/2021.findings-acl.70/)). In practice, generating several distinct elements at the same time, unconditionally although they are meant to be complementary / tied, does not work very well for causal generation (as GANS / Diffusion / any non-autoregressive model). Thus conditioning each output on the very last predicted is almost always guaranteed to give more coherent results. [ Compound Word](https://ojs.aaai.org/index.php/AAAI/article/view/16091) does it partially.\n\nmissing_references: In your introduction, you mention \"tokenizer representing word or subword units\" (l60) without mentioning examples of such method. If think the comparison with your method is very relevant. It could then be nice (if you have enough space) to mention and cite the most popular methods currently used to build vocabularies for text, that the reader can relate to: - [BPE](https://www.derczynski.com/papers/archive/BPE_Gage.pdf) and [its application to natural language](https://aclanthology.org/P16-1162/) - [WordPiece](https://arxiv.org/abs/1609.08144v2) - [Unigram](https://aclanthology.org/P18-1007/) NCE loss introduction: [Noise-contrastive estimation: A new estimation principle for unnormalized statistical models](http://proceedings.mlr.press/v9/gutmann10a.html)\n\ntypos_grammar_style_and_presentation_improvements: You should consider converting your figures to vector format, this will provide the reader a better quality of visualization. Assuming Fig. 3 and 4 are generated with pyplot, you just have to save them as pdf.\nRename \"lexical tokens\" and \"acoustic tokens\" (see reasons to reject); Fig. 1: you could change the contour of the \"Lex Emb\" contours in order to better distinguish methods/functions from models, change \"Transformer\" to \"LM\" (more general), add the signification of \"FC\" somewhere in the main text; l244: \"there is often a linear FC layer before the transformer\" --> this is not really accurate, it isn't a fully-connected layer. You should rephrase with something like \"a learned embedding matrix acting as a lookup table\"; l255: \"measured\" --> did you mean studied ?\nl267: precise acoustic tokens; l322: add signification of NCE + cite [Gutmann et al](http://proceedings.mlr.press/v9/gutmann10a.html); l322 footnote (3): It would be good to include in the main text that L2 reconstruction is applicable, but did not get you good results in practice and include them in appendix; l356 / subsection 3.2.1: You could mention that you are using FAISS (I assumed it was when looking at fig 2 but had to look for appendix to find the answer); l1080 footnote: \"those\" --> \"these\" + missing dot; l1355: add the signification of \"BPE\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "110_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_110_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.6926666666666667,
      "max_similarity": 0.7084,
      "avg_coverage": 0.4475,
      "max_coverage": 0.6111
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 417,
      "avg_human_length": 670.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 6,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "gzQwTaaDAq",
        "similarity": 0.6877,
        "coverage": 0.6111,
        "human_length": 224,
        "human_text": "paper_topic_and_main_contributions: This paper proposes tGSLM, which is a generative spoken language model with features aligned with lexicon encodings. The advantage of this method is that it operates at a scale similar to phonetic/lexical units so that it achieves high efficiency while maintaining the same performance as the fine-grained GSLM.\n\nreasons_to_accept: 1. The alignment between acoustic encoding and lexical encoding to help GSLM is innovative. \n2. Experiments are thorough and in detail.\n\nreasons_to_reject: 1. As the method involves a lot of specific knowledge and dedicated components, it is a bit hard to follow the description of the system. In particular, I was unclear about how the upsampling to HuBERT units was done. Also, see questions for the authors below.\n\nquestions_for_the_authors: 1. How do you get the corresponding Lex tokens for a corresponding 200ms window exactly? Do the sequence of lexical tokens and the sequence of acoustic tokens have the same lengths? \n2. Have you tried any other lengths rather than 200ms?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "KaRXerKTQV",
        "similarity": 0.6819,
        "coverage": 0.5217,
        "human_length": 397,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a method to transform and concatenate frames extracted by wav2vec 2.0 into 200ms vector-based acoustic tokens. The vector-based acoustic tokens can be applied for training a 200ms-tGSLM speech based language model. Performance of this  200ms-tGSLM is comparable to conventional GSLM based on 240ms frames. The proposed method has the potential to improve speech LLM modelling, in terms of computation efficiency and representation ability.\n\nreasons_to_accept: The significant longer sequence length is a challenge of applying NLP sequence-to-sequence technique to speech. There are a lot of researches trying to shorten speech sequences for effective computation and better performance. This paper provides another perspective by proposing a method to generate longer fixed size acoustic tokens of 200ms which is much close to duration of sub-word units. The length the speech sequence is now comparable to the sequence with sub-word tokens, which has the potential for improving speech LLM training. While the choice of 200ms is somehow heuristic, the duration aligns well with average syllable length of most languages. The work includes experiments of different tasks and the results are encouraging.\n\nreasons_to_reject: The performance of 200ms-tGSLM is not necessary better than conventional 40ms based GSLM, except the MMOS for generated speech. However, I do not think that this is  a strong reason to reject this paper.\n\nquestions_for_the_authors: A. Section 3.2.2: For my understanding, the generated acoustic token vectors are converted into Hubert before applying to Tacotron2.0 vocoder. I understand that the authors try to reuse the available vocoder for comparison. However, directly generating time domain signals from the 200ms acoustic tokens with techniques such as GAN should be feasible, and may achieve better results. Have the authors tried the experiments to generate speech directly from the 200ms tokens?\nB.Section 3.1.2: The quantisation parameter $q$ is pre-computed before training. How to determine the PCA size and the K-mean cluster size? Is it possible to replace this pre-computed parameter with other quantisation such as VQ such that this bottle net layer can be trained end-to-end?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "QvPuua0GZC",
        "similarity": 0.7084,
        "coverage": 0.2097,
        "human_length": 1390,
        "human_text": "paper_topic_and_main_contributions: The paper treats of \"audio\" language modeling, that is language represented as audio spoken modality. This task is tackled by training a language model (LM) on speech data represented as discrete tokens. Such model work similarly to text LMs, with a fixed-size vocabulary, cross-entropy loss during training and multinomial sampling during inference, and can be used jointly with vocoder models to autoregressively generate speech and audio. \nIn previous works, the vocabulary is built from sets of learned speech sequence embeddings (SSE) of sequences of speech. These sequences of speech are often short, between 20 to 40ms, and may however not be representative of any relevant information (i.e. when shorter than a phoneme or syllable). An other drawback is the extensive token sequence length created by that such method, which is a known bottleneck of Transformers / attention mechanism. \nIn this paper, the authors propose to use tokens of longer speech sequences, which would reduce the token sequence length and in turn reduce the overall complexity / memory usage, while having more semantically rich audio embeddings. Doing so will however create a very large number of possible SSEs / tokens, making having a finite and well balanced vocabulary intractable (called clustering problem). The authors propose then to directly work in a continuous space, that is not using a finite vocabulary but directly feed the model with continuous vectors. The model cannot directly be fed the SEEs, as \"as two speech segments may sound the same, i.e. be close in the acoustic space\", so an additional function, learned with the model (\"lexical embedder\") mapping the SEEs to input embeddings (called here \"lexical tokens\"), is designed. And as there is no finite vocabulary, the model cannot be trained with the cross-entropy loss commonly used. The authors resort to use a contrastive learning loss. Another modification is made on the output of the model: the authors used three separate output heads (fully connecter layers) to predict the next three tokens. This modification allows to \"extend\" the scope of prediction of the model, which would be fairly limited by autoregressively sampling one token representing a piece of a word after another. \nCompared to the [GSLM](https://aclanthology.org/2021.tacl-1.79/) baseline, the method proposed by the authors allows to significantly reduce the memory usage (by reducing the input sequence lengths) while having results of better quality (measured by perplexity and zero-shot metrics, along with human preferences) and interpretable embeddings.\n\nreasons_to_accept: - The overall goal is very well motivated. Tackling audio modeling as a sequential problem is a very good direction offering a lot of flexibility in real-condition usages. I believe there is a lot of progress to be made, with significant impact, on the representation / tokenization of audio; - The implementation choices of tGSLM are well made and well justified (e.g. 200ms segmentation). Tackling this problem required a fair amount of model adaptations (LexEmb, using contrastive loss, FAISS for inference...) that the authors cleverly overcame; - The tasks and set of metrics are well chosen; - The interpretation of the \"lexical token\" embeddings is valuable and empirically shows that the method works; - The results show that LMs can generate speech using word-size frames directly fed as continuous embeddings; - The paper is globally well written and structured; - The references are very well documented and completed. Only one or two has missing URLs.\n\nreasons_to_reject: - Although the experimental setup is well made (tasks + metrics), I think the paper can really benefits from one or two more baselines, even though this kind of audio representation is fairly new and that you answer well to the main goal of the paper that you set (i.e. generate speech using word-size audio embeddings). The authors could add tokenizations from [AudioLM](https://arxiv.org/abs/2209.03143) or [BEATs](https://arxiv.org/abs/2212.09058). Eventually a baseline without $LexEmb$, that could highlight its necessity. I think the reader would benefit to see how tGSLM compares to these baselines.\n- There is no information about generation speed at inference. This information is IMO as well important as the memory usage. It as a decisive point when designing a system / product. Here, considering the additional stack (FAISS) compared to models using finite vocabularies, the reader could want to know id it is worth to spend the time needed to implement it.\n- **(no big deal)** The memory gain estimation in A.5 is valuable. But it would be great to also consider the FAISS memory consumption for inference, and give concrete examples with numbers, e.g. how $N$ pairs of SEE / lexical token translate into memory consumption. This $N$ could be a limitation depending on the hardware being used. Also as $N$ is correlated to the lengths of the speech sequences, it could raise questions when using such model in real conditions.\n\nquestions_for_the_authors: **A.** I think the \"lexical tokens\" and \"acoustic tokens\" namings are confusing at first read, and not really accurate. In the literature, a token is a discrete element, from a finite ensemble. I would recommend to rename them to something that might indicate that these elements are actually continuous / vectors from a continuous space. What is your opinion on this?\n**B.** Did you made some benchmarks on the generation speed? I think it could be express in tokens/sec and sec_of_audio/sec. As your model now has additional modules (namely LexEmb, multiple output heads, using FAISS search), I assume that the generation speed gain is not linear to the token sequence reduction; **C.** Maybe a suggestion for future research: what do you think about conditioning the output heads on the results of the previous one? By that I mean that $\\mathbf{y}\\_i = h(\\cdot)$ could be conditioned on $\\mathbf{y}\\_{i-1}$, e.g. $\\mathbf{y}\\_i = h(\\mathbf{x}, \\mathbf{y}\\_{i-1})$. Some models applied the same method of yours for symbolic music modeling ([Compound Word](https://ojs.aaai.org/index.php/AAAI/article/view/16091), [MusicBERT](https://aclanthology.org/2021.findings-acl.70/)). In practice, generating several distinct elements at the same time, unconditionally although they are meant to be complementary / tied, does not work very well for causal generation (as GANS / Diffusion / any non-autoregressive model). Thus conditioning each output on the very last predicted is almost always guaranteed to give more coherent results. [ Compound Word](https://ojs.aaai.org/index.php/AAAI/article/view/16091) does it partially.\n\nmissing_references: In your introduction, you mention \"tokenizer representing word or subword units\" (l60) without mentioning examples of such method. If think the comparison with your method is very relevant. It could then be nice (if you have enough space) to mention and cite the most popular methods currently used to build vocabularies for text, that the reader can relate to: - [BPE](https://www.derczynski.com/papers/archive/BPE_Gage.pdf) and [its application to natural language](https://aclanthology.org/P16-1162/) - [WordPiece](https://arxiv.org/abs/1609.08144v2) - [Unigram](https://aclanthology.org/P18-1007/) NCE loss introduction: [Noise-contrastive estimation: A new estimation principle for unnormalized statistical models](http://proceedings.mlr.press/v9/gutmann10a.html)\n\ntypos_grammar_style_and_presentation_improvements: You should consider converting your figures to vector format, this will provide the reader a better quality of visualization. Assuming Fig. 3 and 4 are generated with pyplot, you just have to save them as pdf.\nRename \"lexical tokens\" and \"acoustic tokens\" (see reasons to reject); Fig. 1: you could change the contour of the \"Lex Emb\" contours in order to better distinguish methods/functions from models, change \"Transformer\" to \"LM\" (more general), add the signification of \"FC\" somewhere in the main text; l244: \"there is often a linear FC layer before the transformer\" --> this is not really accurate, it isn't a fully-connected layer. You should rephrase with something like \"a learned embedding matrix acting as a lookup table\"; l255: \"measured\" --> did you mean studied ?\nl267: precise acoustic tokens; l322: add signification of NCE + cite [Gutmann et al](http://proceedings.mlr.press/v9/gutmann10a.html); l322 footnote (3): It would be good to include in the main text that L2 reconstruction is applicable, but did not get you good results in practice and include them in appendix; l356 / subsection 3.2.1: You could mention that you are using FAISS (I assumed it was when looking at fig 2 but had to look for appendix to find the answer); l1080 footnote: \"those\" --> \"these\" + missing dot; l1355: add the signification of \"BPE\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "gzQwTaaDAq",
        "length": 224,
        "human_text": "paper_topic_and_main_contributions: This paper proposes tGSLM, which is a generative spoken language model with features aligned with lexicon encodings. The advantage of this method is that it operates at a scale similar to phonetic/lexical units so that it achieves high efficiency while maintaining the same performance as the fine-grained GSLM.\n\nreasons_to_accept: 1. The alignment between acoustic encoding and lexical encoding to help GSLM is innovative. \n2. Experiments are thorough and in detail.\n\nreasons_to_reject: 1. As the method involves a lot of specific knowledge and dedicated components, it is a bit hard to follow the description of the system. In particular, I was unclear about how the upsampling to HuBERT units was done. Also, see questions for the authors below.\n\nquestions_for_the_authors: 1. How do you get the corresponding Lex tokens for a corresponding 200ms window exactly? Do the sequence of lexical tokens and the sequence of acoustic tokens have the same lengths? \n2. Have you tried any other lengths rather than 200ms?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "KaRXerKTQV",
        "length": 397,
        "human_text": "paper_topic_and_main_contributions: The paper proposes a method to transform and concatenate frames extracted by wav2vec 2.0 into 200ms vector-based acoustic tokens. The vector-based acoustic tokens can be applied for training a 200ms-tGSLM speech based language model. Performance of this  200ms-tGSLM is comparable to conventional GSLM based on 240ms frames. The proposed method has the potential to improve speech LLM modelling, in terms of computation efficiency and representation ability.\n\nreasons_to_accept: The significant longer sequence length is a challenge of applying NLP sequence-to-sequence technique to speech. There are a lot of researches trying to shorten speech sequences for effective computation and better performance. This paper provides another perspective by proposing a method to generate longer fixed size acoustic tokens of 200ms which is much close to duration of sub-word units. The length the speech sequence is now comparable to the sequence with sub-word tokens, which has the potential for improving speech LLM training. While the choice of 200ms is somehow heuristic, the duration aligns well with average syllable length of most languages. The work includes experiments of different tasks and the results are encouraging.\n\nreasons_to_reject: The performance of 200ms-tGSLM is not necessary better than conventional 40ms based GSLM, except the MMOS for generated speech. However, I do not think that this is  a strong reason to reject this paper.\n\nquestions_for_the_authors: A. Section 3.2.2: For my understanding, the generated acoustic token vectors are converted into Hubert before applying to Tacotron2.0 vocoder. I understand that the authors try to reuse the available vocoder for comparison. However, directly generating time domain signals from the 200ms acoustic tokens with techniques such as GAN should be feasible, and may achieve better results. Have the authors tried the experiments to generate speech directly from the 200ms tokens?\nB.Section 3.1.2: The quantisation parameter $q$ is pre-computed before training. How to determine the PCA size and the K-mean cluster size? Is it possible to replace this pre-computed parameter with other quantisation such as VQ such that this bottle net layer can be trained end-to-end?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "QvPuua0GZC",
        "length": 1390,
        "human_text": "paper_topic_and_main_contributions: The paper treats of \"audio\" language modeling, that is language represented as audio spoken modality. This task is tackled by training a language model (LM) on speech data represented as discrete tokens. Such model work similarly to text LMs, with a fixed-size vocabulary, cross-entropy loss during training and multinomial sampling during inference, and can be used jointly with vocoder models to autoregressively generate speech and audio. \nIn previous works, the vocabulary is built from sets of learned speech sequence embeddings (SSE) of sequences of speech. These sequences of speech are often short, between 20 to 40ms, and may however not be representative of any relevant information (i.e. when shorter than a phoneme or syllable). An other drawback is the extensive token sequence length created by that such method, which is a known bottleneck of Transformers / attention mechanism. \nIn this paper, the authors propose to use tokens of longer speech sequences, which would reduce the token sequence length and in turn reduce the overall complexity / memory usage, while having more semantically rich audio embeddings. Doing so will however create a very large number of possible SSEs / tokens, making having a finite and well balanced vocabulary intractable (called clustering problem). The authors propose then to directly work in a continuous space, that is not using a finite vocabulary but directly feed the model with continuous vectors. The model cannot directly be fed the SEEs, as \"as two speech segments may sound the same, i.e. be close in the acoustic space\", so an additional function, learned with the model (\"lexical embedder\") mapping the SEEs to input embeddings (called here \"lexical tokens\"), is designed. And as there is no finite vocabulary, the model cannot be trained with the cross-entropy loss commonly used. The authors resort to use a contrastive learning loss. Another modification is made on the output of the model: the authors used three separate output heads (fully connecter layers) to predict the next three tokens. This modification allows to \"extend\" the scope of prediction of the model, which would be fairly limited by autoregressively sampling one token representing a piece of a word after another. \nCompared to the [GSLM](https://aclanthology.org/2021.tacl-1.79/) baseline, the method proposed by the authors allows to significantly reduce the memory usage (by reducing the input sequence lengths) while having results of better quality (measured by perplexity and zero-shot metrics, along with human preferences) and interpretable embeddings.\n\nreasons_to_accept: - The overall goal is very well motivated. Tackling audio modeling as a sequential problem is a very good direction offering a lot of flexibility in real-condition usages. I believe there is a lot of progress to be made, with significant impact, on the representation / tokenization of audio; - The implementation choices of tGSLM are well made and well justified (e.g. 200ms segmentation). Tackling this problem required a fair amount of model adaptations (LexEmb, using contrastive loss, FAISS for inference...) that the authors cleverly overcame; - The tasks and set of metrics are well chosen; - The interpretation of the \"lexical token\" embeddings is valuable and empirically shows that the method works; - The results show that LMs can generate speech using word-size frames directly fed as continuous embeddings; - The paper is globally well written and structured; - The references are very well documented and completed. Only one or two has missing URLs.\n\nreasons_to_reject: - Although the experimental setup is well made (tasks + metrics), I think the paper can really benefits from one or two more baselines, even though this kind of audio representation is fairly new and that you answer well to the main goal of the paper that you set (i.e. generate speech using word-size audio embeddings). The authors could add tokenizations from [AudioLM](https://arxiv.org/abs/2209.03143) or [BEATs](https://arxiv.org/abs/2212.09058). Eventually a baseline without $LexEmb$, that could highlight its necessity. I think the reader would benefit to see how tGSLM compares to these baselines.\n- There is no information about generation speed at inference. This information is IMO as well important as the memory usage. It as a decisive point when designing a system / product. Here, considering the additional stack (FAISS) compared to models using finite vocabularies, the reader could want to know id it is worth to spend the time needed to implement it.\n- **(no big deal)** The memory gain estimation in A.5 is valuable. But it would be great to also consider the FAISS memory consumption for inference, and give concrete examples with numbers, e.g. how $N$ pairs of SEE / lexical token translate into memory consumption. This $N$ could be a limitation depending on the hardware being used. Also as $N$ is correlated to the lengths of the speech sequences, it could raise questions when using such model in real conditions.\n\nquestions_for_the_authors: **A.** I think the \"lexical tokens\" and \"acoustic tokens\" namings are confusing at first read, and not really accurate. In the literature, a token is a discrete element, from a finite ensemble. I would recommend to rename them to something that might indicate that these elements are actually continuous / vectors from a continuous space. What is your opinion on this?\n**B.** Did you made some benchmarks on the generation speed? I think it could be express in tokens/sec and sec_of_audio/sec. As your model now has additional modules (namely LexEmb, multiple output heads, using FAISS search), I assume that the generation speed gain is not linear to the token sequence reduction; **C.** Maybe a suggestion for future research: what do you think about conditioning the output heads on the results of the previous one? By that I mean that $\\mathbf{y}\\_i = h(\\cdot)$ could be conditioned on $\\mathbf{y}\\_{i-1}$, e.g. $\\mathbf{y}\\_i = h(\\mathbf{x}, \\mathbf{y}\\_{i-1})$. Some models applied the same method of yours for symbolic music modeling ([Compound Word](https://ojs.aaai.org/index.php/AAAI/article/view/16091), [MusicBERT](https://aclanthology.org/2021.findings-acl.70/)). In practice, generating several distinct elements at the same time, unconditionally although they are meant to be complementary / tied, does not work very well for causal generation (as GANS / Diffusion / any non-autoregressive model). Thus conditioning each output on the very last predicted is almost always guaranteed to give more coherent results. [ Compound Word](https://ojs.aaai.org/index.php/AAAI/article/view/16091) does it partially.\n\nmissing_references: In your introduction, you mention \"tokenizer representing word or subword units\" (l60) without mentioning examples of such method. If think the comparison with your method is very relevant. It could then be nice (if you have enough space) to mention and cite the most popular methods currently used to build vocabularies for text, that the reader can relate to: - [BPE](https://www.derczynski.com/papers/archive/BPE_Gage.pdf) and [its application to natural language](https://aclanthology.org/P16-1162/) - [WordPiece](https://arxiv.org/abs/1609.08144v2) - [Unigram](https://aclanthology.org/P18-1007/) NCE loss introduction: [Noise-contrastive estimation: A new estimation principle for unnormalized statistical models](http://proceedings.mlr.press/v9/gutmann10a.html)\n\ntypos_grammar_style_and_presentation_improvements: You should consider converting your figures to vector format, this will provide the reader a better quality of visualization. Assuming Fig. 3 and 4 are generated with pyplot, you just have to save them as pdf.\nRename \"lexical tokens\" and \"acoustic tokens\" (see reasons to reject); Fig. 1: you could change the contour of the \"Lex Emb\" contours in order to better distinguish methods/functions from models, change \"Transformer\" to \"LM\" (more general), add the signification of \"FC\" somewhere in the main text; l244: \"there is often a linear FC layer before the transformer\" --> this is not really accurate, it isn't a fully-connected layer. You should rephrase with something like \"a learned embedding matrix acting as a lookup table\"; l255: \"measured\" --> did you mean studied ?\nl267: precise acoustic tokens; l322: add signification of NCE + cite [Gutmann et al](http://proceedings.mlr.press/v9/gutmann10a.html); l322 footnote (3): It would be good to include in the main text that L2 reconstruction is applicable, but did not get you good results in practice and include them in appendix; l356 / subsection 3.2.1: You could mention that you are using FAISS (I assumed it was when looking at fig 2 but had to look for appendix to find the answer); l1080 footnote: \"those\" --> \"these\" + missing dot; l1355: add the signification of \"BPE\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "184_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_184_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7233333333333333,
      "max_similarity": 0.7322,
      "avg_coverage": 0.35546666666666665,
      "max_coverage": 0.4231
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 507,
      "avg_human_length": 559.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 6,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "m0usOksTlX",
        "similarity": 0.7322,
        "coverage": 0.2683,
        "human_length": 452,
        "human_text": "paper_topic_and_main_contributions: 1. This paper proposes a framework called SUBSUMM for large-scale and multi-perspective opinion summarization. It introduces a review sampling strategy set based on sentiment analysis and contrastive information valuation to select high-quality review subsets from a large set of reviews. \n2. It proposes a two-stage training scheme that allows the model to learn from sub-optimal and optimal review subsets successively. The second stage uses contrastive learning to assign higher probability to better candidate summaries. \n3. Experiments on two datasets AmaSum and Rotten Tomatoes show SUBSUMM outperforms previous state-of-the-art models in generating pros, cons and verdict summaries from hundreds of reviews.\n\nreasons_to_accept: 1. It addresses an important problem of summarizing large volumes of opinion text from different perspectives, which is useful for many real-world applications like marketing analysis and decision making. \n2. The proposed framework SUBSUMM achieves new state-of-the-art results on two benchmark datasets, and the in-depth analysis offers insights into the contribution of different components of the framework.\n\nreasons_to_reject: 1. The problem setup itself is somewhat artificial - summarizing a very large set of reviews (average 76-74 reviews per product) when most real products would not have that many reviews. \n2. There is no component in the framework that specifically models aspect information which is crucial for opinion summarization. It is mainly driven by ROUGE scores. \n3. The two-stage training technique feels a bit ad-hoc. Ideally, the model should be able to learn from all data rather than just optimal subsets in later stages. Also, no mechanisms are introduced to reduce repetition and ensure diversity in the generated summaries.\n\nquestions_for_the_authors: 1. Have you tested SUBSUMM on any other datasets beyond online reviews to prove its generalization ability? \n2. The sampling strategies use sentiment analysis and ROUGE with reference summaries. How would the framework adapt to a purely unsupervised setting? \n3. What is the inference time of SUBSUMM on large review sets? Is it applicable for real-time usage? \n4. What were the main challenges faced in training the 2-stage model? Did you try any other scheduling or curriculum strategies?\n\nmissing_references: 1. Suyu Ge, Jiaxin Huang, Yu Meng, Jiawei Han. FineSum: Target-Oriented, Fine-Grained Opinion Summarization. WSDM 2023.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "shikfCDLTR",
        "similarity": 0.7144,
        "coverage": 0.4231,
        "human_length": 468,
        "human_text": "paper_topic_and_main_contributions: - This paper performs supervised opinion summarization with a long input setting. The study proposes a two-stage approach to perform the task. The experimental results show that the proposed method improves over the strong supervised opinion summarization baselines.\n\nreasons_to_accept: - It shows improvement compared to the strong supervised opinion summarization baselines.\n- It performs human evaluation using best-worst scaling (which is preferable) and demonstrates the effectiveness of the proposed system.\n\nreasons_to_reject: - The biggest issue with this study is the lack of comparison with long-document summarization studies, such as sparse attention-based methods and reduce-then-generate approaches.\n- I calculated the dataset statistics for your specific datasets and found that sparse attention-based encoder-decoder models, which often take 16k tokens as input, can handle all the reviews as input.\n- So, you should try the long-document summarization methods for a more comprehensive comparison.\n- I feel the BRIO and ChatGPT baselines are a bit unfair. Randomly sampled input is used for them. For training BRIO, I believe you should use an oracle set of reviews as input and perform a pipeline-based method for inference. For ChatGPT, you should use better retriever for the sampling stage.\n- As of 2023, I feel it is hardly acceptable to show a ROUGE score and claim that it is a superior model to ChatGPT. It is evident that ChatGPT is not aligned with the training set distribution, making it hard to achieve high ROUGE scores. However, with human evaluation, it can perform better. So, it feels a bit suspicious to claim that the proposed method is better than ChatGPT.\n\nmissing_references: - [1] [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) (Beltagy et al., arXiv) - [2] [Big Bird: Transformers for Longer Sequences](https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf) (Zaheer et al., NeurIPS 2020) - [3] [DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization](https://aclanthology.org/2022.acl-long.118) (Mao et al., ACL 2022) - [4] [Long Document Summarization with Top-down and Bottom-up Inference](https://aclanthology.org/2023.findings-eacl.94) (Pang et al., Findings 2023)\n\nethical_concerns: No\n\nquestions_for_the_authors: - I think this study is a really nice attempt. I'm really sad this is not an ARR submission. However, if you can address all the concerns raised in the reasons to reject during the rebuttal, I would consider raising the score.\n# after rebuttal Thanks for taking your time! I updated the score.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "GXVGbMtFfA",
        "similarity": 0.7234,
        "coverage": 0.375,
        "human_length": 759,
        "human_text": "paper_topic_and_main_contributions: In the realm of opinion summarization, one of the primary challenges lies in creating comprehensive summaries that encompass diverse perspectives. The task becomes even more complex when dealing with a large number of reviews. However, this paper introduces a novel approach to address this issue by leveraging sentiment analysis and contrasting information from various reviews to identify the most suitable ones for each type of summary. The proposed model undergoes two stages of training. In the first stage, it is trained using Maximum Likelihood Estimation (MLE) to generate the initial summaries. Subsequently, in the second stage, the model is further refined by utilizing its capability to produce reasonable summaries, and it is trained on its own generated outputs. This innovative method shows promise in enhancing opinion summarization and streamlining the selection of reviews for different perspectives, ultimately leading to more effective and insightful summaries.\n\nreasons_to_accept: - ***Refreshing idea*** This idea presents a refreshing and novel approach by combining various techniques to achieve opinion summarization.\n- ***Good empirical analysis***          The paper conducts a robust empirical analysis, considering a substantial number of baselines and recent advances for comparison. Furthermore, the authors conduct thorough ablation studies, effectively justifying their choice for the final system design.\n\nreasons_to_reject: - Mismatch between motivation and experiments - One notable aspect where the paper falls short is the absence of comparisons with models capable of handling long context, such as the Longformer or any other recently released model with extended context length. Including such comparisons would have enriched the analysis and strengthened the overall contribution of the study.\n- Experiemnts with different values of K and N - The paper lacks experiments for different values of K, which is a significant limitation. By exploring various values of K, the authors could have gained valuable insights into the performance and behavior of their method under different settings. Additionally, it is essential to assess how well the proposed method performs when N (the number of samples or data points) is higher compared to other existing methods, as this was a primary motivation behind the research. By conducting experiments with varying N, the authors could have demonstrated the scalability and effectiveness of their approach and provided a more comprehensive evaluation of its performance in real-world scenarios. This absence of experimentation limits the paper's ability to fully support its main claims and conclusions. Including experiments with different values of K and diverse N settings would significantly enhance the paper's scientific rigor and strengthen its contribution to the field.\n\nquestions_for_the_authors: - Clarification on K Value in Experiments:  The paper would benefit from explicitly stating the value of K used in the experiments within the main body of the text. If this information is currently relegated to the appendix, it should be brought forward to the main part of the paper for better accessibility and understanding.\n- The paper lacks clarity on how different subsets of reviews are chosen from Rotten Tomatoes data. It's also unclear which category the summary belongs to\u2014pros, cons, or verdict. These aspects require further explanation for better understanding.\n- Experiments involving retrieve and generate models, while not strictly mandatory, have become increasingly pertinent in the current age. The reviewer is understandably curious about your paper's positioning within the framework of \"Retrieve relevant context and generate.\" To address this curiosity, it would be valuable if you could furnish experiments demonstrating the application of your retrieval strategy on a Language Model (LM) like LLaMa and observe the resulting outcomes. Such experiments would serve to validate the relevance and effectiveness of your retrieval strategies independently of the specific language model employed\n\nmissing_references: Line 130 - \u201cThese methods work well\u2026\u201d - Is this an opinion of the paper. Can you add a previous work that shows that the previous works dont work when the number of reviews increase\n\ntypos_grammar_style_and_presentation_improvements: The first line in the introduction is too cumbersome to read. I think you can remove the heavy sounding words and simplify it.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "m0usOksTlX",
        "length": 452,
        "human_text": "paper_topic_and_main_contributions: 1. This paper proposes a framework called SUBSUMM for large-scale and multi-perspective opinion summarization. It introduces a review sampling strategy set based on sentiment analysis and contrastive information valuation to select high-quality review subsets from a large set of reviews. \n2. It proposes a two-stage training scheme that allows the model to learn from sub-optimal and optimal review subsets successively. The second stage uses contrastive learning to assign higher probability to better candidate summaries. \n3. Experiments on two datasets AmaSum and Rotten Tomatoes show SUBSUMM outperforms previous state-of-the-art models in generating pros, cons and verdict summaries from hundreds of reviews.\n\nreasons_to_accept: 1. It addresses an important problem of summarizing large volumes of opinion text from different perspectives, which is useful for many real-world applications like marketing analysis and decision making. \n2. The proposed framework SUBSUMM achieves new state-of-the-art results on two benchmark datasets, and the in-depth analysis offers insights into the contribution of different components of the framework.\n\nreasons_to_reject: 1. The problem setup itself is somewhat artificial - summarizing a very large set of reviews (average 76-74 reviews per product) when most real products would not have that many reviews. \n2. There is no component in the framework that specifically models aspect information which is crucial for opinion summarization. It is mainly driven by ROUGE scores. \n3. The two-stage training technique feels a bit ad-hoc. Ideally, the model should be able to learn from all data rather than just optimal subsets in later stages. Also, no mechanisms are introduced to reduce repetition and ensure diversity in the generated summaries.\n\nquestions_for_the_authors: 1. Have you tested SUBSUMM on any other datasets beyond online reviews to prove its generalization ability? \n2. The sampling strategies use sentiment analysis and ROUGE with reference summaries. How would the framework adapt to a purely unsupervised setting? \n3. What is the inference time of SUBSUMM on large review sets? Is it applicable for real-time usage? \n4. What were the main challenges faced in training the 2-stage model? Did you try any other scheduling or curriculum strategies?\n\nmissing_references: 1. Suyu Ge, Jiaxin Huang, Yu Meng, Jiawei Han. FineSum: Target-Oriented, Fine-Grained Opinion Summarization. WSDM 2023.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "shikfCDLTR",
        "length": 468,
        "human_text": "paper_topic_and_main_contributions: - This paper performs supervised opinion summarization with a long input setting. The study proposes a two-stage approach to perform the task. The experimental results show that the proposed method improves over the strong supervised opinion summarization baselines.\n\nreasons_to_accept: - It shows improvement compared to the strong supervised opinion summarization baselines.\n- It performs human evaluation using best-worst scaling (which is preferable) and demonstrates the effectiveness of the proposed system.\n\nreasons_to_reject: - The biggest issue with this study is the lack of comparison with long-document summarization studies, such as sparse attention-based methods and reduce-then-generate approaches.\n- I calculated the dataset statistics for your specific datasets and found that sparse attention-based encoder-decoder models, which often take 16k tokens as input, can handle all the reviews as input.\n- So, you should try the long-document summarization methods for a more comprehensive comparison.\n- I feel the BRIO and ChatGPT baselines are a bit unfair. Randomly sampled input is used for them. For training BRIO, I believe you should use an oracle set of reviews as input and perform a pipeline-based method for inference. For ChatGPT, you should use better retriever for the sampling stage.\n- As of 2023, I feel it is hardly acceptable to show a ROUGE score and claim that it is a superior model to ChatGPT. It is evident that ChatGPT is not aligned with the training set distribution, making it hard to achieve high ROUGE scores. However, with human evaluation, it can perform better. So, it feels a bit suspicious to claim that the proposed method is better than ChatGPT.\n\nmissing_references: - [1] [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) (Beltagy et al., arXiv) - [2] [Big Bird: Transformers for Longer Sequences](https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf) (Zaheer et al., NeurIPS 2020) - [3] [DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization](https://aclanthology.org/2022.acl-long.118) (Mao et al., ACL 2022) - [4] [Long Document Summarization with Top-down and Bottom-up Inference](https://aclanthology.org/2023.findings-eacl.94) (Pang et al., Findings 2023)\n\nethical_concerns: No\n\nquestions_for_the_authors: - I think this study is a really nice attempt. I'm really sad this is not an ARR submission. However, if you can address all the concerns raised in the reasons to reject during the rebuttal, I would consider raising the score.\n# after rebuttal Thanks for taking your time! I updated the score.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "GXVGbMtFfA",
        "length": 759,
        "human_text": "paper_topic_and_main_contributions: In the realm of opinion summarization, one of the primary challenges lies in creating comprehensive summaries that encompass diverse perspectives. The task becomes even more complex when dealing with a large number of reviews. However, this paper introduces a novel approach to address this issue by leveraging sentiment analysis and contrasting information from various reviews to identify the most suitable ones for each type of summary. The proposed model undergoes two stages of training. In the first stage, it is trained using Maximum Likelihood Estimation (MLE) to generate the initial summaries. Subsequently, in the second stage, the model is further refined by utilizing its capability to produce reasonable summaries, and it is trained on its own generated outputs. This innovative method shows promise in enhancing opinion summarization and streamlining the selection of reviews for different perspectives, ultimately leading to more effective and insightful summaries.\n\nreasons_to_accept: - ***Refreshing idea*** This idea presents a refreshing and novel approach by combining various techniques to achieve opinion summarization.\n- ***Good empirical analysis***          The paper conducts a robust empirical analysis, considering a substantial number of baselines and recent advances for comparison. Furthermore, the authors conduct thorough ablation studies, effectively justifying their choice for the final system design.\n\nreasons_to_reject: - Mismatch between motivation and experiments - One notable aspect where the paper falls short is the absence of comparisons with models capable of handling long context, such as the Longformer or any other recently released model with extended context length. Including such comparisons would have enriched the analysis and strengthened the overall contribution of the study.\n- Experiemnts with different values of K and N - The paper lacks experiments for different values of K, which is a significant limitation. By exploring various values of K, the authors could have gained valuable insights into the performance and behavior of their method under different settings. Additionally, it is essential to assess how well the proposed method performs when N (the number of samples or data points) is higher compared to other existing methods, as this was a primary motivation behind the research. By conducting experiments with varying N, the authors could have demonstrated the scalability and effectiveness of their approach and provided a more comprehensive evaluation of its performance in real-world scenarios. This absence of experimentation limits the paper's ability to fully support its main claims and conclusions. Including experiments with different values of K and diverse N settings would significantly enhance the paper's scientific rigor and strengthen its contribution to the field.\n\nquestions_for_the_authors: - Clarification on K Value in Experiments:  The paper would benefit from explicitly stating the value of K used in the experiments within the main body of the text. If this information is currently relegated to the appendix, it should be brought forward to the main part of the paper for better accessibility and understanding.\n- The paper lacks clarity on how different subsets of reviews are chosen from Rotten Tomatoes data. It's also unclear which category the summary belongs to\u2014pros, cons, or verdict. These aspects require further explanation for better understanding.\n- Experiments involving retrieve and generate models, while not strictly mandatory, have become increasingly pertinent in the current age. The reviewer is understandably curious about your paper's positioning within the framework of \"Retrieve relevant context and generate.\" To address this curiosity, it would be valuable if you could furnish experiments demonstrating the application of your retrieval strategy on a Language Model (LM) like LLaMa and observe the resulting outcomes. Such experiments would serve to validate the relevance and effectiveness of your retrieval strategies independently of the specific language model employed\n\nmissing_references: Line 130 - \u201cThese methods work well\u2026\u201d - Is this an opinion of the paper. Can you add a previous work that shows that the previous works dont work when the number of reviews increase\n\ntypos_grammar_style_and_presentation_improvements: The first line in the introduction is too cumbersome to read. I think you can remove the heavy sounding words and simplify it.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "184_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_184_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7282333333333334,
      "max_similarity": 0.7344,
      "avg_coverage": 0.2975333333333333,
      "max_coverage": 0.4231
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 510,
      "avg_human_length": 559.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 6,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "m0usOksTlX",
        "similarity": 0.7344,
        "coverage": 0.2195,
        "human_length": 452,
        "human_text": "paper_topic_and_main_contributions: 1. This paper proposes a framework called SUBSUMM for large-scale and multi-perspective opinion summarization. It introduces a review sampling strategy set based on sentiment analysis and contrastive information valuation to select high-quality review subsets from a large set of reviews. \n2. It proposes a two-stage training scheme that allows the model to learn from sub-optimal and optimal review subsets successively. The second stage uses contrastive learning to assign higher probability to better candidate summaries. \n3. Experiments on two datasets AmaSum and Rotten Tomatoes show SUBSUMM outperforms previous state-of-the-art models in generating pros, cons and verdict summaries from hundreds of reviews.\n\nreasons_to_accept: 1. It addresses an important problem of summarizing large volumes of opinion text from different perspectives, which is useful for many real-world applications like marketing analysis and decision making. \n2. The proposed framework SUBSUMM achieves new state-of-the-art results on two benchmark datasets, and the in-depth analysis offers insights into the contribution of different components of the framework.\n\nreasons_to_reject: 1. The problem setup itself is somewhat artificial - summarizing a very large set of reviews (average 76-74 reviews per product) when most real products would not have that many reviews. \n2. There is no component in the framework that specifically models aspect information which is crucial for opinion summarization. It is mainly driven by ROUGE scores. \n3. The two-stage training technique feels a bit ad-hoc. Ideally, the model should be able to learn from all data rather than just optimal subsets in later stages. Also, no mechanisms are introduced to reduce repetition and ensure diversity in the generated summaries.\n\nquestions_for_the_authors: 1. Have you tested SUBSUMM on any other datasets beyond online reviews to prove its generalization ability? \n2. The sampling strategies use sentiment analysis and ROUGE with reference summaries. How would the framework adapt to a purely unsupervised setting? \n3. What is the inference time of SUBSUMM on large review sets? Is it applicable for real-time usage? \n4. What were the main challenges faced in training the 2-stage model? Did you try any other scheduling or curriculum strategies?\n\nmissing_references: 1. Suyu Ge, Jiaxin Huang, Yu Meng, Jiawei Han. FineSum: Target-Oriented, Fine-Grained Opinion Summarization. WSDM 2023.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "shikfCDLTR",
        "similarity": 0.7217,
        "coverage": 0.4231,
        "human_length": 468,
        "human_text": "paper_topic_and_main_contributions: - This paper performs supervised opinion summarization with a long input setting. The study proposes a two-stage approach to perform the task. The experimental results show that the proposed method improves over the strong supervised opinion summarization baselines.\n\nreasons_to_accept: - It shows improvement compared to the strong supervised opinion summarization baselines.\n- It performs human evaluation using best-worst scaling (which is preferable) and demonstrates the effectiveness of the proposed system.\n\nreasons_to_reject: - The biggest issue with this study is the lack of comparison with long-document summarization studies, such as sparse attention-based methods and reduce-then-generate approaches.\n- I calculated the dataset statistics for your specific datasets and found that sparse attention-based encoder-decoder models, which often take 16k tokens as input, can handle all the reviews as input.\n- So, you should try the long-document summarization methods for a more comprehensive comparison.\n- I feel the BRIO and ChatGPT baselines are a bit unfair. Randomly sampled input is used for them. For training BRIO, I believe you should use an oracle set of reviews as input and perform a pipeline-based method for inference. For ChatGPT, you should use better retriever for the sampling stage.\n- As of 2023, I feel it is hardly acceptable to show a ROUGE score and claim that it is a superior model to ChatGPT. It is evident that ChatGPT is not aligned with the training set distribution, making it hard to achieve high ROUGE scores. However, with human evaluation, it can perform better. So, it feels a bit suspicious to claim that the proposed method is better than ChatGPT.\n\nmissing_references: - [1] [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) (Beltagy et al., arXiv) - [2] [Big Bird: Transformers for Longer Sequences](https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf) (Zaheer et al., NeurIPS 2020) - [3] [DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization](https://aclanthology.org/2022.acl-long.118) (Mao et al., ACL 2022) - [4] [Long Document Summarization with Top-down and Bottom-up Inference](https://aclanthology.org/2023.findings-eacl.94) (Pang et al., Findings 2023)\n\nethical_concerns: No\n\nquestions_for_the_authors: - I think this study is a really nice attempt. I'm really sad this is not an ARR submission. However, if you can address all the concerns raised in the reasons to reject during the rebuttal, I would consider raising the score.\n# after rebuttal Thanks for taking your time! I updated the score.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "GXVGbMtFfA",
        "similarity": 0.7286,
        "coverage": 0.25,
        "human_length": 759,
        "human_text": "paper_topic_and_main_contributions: In the realm of opinion summarization, one of the primary challenges lies in creating comprehensive summaries that encompass diverse perspectives. The task becomes even more complex when dealing with a large number of reviews. However, this paper introduces a novel approach to address this issue by leveraging sentiment analysis and contrasting information from various reviews to identify the most suitable ones for each type of summary. The proposed model undergoes two stages of training. In the first stage, it is trained using Maximum Likelihood Estimation (MLE) to generate the initial summaries. Subsequently, in the second stage, the model is further refined by utilizing its capability to produce reasonable summaries, and it is trained on its own generated outputs. This innovative method shows promise in enhancing opinion summarization and streamlining the selection of reviews for different perspectives, ultimately leading to more effective and insightful summaries.\n\nreasons_to_accept: - ***Refreshing idea*** This idea presents a refreshing and novel approach by combining various techniques to achieve opinion summarization.\n- ***Good empirical analysis***          The paper conducts a robust empirical analysis, considering a substantial number of baselines and recent advances for comparison. Furthermore, the authors conduct thorough ablation studies, effectively justifying their choice for the final system design.\n\nreasons_to_reject: - Mismatch between motivation and experiments - One notable aspect where the paper falls short is the absence of comparisons with models capable of handling long context, such as the Longformer or any other recently released model with extended context length. Including such comparisons would have enriched the analysis and strengthened the overall contribution of the study.\n- Experiemnts with different values of K and N - The paper lacks experiments for different values of K, which is a significant limitation. By exploring various values of K, the authors could have gained valuable insights into the performance and behavior of their method under different settings. Additionally, it is essential to assess how well the proposed method performs when N (the number of samples or data points) is higher compared to other existing methods, as this was a primary motivation behind the research. By conducting experiments with varying N, the authors could have demonstrated the scalability and effectiveness of their approach and provided a more comprehensive evaluation of its performance in real-world scenarios. This absence of experimentation limits the paper's ability to fully support its main claims and conclusions. Including experiments with different values of K and diverse N settings would significantly enhance the paper's scientific rigor and strengthen its contribution to the field.\n\nquestions_for_the_authors: - Clarification on K Value in Experiments:  The paper would benefit from explicitly stating the value of K used in the experiments within the main body of the text. If this information is currently relegated to the appendix, it should be brought forward to the main part of the paper for better accessibility and understanding.\n- The paper lacks clarity on how different subsets of reviews are chosen from Rotten Tomatoes data. It's also unclear which category the summary belongs to\u2014pros, cons, or verdict. These aspects require further explanation for better understanding.\n- Experiments involving retrieve and generate models, while not strictly mandatory, have become increasingly pertinent in the current age. The reviewer is understandably curious about your paper's positioning within the framework of \"Retrieve relevant context and generate.\" To address this curiosity, it would be valuable if you could furnish experiments demonstrating the application of your retrieval strategy on a Language Model (LM) like LLaMa and observe the resulting outcomes. Such experiments would serve to validate the relevance and effectiveness of your retrieval strategies independently of the specific language model employed\n\nmissing_references: Line 130 - \u201cThese methods work well\u2026\u201d - Is this an opinion of the paper. Can you add a previous work that shows that the previous works dont work when the number of reviews increase\n\ntypos_grammar_style_and_presentation_improvements: The first line in the introduction is too cumbersome to read. I think you can remove the heavy sounding words and simplify it.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "m0usOksTlX",
        "length": 452,
        "human_text": "paper_topic_and_main_contributions: 1. This paper proposes a framework called SUBSUMM for large-scale and multi-perspective opinion summarization. It introduces a review sampling strategy set based on sentiment analysis and contrastive information valuation to select high-quality review subsets from a large set of reviews. \n2. It proposes a two-stage training scheme that allows the model to learn from sub-optimal and optimal review subsets successively. The second stage uses contrastive learning to assign higher probability to better candidate summaries. \n3. Experiments on two datasets AmaSum and Rotten Tomatoes show SUBSUMM outperforms previous state-of-the-art models in generating pros, cons and verdict summaries from hundreds of reviews.\n\nreasons_to_accept: 1. It addresses an important problem of summarizing large volumes of opinion text from different perspectives, which is useful for many real-world applications like marketing analysis and decision making. \n2. The proposed framework SUBSUMM achieves new state-of-the-art results on two benchmark datasets, and the in-depth analysis offers insights into the contribution of different components of the framework.\n\nreasons_to_reject: 1. The problem setup itself is somewhat artificial - summarizing a very large set of reviews (average 76-74 reviews per product) when most real products would not have that many reviews. \n2. There is no component in the framework that specifically models aspect information which is crucial for opinion summarization. It is mainly driven by ROUGE scores. \n3. The two-stage training technique feels a bit ad-hoc. Ideally, the model should be able to learn from all data rather than just optimal subsets in later stages. Also, no mechanisms are introduced to reduce repetition and ensure diversity in the generated summaries.\n\nquestions_for_the_authors: 1. Have you tested SUBSUMM on any other datasets beyond online reviews to prove its generalization ability? \n2. The sampling strategies use sentiment analysis and ROUGE with reference summaries. How would the framework adapt to a purely unsupervised setting? \n3. What is the inference time of SUBSUMM on large review sets? Is it applicable for real-time usage? \n4. What were the main challenges faced in training the 2-stage model? Did you try any other scheduling or curriculum strategies?\n\nmissing_references: 1. Suyu Ge, Jiaxin Huang, Yu Meng, Jiawei Han. FineSum: Target-Oriented, Fine-Grained Opinion Summarization. WSDM 2023.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "shikfCDLTR",
        "length": 468,
        "human_text": "paper_topic_and_main_contributions: - This paper performs supervised opinion summarization with a long input setting. The study proposes a two-stage approach to perform the task. The experimental results show that the proposed method improves over the strong supervised opinion summarization baselines.\n\nreasons_to_accept: - It shows improvement compared to the strong supervised opinion summarization baselines.\n- It performs human evaluation using best-worst scaling (which is preferable) and demonstrates the effectiveness of the proposed system.\n\nreasons_to_reject: - The biggest issue with this study is the lack of comparison with long-document summarization studies, such as sparse attention-based methods and reduce-then-generate approaches.\n- I calculated the dataset statistics for your specific datasets and found that sparse attention-based encoder-decoder models, which often take 16k tokens as input, can handle all the reviews as input.\n- So, you should try the long-document summarization methods for a more comprehensive comparison.\n- I feel the BRIO and ChatGPT baselines are a bit unfair. Randomly sampled input is used for them. For training BRIO, I believe you should use an oracle set of reviews as input and perform a pipeline-based method for inference. For ChatGPT, you should use better retriever for the sampling stage.\n- As of 2023, I feel it is hardly acceptable to show a ROUGE score and claim that it is a superior model to ChatGPT. It is evident that ChatGPT is not aligned with the training set distribution, making it hard to achieve high ROUGE scores. However, with human evaluation, it can perform better. So, it feels a bit suspicious to claim that the proposed method is better than ChatGPT.\n\nmissing_references: - [1] [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) (Beltagy et al., arXiv) - [2] [Big Bird: Transformers for Longer Sequences](https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf) (Zaheer et al., NeurIPS 2020) - [3] [DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization](https://aclanthology.org/2022.acl-long.118) (Mao et al., ACL 2022) - [4] [Long Document Summarization with Top-down and Bottom-up Inference](https://aclanthology.org/2023.findings-eacl.94) (Pang et al., Findings 2023)\n\nethical_concerns: No\n\nquestions_for_the_authors: - I think this study is a really nice attempt. I'm really sad this is not an ARR submission. However, if you can address all the concerns raised in the reasons to reject during the rebuttal, I would consider raising the score.\n# after rebuttal Thanks for taking your time! I updated the score.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "GXVGbMtFfA",
        "length": 759,
        "human_text": "paper_topic_and_main_contributions: In the realm of opinion summarization, one of the primary challenges lies in creating comprehensive summaries that encompass diverse perspectives. The task becomes even more complex when dealing with a large number of reviews. However, this paper introduces a novel approach to address this issue by leveraging sentiment analysis and contrasting information from various reviews to identify the most suitable ones for each type of summary. The proposed model undergoes two stages of training. In the first stage, it is trained using Maximum Likelihood Estimation (MLE) to generate the initial summaries. Subsequently, in the second stage, the model is further refined by utilizing its capability to produce reasonable summaries, and it is trained on its own generated outputs. This innovative method shows promise in enhancing opinion summarization and streamlining the selection of reviews for different perspectives, ultimately leading to more effective and insightful summaries.\n\nreasons_to_accept: - ***Refreshing idea*** This idea presents a refreshing and novel approach by combining various techniques to achieve opinion summarization.\n- ***Good empirical analysis***          The paper conducts a robust empirical analysis, considering a substantial number of baselines and recent advances for comparison. Furthermore, the authors conduct thorough ablation studies, effectively justifying their choice for the final system design.\n\nreasons_to_reject: - Mismatch between motivation and experiments - One notable aspect where the paper falls short is the absence of comparisons with models capable of handling long context, such as the Longformer or any other recently released model with extended context length. Including such comparisons would have enriched the analysis and strengthened the overall contribution of the study.\n- Experiemnts with different values of K and N - The paper lacks experiments for different values of K, which is a significant limitation. By exploring various values of K, the authors could have gained valuable insights into the performance and behavior of their method under different settings. Additionally, it is essential to assess how well the proposed method performs when N (the number of samples or data points) is higher compared to other existing methods, as this was a primary motivation behind the research. By conducting experiments with varying N, the authors could have demonstrated the scalability and effectiveness of their approach and provided a more comprehensive evaluation of its performance in real-world scenarios. This absence of experimentation limits the paper's ability to fully support its main claims and conclusions. Including experiments with different values of K and diverse N settings would significantly enhance the paper's scientific rigor and strengthen its contribution to the field.\n\nquestions_for_the_authors: - Clarification on K Value in Experiments:  The paper would benefit from explicitly stating the value of K used in the experiments within the main body of the text. If this information is currently relegated to the appendix, it should be brought forward to the main part of the paper for better accessibility and understanding.\n- The paper lacks clarity on how different subsets of reviews are chosen from Rotten Tomatoes data. It's also unclear which category the summary belongs to\u2014pros, cons, or verdict. These aspects require further explanation for better understanding.\n- Experiments involving retrieve and generate models, while not strictly mandatory, have become increasingly pertinent in the current age. The reviewer is understandably curious about your paper's positioning within the framework of \"Retrieve relevant context and generate.\" To address this curiosity, it would be valuable if you could furnish experiments demonstrating the application of your retrieval strategy on a Language Model (LM) like LLaMa and observe the resulting outcomes. Such experiments would serve to validate the relevance and effectiveness of your retrieval strategies independently of the specific language model employed\n\nmissing_references: Line 130 - \u201cThese methods work well\u2026\u201d - Is this an opinion of the paper. Can you add a previous work that shows that the previous works dont work when the number of reviews increase\n\ntypos_grammar_style_and_presentation_improvements: The first line in the introduction is too cumbersome to read. I think you can remove the heavy sounding words and simplify it.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "94_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_94_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7206,
      "max_similarity": 0.7491,
      "avg_coverage": 0.4705,
      "max_coverage": 0.5217
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 470,
      "avg_human_length": 389.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 6,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "fPkXoEJGeO",
        "similarity": 0.6944,
        "coverage": 0.4667,
        "human_length": 415,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a framework for leveraging pure textual data as a way to improve the modeling capability of speech tasks that originally required paired audio-text pairs. This makes it possible to grow the representation capability and accuracy of models by leveraging linguistic information proyected into the acoustic latents. This, the authors hypothesize that promotes cross-modal (linguistic->acoustic) knowledge transfer. Results are demonstrated on both speech recognition and spoken natural language understanding tasks, improving over baselines and meeting state of the art performance on smaller models and (paired) datasets.\n\nreasons_to_accept: This paper proposes an interesting formulation to include unpaired data to speech models. While there has been an abundant recent amount of work to incorporate unscripted audio (audio without text pairs) there has been limited recent work on the 'other side of the coin'. This has been mostly carried out by leveraging pre-trained Language Models, so in this paper the authors propose a lighter approach to use linguistic information in a cross-modal way. This is demonstrated to have a positive impact across multiple downstream tasks, which reinforces the value of linguistic information in multiple speech-related tasks beyond the conventional domains.\n\nreasons_to_reject: Not a strong reason to reject, but I am left wondering if the impact of the proposed approach happens because the latent acoustic representation trained with the datasets is incomplete (in terms, for example, of linguistic coverage). As there is an abundance of speech-text paired datasets in the public domain, it could be that the benefits of the approach are restricted to lower-resource languages in which the technique was untested, leading to a lower impact of the proposed knowledge.\n\nmissing_references: I believe the paper would benefit from covering the problem also from the point of view of acoustic->linguistic cross-modal training, such as https://arxiv.org/pdf/2302.03540.pdf. Even if the focus is not on how to properly develop and train speech latent encoders, there is potential interaction and extended impact in covering both literatures.\nI also miss references to the recent trends such as SpeechGPT models that are potential competitors (or beneficiaries?) of this approach https://arxiv.org/pdf/2305.11000.pdf\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "KHWRlK3vI0",
        "similarity": 0.7183,
        "coverage": 0.5217,
        "human_length": 431,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel data augmentation framework aimed at enhancing low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) by leveraging unpaired text data. The authors introduce latent synthesizers, which generate continuous latent features from text. These latents correspond to the output features of the speech encoder, enabling the latent synthesizer to produce simplified representations compared to traditional mel-spectrogram and waveform approaches. The study investigates fixed-projection and diffusion-based models as potential latent synthesizers. Experimental evaluations demonstrate that the proposed augmentation technique leads to substantial improvements in both ASR and SLU quality.\n\nreasons_to_accept: The proposed method is novel to some extent. If the authors conducted more experiments in the speech translation task, the claims would be stronger.\n\nreasons_to_reject: \u30fbA comparison with data augmentation methods utilizing discrete acoustic units like SpeechUT is absent. These discrete units simplify speech by emphasizing phonetic and semantic information. An alternative to latent synthesizers involves training Transformer models to generate these discrete units from text [Popuri+ 2022]. While this approach necessitates an additional module to map discrete units to the continuous space, prior research has demonstrated quality enhancements through this technique.\n\u30fbComparisons with TTS augmentation are unfair due to the encoder architectures (Transformer vs Conformer). Consequently, the results don't definitively indicate the superior data augmentation method.\n\u30fbThe investigation into the efficacy of external language model fusion is lacking. Since the text data employed in data augmentation originates from the same source as LM training data (specifically Librispeech 960h in this study), we seek clarification on two aspects: 1) the relative effectiveness of the proposed method compared to shallow fusion, and 2) whether the proposed method complements shallow fusion.\n\u30fbThe results in Section 4.2.1 highlight the dependence of data augmentation effectiveness on the domains of training data used for latent synthesizers. This restriction confines the applicability of the proposed method. To investigate domain transferability, employing an ASR model trained on an out-of-domain dataset for initializing the Guiding Net would be desirable.\n\nmissing_references: \u30fbPopuri et al., Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation, INTERSPEECH, 2022.\n\u30fbSiuzdak et al., WavThruVec: Latent speech representation as intermediate features for neural speech synthesis, INTERSPEECH, 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "NCkwmCINrz",
        "similarity": 0.7491,
        "coverage": 0.4231,
        "human_length": 321,
        "human_text": "paper_topic_and_main_contributions: The researchers introduce a framework called LaSyn, designed to efficiently utilize textual data for end-to-end (E2E) speech processing models. This framework facilitates the transfer of cross-modal knowledge from text to E2E speech processing models through latent synthesis. Two implementations of the latent synthesizer are designed as the core of LaSyn: a fixed-projection latent synthesizer and a diffusion latent synthesizer, which incorporates recent advancements in diffusion models. By leveraging LaSyn to enhance E2E speech models with textual data, the researchers achieve competitive results multiple ASR/SLU tasks.\n\nreasons_to_accept: 1. Two novel designs of latent synthesizer are tailored to the end goal of text-to-speech representation simulation. \n2. The effectiveness of the approach is empirically proven by promising results on multiple tasks/datasets. \n3. Good paper writing and clear presentation.\n\nreasons_to_reject: 1. The ASR experiments are limited. Librispeech is a widely used dataset, but there are various more competitive models (e.g. Wave2Vec2, HuBERT etc) than the baselines in the paper. Moreover, since Librispeech is clean read speech data and not a very challenging dataset, experimenting on it alone may not reflect the ASR performance comprehensively. \n2. When targeting low-resource languages/domains, the availability of G2P are also be limited (section 3.2.2).\n\nmissing_references: Relevant papers on improving/adapting latent speech representation with text only data.\n1. Text-Only Domain Adaptation Based on Intermediate CTC. https://www.isca-speech.org/archive/pdfs/interspeech_2022/sato22_interspeech.pdf 2. Integrating Text Inputs For Training and Adapting RNN Transducer ASR Models. https://arxiv.org/pdf/2202.13155.pdf\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "fPkXoEJGeO",
        "length": 415,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a framework for leveraging pure textual data as a way to improve the modeling capability of speech tasks that originally required paired audio-text pairs. This makes it possible to grow the representation capability and accuracy of models by leveraging linguistic information proyected into the acoustic latents. This, the authors hypothesize that promotes cross-modal (linguistic->acoustic) knowledge transfer. Results are demonstrated on both speech recognition and spoken natural language understanding tasks, improving over baselines and meeting state of the art performance on smaller models and (paired) datasets.\n\nreasons_to_accept: This paper proposes an interesting formulation to include unpaired data to speech models. While there has been an abundant recent amount of work to incorporate unscripted audio (audio without text pairs) there has been limited recent work on the 'other side of the coin'. This has been mostly carried out by leveraging pre-trained Language Models, so in this paper the authors propose a lighter approach to use linguistic information in a cross-modal way. This is demonstrated to have a positive impact across multiple downstream tasks, which reinforces the value of linguistic information in multiple speech-related tasks beyond the conventional domains.\n\nreasons_to_reject: Not a strong reason to reject, but I am left wondering if the impact of the proposed approach happens because the latent acoustic representation trained with the datasets is incomplete (in terms, for example, of linguistic coverage). As there is an abundance of speech-text paired datasets in the public domain, it could be that the benefits of the approach are restricted to lower-resource languages in which the technique was untested, leading to a lower impact of the proposed knowledge.\n\nmissing_references: I believe the paper would benefit from covering the problem also from the point of view of acoustic->linguistic cross-modal training, such as https://arxiv.org/pdf/2302.03540.pdf. Even if the focus is not on how to properly develop and train speech latent encoders, there is potential interaction and extended impact in covering both literatures.\nI also miss references to the recent trends such as SpeechGPT models that are potential competitors (or beneficiaries?) of this approach https://arxiv.org/pdf/2305.11000.pdf\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "KHWRlK3vI0",
        "length": 431,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel data augmentation framework aimed at enhancing low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) by leveraging unpaired text data. The authors introduce latent synthesizers, which generate continuous latent features from text. These latents correspond to the output features of the speech encoder, enabling the latent synthesizer to produce simplified representations compared to traditional mel-spectrogram and waveform approaches. The study investigates fixed-projection and diffusion-based models as potential latent synthesizers. Experimental evaluations demonstrate that the proposed augmentation technique leads to substantial improvements in both ASR and SLU quality.\n\nreasons_to_accept: The proposed method is novel to some extent. If the authors conducted more experiments in the speech translation task, the claims would be stronger.\n\nreasons_to_reject: \u30fbA comparison with data augmentation methods utilizing discrete acoustic units like SpeechUT is absent. These discrete units simplify speech by emphasizing phonetic and semantic information. An alternative to latent synthesizers involves training Transformer models to generate these discrete units from text [Popuri+ 2022]. While this approach necessitates an additional module to map discrete units to the continuous space, prior research has demonstrated quality enhancements through this technique.\n\u30fbComparisons with TTS augmentation are unfair due to the encoder architectures (Transformer vs Conformer). Consequently, the results don't definitively indicate the superior data augmentation method.\n\u30fbThe investigation into the efficacy of external language model fusion is lacking. Since the text data employed in data augmentation originates from the same source as LM training data (specifically Librispeech 960h in this study), we seek clarification on two aspects: 1) the relative effectiveness of the proposed method compared to shallow fusion, and 2) whether the proposed method complements shallow fusion.\n\u30fbThe results in Section 4.2.1 highlight the dependence of data augmentation effectiveness on the domains of training data used for latent synthesizers. This restriction confines the applicability of the proposed method. To investigate domain transferability, employing an ASR model trained on an out-of-domain dataset for initializing the Guiding Net would be desirable.\n\nmissing_references: \u30fbPopuri et al., Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation, INTERSPEECH, 2022.\n\u30fbSiuzdak et al., WavThruVec: Latent speech representation as intermediate features for neural speech synthesis, INTERSPEECH, 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "NCkwmCINrz",
        "length": 321,
        "human_text": "paper_topic_and_main_contributions: The researchers introduce a framework called LaSyn, designed to efficiently utilize textual data for end-to-end (E2E) speech processing models. This framework facilitates the transfer of cross-modal knowledge from text to E2E speech processing models through latent synthesis. Two implementations of the latent synthesizer are designed as the core of LaSyn: a fixed-projection latent synthesizer and a diffusion latent synthesizer, which incorporates recent advancements in diffusion models. By leveraging LaSyn to enhance E2E speech models with textual data, the researchers achieve competitive results multiple ASR/SLU tasks.\n\nreasons_to_accept: 1. Two novel designs of latent synthesizer are tailored to the end goal of text-to-speech representation simulation. \n2. The effectiveness of the approach is empirically proven by promising results on multiple tasks/datasets. \n3. Good paper writing and clear presentation.\n\nreasons_to_reject: 1. The ASR experiments are limited. Librispeech is a widely used dataset, but there are various more competitive models (e.g. Wave2Vec2, HuBERT etc) than the baselines in the paper. Moreover, since Librispeech is clean read speech data and not a very challenging dataset, experimenting on it alone may not reflect the ASR performance comprehensively. \n2. When targeting low-resource languages/domains, the availability of G2P are also be limited (section 3.2.2).\n\nmissing_references: Relevant papers on improving/adapting latent speech representation with text only data.\n1. Text-Only Domain Adaptation Based on Intermediate CTC. https://www.isca-speech.org/archive/pdfs/interspeech_2022/sato22_interspeech.pdf 2. Integrating Text Inputs For Training and Adapting RNN Transducer ASR Models. https://arxiv.org/pdf/2202.13155.pdf\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "94_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_94_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7210666666666666,
      "max_similarity": 0.7422,
      "avg_coverage": 0.4833,
      "max_coverage": 0.5217
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 569,
      "avg_human_length": 389.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 6,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "fPkXoEJGeO",
        "similarity": 0.6998,
        "coverage": 0.4667,
        "human_length": 415,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a framework for leveraging pure textual data as a way to improve the modeling capability of speech tasks that originally required paired audio-text pairs. This makes it possible to grow the representation capability and accuracy of models by leveraging linguistic information proyected into the acoustic latents. This, the authors hypothesize that promotes cross-modal (linguistic->acoustic) knowledge transfer. Results are demonstrated on both speech recognition and spoken natural language understanding tasks, improving over baselines and meeting state of the art performance on smaller models and (paired) datasets.\n\nreasons_to_accept: This paper proposes an interesting formulation to include unpaired data to speech models. While there has been an abundant recent amount of work to incorporate unscripted audio (audio without text pairs) there has been limited recent work on the 'other side of the coin'. This has been mostly carried out by leveraging pre-trained Language Models, so in this paper the authors propose a lighter approach to use linguistic information in a cross-modal way. This is demonstrated to have a positive impact across multiple downstream tasks, which reinforces the value of linguistic information in multiple speech-related tasks beyond the conventional domains.\n\nreasons_to_reject: Not a strong reason to reject, but I am left wondering if the impact of the proposed approach happens because the latent acoustic representation trained with the datasets is incomplete (in terms, for example, of linguistic coverage). As there is an abundance of speech-text paired datasets in the public domain, it could be that the benefits of the approach are restricted to lower-resource languages in which the technique was untested, leading to a lower impact of the proposed knowledge.\n\nmissing_references: I believe the paper would benefit from covering the problem also from the point of view of acoustic->linguistic cross-modal training, such as https://arxiv.org/pdf/2302.03540.pdf. Even if the focus is not on how to properly develop and train speech latent encoders, there is potential interaction and extended impact in covering both literatures.\nI also miss references to the recent trends such as SpeechGPT models that are potential competitors (or beneficiaries?) of this approach https://arxiv.org/pdf/2305.11000.pdf\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "KHWRlK3vI0",
        "similarity": 0.7212,
        "coverage": 0.5217,
        "human_length": 431,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel data augmentation framework aimed at enhancing low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) by leveraging unpaired text data. The authors introduce latent synthesizers, which generate continuous latent features from text. These latents correspond to the output features of the speech encoder, enabling the latent synthesizer to produce simplified representations compared to traditional mel-spectrogram and waveform approaches. The study investigates fixed-projection and diffusion-based models as potential latent synthesizers. Experimental evaluations demonstrate that the proposed augmentation technique leads to substantial improvements in both ASR and SLU quality.\n\nreasons_to_accept: The proposed method is novel to some extent. If the authors conducted more experiments in the speech translation task, the claims would be stronger.\n\nreasons_to_reject: \u30fbA comparison with data augmentation methods utilizing discrete acoustic units like SpeechUT is absent. These discrete units simplify speech by emphasizing phonetic and semantic information. An alternative to latent synthesizers involves training Transformer models to generate these discrete units from text [Popuri+ 2022]. While this approach necessitates an additional module to map discrete units to the continuous space, prior research has demonstrated quality enhancements through this technique.\n\u30fbComparisons with TTS augmentation are unfair due to the encoder architectures (Transformer vs Conformer). Consequently, the results don't definitively indicate the superior data augmentation method.\n\u30fbThe investigation into the efficacy of external language model fusion is lacking. Since the text data employed in data augmentation originates from the same source as LM training data (specifically Librispeech 960h in this study), we seek clarification on two aspects: 1) the relative effectiveness of the proposed method compared to shallow fusion, and 2) whether the proposed method complements shallow fusion.\n\u30fbThe results in Section 4.2.1 highlight the dependence of data augmentation effectiveness on the domains of training data used for latent synthesizers. This restriction confines the applicability of the proposed method. To investigate domain transferability, employing an ASR model trained on an out-of-domain dataset for initializing the Guiding Net would be desirable.\n\nmissing_references: \u30fbPopuri et al., Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation, INTERSPEECH, 2022.\n\u30fbSiuzdak et al., WavThruVec: Latent speech representation as intermediate features for neural speech synthesis, INTERSPEECH, 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "NCkwmCINrz",
        "similarity": 0.7422,
        "coverage": 0.4615,
        "human_length": 321,
        "human_text": "paper_topic_and_main_contributions: The researchers introduce a framework called LaSyn, designed to efficiently utilize textual data for end-to-end (E2E) speech processing models. This framework facilitates the transfer of cross-modal knowledge from text to E2E speech processing models through latent synthesis. Two implementations of the latent synthesizer are designed as the core of LaSyn: a fixed-projection latent synthesizer and a diffusion latent synthesizer, which incorporates recent advancements in diffusion models. By leveraging LaSyn to enhance E2E speech models with textual data, the researchers achieve competitive results multiple ASR/SLU tasks.\n\nreasons_to_accept: 1. Two novel designs of latent synthesizer are tailored to the end goal of text-to-speech representation simulation. \n2. The effectiveness of the approach is empirically proven by promising results on multiple tasks/datasets. \n3. Good paper writing and clear presentation.\n\nreasons_to_reject: 1. The ASR experiments are limited. Librispeech is a widely used dataset, but there are various more competitive models (e.g. Wave2Vec2, HuBERT etc) than the baselines in the paper. Moreover, since Librispeech is clean read speech data and not a very challenging dataset, experimenting on it alone may not reflect the ASR performance comprehensively. \n2. When targeting low-resource languages/domains, the availability of G2P are also be limited (section 3.2.2).\n\nmissing_references: Relevant papers on improving/adapting latent speech representation with text only data.\n1. Text-Only Domain Adaptation Based on Intermediate CTC. https://www.isca-speech.org/archive/pdfs/interspeech_2022/sato22_interspeech.pdf 2. Integrating Text Inputs For Training and Adapting RNN Transducer ASR Models. https://arxiv.org/pdf/2202.13155.pdf\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "fPkXoEJGeO",
        "length": 415,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a framework for leveraging pure textual data as a way to improve the modeling capability of speech tasks that originally required paired audio-text pairs. This makes it possible to grow the representation capability and accuracy of models by leveraging linguistic information proyected into the acoustic latents. This, the authors hypothesize that promotes cross-modal (linguistic->acoustic) knowledge transfer. Results are demonstrated on both speech recognition and spoken natural language understanding tasks, improving over baselines and meeting state of the art performance on smaller models and (paired) datasets.\n\nreasons_to_accept: This paper proposes an interesting formulation to include unpaired data to speech models. While there has been an abundant recent amount of work to incorporate unscripted audio (audio without text pairs) there has been limited recent work on the 'other side of the coin'. This has been mostly carried out by leveraging pre-trained Language Models, so in this paper the authors propose a lighter approach to use linguistic information in a cross-modal way. This is demonstrated to have a positive impact across multiple downstream tasks, which reinforces the value of linguistic information in multiple speech-related tasks beyond the conventional domains.\n\nreasons_to_reject: Not a strong reason to reject, but I am left wondering if the impact of the proposed approach happens because the latent acoustic representation trained with the datasets is incomplete (in terms, for example, of linguistic coverage). As there is an abundance of speech-text paired datasets in the public domain, it could be that the benefits of the approach are restricted to lower-resource languages in which the technique was untested, leading to a lower impact of the proposed knowledge.\n\nmissing_references: I believe the paper would benefit from covering the problem also from the point of view of acoustic->linguistic cross-modal training, such as https://arxiv.org/pdf/2302.03540.pdf. Even if the focus is not on how to properly develop and train speech latent encoders, there is potential interaction and extended impact in covering both literatures.\nI also miss references to the recent trends such as SpeechGPT models that are potential competitors (or beneficiaries?) of this approach https://arxiv.org/pdf/2305.11000.pdf\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "KHWRlK3vI0",
        "length": 431,
        "human_text": "paper_topic_and_main_contributions: This paper presents a novel data augmentation framework aimed at enhancing low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) by leveraging unpaired text data. The authors introduce latent synthesizers, which generate continuous latent features from text. These latents correspond to the output features of the speech encoder, enabling the latent synthesizer to produce simplified representations compared to traditional mel-spectrogram and waveform approaches. The study investigates fixed-projection and diffusion-based models as potential latent synthesizers. Experimental evaluations demonstrate that the proposed augmentation technique leads to substantial improvements in both ASR and SLU quality.\n\nreasons_to_accept: The proposed method is novel to some extent. If the authors conducted more experiments in the speech translation task, the claims would be stronger.\n\nreasons_to_reject: \u30fbA comparison with data augmentation methods utilizing discrete acoustic units like SpeechUT is absent. These discrete units simplify speech by emphasizing phonetic and semantic information. An alternative to latent synthesizers involves training Transformer models to generate these discrete units from text [Popuri+ 2022]. While this approach necessitates an additional module to map discrete units to the continuous space, prior research has demonstrated quality enhancements through this technique.\n\u30fbComparisons with TTS augmentation are unfair due to the encoder architectures (Transformer vs Conformer). Consequently, the results don't definitively indicate the superior data augmentation method.\n\u30fbThe investigation into the efficacy of external language model fusion is lacking. Since the text data employed in data augmentation originates from the same source as LM training data (specifically Librispeech 960h in this study), we seek clarification on two aspects: 1) the relative effectiveness of the proposed method compared to shallow fusion, and 2) whether the proposed method complements shallow fusion.\n\u30fbThe results in Section 4.2.1 highlight the dependence of data augmentation effectiveness on the domains of training data used for latent synthesizers. This restriction confines the applicability of the proposed method. To investigate domain transferability, employing an ASR model trained on an out-of-domain dataset for initializing the Guiding Net would be desirable.\n\nmissing_references: \u30fbPopuri et al., Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation, INTERSPEECH, 2022.\n\u30fbSiuzdak et al., WavThruVec: Latent speech representation as intermediate features for neural speech synthesis, INTERSPEECH, 2022.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "NCkwmCINrz",
        "length": 321,
        "human_text": "paper_topic_and_main_contributions: The researchers introduce a framework called LaSyn, designed to efficiently utilize textual data for end-to-end (E2E) speech processing models. This framework facilitates the transfer of cross-modal knowledge from text to E2E speech processing models through latent synthesis. Two implementations of the latent synthesizer are designed as the core of LaSyn: a fixed-projection latent synthesizer and a diffusion latent synthesizer, which incorporates recent advancements in diffusion models. By leveraging LaSyn to enhance E2E speech models with textual data, the researchers achieve competitive results multiple ASR/SLU tasks.\n\nreasons_to_accept: 1. Two novel designs of latent synthesizer are tailored to the end goal of text-to-speech representation simulation. \n2. The effectiveness of the approach is empirically proven by promising results on multiple tasks/datasets. \n3. Good paper writing and clear presentation.\n\nreasons_to_reject: 1. The ASR experiments are limited. Librispeech is a widely used dataset, but there are various more competitive models (e.g. Wave2Vec2, HuBERT etc) than the baselines in the paper. Moreover, since Librispeech is clean read speech data and not a very challenging dataset, experimenting on it alone may not reflect the ASR performance comprehensively. \n2. When targeting low-resource languages/domains, the availability of G2P are also be limited (section 3.2.2).\n\nmissing_references: Relevant papers on improving/adapting latent speech representation with text only data.\n1. Text-Only Domain Adaptation Based on Intermediate CTC. https://www.isca-speech.org/archive/pdfs/interspeech_2022/sato22_interspeech.pdf 2. Integrating Text Inputs For Training and Adapting RNN Transducer ASR Models. https://arxiv.org/pdf/2202.13155.pdf\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "6_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_6_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7110333333333334,
      "max_similarity": 0.7165,
      "avg_coverage": 0.6311,
      "max_coverage": 0.7333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 888,
      "avg_human_length": 259.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 11,
      "suggestions_count": 17
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "vu8vU7YCdJ",
        "similarity": 0.7103,
        "coverage": 0.4643,
        "human_length": 255,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an attention prompt tuning method for parameter-efficient tuning PLMs. This paper has proved that existing prompt tuning can be considered as a special case of attention prompt tuning. The main contributions lie in that extensive experiments has verified the effectiveness of this kind of prompt tuning.\n\nreasons_to_accept: 1. Novel method. The proposed attention prompt tuning is novel compared to previous method and can be considered as a unified form. \n2. Insightful analysis. The empirical study is useful to build a connection between the proposed tuning and previous tuning. \n3.  Extensive experiments. The authors conduct many experiments to validate the proposed method. \n4. Good writing.\n\nreasons_to_reject: 1. Missing some detailed comparison with previous methods. \n2. Missing some important baselines.\n\nquestions_for_the_authors: 1. You should make a comparison among prompt tuning methods including their detailed trainable parameters and training time. \n2. LoRA [1] is an important parameter-efficient tuning method for large language models. You should compare with it. \n[1] LoRA: Low-Rank Adaptation of Large Language Models\n\nmissing_references: [1] LoRA: Low-Rank Adaptation of Large Language Models. \n[2] A Survey of Large Language Models. \n[3] Challenges and Applications of Large Language Models\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "J6qUJiyFen",
        "similarity": 0.7063,
        "coverage": 0.7333,
        "human_length": 217,
        "human_text": "paper_topic_and_main_contributions: This paper proposes another parameter efficient fine-tuning method of large language models: attention prompt tuning. APROMPT incorporates three sets of learnable prompts: query, key, and value prompts. These prompts are prepended to the respective matrices in the self-attention block within the Transformer layer. During model tuning, these attention prompts are learned alongside the original input prompts. Comprehensive experiments on various tasks in the SuperGLUE benchmark show the effectiveness of the proposed method.\n\nreasons_to_accept: 1. Clear writing. \n2. Thorough analysis and discussion.\n\nreasons_to_reject: My main concern of the paper is on the baselines. As newly-proposed parameter efficient fine-tuning method, it should not only be compared with prompt tuning approaches, but also other methods such as Adaptor (https://arxiv.org/pdf/1902.00751.pdf) and LoRA (https://arxiv.org/abs/2106.09685), since they are more widely used.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "7HnEmyXrb0",
        "similarity": 0.7165,
        "coverage": 0.6957,
        "human_length": 305,
        "human_text": "paper_topic_and_main_contributions: The paper presents Attention Prompt tuning (APrompt), an innovative method for adapting large pre-trained language models. Unlike traditional tuning that focuses on input layers, APrompt incorporates prompts into the attention layer. Tested on the SuperGLUE benchmark, APrompt outperforms standard approaches across different model scales, with additional studies confirming its effectiveness and efficiency.\n\nreasons_to_accept: 1. Views the existing prompt tuning method as a specialized form of attention prompts.  2. Proposes a novel prompting method where the prompts are applied to the attention directly.  3. Extensive experiments on three different model size and SuperGLUE benchmark shows the effectiveness of the proposed method.  4. The paper is well-written and the figures help the understanding.\n\nreasons_to_reject: 1. I have a hard time understanding why in line 213, Q is not calculated using $X_{new}$. It seems like the authors argue this because only the original text tokens X are updated. However, to the best of my knowledge, the features of the prompts are also updated as it goes through the transformer layers.  2. In Table 8, the authors report the training time of APrompt on the SuperGLUE dataset. But I feel its necessary to report the training time of other baselines as well.\n\nquestions_for_the_authors: Please refer to [reasons to reject].\n\ntypos_grammar_style_and_presentation_improvements: In Figure 3 (c), Matmul -> MatMul\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "vu8vU7YCdJ",
        "length": 255,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an attention prompt tuning method for parameter-efficient tuning PLMs. This paper has proved that existing prompt tuning can be considered as a special case of attention prompt tuning. The main contributions lie in that extensive experiments has verified the effectiveness of this kind of prompt tuning.\n\nreasons_to_accept: 1. Novel method. The proposed attention prompt tuning is novel compared to previous method and can be considered as a unified form. \n2. Insightful analysis. The empirical study is useful to build a connection between the proposed tuning and previous tuning. \n3.  Extensive experiments. The authors conduct many experiments to validate the proposed method. \n4. Good writing.\n\nreasons_to_reject: 1. Missing some detailed comparison with previous methods. \n2. Missing some important baselines.\n\nquestions_for_the_authors: 1. You should make a comparison among prompt tuning methods including their detailed trainable parameters and training time. \n2. LoRA [1] is an important parameter-efficient tuning method for large language models. You should compare with it. \n[1] LoRA: Low-Rank Adaptation of Large Language Models\n\nmissing_references: [1] LoRA: Low-Rank Adaptation of Large Language Models. \n[2] A Survey of Large Language Models. \n[3] Challenges and Applications of Large Language Models\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "J6qUJiyFen",
        "length": 217,
        "human_text": "paper_topic_and_main_contributions: This paper proposes another parameter efficient fine-tuning method of large language models: attention prompt tuning. APROMPT incorporates three sets of learnable prompts: query, key, and value prompts. These prompts are prepended to the respective matrices in the self-attention block within the Transformer layer. During model tuning, these attention prompts are learned alongside the original input prompts. Comprehensive experiments on various tasks in the SuperGLUE benchmark show the effectiveness of the proposed method.\n\nreasons_to_accept: 1. Clear writing. \n2. Thorough analysis and discussion.\n\nreasons_to_reject: My main concern of the paper is on the baselines. As newly-proposed parameter efficient fine-tuning method, it should not only be compared with prompt tuning approaches, but also other methods such as Adaptor (https://arxiv.org/pdf/1902.00751.pdf) and LoRA (https://arxiv.org/abs/2106.09685), since they are more widely used.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "7HnEmyXrb0",
        "length": 305,
        "human_text": "paper_topic_and_main_contributions: The paper presents Attention Prompt tuning (APrompt), an innovative method for adapting large pre-trained language models. Unlike traditional tuning that focuses on input layers, APrompt incorporates prompts into the attention layer. Tested on the SuperGLUE benchmark, APrompt outperforms standard approaches across different model scales, with additional studies confirming its effectiveness and efficiency.\n\nreasons_to_accept: 1. Views the existing prompt tuning method as a specialized form of attention prompts.  2. Proposes a novel prompting method where the prompts are applied to the attention directly.  3. Extensive experiments on three different model size and SuperGLUE benchmark shows the effectiveness of the proposed method.  4. The paper is well-written and the figures help the understanding.\n\nreasons_to_reject: 1. I have a hard time understanding why in line 213, Q is not calculated using $X_{new}$. It seems like the authors argue this because only the original text tokens X are updated. However, to the best of my knowledge, the features of the prompts are also updated as it goes through the transformer layers.  2. In Table 8, the authors report the training time of APrompt on the SuperGLUE dataset. But I feel its necessary to report the training time of other baselines as well.\n\nquestions_for_the_authors: Please refer to [reasons to reject].\n\ntypos_grammar_style_and_presentation_improvements: In Figure 3 (c), Matmul -> MatMul\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "6_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_6_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7224,
      "max_similarity": 0.7243,
      "avg_coverage": 0.6021,
      "max_coverage": 0.7333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 801,
      "avg_human_length": 259.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 11,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "vu8vU7YCdJ",
        "similarity": 0.7243,
        "coverage": 0.4643,
        "human_length": 255,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an attention prompt tuning method for parameter-efficient tuning PLMs. This paper has proved that existing prompt tuning can be considered as a special case of attention prompt tuning. The main contributions lie in that extensive experiments has verified the effectiveness of this kind of prompt tuning.\n\nreasons_to_accept: 1. Novel method. The proposed attention prompt tuning is novel compared to previous method and can be considered as a unified form. \n2. Insightful analysis. The empirical study is useful to build a connection between the proposed tuning and previous tuning. \n3.  Extensive experiments. The authors conduct many experiments to validate the proposed method. \n4. Good writing.\n\nreasons_to_reject: 1. Missing some detailed comparison with previous methods. \n2. Missing some important baselines.\n\nquestions_for_the_authors: 1. You should make a comparison among prompt tuning methods including their detailed trainable parameters and training time. \n2. LoRA [1] is an important parameter-efficient tuning method for large language models. You should compare with it. \n[1] LoRA: Low-Rank Adaptation of Large Language Models\n\nmissing_references: [1] LoRA: Low-Rank Adaptation of Large Language Models. \n[2] A Survey of Large Language Models. \n[3] Challenges and Applications of Large Language Models\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "J6qUJiyFen",
        "similarity": 0.7209,
        "coverage": 0.7333,
        "human_length": 217,
        "human_text": "paper_topic_and_main_contributions: This paper proposes another parameter efficient fine-tuning method of large language models: attention prompt tuning. APROMPT incorporates three sets of learnable prompts: query, key, and value prompts. These prompts are prepended to the respective matrices in the self-attention block within the Transformer layer. During model tuning, these attention prompts are learned alongside the original input prompts. Comprehensive experiments on various tasks in the SuperGLUE benchmark show the effectiveness of the proposed method.\n\nreasons_to_accept: 1. Clear writing. \n2. Thorough analysis and discussion.\n\nreasons_to_reject: My main concern of the paper is on the baselines. As newly-proposed parameter efficient fine-tuning method, it should not only be compared with prompt tuning approaches, but also other methods such as Adaptor (https://arxiv.org/pdf/1902.00751.pdf) and LoRA (https://arxiv.org/abs/2106.09685), since they are more widely used.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "7HnEmyXrb0",
        "similarity": 0.722,
        "coverage": 0.6087,
        "human_length": 305,
        "human_text": "paper_topic_and_main_contributions: The paper presents Attention Prompt tuning (APrompt), an innovative method for adapting large pre-trained language models. Unlike traditional tuning that focuses on input layers, APrompt incorporates prompts into the attention layer. Tested on the SuperGLUE benchmark, APrompt outperforms standard approaches across different model scales, with additional studies confirming its effectiveness and efficiency.\n\nreasons_to_accept: 1. Views the existing prompt tuning method as a specialized form of attention prompts.  2. Proposes a novel prompting method where the prompts are applied to the attention directly.  3. Extensive experiments on three different model size and SuperGLUE benchmark shows the effectiveness of the proposed method.  4. The paper is well-written and the figures help the understanding.\n\nreasons_to_reject: 1. I have a hard time understanding why in line 213, Q is not calculated using $X_{new}$. It seems like the authors argue this because only the original text tokens X are updated. However, to the best of my knowledge, the features of the prompts are also updated as it goes through the transformer layers.  2. In Table 8, the authors report the training time of APrompt on the SuperGLUE dataset. But I feel its necessary to report the training time of other baselines as well.\n\nquestions_for_the_authors: Please refer to [reasons to reject].\n\ntypos_grammar_style_and_presentation_improvements: In Figure 3 (c), Matmul -> MatMul\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "vu8vU7YCdJ",
        "length": 255,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an attention prompt tuning method for parameter-efficient tuning PLMs. This paper has proved that existing prompt tuning can be considered as a special case of attention prompt tuning. The main contributions lie in that extensive experiments has verified the effectiveness of this kind of prompt tuning.\n\nreasons_to_accept: 1. Novel method. The proposed attention prompt tuning is novel compared to previous method and can be considered as a unified form. \n2. Insightful analysis. The empirical study is useful to build a connection between the proposed tuning and previous tuning. \n3.  Extensive experiments. The authors conduct many experiments to validate the proposed method. \n4. Good writing.\n\nreasons_to_reject: 1. Missing some detailed comparison with previous methods. \n2. Missing some important baselines.\n\nquestions_for_the_authors: 1. You should make a comparison among prompt tuning methods including their detailed trainable parameters and training time. \n2. LoRA [1] is an important parameter-efficient tuning method for large language models. You should compare with it. \n[1] LoRA: Low-Rank Adaptation of Large Language Models\n\nmissing_references: [1] LoRA: Low-Rank Adaptation of Large Language Models. \n[2] A Survey of Large Language Models. \n[3] Challenges and Applications of Large Language Models\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "J6qUJiyFen",
        "length": 217,
        "human_text": "paper_topic_and_main_contributions: This paper proposes another parameter efficient fine-tuning method of large language models: attention prompt tuning. APROMPT incorporates three sets of learnable prompts: query, key, and value prompts. These prompts are prepended to the respective matrices in the self-attention block within the Transformer layer. During model tuning, these attention prompts are learned alongside the original input prompts. Comprehensive experiments on various tasks in the SuperGLUE benchmark show the effectiveness of the proposed method.\n\nreasons_to_accept: 1. Clear writing. \n2. Thorough analysis and discussion.\n\nreasons_to_reject: My main concern of the paper is on the baselines. As newly-proposed parameter efficient fine-tuning method, it should not only be compared with prompt tuning approaches, but also other methods such as Adaptor (https://arxiv.org/pdf/1902.00751.pdf) and LoRA (https://arxiv.org/abs/2106.09685), since they are more widely used.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "7HnEmyXrb0",
        "length": 305,
        "human_text": "paper_topic_and_main_contributions: The paper presents Attention Prompt tuning (APrompt), an innovative method for adapting large pre-trained language models. Unlike traditional tuning that focuses on input layers, APrompt incorporates prompts into the attention layer. Tested on the SuperGLUE benchmark, APrompt outperforms standard approaches across different model scales, with additional studies confirming its effectiveness and efficiency.\n\nreasons_to_accept: 1. Views the existing prompt tuning method as a specialized form of attention prompts.  2. Proposes a novel prompting method where the prompts are applied to the attention directly.  3. Extensive experiments on three different model size and SuperGLUE benchmark shows the effectiveness of the proposed method.  4. The paper is well-written and the figures help the understanding.\n\nreasons_to_reject: 1. I have a hard time understanding why in line 213, Q is not calculated using $X_{new}$. It seems like the authors argue this because only the original text tokens X are updated. However, to the best of my knowledge, the features of the prompts are also updated as it goes through the transformer layers.  2. In Table 8, the authors report the training time of APrompt on the SuperGLUE dataset. But I feel its necessary to report the training time of other baselines as well.\n\nquestions_for_the_authors: Please refer to [reasons to reject].\n\ntypos_grammar_style_and_presentation_improvements: In Figure 3 (c), Matmul -> MatMul\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "76_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_76_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7057000000000001,
      "max_similarity": 0.7133,
      "avg_coverage": 0.6228000000000001,
      "max_coverage": 0.6842
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 655,
      "avg_human_length": 456.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 10,
      "suggestions_count": 16
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "MgtMmeNwBz",
        "similarity": 0.7133,
        "coverage": 0.5,
        "human_length": 753,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a system for automatic narrative detection with an application of characterizing the use of narrative in health misinformation on Twitter. The authors manually annotated an existing misinformation tweet dataset for the presence of narrative. They then built text classifiers to predict narrative style, and found that a fine-tuned RoBERTa model performed the best. Using that classifier to label the rest of the dataset, they explore different properties of the relationship between narrative style and misinformation. They find that narrative style is related to higher engagement and that for one of their datasets, narrative is associated with higher engagement with misinformation.\n\nreasons_to_accept: The paper provides a good motivation that narrative is an issue that needs to be addressed with misinformation and takes first steps toward a computational and quantitative analysis.\n\nreasons_to_reject: Since the RoBERTa classifier chosen to annotate the reset of the dataset surely has some systematic errors, it would be more convincing to present results with respect to engagement (section 5) also just for the subset of manually annotated tweets to verify that there is no significant difference between those and the tweets that were annotated for narrative with the classifier.\nOtherwise, this paper would benefit most from a more thorough discussion and interpretation of all the results, which as it currently stand are difficult to tie together and so lack coherent findings that could benefit future work.\nThe results from the GLM seem inconclusive on the relationship between misinformation and narrative. As noted, there is a positive relationship for misinfo x narrative for ANTiVax but not CMU-MisCov19. The paper says that this indicates that false narratives about getting the vaccine get more engagement than other misinformation narratives (lines 433-435). Examples of those other types of misinformation narratives would be informative. I assume false narratives about getting the vaccine would be narratives about people getting the vaccine and then terrible things happening to them? Examples here would be valuable as well.\nEffect sizes or something else may help readers interpret the strength of these relationships, which is currently lacking. There is a positive relationship for both datasets for narrative x misinfo x follower count, but with very small coefficient values. The range of coefficient values varies dramatically overall (the -3.5 coefficient for misinfo x narrative for CMU-MisCov19 has a particularly large absolute value, which may be worth talking about).\nA more in-depth discussion of the results is needed to properly interpret and contextualize all of the results given. For example, it seems counterintuitive and interesting that there is more narrativity with informed tweets (if I'm reading section 5.1 right). What might be evidence from the tweets to explain this? Are people just sharing stories of getting the vaccine? In general, more interpretation of main themes coming from results in each subsection of section 5 would be helpful.\nThe LIWC analysis in section 5.3.3 seems to yield unsurprising results when it comes to what terms narratives would use more. More informative would be differences between narrative info and narrative misinfo, but the only difference detected is a focus on death terms. Unsurprising results are okay in general, and maybe the focus on death can contribute to what is not known about COVID misinformation (it is perhaps interesting that the informed tweets are not talking about avoiding death through taking the vaccine, for example), but more interpretation and discussion would be needed. The narrative arc analysis in section 5.3.2 doesn't seem to add any value, since as noted, tweets just seem too short for this tool to work properly and it seems like most of the results are statistically insignificant.\n\ntypos_grammar_style_and_presentation_improvements: - What does the Hydrated % row refer to in Table 1?\n- Figure 1 text is too small to easily read, other figures like Fig 2 and tables too - Line 338 , instead of .\n- Line 417: model -> models - Line 632: authentic used -> authentic, used\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "WBYXSNHZjo",
        "similarity": 0.7035,
        "coverage": 0.6842,
        "human_length": 326,
        "human_text": "paper_topic_and_main_contributions: This paper explores the relationship between narrative communication and health misinformation on social media, focusing on Twitter. The authors manually labeled a subset of tweets from misinformation datasets to identify narratives and then used the best model (their case RoBERTa) to extend these labels to the entire datasets. They found that narrativity can increase engagement with misinformation, especially for users with many followers.\n\nreasons_to_accept: The analysis part of the paper where the authors used statistical modeling to find out the relationship among narrativity, misinformation, follower count, and user engagement.\n\nreasons_to_reject: 1. Line 327, 14.5k tweets were labeled by the annotators and the best RoBERTa model. If the annotators did 3k annotation,  It is not clear how the rest of the tweets are labeled by the model. Does it mean around 11.5k tweets were used as test data?\n2. As data annotation was mentioned as one of the contributions, the authors could show some examples of annotated data and annotation procedures.\n3. Authors could include a thematic analysis of the data.\n\nmissing_references: Authors should explore the stance, morality, and reason detection task on COVID-19 by Pacheco et al 2022 (A holistic framework for analyzing the covid-19 vaccine debate).  Also for COVID-19 moral foundation and theme, authors could look into work by Islam and Goldwasser 2022 (Understanding COVID-19 Vaccine Campaign on Facebook using Minimal Supervision).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "KoZECf8FdT",
        "similarity": 0.7003,
        "coverage": 0.6842,
        "human_length": 289,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors annotate an existing misinformation tweet dataset for the presence of narratives. The authors selected two existing medical datasets. They then followed the annotation process through an iterative process involving discussion and revision. They first annotated some samples, and then the rest of the samples were automatically annotated using machine learning techniques.\nThe conclusions that the authors made are: 1. narratives involve higher user engagement than misinformation 2. narratives may help increase engagement of misinformation The key contribution of the work is the dataset and the follow-up conclusions that the paper makes.\n\nreasons_to_accept: A new dataset that might help modelling user narratives online especially related to misinformation. \nThe studies that the paper presents are interesting and useful to understand the importance of user narratives.\n\nreasons_to_reject: One of the key concerns is the quality of the automatically annotated samples. How much control we have to maintain the quality of the automatically generated samples is something that needs to be mentioned in the paper. While the authors have used different machine learning models, these models have their own pros and cons. \nBesides that, how do we know that the number of instances that have been manually annotated are enough for the computational model to learn reliably?\n\nquestions_for_the_authors: Questions are mentioned above.\nThere is an assumption that the dataset would be available for free use later.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "MgtMmeNwBz",
        "length": 753,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a system for automatic narrative detection with an application of characterizing the use of narrative in health misinformation on Twitter. The authors manually annotated an existing misinformation tweet dataset for the presence of narrative. They then built text classifiers to predict narrative style, and found that a fine-tuned RoBERTa model performed the best. Using that classifier to label the rest of the dataset, they explore different properties of the relationship between narrative style and misinformation. They find that narrative style is related to higher engagement and that for one of their datasets, narrative is associated with higher engagement with misinformation.\n\nreasons_to_accept: The paper provides a good motivation that narrative is an issue that needs to be addressed with misinformation and takes first steps toward a computational and quantitative analysis.\n\nreasons_to_reject: Since the RoBERTa classifier chosen to annotate the reset of the dataset surely has some systematic errors, it would be more convincing to present results with respect to engagement (section 5) also just for the subset of manually annotated tweets to verify that there is no significant difference between those and the tweets that were annotated for narrative with the classifier.\nOtherwise, this paper would benefit most from a more thorough discussion and interpretation of all the results, which as it currently stand are difficult to tie together and so lack coherent findings that could benefit future work.\nThe results from the GLM seem inconclusive on the relationship between misinformation and narrative. As noted, there is a positive relationship for misinfo x narrative for ANTiVax but not CMU-MisCov19. The paper says that this indicates that false narratives about getting the vaccine get more engagement than other misinformation narratives (lines 433-435). Examples of those other types of misinformation narratives would be informative. I assume false narratives about getting the vaccine would be narratives about people getting the vaccine and then terrible things happening to them? Examples here would be valuable as well.\nEffect sizes or something else may help readers interpret the strength of these relationships, which is currently lacking. There is a positive relationship for both datasets for narrative x misinfo x follower count, but with very small coefficient values. The range of coefficient values varies dramatically overall (the -3.5 coefficient for misinfo x narrative for CMU-MisCov19 has a particularly large absolute value, which may be worth talking about).\nA more in-depth discussion of the results is needed to properly interpret and contextualize all of the results given. For example, it seems counterintuitive and interesting that there is more narrativity with informed tweets (if I'm reading section 5.1 right). What might be evidence from the tweets to explain this? Are people just sharing stories of getting the vaccine? In general, more interpretation of main themes coming from results in each subsection of section 5 would be helpful.\nThe LIWC analysis in section 5.3.3 seems to yield unsurprising results when it comes to what terms narratives would use more. More informative would be differences between narrative info and narrative misinfo, but the only difference detected is a focus on death terms. Unsurprising results are okay in general, and maybe the focus on death can contribute to what is not known about COVID misinformation (it is perhaps interesting that the informed tweets are not talking about avoiding death through taking the vaccine, for example), but more interpretation and discussion would be needed. The narrative arc analysis in section 5.3.2 doesn't seem to add any value, since as noted, tweets just seem too short for this tool to work properly and it seems like most of the results are statistically insignificant.\n\ntypos_grammar_style_and_presentation_improvements: - What does the Hydrated % row refer to in Table 1?\n- Figure 1 text is too small to easily read, other figures like Fig 2 and tables too - Line 338 , instead of .\n- Line 417: model -> models - Line 632: authentic used -> authentic, used\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "WBYXSNHZjo",
        "length": 326,
        "human_text": "paper_topic_and_main_contributions: This paper explores the relationship between narrative communication and health misinformation on social media, focusing on Twitter. The authors manually labeled a subset of tweets from misinformation datasets to identify narratives and then used the best model (their case RoBERTa) to extend these labels to the entire datasets. They found that narrativity can increase engagement with misinformation, especially for users with many followers.\n\nreasons_to_accept: The analysis part of the paper where the authors used statistical modeling to find out the relationship among narrativity, misinformation, follower count, and user engagement.\n\nreasons_to_reject: 1. Line 327, 14.5k tweets were labeled by the annotators and the best RoBERTa model. If the annotators did 3k annotation,  It is not clear how the rest of the tweets are labeled by the model. Does it mean around 11.5k tweets were used as test data?\n2. As data annotation was mentioned as one of the contributions, the authors could show some examples of annotated data and annotation procedures.\n3. Authors could include a thematic analysis of the data.\n\nmissing_references: Authors should explore the stance, morality, and reason detection task on COVID-19 by Pacheco et al 2022 (A holistic framework for analyzing the covid-19 vaccine debate).  Also for COVID-19 moral foundation and theme, authors could look into work by Islam and Goldwasser 2022 (Understanding COVID-19 Vaccine Campaign on Facebook using Minimal Supervision).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "KoZECf8FdT",
        "length": 289,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors annotate an existing misinformation tweet dataset for the presence of narratives. The authors selected two existing medical datasets. They then followed the annotation process through an iterative process involving discussion and revision. They first annotated some samples, and then the rest of the samples were automatically annotated using machine learning techniques.\nThe conclusions that the authors made are: 1. narratives involve higher user engagement than misinformation 2. narratives may help increase engagement of misinformation The key contribution of the work is the dataset and the follow-up conclusions that the paper makes.\n\nreasons_to_accept: A new dataset that might help modelling user narratives online especially related to misinformation. \nThe studies that the paper presents are interesting and useful to understand the importance of user narratives.\n\nreasons_to_reject: One of the key concerns is the quality of the automatically annotated samples. How much control we have to maintain the quality of the automatically generated samples is something that needs to be mentioned in the paper. While the authors have used different machine learning models, these models have their own pros and cons. \nBesides that, how do we know that the number of instances that have been manually annotated are enough for the computational model to learn reliably?\n\nquestions_for_the_authors: Questions are mentioned above.\nThere is an assumption that the dataset would be available for free use later.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "76_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_76_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7091,
      "max_similarity": 0.7164,
      "avg_coverage": 0.6787000000000001,
      "max_coverage": 0.7368
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 743,
      "avg_human_length": 456.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 10,
      "suggestions_count": 15
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "MgtMmeNwBz",
        "similarity": 0.7164,
        "coverage": 0.5625,
        "human_length": 753,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a system for automatic narrative detection with an application of characterizing the use of narrative in health misinformation on Twitter. The authors manually annotated an existing misinformation tweet dataset for the presence of narrative. They then built text classifiers to predict narrative style, and found that a fine-tuned RoBERTa model performed the best. Using that classifier to label the rest of the dataset, they explore different properties of the relationship between narrative style and misinformation. They find that narrative style is related to higher engagement and that for one of their datasets, narrative is associated with higher engagement with misinformation.\n\nreasons_to_accept: The paper provides a good motivation that narrative is an issue that needs to be addressed with misinformation and takes first steps toward a computational and quantitative analysis.\n\nreasons_to_reject: Since the RoBERTa classifier chosen to annotate the reset of the dataset surely has some systematic errors, it would be more convincing to present results with respect to engagement (section 5) also just for the subset of manually annotated tweets to verify that there is no significant difference between those and the tweets that were annotated for narrative with the classifier.\nOtherwise, this paper would benefit most from a more thorough discussion and interpretation of all the results, which as it currently stand are difficult to tie together and so lack coherent findings that could benefit future work.\nThe results from the GLM seem inconclusive on the relationship between misinformation and narrative. As noted, there is a positive relationship for misinfo x narrative for ANTiVax but not CMU-MisCov19. The paper says that this indicates that false narratives about getting the vaccine get more engagement than other misinformation narratives (lines 433-435). Examples of those other types of misinformation narratives would be informative. I assume false narratives about getting the vaccine would be narratives about people getting the vaccine and then terrible things happening to them? Examples here would be valuable as well.\nEffect sizes or something else may help readers interpret the strength of these relationships, which is currently lacking. There is a positive relationship for both datasets for narrative x misinfo x follower count, but with very small coefficient values. The range of coefficient values varies dramatically overall (the -3.5 coefficient for misinfo x narrative for CMU-MisCov19 has a particularly large absolute value, which may be worth talking about).\nA more in-depth discussion of the results is needed to properly interpret and contextualize all of the results given. For example, it seems counterintuitive and interesting that there is more narrativity with informed tweets (if I'm reading section 5.1 right). What might be evidence from the tweets to explain this? Are people just sharing stories of getting the vaccine? In general, more interpretation of main themes coming from results in each subsection of section 5 would be helpful.\nThe LIWC analysis in section 5.3.3 seems to yield unsurprising results when it comes to what terms narratives would use more. More informative would be differences between narrative info and narrative misinfo, but the only difference detected is a focus on death terms. Unsurprising results are okay in general, and maybe the focus on death can contribute to what is not known about COVID misinformation (it is perhaps interesting that the informed tweets are not talking about avoiding death through taking the vaccine, for example), but more interpretation and discussion would be needed. The narrative arc analysis in section 5.3.2 doesn't seem to add any value, since as noted, tweets just seem too short for this tool to work properly and it seems like most of the results are statistically insignificant.\n\ntypos_grammar_style_and_presentation_improvements: - What does the Hydrated % row refer to in Table 1?\n- Figure 1 text is too small to easily read, other figures like Fig 2 and tables too - Line 338 , instead of .\n- Line 417: model -> models - Line 632: authentic used -> authentic, used\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "WBYXSNHZjo",
        "similarity": 0.7072,
        "coverage": 0.7368,
        "human_length": 326,
        "human_text": "paper_topic_and_main_contributions: This paper explores the relationship between narrative communication and health misinformation on social media, focusing on Twitter. The authors manually labeled a subset of tweets from misinformation datasets to identify narratives and then used the best model (their case RoBERTa) to extend these labels to the entire datasets. They found that narrativity can increase engagement with misinformation, especially for users with many followers.\n\nreasons_to_accept: The analysis part of the paper where the authors used statistical modeling to find out the relationship among narrativity, misinformation, follower count, and user engagement.\n\nreasons_to_reject: 1. Line 327, 14.5k tweets were labeled by the annotators and the best RoBERTa model. If the annotators did 3k annotation,  It is not clear how the rest of the tweets are labeled by the model. Does it mean around 11.5k tweets were used as test data?\n2. As data annotation was mentioned as one of the contributions, the authors could show some examples of annotated data and annotation procedures.\n3. Authors could include a thematic analysis of the data.\n\nmissing_references: Authors should explore the stance, morality, and reason detection task on COVID-19 by Pacheco et al 2022 (A holistic framework for analyzing the covid-19 vaccine debate).  Also for COVID-19 moral foundation and theme, authors could look into work by Islam and Goldwasser 2022 (Understanding COVID-19 Vaccine Campaign on Facebook using Minimal Supervision).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "KoZECf8FdT",
        "similarity": 0.7037,
        "coverage": 0.7368,
        "human_length": 289,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors annotate an existing misinformation tweet dataset for the presence of narratives. The authors selected two existing medical datasets. They then followed the annotation process through an iterative process involving discussion and revision. They first annotated some samples, and then the rest of the samples were automatically annotated using machine learning techniques.\nThe conclusions that the authors made are: 1. narratives involve higher user engagement than misinformation 2. narratives may help increase engagement of misinformation The key contribution of the work is the dataset and the follow-up conclusions that the paper makes.\n\nreasons_to_accept: A new dataset that might help modelling user narratives online especially related to misinformation. \nThe studies that the paper presents are interesting and useful to understand the importance of user narratives.\n\nreasons_to_reject: One of the key concerns is the quality of the automatically annotated samples. How much control we have to maintain the quality of the automatically generated samples is something that needs to be mentioned in the paper. While the authors have used different machine learning models, these models have their own pros and cons. \nBesides that, how do we know that the number of instances that have been manually annotated are enough for the computational model to learn reliably?\n\nquestions_for_the_authors: Questions are mentioned above.\nThere is an assumption that the dataset would be available for free use later.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "MgtMmeNwBz",
        "length": 753,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a system for automatic narrative detection with an application of characterizing the use of narrative in health misinformation on Twitter. The authors manually annotated an existing misinformation tweet dataset for the presence of narrative. They then built text classifiers to predict narrative style, and found that a fine-tuned RoBERTa model performed the best. Using that classifier to label the rest of the dataset, they explore different properties of the relationship between narrative style and misinformation. They find that narrative style is related to higher engagement and that for one of their datasets, narrative is associated with higher engagement with misinformation.\n\nreasons_to_accept: The paper provides a good motivation that narrative is an issue that needs to be addressed with misinformation and takes first steps toward a computational and quantitative analysis.\n\nreasons_to_reject: Since the RoBERTa classifier chosen to annotate the reset of the dataset surely has some systematic errors, it would be more convincing to present results with respect to engagement (section 5) also just for the subset of manually annotated tweets to verify that there is no significant difference between those and the tweets that were annotated for narrative with the classifier.\nOtherwise, this paper would benefit most from a more thorough discussion and interpretation of all the results, which as it currently stand are difficult to tie together and so lack coherent findings that could benefit future work.\nThe results from the GLM seem inconclusive on the relationship between misinformation and narrative. As noted, there is a positive relationship for misinfo x narrative for ANTiVax but not CMU-MisCov19. The paper says that this indicates that false narratives about getting the vaccine get more engagement than other misinformation narratives (lines 433-435). Examples of those other types of misinformation narratives would be informative. I assume false narratives about getting the vaccine would be narratives about people getting the vaccine and then terrible things happening to them? Examples here would be valuable as well.\nEffect sizes or something else may help readers interpret the strength of these relationships, which is currently lacking. There is a positive relationship for both datasets for narrative x misinfo x follower count, but with very small coefficient values. The range of coefficient values varies dramatically overall (the -3.5 coefficient for misinfo x narrative for CMU-MisCov19 has a particularly large absolute value, which may be worth talking about).\nA more in-depth discussion of the results is needed to properly interpret and contextualize all of the results given. For example, it seems counterintuitive and interesting that there is more narrativity with informed tweets (if I'm reading section 5.1 right). What might be evidence from the tweets to explain this? Are people just sharing stories of getting the vaccine? In general, more interpretation of main themes coming from results in each subsection of section 5 would be helpful.\nThe LIWC analysis in section 5.3.3 seems to yield unsurprising results when it comes to what terms narratives would use more. More informative would be differences between narrative info and narrative misinfo, but the only difference detected is a focus on death terms. Unsurprising results are okay in general, and maybe the focus on death can contribute to what is not known about COVID misinformation (it is perhaps interesting that the informed tweets are not talking about avoiding death through taking the vaccine, for example), but more interpretation and discussion would be needed. The narrative arc analysis in section 5.3.2 doesn't seem to add any value, since as noted, tweets just seem too short for this tool to work properly and it seems like most of the results are statistically insignificant.\n\ntypos_grammar_style_and_presentation_improvements: - What does the Hydrated % row refer to in Table 1?\n- Figure 1 text is too small to easily read, other figures like Fig 2 and tables too - Line 338 , instead of .\n- Line 417: model -> models - Line 632: authentic used -> authentic, used\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "WBYXSNHZjo",
        "length": 326,
        "human_text": "paper_topic_and_main_contributions: This paper explores the relationship between narrative communication and health misinformation on social media, focusing on Twitter. The authors manually labeled a subset of tweets from misinformation datasets to identify narratives and then used the best model (their case RoBERTa) to extend these labels to the entire datasets. They found that narrativity can increase engagement with misinformation, especially for users with many followers.\n\nreasons_to_accept: The analysis part of the paper where the authors used statistical modeling to find out the relationship among narrativity, misinformation, follower count, and user engagement.\n\nreasons_to_reject: 1. Line 327, 14.5k tweets were labeled by the annotators and the best RoBERTa model. If the annotators did 3k annotation,  It is not clear how the rest of the tweets are labeled by the model. Does it mean around 11.5k tweets were used as test data?\n2. As data annotation was mentioned as one of the contributions, the authors could show some examples of annotated data and annotation procedures.\n3. Authors could include a thematic analysis of the data.\n\nmissing_references: Authors should explore the stance, morality, and reason detection task on COVID-19 by Pacheco et al 2022 (A holistic framework for analyzing the covid-19 vaccine debate).  Also for COVID-19 moral foundation and theme, authors could look into work by Islam and Goldwasser 2022 (Understanding COVID-19 Vaccine Campaign on Facebook using Minimal Supervision).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "KoZECf8FdT",
        "length": 289,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors annotate an existing misinformation tweet dataset for the presence of narratives. The authors selected two existing medical datasets. They then followed the annotation process through an iterative process involving discussion and revision. They first annotated some samples, and then the rest of the samples were automatically annotated using machine learning techniques.\nThe conclusions that the authors made are: 1. narratives involve higher user engagement than misinformation 2. narratives may help increase engagement of misinformation The key contribution of the work is the dataset and the follow-up conclusions that the paper makes.\n\nreasons_to_accept: A new dataset that might help modelling user narratives online especially related to misinformation. \nThe studies that the paper presents are interesting and useful to understand the importance of user narratives.\n\nreasons_to_reject: One of the key concerns is the quality of the automatically annotated samples. How much control we have to maintain the quality of the automatically generated samples is something that needs to be mentioned in the paper. While the authors have used different machine learning models, these models have their own pros and cons. \nBesides that, how do we know that the number of instances that have been manually annotated are enough for the computational model to learn reliably?\n\nquestions_for_the_authors: Questions are mentioned above.\nThere is an assumption that the dataset would be available for free use later.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "106_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_106_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.743675,
      "max_similarity": 0.7802,
      "avg_coverage": 0.3374,
      "max_coverage": 0.4074
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 585,
      "avg_human_length": 471.5
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 8,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "umVjrSPuCP",
        "similarity": 0.7353,
        "coverage": 0.2553,
        "human_length": 808,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the combination of LLMs and CRSs in the context of product recommendations. Authors approach the topic from two viewpoints --- either \"LLMs assisting CRSs\", or \"CRSs assisting LLMs\" --- and across four sub-tasks implied in recommendation-making: intent understanding, preference elicitation, product recommendation, and response generation. Evaluation measures depend on the sub-task: precision, recall, and F1 for intent understanding and preference elicitation; accuracy, hit@5 and MRR@5 for product recommendation; and distinct@1, informativeness, and relevance for response generation. \nAuthors rely on the U-NEED dataset, which comprises 1135-1748 dialogues organized across five product categories: Beauty, Phones, Fashion, Shoes, and Eletronics. More simple baselines vary for each sub-task, but more complex baselines generally include both CRSs (variations of UniMIND) and LLMs (ChatGLM-6B and Chinese-Alpaca) in isolation, as well as the proposed methods: LLMs combined into CRSs (ChatGLM/Alpaca combined into UniMIND), or CRSs combined into LLMs (UniMIND combined into ChatGLM/Alpaca). Findings are somewhat unclear when comparing the proposed methods against the more complex baselines, with marginal gains associated to either \"LLMs assisting CRSs\" or \"CRSs assisting LLMs\" depending on the sub-task.\n\nreasons_to_accept: 1. The paper focuses on a relevant topic with direct, real-world applicability. \n2. Experiments contemplate five product categories, and include sub-tasks that are consensually important to conversational recommendations. The product recommendation sub-task has received increasing attention from the NLP community, particularly in how to leverage LLMs for the task. \n3. Interestingly, it is for the product recommendation sub-task that authors report the most promising gains (as seen in aggregated accuracy of ALLM-CCRS, Table 3). A paper focused on \"LLMs assisting CRSs\" applied to the product recommendation sub-task could lead to potentially helpful findings to the community.\n\nreasons_to_reject: 1. Since there are smaller LMs behind UniMIND (e.g., BART) and larger LMs representing LLMs, the experimental set up makes it hard to understand if the gains are due to the proposed methods or simply due to increased model capacity. Comparing the combination of LLMs + CRSs to the LLMs alone shows small gains (e.g., on Tables 1 and 4), so one conclusion can be that these marginal gains are due to the additional parameters introduced by the CRS --- in this case, would a slightly larger LLM be enough to achieve similar gains? \n2. Methodology aside, I am not sure how novel and practically helpful the findings are. Essentially, the main takeaway is that it depends on the nature of the sub-task how to combine LLMs and CRSs. This takeaway has already been contributed by various previous works, for example: a) \"What does BERT know about books, movies and music? Probing BERT for conversational recommendation\" (Penha et al., 2020) shows how LLMs can be helpful in recommendation-making; b) \"'It doesn't look good for a date': Transforming Critiques into Preferences for Conversational Recommendation Systems\" (Bursztyn et al., 2021) shows how LLMs can be helpful in preference interpretation; c) \"ReXPlug: Explainable Recommendation using Plug-and-Play Language Model\" (Hada and Shevade, 2021) shows how LLMs can be helpful in recommendation explanation. \nHowever, these previous contributions are not properly acknowledged in the paper. \n3. The paper makes too big of a claim in that it is the first to investigate the combination of LLMs + CRSs (e.g., line 14, lines 97-100). Lines 135-137 are not accurate (see #2 above). \n4. The majority of the Appendices is copied *verbatim* from \"U-NEED: A Fine-grained Dataset for User Needs-Centric E-Commerce Conversational Recommendation\" (Liu et al., 2023), even duplicated in lines 979-994.\n\nquestions_for_the_authors: 1. Given the observations in Reasons to Reject (#1), how do you view the difference in model capacity between the proposed methods and the baselines? \n2. Lines 473-474 claim that \"CRSs assisting LLMs\" can improve the \"robustness\" of the LLMs alone. Are CRSs improving LLMs' robustness or are they improving their domain knowledge?\n\nmissing_references: As described in Reasons to Reject: \"What does BERT know about books, movies and music? Probing BERT for conversational recommendation\" (Penha et al., RecSys 2020); \"'It doesn't look good for a date': Transforming Critiques into Preferences for Conversational Recommendation Systems\" (Bursztyn et al., EMNLP 2021); \"ReXPlug: Explainable Recommendation using Plug-and-Play Language Model\" (Hada and Shevade, SIGIR 2021).\n\ntypos_grammar_style_and_presentation_improvements: 1. In Table 4, Dist-1 for Chinese-Alpaca should be marked in bold --- not CCRS-ALLM. \n2. Duplicated information in lines 309-313 and lines 979-994. \n3. Verb \"guess\" used twice in lines 407 and 485 --- less colloquial verbs like \"hypothesize\" would be more appropriate. \n4. Distinct-1 should be supported by a citation.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "FaymUJy3en",
        "similarity": 0.7802,
        "coverage": 0.3421,
        "human_length": 413,
        "human_text": "paper_topic_and_main_contributions: The paper explores the collaboration of conversational recommender systems (CRSs) and large language models (LLMs) in E-commerce pre-sales dialogues. The paper proposes two methods of collaboration: CRS assisting LLM and LLM assisting CRS. The paper evaluates the effectiveness of these methods on a real-world dataset of pre-sales dialogues, involving four tasks: dialogue understanding, needs elicitation, recommendation, and dialogue generation. The paper shows that the collaboration of CRS and LLM can improve the performance of these tasks.\n\nreasons_to_accept: 1. Using LLMs to assist in recommendation tasks is an interesting problem. \n2. The authors' proposal to combine LLMs with traditional CRS models is well-motivated.\n\nreasons_to_reject: 1. The paper's writing needs improvement. Four different tasks are defined in the paper, but the details of each task are not clearly explained. For specifics, see the Typos, Grammar, Style, and Presentation Improvements section. \n2. The proposed method only achieves improvements on some specific CRS tasks. \n3. There is a lack of explanation for instances where no improvement is observed. For example, in line 406, \"In contrast, the performance in task 1 across all categories declines.\" \n4. The authors only conducted experiments on a single dataset, which could be a limitation. \n5. No analysis of computational overhead is provided.\n\ntypos_grammar_style_and_presentation_improvements: The paper's writing and the content of the corresponding figures need revision to make them easier to understand. For example, 1. Section 3.3 does not introduce each task in multi-task training, and there is no specific explanation or example for the design of prompts. Figure 3 and the Figures in the Appendix are also difficult to understand. \n2. The variable name definitions in Section 3.3 are problematic. $X_S$ is defined but not used. $X_R$ and $Z_R$ are not defined. The user needs-based recommendation task corresponding to Eq. 2 and Eq. 3 is not defined. Section 3.5 has similar issues. \n3. In the experimental analysis section, the author uses tasks 1, 2, 3, and 4 to represent the tasks. Using task in Table 1, 2, 3, 4 or directly using the task name are more suitable.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "Lq9nCsXTw3",
        "similarity": 0.7363,
        "coverage": 0.4074,
        "human_length": 348,
        "human_text": "paper_topic_and_main_contributions: This work proposes to combine a large language model (LLM) and a conversational recommender system (CRS) for precise recommendation. Experiments show the effectiveness of this kind of collaboration.\n\nreasons_to_accept: 1. The authors explore a novel setting in conversational recommendation, i.e., LLM collaborating with CRS.\n\nreasons_to_reject: 1. The motivation for using CRSs for recommendation is not clear. The authors should justify why it is not sufficient to fine-tune an LLM on the recommendation task. The setting **no collaboration** in Table 1,2,3,4 shows that the proposed method has only a marginal advantage over the single LLM, despite the incorporation of the collaboration mechanism. \n2. The combination of the CRSs and the LLM is kind of simple and ad hoc. The authors should explain whether there are any alternatives or ablations to the proposed interaction mechanism. \n3. The term **task 3** in Line 401 is undefined and confusing. The authors should clarify what it refers to and how it relates to the previous tasks or sections of the paper. \n4. It might be inappropriate to move part of the analysis of the **main** result into the appendix. The authors should include the most important findings and insights in the main paper. Moreover, the figure/table referred in Appendix A is missing or mislabeled. \n5. The writing of the experiment section is messy and hard to follow. The authors should improve the clarity and coherence of their presentation, and avoid grammatical errors and typos.\n\nquestions_for_the_authors: Please refer to **Reasons To Reject**.\n\nmissing_references: None.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "UEu3QA18iD",
        "similarity": 0.7229,
        "coverage": 0.3448,
        "human_length": 317,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors study the effectiveness of combing large language model with conversational recommendation system in e-commerce pre-sales dialogues. They propose two collaboration methods, i.e., conversational recommender systems assisting large language model and large language model assisting conversational recommender systems. The authors have performed extensive experiments on real datasets to demonstrate the effectiveness of the proposed methods.\n\nreasons_to_accept: 1. The authors propose to study the combination of large language model and conversational recommendation system in pre-sales dialogue in e-commerce. This idea seems novel.\n2. The authors propose two collaboration methods: 1) conversational recommendation system assisting large language model, and 2) large language model assisting conversational recommendation system.\n3. The authors have performed extensive experiments on a real-world pre-sale dialogue dataset in e-commerce.\n4. This paper is well-structured and clearly written.\n\nreasons_to_reject: 1. The main objective of this work is to explore the integration of conversational recommendation system with large language model in e-commerce pre-sale dialogues. However, in the proposed models, the specific properties of e-commerce pre-sales dialogues are not explored.\n2. According to my understanding, the proposed two collaboration models can also be applied to other conversational recommendation scenarios, e.g., movie recommendation. However, in this work, the authors only study the performance of the proposed methods on one e-commerce pre-sale dialogue dataset. Thus, the experimental analysis is not sufficient.\n\nquestions_for_the_authors: 1. What are the specific properties of e-commerce pre-sale dialogues have been explored by the proposed model?\n2. Why using different evaluation metrics in Table 1, 2, and 3?\n\nmissing_references: N.A.\n\ntypos_grammar_style_and_presentation_improvements: N.A.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "umVjrSPuCP",
        "length": 808,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the combination of LLMs and CRSs in the context of product recommendations. Authors approach the topic from two viewpoints --- either \"LLMs assisting CRSs\", or \"CRSs assisting LLMs\" --- and across four sub-tasks implied in recommendation-making: intent understanding, preference elicitation, product recommendation, and response generation. Evaluation measures depend on the sub-task: precision, recall, and F1 for intent understanding and preference elicitation; accuracy, hit@5 and MRR@5 for product recommendation; and distinct@1, informativeness, and relevance for response generation. \nAuthors rely on the U-NEED dataset, which comprises 1135-1748 dialogues organized across five product categories: Beauty, Phones, Fashion, Shoes, and Eletronics. More simple baselines vary for each sub-task, but more complex baselines generally include both CRSs (variations of UniMIND) and LLMs (ChatGLM-6B and Chinese-Alpaca) in isolation, as well as the proposed methods: LLMs combined into CRSs (ChatGLM/Alpaca combined into UniMIND), or CRSs combined into LLMs (UniMIND combined into ChatGLM/Alpaca). Findings are somewhat unclear when comparing the proposed methods against the more complex baselines, with marginal gains associated to either \"LLMs assisting CRSs\" or \"CRSs assisting LLMs\" depending on the sub-task.\n\nreasons_to_accept: 1. The paper focuses on a relevant topic with direct, real-world applicability. \n2. Experiments contemplate five product categories, and include sub-tasks that are consensually important to conversational recommendations. The product recommendation sub-task has received increasing attention from the NLP community, particularly in how to leverage LLMs for the task. \n3. Interestingly, it is for the product recommendation sub-task that authors report the most promising gains (as seen in aggregated accuracy of ALLM-CCRS, Table 3). A paper focused on \"LLMs assisting CRSs\" applied to the product recommendation sub-task could lead to potentially helpful findings to the community.\n\nreasons_to_reject: 1. Since there are smaller LMs behind UniMIND (e.g., BART) and larger LMs representing LLMs, the experimental set up makes it hard to understand if the gains are due to the proposed methods or simply due to increased model capacity. Comparing the combination of LLMs + CRSs to the LLMs alone shows small gains (e.g., on Tables 1 and 4), so one conclusion can be that these marginal gains are due to the additional parameters introduced by the CRS --- in this case, would a slightly larger LLM be enough to achieve similar gains? \n2. Methodology aside, I am not sure how novel and practically helpful the findings are. Essentially, the main takeaway is that it depends on the nature of the sub-task how to combine LLMs and CRSs. This takeaway has already been contributed by various previous works, for example: a) \"What does BERT know about books, movies and music? Probing BERT for conversational recommendation\" (Penha et al., 2020) shows how LLMs can be helpful in recommendation-making; b) \"'It doesn't look good for a date': Transforming Critiques into Preferences for Conversational Recommendation Systems\" (Bursztyn et al., 2021) shows how LLMs can be helpful in preference interpretation; c) \"ReXPlug: Explainable Recommendation using Plug-and-Play Language Model\" (Hada and Shevade, 2021) shows how LLMs can be helpful in recommendation explanation. \nHowever, these previous contributions are not properly acknowledged in the paper. \n3. The paper makes too big of a claim in that it is the first to investigate the combination of LLMs + CRSs (e.g., line 14, lines 97-100). Lines 135-137 are not accurate (see #2 above). \n4. The majority of the Appendices is copied *verbatim* from \"U-NEED: A Fine-grained Dataset for User Needs-Centric E-Commerce Conversational Recommendation\" (Liu et al., 2023), even duplicated in lines 979-994.\n\nquestions_for_the_authors: 1. Given the observations in Reasons to Reject (#1), how do you view the difference in model capacity between the proposed methods and the baselines? \n2. Lines 473-474 claim that \"CRSs assisting LLMs\" can improve the \"robustness\" of the LLMs alone. Are CRSs improving LLMs' robustness or are they improving their domain knowledge?\n\nmissing_references: As described in Reasons to Reject: \"What does BERT know about books, movies and music? Probing BERT for conversational recommendation\" (Penha et al., RecSys 2020); \"'It doesn't look good for a date': Transforming Critiques into Preferences for Conversational Recommendation Systems\" (Bursztyn et al., EMNLP 2021); \"ReXPlug: Explainable Recommendation using Plug-and-Play Language Model\" (Hada and Shevade, SIGIR 2021).\n\ntypos_grammar_style_and_presentation_improvements: 1. In Table 4, Dist-1 for Chinese-Alpaca should be marked in bold --- not CCRS-ALLM. \n2. Duplicated information in lines 309-313 and lines 979-994. \n3. Verb \"guess\" used twice in lines 407 and 485 --- less colloquial verbs like \"hypothesize\" would be more appropriate. \n4. Distinct-1 should be supported by a citation.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "FaymUJy3en",
        "length": 413,
        "human_text": "paper_topic_and_main_contributions: The paper explores the collaboration of conversational recommender systems (CRSs) and large language models (LLMs) in E-commerce pre-sales dialogues. The paper proposes two methods of collaboration: CRS assisting LLM and LLM assisting CRS. The paper evaluates the effectiveness of these methods on a real-world dataset of pre-sales dialogues, involving four tasks: dialogue understanding, needs elicitation, recommendation, and dialogue generation. The paper shows that the collaboration of CRS and LLM can improve the performance of these tasks.\n\nreasons_to_accept: 1. Using LLMs to assist in recommendation tasks is an interesting problem. \n2. The authors' proposal to combine LLMs with traditional CRS models is well-motivated.\n\nreasons_to_reject: 1. The paper's writing needs improvement. Four different tasks are defined in the paper, but the details of each task are not clearly explained. For specifics, see the Typos, Grammar, Style, and Presentation Improvements section. \n2. The proposed method only achieves improvements on some specific CRS tasks. \n3. There is a lack of explanation for instances where no improvement is observed. For example, in line 406, \"In contrast, the performance in task 1 across all categories declines.\" \n4. The authors only conducted experiments on a single dataset, which could be a limitation. \n5. No analysis of computational overhead is provided.\n\ntypos_grammar_style_and_presentation_improvements: The paper's writing and the content of the corresponding figures need revision to make them easier to understand. For example, 1. Section 3.3 does not introduce each task in multi-task training, and there is no specific explanation or example for the design of prompts. Figure 3 and the Figures in the Appendix are also difficult to understand. \n2. The variable name definitions in Section 3.3 are problematic. $X_S$ is defined but not used. $X_R$ and $Z_R$ are not defined. The user needs-based recommendation task corresponding to Eq. 2 and Eq. 3 is not defined. Section 3.5 has similar issues. \n3. In the experimental analysis section, the author uses tasks 1, 2, 3, and 4 to represent the tasks. Using task in Table 1, 2, 3, 4 or directly using the task name are more suitable.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "Lq9nCsXTw3",
        "length": 348,
        "human_text": "paper_topic_and_main_contributions: This work proposes to combine a large language model (LLM) and a conversational recommender system (CRS) for precise recommendation. Experiments show the effectiveness of this kind of collaboration.\n\nreasons_to_accept: 1. The authors explore a novel setting in conversational recommendation, i.e., LLM collaborating with CRS.\n\nreasons_to_reject: 1. The motivation for using CRSs for recommendation is not clear. The authors should justify why it is not sufficient to fine-tune an LLM on the recommendation task. The setting **no collaboration** in Table 1,2,3,4 shows that the proposed method has only a marginal advantage over the single LLM, despite the incorporation of the collaboration mechanism. \n2. The combination of the CRSs and the LLM is kind of simple and ad hoc. The authors should explain whether there are any alternatives or ablations to the proposed interaction mechanism. \n3. The term **task 3** in Line 401 is undefined and confusing. The authors should clarify what it refers to and how it relates to the previous tasks or sections of the paper. \n4. It might be inappropriate to move part of the analysis of the **main** result into the appendix. The authors should include the most important findings and insights in the main paper. Moreover, the figure/table referred in Appendix A is missing or mislabeled. \n5. The writing of the experiment section is messy and hard to follow. The authors should improve the clarity and coherence of their presentation, and avoid grammatical errors and typos.\n\nquestions_for_the_authors: Please refer to **Reasons To Reject**.\n\nmissing_references: None.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "UEu3QA18iD",
        "length": 317,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors study the effectiveness of combing large language model with conversational recommendation system in e-commerce pre-sales dialogues. They propose two collaboration methods, i.e., conversational recommender systems assisting large language model and large language model assisting conversational recommender systems. The authors have performed extensive experiments on real datasets to demonstrate the effectiveness of the proposed methods.\n\nreasons_to_accept: 1. The authors propose to study the combination of large language model and conversational recommendation system in pre-sales dialogue in e-commerce. This idea seems novel.\n2. The authors propose two collaboration methods: 1) conversational recommendation system assisting large language model, and 2) large language model assisting conversational recommendation system.\n3. The authors have performed extensive experiments on a real-world pre-sale dialogue dataset in e-commerce.\n4. This paper is well-structured and clearly written.\n\nreasons_to_reject: 1. The main objective of this work is to explore the integration of conversational recommendation system with large language model in e-commerce pre-sale dialogues. However, in the proposed models, the specific properties of e-commerce pre-sales dialogues are not explored.\n2. According to my understanding, the proposed two collaboration models can also be applied to other conversational recommendation scenarios, e.g., movie recommendation. However, in this work, the authors only study the performance of the proposed methods on one e-commerce pre-sale dialogue dataset. Thus, the experimental analysis is not sufficient.\n\nquestions_for_the_authors: 1. What are the specific properties of e-commerce pre-sale dialogues have been explored by the proposed model?\n2. Why using different evaluation metrics in Table 1, 2, and 3?\n\nmissing_references: N.A.\n\ntypos_grammar_style_and_presentation_improvements: N.A.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "106_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_106_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.7499,
      "max_similarity": 0.7844,
      "avg_coverage": 0.352775,
      "max_coverage": 0.4444
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 650,
      "avg_human_length": 471.5
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 7,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "umVjrSPuCP",
        "similarity": 0.7444,
        "coverage": 0.2979,
        "human_length": 808,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the combination of LLMs and CRSs in the context of product recommendations. Authors approach the topic from two viewpoints --- either \"LLMs assisting CRSs\", or \"CRSs assisting LLMs\" --- and across four sub-tasks implied in recommendation-making: intent understanding, preference elicitation, product recommendation, and response generation. Evaluation measures depend on the sub-task: precision, recall, and F1 for intent understanding and preference elicitation; accuracy, hit@5 and MRR@5 for product recommendation; and distinct@1, informativeness, and relevance for response generation. \nAuthors rely on the U-NEED dataset, which comprises 1135-1748 dialogues organized across five product categories: Beauty, Phones, Fashion, Shoes, and Eletronics. More simple baselines vary for each sub-task, but more complex baselines generally include both CRSs (variations of UniMIND) and LLMs (ChatGLM-6B and Chinese-Alpaca) in isolation, as well as the proposed methods: LLMs combined into CRSs (ChatGLM/Alpaca combined into UniMIND), or CRSs combined into LLMs (UniMIND combined into ChatGLM/Alpaca). Findings are somewhat unclear when comparing the proposed methods against the more complex baselines, with marginal gains associated to either \"LLMs assisting CRSs\" or \"CRSs assisting LLMs\" depending on the sub-task.\n\nreasons_to_accept: 1. The paper focuses on a relevant topic with direct, real-world applicability. \n2. Experiments contemplate five product categories, and include sub-tasks that are consensually important to conversational recommendations. The product recommendation sub-task has received increasing attention from the NLP community, particularly in how to leverage LLMs for the task. \n3. Interestingly, it is for the product recommendation sub-task that authors report the most promising gains (as seen in aggregated accuracy of ALLM-CCRS, Table 3). A paper focused on \"LLMs assisting CRSs\" applied to the product recommendation sub-task could lead to potentially helpful findings to the community.\n\nreasons_to_reject: 1. Since there are smaller LMs behind UniMIND (e.g., BART) and larger LMs representing LLMs, the experimental set up makes it hard to understand if the gains are due to the proposed methods or simply due to increased model capacity. Comparing the combination of LLMs + CRSs to the LLMs alone shows small gains (e.g., on Tables 1 and 4), so one conclusion can be that these marginal gains are due to the additional parameters introduced by the CRS --- in this case, would a slightly larger LLM be enough to achieve similar gains? \n2. Methodology aside, I am not sure how novel and practically helpful the findings are. Essentially, the main takeaway is that it depends on the nature of the sub-task how to combine LLMs and CRSs. This takeaway has already been contributed by various previous works, for example: a) \"What does BERT know about books, movies and music? Probing BERT for conversational recommendation\" (Penha et al., 2020) shows how LLMs can be helpful in recommendation-making; b) \"'It doesn't look good for a date': Transforming Critiques into Preferences for Conversational Recommendation Systems\" (Bursztyn et al., 2021) shows how LLMs can be helpful in preference interpretation; c) \"ReXPlug: Explainable Recommendation using Plug-and-Play Language Model\" (Hada and Shevade, 2021) shows how LLMs can be helpful in recommendation explanation. \nHowever, these previous contributions are not properly acknowledged in the paper. \n3. The paper makes too big of a claim in that it is the first to investigate the combination of LLMs + CRSs (e.g., line 14, lines 97-100). Lines 135-137 are not accurate (see #2 above). \n4. The majority of the Appendices is copied *verbatim* from \"U-NEED: A Fine-grained Dataset for User Needs-Centric E-Commerce Conversational Recommendation\" (Liu et al., 2023), even duplicated in lines 979-994.\n\nquestions_for_the_authors: 1. Given the observations in Reasons to Reject (#1), how do you view the difference in model capacity between the proposed methods and the baselines? \n2. Lines 473-474 claim that \"CRSs assisting LLMs\" can improve the \"robustness\" of the LLMs alone. Are CRSs improving LLMs' robustness or are they improving their domain knowledge?\n\nmissing_references: As described in Reasons to Reject: \"What does BERT know about books, movies and music? Probing BERT for conversational recommendation\" (Penha et al., RecSys 2020); \"'It doesn't look good for a date': Transforming Critiques into Preferences for Conversational Recommendation Systems\" (Bursztyn et al., EMNLP 2021); \"ReXPlug: Explainable Recommendation using Plug-and-Play Language Model\" (Hada and Shevade, SIGIR 2021).\n\ntypos_grammar_style_and_presentation_improvements: 1. In Table 4, Dist-1 for Chinese-Alpaca should be marked in bold --- not CCRS-ALLM. \n2. Duplicated information in lines 309-313 and lines 979-994. \n3. Verb \"guess\" used twice in lines 407 and 485 --- less colloquial verbs like \"hypothesize\" would be more appropriate. \n4. Distinct-1 should be supported by a citation.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "FaymUJy3en",
        "similarity": 0.7844,
        "coverage": 0.2895,
        "human_length": 413,
        "human_text": "paper_topic_and_main_contributions: The paper explores the collaboration of conversational recommender systems (CRSs) and large language models (LLMs) in E-commerce pre-sales dialogues. The paper proposes two methods of collaboration: CRS assisting LLM and LLM assisting CRS. The paper evaluates the effectiveness of these methods on a real-world dataset of pre-sales dialogues, involving four tasks: dialogue understanding, needs elicitation, recommendation, and dialogue generation. The paper shows that the collaboration of CRS and LLM can improve the performance of these tasks.\n\nreasons_to_accept: 1. Using LLMs to assist in recommendation tasks is an interesting problem. \n2. The authors' proposal to combine LLMs with traditional CRS models is well-motivated.\n\nreasons_to_reject: 1. The paper's writing needs improvement. Four different tasks are defined in the paper, but the details of each task are not clearly explained. For specifics, see the Typos, Grammar, Style, and Presentation Improvements section. \n2. The proposed method only achieves improvements on some specific CRS tasks. \n3. There is a lack of explanation for instances where no improvement is observed. For example, in line 406, \"In contrast, the performance in task 1 across all categories declines.\" \n4. The authors only conducted experiments on a single dataset, which could be a limitation. \n5. No analysis of computational overhead is provided.\n\ntypos_grammar_style_and_presentation_improvements: The paper's writing and the content of the corresponding figures need revision to make them easier to understand. For example, 1. Section 3.3 does not introduce each task in multi-task training, and there is no specific explanation or example for the design of prompts. Figure 3 and the Figures in the Appendix are also difficult to understand. \n2. The variable name definitions in Section 3.3 are problematic. $X_S$ is defined but not used. $X_R$ and $Z_R$ are not defined. The user needs-based recommendation task corresponding to Eq. 2 and Eq. 3 is not defined. Section 3.5 has similar issues. \n3. In the experimental analysis section, the author uses tasks 1, 2, 3, and 4 to represent the tasks. Using task in Table 1, 2, 3, 4 or directly using the task name are more suitable.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "Lq9nCsXTw3",
        "similarity": 0.7481,
        "coverage": 0.4444,
        "human_length": 348,
        "human_text": "paper_topic_and_main_contributions: This work proposes to combine a large language model (LLM) and a conversational recommender system (CRS) for precise recommendation. Experiments show the effectiveness of this kind of collaboration.\n\nreasons_to_accept: 1. The authors explore a novel setting in conversational recommendation, i.e., LLM collaborating with CRS.\n\nreasons_to_reject: 1. The motivation for using CRSs for recommendation is not clear. The authors should justify why it is not sufficient to fine-tune an LLM on the recommendation task. The setting **no collaboration** in Table 1,2,3,4 shows that the proposed method has only a marginal advantage over the single LLM, despite the incorporation of the collaboration mechanism. \n2. The combination of the CRSs and the LLM is kind of simple and ad hoc. The authors should explain whether there are any alternatives or ablations to the proposed interaction mechanism. \n3. The term **task 3** in Line 401 is undefined and confusing. The authors should clarify what it refers to and how it relates to the previous tasks or sections of the paper. \n4. It might be inappropriate to move part of the analysis of the **main** result into the appendix. The authors should include the most important findings and insights in the main paper. Moreover, the figure/table referred in Appendix A is missing or mislabeled. \n5. The writing of the experiment section is messy and hard to follow. The authors should improve the clarity and coherence of their presentation, and avoid grammatical errors and typos.\n\nquestions_for_the_authors: Please refer to **Reasons To Reject**.\n\nmissing_references: None.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "UEu3QA18iD",
        "similarity": 0.7227,
        "coverage": 0.3793,
        "human_length": 317,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors study the effectiveness of combing large language model with conversational recommendation system in e-commerce pre-sales dialogues. They propose two collaboration methods, i.e., conversational recommender systems assisting large language model and large language model assisting conversational recommender systems. The authors have performed extensive experiments on real datasets to demonstrate the effectiveness of the proposed methods.\n\nreasons_to_accept: 1. The authors propose to study the combination of large language model and conversational recommendation system in pre-sales dialogue in e-commerce. This idea seems novel.\n2. The authors propose two collaboration methods: 1) conversational recommendation system assisting large language model, and 2) large language model assisting conversational recommendation system.\n3. The authors have performed extensive experiments on a real-world pre-sale dialogue dataset in e-commerce.\n4. This paper is well-structured and clearly written.\n\nreasons_to_reject: 1. The main objective of this work is to explore the integration of conversational recommendation system with large language model in e-commerce pre-sale dialogues. However, in the proposed models, the specific properties of e-commerce pre-sales dialogues are not explored.\n2. According to my understanding, the proposed two collaboration models can also be applied to other conversational recommendation scenarios, e.g., movie recommendation. However, in this work, the authors only study the performance of the proposed methods on one e-commerce pre-sale dialogue dataset. Thus, the experimental analysis is not sufficient.\n\nquestions_for_the_authors: 1. What are the specific properties of e-commerce pre-sale dialogues have been explored by the proposed model?\n2. Why using different evaluation metrics in Table 1, 2, and 3?\n\nmissing_references: N.A.\n\ntypos_grammar_style_and_presentation_improvements: N.A.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "umVjrSPuCP",
        "length": 808,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the combination of LLMs and CRSs in the context of product recommendations. Authors approach the topic from two viewpoints --- either \"LLMs assisting CRSs\", or \"CRSs assisting LLMs\" --- and across four sub-tasks implied in recommendation-making: intent understanding, preference elicitation, product recommendation, and response generation. Evaluation measures depend on the sub-task: precision, recall, and F1 for intent understanding and preference elicitation; accuracy, hit@5 and MRR@5 for product recommendation; and distinct@1, informativeness, and relevance for response generation. \nAuthors rely on the U-NEED dataset, which comprises 1135-1748 dialogues organized across five product categories: Beauty, Phones, Fashion, Shoes, and Eletronics. More simple baselines vary for each sub-task, but more complex baselines generally include both CRSs (variations of UniMIND) and LLMs (ChatGLM-6B and Chinese-Alpaca) in isolation, as well as the proposed methods: LLMs combined into CRSs (ChatGLM/Alpaca combined into UniMIND), or CRSs combined into LLMs (UniMIND combined into ChatGLM/Alpaca). Findings are somewhat unclear when comparing the proposed methods against the more complex baselines, with marginal gains associated to either \"LLMs assisting CRSs\" or \"CRSs assisting LLMs\" depending on the sub-task.\n\nreasons_to_accept: 1. The paper focuses on a relevant topic with direct, real-world applicability. \n2. Experiments contemplate five product categories, and include sub-tasks that are consensually important to conversational recommendations. The product recommendation sub-task has received increasing attention from the NLP community, particularly in how to leverage LLMs for the task. \n3. Interestingly, it is for the product recommendation sub-task that authors report the most promising gains (as seen in aggregated accuracy of ALLM-CCRS, Table 3). A paper focused on \"LLMs assisting CRSs\" applied to the product recommendation sub-task could lead to potentially helpful findings to the community.\n\nreasons_to_reject: 1. Since there are smaller LMs behind UniMIND (e.g., BART) and larger LMs representing LLMs, the experimental set up makes it hard to understand if the gains are due to the proposed methods or simply due to increased model capacity. Comparing the combination of LLMs + CRSs to the LLMs alone shows small gains (e.g., on Tables 1 and 4), so one conclusion can be that these marginal gains are due to the additional parameters introduced by the CRS --- in this case, would a slightly larger LLM be enough to achieve similar gains? \n2. Methodology aside, I am not sure how novel and practically helpful the findings are. Essentially, the main takeaway is that it depends on the nature of the sub-task how to combine LLMs and CRSs. This takeaway has already been contributed by various previous works, for example: a) \"What does BERT know about books, movies and music? Probing BERT for conversational recommendation\" (Penha et al., 2020) shows how LLMs can be helpful in recommendation-making; b) \"'It doesn't look good for a date': Transforming Critiques into Preferences for Conversational Recommendation Systems\" (Bursztyn et al., 2021) shows how LLMs can be helpful in preference interpretation; c) \"ReXPlug: Explainable Recommendation using Plug-and-Play Language Model\" (Hada and Shevade, 2021) shows how LLMs can be helpful in recommendation explanation. \nHowever, these previous contributions are not properly acknowledged in the paper. \n3. The paper makes too big of a claim in that it is the first to investigate the combination of LLMs + CRSs (e.g., line 14, lines 97-100). Lines 135-137 are not accurate (see #2 above). \n4. The majority of the Appendices is copied *verbatim* from \"U-NEED: A Fine-grained Dataset for User Needs-Centric E-Commerce Conversational Recommendation\" (Liu et al., 2023), even duplicated in lines 979-994.\n\nquestions_for_the_authors: 1. Given the observations in Reasons to Reject (#1), how do you view the difference in model capacity between the proposed methods and the baselines? \n2. Lines 473-474 claim that \"CRSs assisting LLMs\" can improve the \"robustness\" of the LLMs alone. Are CRSs improving LLMs' robustness or are they improving their domain knowledge?\n\nmissing_references: As described in Reasons to Reject: \"What does BERT know about books, movies and music? Probing BERT for conversational recommendation\" (Penha et al., RecSys 2020); \"'It doesn't look good for a date': Transforming Critiques into Preferences for Conversational Recommendation Systems\" (Bursztyn et al., EMNLP 2021); \"ReXPlug: Explainable Recommendation using Plug-and-Play Language Model\" (Hada and Shevade, SIGIR 2021).\n\ntypos_grammar_style_and_presentation_improvements: 1. In Table 4, Dist-1 for Chinese-Alpaca should be marked in bold --- not CCRS-ALLM. \n2. Duplicated information in lines 309-313 and lines 979-994. \n3. Verb \"guess\" used twice in lines 407 and 485 --- less colloquial verbs like \"hypothesize\" would be more appropriate. \n4. Distinct-1 should be supported by a citation.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "FaymUJy3en",
        "length": 413,
        "human_text": "paper_topic_and_main_contributions: The paper explores the collaboration of conversational recommender systems (CRSs) and large language models (LLMs) in E-commerce pre-sales dialogues. The paper proposes two methods of collaboration: CRS assisting LLM and LLM assisting CRS. The paper evaluates the effectiveness of these methods on a real-world dataset of pre-sales dialogues, involving four tasks: dialogue understanding, needs elicitation, recommendation, and dialogue generation. The paper shows that the collaboration of CRS and LLM can improve the performance of these tasks.\n\nreasons_to_accept: 1. Using LLMs to assist in recommendation tasks is an interesting problem. \n2. The authors' proposal to combine LLMs with traditional CRS models is well-motivated.\n\nreasons_to_reject: 1. The paper's writing needs improvement. Four different tasks are defined in the paper, but the details of each task are not clearly explained. For specifics, see the Typos, Grammar, Style, and Presentation Improvements section. \n2. The proposed method only achieves improvements on some specific CRS tasks. \n3. There is a lack of explanation for instances where no improvement is observed. For example, in line 406, \"In contrast, the performance in task 1 across all categories declines.\" \n4. The authors only conducted experiments on a single dataset, which could be a limitation. \n5. No analysis of computational overhead is provided.\n\ntypos_grammar_style_and_presentation_improvements: The paper's writing and the content of the corresponding figures need revision to make them easier to understand. For example, 1. Section 3.3 does not introduce each task in multi-task training, and there is no specific explanation or example for the design of prompts. Figure 3 and the Figures in the Appendix are also difficult to understand. \n2. The variable name definitions in Section 3.3 are problematic. $X_S$ is defined but not used. $X_R$ and $Z_R$ are not defined. The user needs-based recommendation task corresponding to Eq. 2 and Eq. 3 is not defined. Section 3.5 has similar issues. \n3. In the experimental analysis section, the author uses tasks 1, 2, 3, and 4 to represent the tasks. Using task in Table 1, 2, 3, 4 or directly using the task name are more suitable.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "Lq9nCsXTw3",
        "length": 348,
        "human_text": "paper_topic_and_main_contributions: This work proposes to combine a large language model (LLM) and a conversational recommender system (CRS) for precise recommendation. Experiments show the effectiveness of this kind of collaboration.\n\nreasons_to_accept: 1. The authors explore a novel setting in conversational recommendation, i.e., LLM collaborating with CRS.\n\nreasons_to_reject: 1. The motivation for using CRSs for recommendation is not clear. The authors should justify why it is not sufficient to fine-tune an LLM on the recommendation task. The setting **no collaboration** in Table 1,2,3,4 shows that the proposed method has only a marginal advantage over the single LLM, despite the incorporation of the collaboration mechanism. \n2. The combination of the CRSs and the LLM is kind of simple and ad hoc. The authors should explain whether there are any alternatives or ablations to the proposed interaction mechanism. \n3. The term **task 3** in Line 401 is undefined and confusing. The authors should clarify what it refers to and how it relates to the previous tasks or sections of the paper. \n4. It might be inappropriate to move part of the analysis of the **main** result into the appendix. The authors should include the most important findings and insights in the main paper. Moreover, the figure/table referred in Appendix A is missing or mislabeled. \n5. The writing of the experiment section is messy and hard to follow. The authors should improve the clarity and coherence of their presentation, and avoid grammatical errors and typos.\n\nquestions_for_the_authors: Please refer to **Reasons To Reject**.\n\nmissing_references: None.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "UEu3QA18iD",
        "length": 317,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors study the effectiveness of combing large language model with conversational recommendation system in e-commerce pre-sales dialogues. They propose two collaboration methods, i.e., conversational recommender systems assisting large language model and large language model assisting conversational recommender systems. The authors have performed extensive experiments on real datasets to demonstrate the effectiveness of the proposed methods.\n\nreasons_to_accept: 1. The authors propose to study the combination of large language model and conversational recommendation system in pre-sales dialogue in e-commerce. This idea seems novel.\n2. The authors propose two collaboration methods: 1) conversational recommendation system assisting large language model, and 2) large language model assisting conversational recommendation system.\n3. The authors have performed extensive experiments on a real-world pre-sale dialogue dataset in e-commerce.\n4. This paper is well-structured and clearly written.\n\nreasons_to_reject: 1. The main objective of this work is to explore the integration of conversational recommendation system with large language model in e-commerce pre-sale dialogues. However, in the proposed models, the specific properties of e-commerce pre-sales dialogues are not explored.\n2. According to my understanding, the proposed two collaboration models can also be applied to other conversational recommendation scenarios, e.g., movie recommendation. However, in this work, the authors only study the performance of the proposed methods on one e-commerce pre-sale dialogue dataset. Thus, the experimental analysis is not sufficient.\n\nquestions_for_the_authors: 1. What are the specific properties of e-commerce pre-sale dialogues have been explored by the proposed model?\n2. Why using different evaluation metrics in Table 1, 2, and 3?\n\nmissing_references: N.A.\n\ntypos_grammar_style_and_presentation_improvements: N.A.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "53_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_53_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7077666666666667,
      "max_similarity": 0.7176,
      "avg_coverage": 0.5222666666666667,
      "max_coverage": 0.6667
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 701,
      "avg_human_length": 354.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 13,
      "suggestions_count": 9
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "34uvjucb98",
        "similarity": 0.7176,
        "coverage": 0.4815,
        "human_length": 265,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a normal-abnormal decoupling memory network for medical report generation. The proposed approach optimizes the visual extraction network only based on anomaly semantics but not normal semantics. The experiments show that the proposed method could outperform baselines.\n\nreasons_to_accept: 1. It is interesting to separately model normal and abnormal reports. \n2. The proposed approach could outperform baselines on the benchmark MIMIC-CXR dataset. \n3. The ablation study shows that each of the component could help improve the performance.\n\nreasons_to_reject: 1. The abnormal mode memory part is not quite clear, which is difficult to interpret. \n2. The proposed method is somewhat incremental. \n3. This paper only includes results of one dataset in the main text.\n\nmissing_references: [1] Jing, Baoyu, Zeya Wang, and Eric Xing. \" Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports.\" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.\n[1] is a prior work which also decomposes normal and abnormal reports.\n\ntypos_grammar_style_and_presentation_improvements: 1. Section 3.2 is not clear, which needs further presentation improvement. \n2. There are some notation issues. For example, there are some issues with $V_l$ and $V_g$. It is not clear what does $k$ mean in section\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "RHLG5RNoTb",
        "similarity": 0.6892,
        "coverage": 0.6667,
        "human_length": 187,
        "human_text": "paper_topic_and_main_contributions: This work aims to deal with the problem of medical report generation. The develop a visual encoder based on anomaly pattern memory to enhance anomaly perception. The performance shown on MIMIC-CXR is promising over multiple SOTA methods.\n\nreasons_to_accept: 1. The idea of using the codebook in VQVAE to store various anomaly information seems to be a right way to go. \n2. The memory loss in the objective functions is novel and makes sense. \n3. The experimental results seem to be very promising. And the baselines are comprehensive.\n\nreasons_to_reject: I didn't find any obvous reason to reject the paper.\n\ntypos_grammar_style_and_presentation_improvements: 1. Line 203, \"Combined with global features $V_l$\" should be \"Combined with global features $V_g$\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "0NlaBNIzU4",
        "similarity": 0.7165,
        "coverage": 0.4186,
        "human_length": 612,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a novel concept to automatically generate radiology reports from radiographic images using normal-abnormal semantic decoupling network with abnormal pattern memory. The authors combine the concepts of abnormal semantics in reports with visual feature extraction to shape the decoder output. Their trained model achieves higher performance than state of the art models in all of the relevant metrics. In addition, the paper clearly analyses the model output by using model visualization which helps in understanding the model better. The writing is crisp, easy to understand and clearly explains the methodologies used in the paper without ambiguity.\n\nreasons_to_accept: 1. The methodology achieves higher than state-of-the-art for MIMIC CXR report generation. \n2. The methodology is novel, and involves an understanding the structure of radiology report findings beyond just caption generation problem for radiographic images. In radiology reports, the radiologists tend to write more about the abnormalities than focus on the normal perceptions in the report, and this architecture utilizes that aspect in their modeling quite well. \n3. The analysis of the model is rigorous and gives us a clear understanding of the types of errors to expect.\n\nreasons_to_reject: 1. Though one of the core concepts of the paper is normal-abnormal semantics in reports, the classification from the reports is largely dependent on keyword-based method, which, the authors accept is largely noisy. The definition of what is an abnormal semantics in a report varies depending on application, and there is no specification of any such definition followed in this paper. This makes the normal/abnormal semantic section a little weak compared to the other sections. \n-- After rebuttal: Even with the rebuttal, I think this portion could have been handled much better.\n\nquestions_for_the_authors: 1. What are the details of GPUs used in the experiment and the number of epochs for training, and GPU hours? \n2. The paper mentions in line 51 that the presence of noise in reports impacts network optimization, and similar mentions of noise has been repeated in the paper a few times. Please elucidate the type of noise being referred to, and how it impacts network optimization 3. Is the paper using only the Finding section of the radiology report throughout the experiment? It is a bit unclear from the description. \n4. Which language model has been used for embeddings for training this model?\n\nmissing_references: Please cite Hungarian algorithm in line 221. Please cite the language model that has been used.\n\ntypos_grammar_style_and_presentation_improvements: 1. The paper focuses on generating the Findings in a radiology report, which is an important detail that is only mentioned in line 871 in the Appendix section of the paper. This can be mentioned in the beginning. \n2. Please correct spacing around period in line 240. \n3. In line 294, Att definition is missing. \n4. In line 326 Cancat needs to be changed to Concat, and defined. \n5. Correct typo : Abnaormal -> Abnormal in table 5\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: The paper does not have an ethics section, and since this pertains to a medical application, it is prudent to include a section detailing failures and biases of the model.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "34uvjucb98",
        "length": 265,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a normal-abnormal decoupling memory network for medical report generation. The proposed approach optimizes the visual extraction network only based on anomaly semantics but not normal semantics. The experiments show that the proposed method could outperform baselines.\n\nreasons_to_accept: 1. It is interesting to separately model normal and abnormal reports. \n2. The proposed approach could outperform baselines on the benchmark MIMIC-CXR dataset. \n3. The ablation study shows that each of the component could help improve the performance.\n\nreasons_to_reject: 1. The abnormal mode memory part is not quite clear, which is difficult to interpret. \n2. The proposed method is somewhat incremental. \n3. This paper only includes results of one dataset in the main text.\n\nmissing_references: [1] Jing, Baoyu, Zeya Wang, and Eric Xing. \" Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports.\" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.\n[1] is a prior work which also decomposes normal and abnormal reports.\n\ntypos_grammar_style_and_presentation_improvements: 1. Section 3.2 is not clear, which needs further presentation improvement. \n2. There are some notation issues. For example, there are some issues with $V_l$ and $V_g$. It is not clear what does $k$ mean in section\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "RHLG5RNoTb",
        "length": 187,
        "human_text": "paper_topic_and_main_contributions: This work aims to deal with the problem of medical report generation. The develop a visual encoder based on anomaly pattern memory to enhance anomaly perception. The performance shown on MIMIC-CXR is promising over multiple SOTA methods.\n\nreasons_to_accept: 1. The idea of using the codebook in VQVAE to store various anomaly information seems to be a right way to go. \n2. The memory loss in the objective functions is novel and makes sense. \n3. The experimental results seem to be very promising. And the baselines are comprehensive.\n\nreasons_to_reject: I didn't find any obvous reason to reject the paper.\n\ntypos_grammar_style_and_presentation_improvements: 1. Line 203, \"Combined with global features $V_l$\" should be \"Combined with global features $V_g$\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "0NlaBNIzU4",
        "length": 612,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a novel concept to automatically generate radiology reports from radiographic images using normal-abnormal semantic decoupling network with abnormal pattern memory. The authors combine the concepts of abnormal semantics in reports with visual feature extraction to shape the decoder output. Their trained model achieves higher performance than state of the art models in all of the relevant metrics. In addition, the paper clearly analyses the model output by using model visualization which helps in understanding the model better. The writing is crisp, easy to understand and clearly explains the methodologies used in the paper without ambiguity.\n\nreasons_to_accept: 1. The methodology achieves higher than state-of-the-art for MIMIC CXR report generation. \n2. The methodology is novel, and involves an understanding the structure of radiology report findings beyond just caption generation problem for radiographic images. In radiology reports, the radiologists tend to write more about the abnormalities than focus on the normal perceptions in the report, and this architecture utilizes that aspect in their modeling quite well. \n3. The analysis of the model is rigorous and gives us a clear understanding of the types of errors to expect.\n\nreasons_to_reject: 1. Though one of the core concepts of the paper is normal-abnormal semantics in reports, the classification from the reports is largely dependent on keyword-based method, which, the authors accept is largely noisy. The definition of what is an abnormal semantics in a report varies depending on application, and there is no specification of any such definition followed in this paper. This makes the normal/abnormal semantic section a little weak compared to the other sections. \n-- After rebuttal: Even with the rebuttal, I think this portion could have been handled much better.\n\nquestions_for_the_authors: 1. What are the details of GPUs used in the experiment and the number of epochs for training, and GPU hours? \n2. The paper mentions in line 51 that the presence of noise in reports impacts network optimization, and similar mentions of noise has been repeated in the paper a few times. Please elucidate the type of noise being referred to, and how it impacts network optimization 3. Is the paper using only the Finding section of the radiology report throughout the experiment? It is a bit unclear from the description. \n4. Which language model has been used for embeddings for training this model?\n\nmissing_references: Please cite Hungarian algorithm in line 221. Please cite the language model that has been used.\n\ntypos_grammar_style_and_presentation_improvements: 1. The paper focuses on generating the Findings in a radiology report, which is an important detail that is only mentioned in line 871 in the Appendix section of the paper. This can be mentioned in the beginning. \n2. Please correct spacing around period in line 240. \n3. In line 294, Att definition is missing. \n4. In line 326 Cancat needs to be changed to Concat, and defined. \n5. Correct typo : Abnaormal -> Abnormal in table 5\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: The paper does not have an ethics section, and since this pertains to a medical application, it is prudent to include a section detailing failures and biases of the model.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "53_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_53_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7113333333333333,
      "max_similarity": 0.7222,
      "avg_coverage": 0.5000333333333333,
      "max_coverage": 0.6
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 693,
      "avg_human_length": 354.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 7,
      "weaknesses_count": 13,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "34uvjucb98",
        "similarity": 0.7174,
        "coverage": 0.4815,
        "human_length": 265,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a normal-abnormal decoupling memory network for medical report generation. The proposed approach optimizes the visual extraction network only based on anomaly semantics but not normal semantics. The experiments show that the proposed method could outperform baselines.\n\nreasons_to_accept: 1. It is interesting to separately model normal and abnormal reports. \n2. The proposed approach could outperform baselines on the benchmark MIMIC-CXR dataset. \n3. The ablation study shows that each of the component could help improve the performance.\n\nreasons_to_reject: 1. The abnormal mode memory part is not quite clear, which is difficult to interpret. \n2. The proposed method is somewhat incremental. \n3. This paper only includes results of one dataset in the main text.\n\nmissing_references: [1] Jing, Baoyu, Zeya Wang, and Eric Xing. \" Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports.\" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.\n[1] is a prior work which also decomposes normal and abnormal reports.\n\ntypos_grammar_style_and_presentation_improvements: 1. Section 3.2 is not clear, which needs further presentation improvement. \n2. There are some notation issues. For example, there are some issues with $V_l$ and $V_g$. It is not clear what does $k$ mean in section\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "RHLG5RNoTb",
        "similarity": 0.6944,
        "coverage": 0.6,
        "human_length": 187,
        "human_text": "paper_topic_and_main_contributions: This work aims to deal with the problem of medical report generation. The develop a visual encoder based on anomaly pattern memory to enhance anomaly perception. The performance shown on MIMIC-CXR is promising over multiple SOTA methods.\n\nreasons_to_accept: 1. The idea of using the codebook in VQVAE to store various anomaly information seems to be a right way to go. \n2. The memory loss in the objective functions is novel and makes sense. \n3. The experimental results seem to be very promising. And the baselines are comprehensive.\n\nreasons_to_reject: I didn't find any obvous reason to reject the paper.\n\ntypos_grammar_style_and_presentation_improvements: 1. Line 203, \"Combined with global features $V_l$\" should be \"Combined with global features $V_g$\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "0NlaBNIzU4",
        "similarity": 0.7222,
        "coverage": 0.4186,
        "human_length": 612,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a novel concept to automatically generate radiology reports from radiographic images using normal-abnormal semantic decoupling network with abnormal pattern memory. The authors combine the concepts of abnormal semantics in reports with visual feature extraction to shape the decoder output. Their trained model achieves higher performance than state of the art models in all of the relevant metrics. In addition, the paper clearly analyses the model output by using model visualization which helps in understanding the model better. The writing is crisp, easy to understand and clearly explains the methodologies used in the paper without ambiguity.\n\nreasons_to_accept: 1. The methodology achieves higher than state-of-the-art for MIMIC CXR report generation. \n2. The methodology is novel, and involves an understanding the structure of radiology report findings beyond just caption generation problem for radiographic images. In radiology reports, the radiologists tend to write more about the abnormalities than focus on the normal perceptions in the report, and this architecture utilizes that aspect in their modeling quite well. \n3. The analysis of the model is rigorous and gives us a clear understanding of the types of errors to expect.\n\nreasons_to_reject: 1. Though one of the core concepts of the paper is normal-abnormal semantics in reports, the classification from the reports is largely dependent on keyword-based method, which, the authors accept is largely noisy. The definition of what is an abnormal semantics in a report varies depending on application, and there is no specification of any such definition followed in this paper. This makes the normal/abnormal semantic section a little weak compared to the other sections. \n-- After rebuttal: Even with the rebuttal, I think this portion could have been handled much better.\n\nquestions_for_the_authors: 1. What are the details of GPUs used in the experiment and the number of epochs for training, and GPU hours? \n2. The paper mentions in line 51 that the presence of noise in reports impacts network optimization, and similar mentions of noise has been repeated in the paper a few times. Please elucidate the type of noise being referred to, and how it impacts network optimization 3. Is the paper using only the Finding section of the radiology report throughout the experiment? It is a bit unclear from the description. \n4. Which language model has been used for embeddings for training this model?\n\nmissing_references: Please cite Hungarian algorithm in line 221. Please cite the language model that has been used.\n\ntypos_grammar_style_and_presentation_improvements: 1. The paper focuses on generating the Findings in a radiology report, which is an important detail that is only mentioned in line 871 in the Appendix section of the paper. This can be mentioned in the beginning. \n2. Please correct spacing around period in line 240. \n3. In line 294, Att definition is missing. \n4. In line 326 Cancat needs to be changed to Concat, and defined. \n5. Correct typo : Abnaormal -> Abnormal in table 5\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: The paper does not have an ethics section, and since this pertains to a medical application, it is prudent to include a section detailing failures and biases of the model.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "34uvjucb98",
        "length": 265,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a normal-abnormal decoupling memory network for medical report generation. The proposed approach optimizes the visual extraction network only based on anomaly semantics but not normal semantics. The experiments show that the proposed method could outperform baselines.\n\nreasons_to_accept: 1. It is interesting to separately model normal and abnormal reports. \n2. The proposed approach could outperform baselines on the benchmark MIMIC-CXR dataset. \n3. The ablation study shows that each of the component could help improve the performance.\n\nreasons_to_reject: 1. The abnormal mode memory part is not quite clear, which is difficult to interpret. \n2. The proposed method is somewhat incremental. \n3. This paper only includes results of one dataset in the main text.\n\nmissing_references: [1] Jing, Baoyu, Zeya Wang, and Eric Xing. \" Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports.\" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.\n[1] is a prior work which also decomposes normal and abnormal reports.\n\ntypos_grammar_style_and_presentation_improvements: 1. Section 3.2 is not clear, which needs further presentation improvement. \n2. There are some notation issues. For example, there are some issues with $V_l$ and $V_g$. It is not clear what does $k$ mean in section\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "RHLG5RNoTb",
        "length": 187,
        "human_text": "paper_topic_and_main_contributions: This work aims to deal with the problem of medical report generation. The develop a visual encoder based on anomaly pattern memory to enhance anomaly perception. The performance shown on MIMIC-CXR is promising over multiple SOTA methods.\n\nreasons_to_accept: 1. The idea of using the codebook in VQVAE to store various anomaly information seems to be a right way to go. \n2. The memory loss in the objective functions is novel and makes sense. \n3. The experimental results seem to be very promising. And the baselines are comprehensive.\n\nreasons_to_reject: I didn't find any obvous reason to reject the paper.\n\ntypos_grammar_style_and_presentation_improvements: 1. Line 203, \"Combined with global features $V_l$\" should be \"Combined with global features $V_g$\"\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "0NlaBNIzU4",
        "length": 612,
        "human_text": "paper_topic_and_main_contributions: The paper introduces a novel concept to automatically generate radiology reports from radiographic images using normal-abnormal semantic decoupling network with abnormal pattern memory. The authors combine the concepts of abnormal semantics in reports with visual feature extraction to shape the decoder output. Their trained model achieves higher performance than state of the art models in all of the relevant metrics. In addition, the paper clearly analyses the model output by using model visualization which helps in understanding the model better. The writing is crisp, easy to understand and clearly explains the methodologies used in the paper without ambiguity.\n\nreasons_to_accept: 1. The methodology achieves higher than state-of-the-art for MIMIC CXR report generation. \n2. The methodology is novel, and involves an understanding the structure of radiology report findings beyond just caption generation problem for radiographic images. In radiology reports, the radiologists tend to write more about the abnormalities than focus on the normal perceptions in the report, and this architecture utilizes that aspect in their modeling quite well. \n3. The analysis of the model is rigorous and gives us a clear understanding of the types of errors to expect.\n\nreasons_to_reject: 1. Though one of the core concepts of the paper is normal-abnormal semantics in reports, the classification from the reports is largely dependent on keyword-based method, which, the authors accept is largely noisy. The definition of what is an abnormal semantics in a report varies depending on application, and there is no specification of any such definition followed in this paper. This makes the normal/abnormal semantic section a little weak compared to the other sections. \n-- After rebuttal: Even with the rebuttal, I think this portion could have been handled much better.\n\nquestions_for_the_authors: 1. What are the details of GPUs used in the experiment and the number of epochs for training, and GPU hours? \n2. The paper mentions in line 51 that the presence of noise in reports impacts network optimization, and similar mentions of noise has been repeated in the paper a few times. Please elucidate the type of noise being referred to, and how it impacts network optimization 3. Is the paper using only the Finding section of the radiology report throughout the experiment? It is a bit unclear from the description. \n4. Which language model has been used for embeddings for training this model?\n\nmissing_references: Please cite Hungarian algorithm in line 221. Please cite the language model that has been used.\n\ntypos_grammar_style_and_presentation_improvements: 1. The paper focuses on generating the Findings in a radiology report, which is an important detail that is only mentioned in line 871 in the Appendix section of the paper. This can be mentioned in the beginning. \n2. Please correct spacing around period in line 240. \n3. In line 294, Att definition is missing. \n4. In line 326 Cancat needs to be changed to Concat, and defined. \n5. Correct typo : Abnaormal -> Abnormal in table 5\n\nethical_concerns: Yes\n\njustification_for_ethical_concerns: The paper does not have an ethics section, and since this pertains to a medical application, it is prudent to include a section detailing failures and biases of the model.\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "82_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_82_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.74905,
      "max_similarity": 0.7938,
      "avg_coverage": 0.368625,
      "max_coverage": 0.5
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 373,
      "avg_human_length": 433.25
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 4,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "vKTY8jqFyT",
        "similarity": 0.7551,
        "coverage": 0.381,
        "human_length": 319,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the clickbait detection task and collects a new Romanian clickbait dataset (low-resource languages) for future research. \nThe main contributions consist of collecting a new Romanian clickbait dataset and adopting contrastive learning into clickbait detection task.\n\nreasons_to_accept: The main contributions consist of collecting a new Romanian clickbait dataset and adopting contrastive learning into clickbait detection task.\n\nreasons_to_reject: 1. The description of the dataset is not specific enough, such as the domains, the average number of sentences of these articles, how many sentences are in the longest article, and how many sentences are in the shortest article. \n2. The data collection method, the annotation process lack of details, for example, what is the annotator agreement score? \n2. Do not describe the difference between their proposed method and Siamese networks. \n3. Do not clarify the difference between fake news detection, satirical news detection and clickbait detection. Maybe the author can explain it in the intro section.\n\nquestions_for_the_authors: A. (Line 200-202) Is this phenomenon announced by the website itself or by the author's study? If it is a study, please give an argument, otherwise I think this conclusion is a strong hypothesis lack of justification. \nB. The conclusion does not give insight and line332 mentioned that \"non-clickbait articles is fairly easy. However, clickbait articles are comparatively\",  is it possible that the data set is biased (4,460 non-clickbait samples and 3,453 clickbait samples)?\n\ntypos_grammar_style_and_presentation_improvements: line 278: for all i = ***, may be more understandable to use \"for all $i \\in [1,n]$\" (Latex)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "hJSHfiA7Ao",
        "similarity": 0.7938,
        "coverage": 0.3243,
        "human_length": 737,
        "human_text": "paper_topic_and_main_contributions: This paper presents the authors' experience in Clickbait Detection, introducing RoCliCo, the first public corpus for Romanian clickbait detection. They train and evaluate four models and an ensemble of these models to establish competitive baselines for future research on their corpus. The authors propose a novel approach using a BERT model based on contrastive learning, which leverages cosine similarity between the title and content to detect clickbait articles. The empirical results indicate significant potential for future research on the Romanian clickbait detection task. The availability of RoCliCo opens up opportunities for collaboration and advancement in clickbait detection methods for the Romanian language.\n\nreasons_to_accept: - The process of creating the dataset is thoroughly explained.\n- A dataset of Clickbait in Romanian with comprehensive annotations holds potential value for researchers.\n- The authors conduct multiple experiments to validate the efficacy of both handcrafted and deep models as baseline methods on the dataset.\n- Reporting the performance of various commonly used models on this dataset serves as a valuable reference for future benchmarking efforts.\n\nreasons_to_reject: - Please provide detailed information on the qualifications of the annotators and the payment process for the labeling task to ensure transparency and reliability.\n- The rationale behind separating the news platforms between the training and test sets (as discussed in Section 3) should be clearly explained to justify the authors' approach.\n- Table 1 should be appropriately placed in the text, precisely where it is mentioned, to enhance readability and facilitate easy reference.\n- To demonstrate the dataset's utility, it is essential to compare it with popular datasets to highlight the unique dimensions it can contribute to various research areas.\n- While the authors have presented a comprehensive overview of the Clickbait Detection task, including additional information on the linguistic aspects of the task would be valuable. This addition would provide readers with a deeper understanding of the study's theoretical foundations and the methodologies used to achieve the results.\n- To ascertain the significance of the reported results, the authors should provide statistical analysis. This will offer readers a more accurate understanding of the effectiveness of the methods employed.\n- The manuscript lacks mention of hyperparameters or other model parameters. It is crucial to explain how these parameters are optimized to ensure reproducibility and enable others to apply the methodology effectively.\n\nquestions_for_the_authors: - Is there any related work for Clickbait Detection? How are their methods different from your work?\n- Please provide detailed information on the qualifications of the annotators and the payment process for the labeling task to ensure transparency and reliability.\n- The rationale behind separating the news platforms between the training and test sets (as discussed in Section 3) should be clearly explained to justify the authors' approach.\n- Table 1 should be appropriately placed in the text, precisely where it is mentioned, to enhance readability and facilitate easy reference.\n- To demonstrate the dataset's utility, it is essential to compare it with popular datasets to highlight the unique dimensions it can contribute to various research areas.\n- While the authors have presented a comprehensive overview of the Clickbait Detection task, including additional information on the linguistic aspects of the task would be valuable. This addition would provide readers with a deeper understanding of the study's theoretical foundations and the methodologies used to achieve the results.\n- To ascertain the significance of the reported results, the authors should provide statistical analysis. This will offer readers a more accurate understanding of the effectiveness of the methods employed.\n- The manuscript lacks mention of hyperparameters or other model parameters. It is crucial to explain how these parameters are optimized to ensure reproducibility and enable others to apply the methodology effectively.\n\ntypos_grammar_style_and_presentation_improvements: - To make the analysis of the results more transparent, authors should present their results in several metrics and as a 4-digit decimal (ex: 0.81xx).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "iEFS0KB6hu",
        "similarity": 0.7063,
        "coverage": 0.5,
        "human_length": 177,
        "human_text": "paper_topic_and_main_contributions: Gather qnd curate a click bait classification dataset in Romanian.\n\nreasons_to_accept: It's a novel dataset and has well established labeling, reliability metrics, and\n\nreasons_to_reject: For classification tasks it would be good to see a PR curve to evaluate overall performance rather than just mean metrics.\n\nquestions_for_the_authors: Why is it important to have this dataset in Romanian? How is it different than other languages?\n\nmissing_references: Other click bait datasets across languages and a side by side comparison.\n\ntypos_grammar_style_and_presentation_improvements: Some simple edits are necessary but nothing unusual.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "JGjbEFECoK",
        "similarity": 0.741,
        "coverage": 0.2692,
        "human_length": 500,
        "human_text": "paper_topic_and_main_contributions: The authors present RoCliCo, a corpus that is a collection of over 7,900 news samples in Romanian that are manually annotated with clickbait and non-clickbait labels. It is the first publicly available corpus for clickbait detection in Romanian news, and it provides a valuable resource for researchers and practitioners to develop and evaluate machine learning models for this task. Experiments with a few machine learning methods to establish competitive baselines, which can serve as a starting point for future research on clickbait detection in Romanian.\n\nreasons_to_accept: This paper is really well-motivated and well-written.\nThe RoCliCo enriches the low-resource natural language corpora and clickbait detection.\nThe selected baseline models show that Romanian clickbait detection is a challenging task.\n\nreasons_to_reject: The number and type of baseline models in the Experiments section are too few, only 4.  The size of RoCliCo is relatively small since the majority of non-clickbait samples can be collected from some authoritative news website.\nThough Cohen\u2019s kappa coefficient among annotators is 0.73, the labeling process lack of cross-check, which can introduce biases and errors in the labeling results.\n\nquestions_for_the_authors: Can you provide more details on the criteria used for selecting the five Romanian news websites included in the corpus? Were there any specific reasons for choosing these websites over others?\nHow did you ensure the quality and consistency of the annotations provided by the volunteers? Did you conduct any inter-annotator agreement analysis or provide any guidelines for the annotators?\nAs the clickbait detection task is quite similar to the natural language inference task (NLI), why do the authors not pick some existing NLI models for comparison?\nSection 3: In the last paragraph, \u201c\u2026 the test data represents a more natural distribution, \u2026\u201d, what is \u201cnatural distribution\u201d? Is imbalance data the same as \u201cnatural distribution\u201d?\nSection 4: In the 1st paragraph, \u201c\u2026 based on part-of-speech, scores, and punctuation patterns.\u201d, what do the scores represent?\nSection 4: In the 2nd paragraph, \u201cEach input is first preprocessed by a tokenizer.\u201d, which tokenizer do the authors employ? NLTK, Gensim, or StanfordCoreNLP?\nSection 4: In the 2nd paragraph, \u201cThe resulting embeddings are passed as input to the neural network.\u201d, what neural network is chosen? MLP, CNN, or Transformer?\nSection 4: In the 3rd paragraph, \u201cThe generated Ro-BERT-based embeddings are passed through a dropout layer, a fully-connected layer, and a classification layer.\u201d, as the former sentence mentioned [SEP], does here the embedding denote [CLS] embedding?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "vKTY8jqFyT",
        "length": 319,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the clickbait detection task and collects a new Romanian clickbait dataset (low-resource languages) for future research. \nThe main contributions consist of collecting a new Romanian clickbait dataset and adopting contrastive learning into clickbait detection task.\n\nreasons_to_accept: The main contributions consist of collecting a new Romanian clickbait dataset and adopting contrastive learning into clickbait detection task.\n\nreasons_to_reject: 1. The description of the dataset is not specific enough, such as the domains, the average number of sentences of these articles, how many sentences are in the longest article, and how many sentences are in the shortest article. \n2. The data collection method, the annotation process lack of details, for example, what is the annotator agreement score? \n2. Do not describe the difference between their proposed method and Siamese networks. \n3. Do not clarify the difference between fake news detection, satirical news detection and clickbait detection. Maybe the author can explain it in the intro section.\n\nquestions_for_the_authors: A. (Line 200-202) Is this phenomenon announced by the website itself or by the author's study? If it is a study, please give an argument, otherwise I think this conclusion is a strong hypothesis lack of justification. \nB. The conclusion does not give insight and line332 mentioned that \"non-clickbait articles is fairly easy. However, clickbait articles are comparatively\",  is it possible that the data set is biased (4,460 non-clickbait samples and 3,453 clickbait samples)?\n\ntypos_grammar_style_and_presentation_improvements: line 278: for all i = ***, may be more understandable to use \"for all $i \\in [1,n]$\" (Latex)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "hJSHfiA7Ao",
        "length": 737,
        "human_text": "paper_topic_and_main_contributions: This paper presents the authors' experience in Clickbait Detection, introducing RoCliCo, the first public corpus for Romanian clickbait detection. They train and evaluate four models and an ensemble of these models to establish competitive baselines for future research on their corpus. The authors propose a novel approach using a BERT model based on contrastive learning, which leverages cosine similarity between the title and content to detect clickbait articles. The empirical results indicate significant potential for future research on the Romanian clickbait detection task. The availability of RoCliCo opens up opportunities for collaboration and advancement in clickbait detection methods for the Romanian language.\n\nreasons_to_accept: - The process of creating the dataset is thoroughly explained.\n- A dataset of Clickbait in Romanian with comprehensive annotations holds potential value for researchers.\n- The authors conduct multiple experiments to validate the efficacy of both handcrafted and deep models as baseline methods on the dataset.\n- Reporting the performance of various commonly used models on this dataset serves as a valuable reference for future benchmarking efforts.\n\nreasons_to_reject: - Please provide detailed information on the qualifications of the annotators and the payment process for the labeling task to ensure transparency and reliability.\n- The rationale behind separating the news platforms between the training and test sets (as discussed in Section 3) should be clearly explained to justify the authors' approach.\n- Table 1 should be appropriately placed in the text, precisely where it is mentioned, to enhance readability and facilitate easy reference.\n- To demonstrate the dataset's utility, it is essential to compare it with popular datasets to highlight the unique dimensions it can contribute to various research areas.\n- While the authors have presented a comprehensive overview of the Clickbait Detection task, including additional information on the linguistic aspects of the task would be valuable. This addition would provide readers with a deeper understanding of the study's theoretical foundations and the methodologies used to achieve the results.\n- To ascertain the significance of the reported results, the authors should provide statistical analysis. This will offer readers a more accurate understanding of the effectiveness of the methods employed.\n- The manuscript lacks mention of hyperparameters or other model parameters. It is crucial to explain how these parameters are optimized to ensure reproducibility and enable others to apply the methodology effectively.\n\nquestions_for_the_authors: - Is there any related work for Clickbait Detection? How are their methods different from your work?\n- Please provide detailed information on the qualifications of the annotators and the payment process for the labeling task to ensure transparency and reliability.\n- The rationale behind separating the news platforms between the training and test sets (as discussed in Section 3) should be clearly explained to justify the authors' approach.\n- Table 1 should be appropriately placed in the text, precisely where it is mentioned, to enhance readability and facilitate easy reference.\n- To demonstrate the dataset's utility, it is essential to compare it with popular datasets to highlight the unique dimensions it can contribute to various research areas.\n- While the authors have presented a comprehensive overview of the Clickbait Detection task, including additional information on the linguistic aspects of the task would be valuable. This addition would provide readers with a deeper understanding of the study's theoretical foundations and the methodologies used to achieve the results.\n- To ascertain the significance of the reported results, the authors should provide statistical analysis. This will offer readers a more accurate understanding of the effectiveness of the methods employed.\n- The manuscript lacks mention of hyperparameters or other model parameters. It is crucial to explain how these parameters are optimized to ensure reproducibility and enable others to apply the methodology effectively.\n\ntypos_grammar_style_and_presentation_improvements: - To make the analysis of the results more transparent, authors should present their results in several metrics and as a 4-digit decimal (ex: 0.81xx).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "iEFS0KB6hu",
        "length": 177,
        "human_text": "paper_topic_and_main_contributions: Gather qnd curate a click bait classification dataset in Romanian.\n\nreasons_to_accept: It's a novel dataset and has well established labeling, reliability metrics, and\n\nreasons_to_reject: For classification tasks it would be good to see a PR curve to evaluate overall performance rather than just mean metrics.\n\nquestions_for_the_authors: Why is it important to have this dataset in Romanian? How is it different than other languages?\n\nmissing_references: Other click bait datasets across languages and a side by side comparison.\n\ntypos_grammar_style_and_presentation_improvements: Some simple edits are necessary but nothing unusual.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "JGjbEFECoK",
        "length": 500,
        "human_text": "paper_topic_and_main_contributions: The authors present RoCliCo, a corpus that is a collection of over 7,900 news samples in Romanian that are manually annotated with clickbait and non-clickbait labels. It is the first publicly available corpus for clickbait detection in Romanian news, and it provides a valuable resource for researchers and practitioners to develop and evaluate machine learning models for this task. Experiments with a few machine learning methods to establish competitive baselines, which can serve as a starting point for future research on clickbait detection in Romanian.\n\nreasons_to_accept: This paper is really well-motivated and well-written.\nThe RoCliCo enriches the low-resource natural language corpora and clickbait detection.\nThe selected baseline models show that Romanian clickbait detection is a challenging task.\n\nreasons_to_reject: The number and type of baseline models in the Experiments section are too few, only 4.  The size of RoCliCo is relatively small since the majority of non-clickbait samples can be collected from some authoritative news website.\nThough Cohen\u2019s kappa coefficient among annotators is 0.73, the labeling process lack of cross-check, which can introduce biases and errors in the labeling results.\n\nquestions_for_the_authors: Can you provide more details on the criteria used for selecting the five Romanian news websites included in the corpus? Were there any specific reasons for choosing these websites over others?\nHow did you ensure the quality and consistency of the annotations provided by the volunteers? Did you conduct any inter-annotator agreement analysis or provide any guidelines for the annotators?\nAs the clickbait detection task is quite similar to the natural language inference task (NLI), why do the authors not pick some existing NLI models for comparison?\nSection 3: In the last paragraph, \u201c\u2026 the test data represents a more natural distribution, \u2026\u201d, what is \u201cnatural distribution\u201d? Is imbalance data the same as \u201cnatural distribution\u201d?\nSection 4: In the 1st paragraph, \u201c\u2026 based on part-of-speech, scores, and punctuation patterns.\u201d, what do the scores represent?\nSection 4: In the 2nd paragraph, \u201cEach input is first preprocessed by a tokenizer.\u201d, which tokenizer do the authors employ? NLTK, Gensim, or StanfordCoreNLP?\nSection 4: In the 2nd paragraph, \u201cThe resulting embeddings are passed as input to the neural network.\u201d, what neural network is chosen? MLP, CNN, or Transformer?\nSection 4: In the 3rd paragraph, \u201cThe generated Ro-BERT-based embeddings are passed through a dropout layer, a fully-connected layer, and a classification layer.\u201d, as the former sentence mentioned [SEP], does here the embedding denote [CLS] embedding?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "82_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_82_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.7549499999999999,
      "max_similarity": 0.786,
      "avg_coverage": 0.421925,
      "max_coverage": 0.6
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 379,
      "avg_human_length": 433.25
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 4,
      "suggestions_count": 4
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "vKTY8jqFyT",
        "similarity": 0.7649,
        "coverage": 0.4286,
        "human_length": 319,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the clickbait detection task and collects a new Romanian clickbait dataset (low-resource languages) for future research. \nThe main contributions consist of collecting a new Romanian clickbait dataset and adopting contrastive learning into clickbait detection task.\n\nreasons_to_accept: The main contributions consist of collecting a new Romanian clickbait dataset and adopting contrastive learning into clickbait detection task.\n\nreasons_to_reject: 1. The description of the dataset is not specific enough, such as the domains, the average number of sentences of these articles, how many sentences are in the longest article, and how many sentences are in the shortest article. \n2. The data collection method, the annotation process lack of details, for example, what is the annotator agreement score? \n2. Do not describe the difference between their proposed method and Siamese networks. \n3. Do not clarify the difference between fake news detection, satirical news detection and clickbait detection. Maybe the author can explain it in the intro section.\n\nquestions_for_the_authors: A. (Line 200-202) Is this phenomenon announced by the website itself or by the author's study? If it is a study, please give an argument, otherwise I think this conclusion is a strong hypothesis lack of justification. \nB. The conclusion does not give insight and line332 mentioned that \"non-clickbait articles is fairly easy. However, clickbait articles are comparatively\",  is it possible that the data set is biased (4,460 non-clickbait samples and 3,453 clickbait samples)?\n\ntypos_grammar_style_and_presentation_improvements: line 278: for all i = ***, may be more understandable to use \"for all $i \\in [1,n]$\" (Latex)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "hJSHfiA7Ao",
        "similarity": 0.786,
        "coverage": 0.3514,
        "human_length": 737,
        "human_text": "paper_topic_and_main_contributions: This paper presents the authors' experience in Clickbait Detection, introducing RoCliCo, the first public corpus for Romanian clickbait detection. They train and evaluate four models and an ensemble of these models to establish competitive baselines for future research on their corpus. The authors propose a novel approach using a BERT model based on contrastive learning, which leverages cosine similarity between the title and content to detect clickbait articles. The empirical results indicate significant potential for future research on the Romanian clickbait detection task. The availability of RoCliCo opens up opportunities for collaboration and advancement in clickbait detection methods for the Romanian language.\n\nreasons_to_accept: - The process of creating the dataset is thoroughly explained.\n- A dataset of Clickbait in Romanian with comprehensive annotations holds potential value for researchers.\n- The authors conduct multiple experiments to validate the efficacy of both handcrafted and deep models as baseline methods on the dataset.\n- Reporting the performance of various commonly used models on this dataset serves as a valuable reference for future benchmarking efforts.\n\nreasons_to_reject: - Please provide detailed information on the qualifications of the annotators and the payment process for the labeling task to ensure transparency and reliability.\n- The rationale behind separating the news platforms between the training and test sets (as discussed in Section 3) should be clearly explained to justify the authors' approach.\n- Table 1 should be appropriately placed in the text, precisely where it is mentioned, to enhance readability and facilitate easy reference.\n- To demonstrate the dataset's utility, it is essential to compare it with popular datasets to highlight the unique dimensions it can contribute to various research areas.\n- While the authors have presented a comprehensive overview of the Clickbait Detection task, including additional information on the linguistic aspects of the task would be valuable. This addition would provide readers with a deeper understanding of the study's theoretical foundations and the methodologies used to achieve the results.\n- To ascertain the significance of the reported results, the authors should provide statistical analysis. This will offer readers a more accurate understanding of the effectiveness of the methods employed.\n- The manuscript lacks mention of hyperparameters or other model parameters. It is crucial to explain how these parameters are optimized to ensure reproducibility and enable others to apply the methodology effectively.\n\nquestions_for_the_authors: - Is there any related work for Clickbait Detection? How are their methods different from your work?\n- Please provide detailed information on the qualifications of the annotators and the payment process for the labeling task to ensure transparency and reliability.\n- The rationale behind separating the news platforms between the training and test sets (as discussed in Section 3) should be clearly explained to justify the authors' approach.\n- Table 1 should be appropriately placed in the text, precisely where it is mentioned, to enhance readability and facilitate easy reference.\n- To demonstrate the dataset's utility, it is essential to compare it with popular datasets to highlight the unique dimensions it can contribute to various research areas.\n- While the authors have presented a comprehensive overview of the Clickbait Detection task, including additional information on the linguistic aspects of the task would be valuable. This addition would provide readers with a deeper understanding of the study's theoretical foundations and the methodologies used to achieve the results.\n- To ascertain the significance of the reported results, the authors should provide statistical analysis. This will offer readers a more accurate understanding of the effectiveness of the methods employed.\n- The manuscript lacks mention of hyperparameters or other model parameters. It is crucial to explain how these parameters are optimized to ensure reproducibility and enable others to apply the methodology effectively.\n\ntypos_grammar_style_and_presentation_improvements: - To make the analysis of the results more transparent, authors should present their results in several metrics and as a 4-digit decimal (ex: 0.81xx).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "iEFS0KB6hu",
        "similarity": 0.7141,
        "coverage": 0.6,
        "human_length": 177,
        "human_text": "paper_topic_and_main_contributions: Gather qnd curate a click bait classification dataset in Romanian.\n\nreasons_to_accept: It's a novel dataset and has well established labeling, reliability metrics, and\n\nreasons_to_reject: For classification tasks it would be good to see a PR curve to evaluate overall performance rather than just mean metrics.\n\nquestions_for_the_authors: Why is it important to have this dataset in Romanian? How is it different than other languages?\n\nmissing_references: Other click bait datasets across languages and a side by side comparison.\n\ntypos_grammar_style_and_presentation_improvements: Some simple edits are necessary but nothing unusual.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "JGjbEFECoK",
        "similarity": 0.7548,
        "coverage": 0.3077,
        "human_length": 500,
        "human_text": "paper_topic_and_main_contributions: The authors present RoCliCo, a corpus that is a collection of over 7,900 news samples in Romanian that are manually annotated with clickbait and non-clickbait labels. It is the first publicly available corpus for clickbait detection in Romanian news, and it provides a valuable resource for researchers and practitioners to develop and evaluate machine learning models for this task. Experiments with a few machine learning methods to establish competitive baselines, which can serve as a starting point for future research on clickbait detection in Romanian.\n\nreasons_to_accept: This paper is really well-motivated and well-written.\nThe RoCliCo enriches the low-resource natural language corpora and clickbait detection.\nThe selected baseline models show that Romanian clickbait detection is a challenging task.\n\nreasons_to_reject: The number and type of baseline models in the Experiments section are too few, only 4.  The size of RoCliCo is relatively small since the majority of non-clickbait samples can be collected from some authoritative news website.\nThough Cohen\u2019s kappa coefficient among annotators is 0.73, the labeling process lack of cross-check, which can introduce biases and errors in the labeling results.\n\nquestions_for_the_authors: Can you provide more details on the criteria used for selecting the five Romanian news websites included in the corpus? Were there any specific reasons for choosing these websites over others?\nHow did you ensure the quality and consistency of the annotations provided by the volunteers? Did you conduct any inter-annotator agreement analysis or provide any guidelines for the annotators?\nAs the clickbait detection task is quite similar to the natural language inference task (NLI), why do the authors not pick some existing NLI models for comparison?\nSection 3: In the last paragraph, \u201c\u2026 the test data represents a more natural distribution, \u2026\u201d, what is \u201cnatural distribution\u201d? Is imbalance data the same as \u201cnatural distribution\u201d?\nSection 4: In the 1st paragraph, \u201c\u2026 based on part-of-speech, scores, and punctuation patterns.\u201d, what do the scores represent?\nSection 4: In the 2nd paragraph, \u201cEach input is first preprocessed by a tokenizer.\u201d, which tokenizer do the authors employ? NLTK, Gensim, or StanfordCoreNLP?\nSection 4: In the 2nd paragraph, \u201cThe resulting embeddings are passed as input to the neural network.\u201d, what neural network is chosen? MLP, CNN, or Transformer?\nSection 4: In the 3rd paragraph, \u201cThe generated Ro-BERT-based embeddings are passed through a dropout layer, a fully-connected layer, and a classification layer.\u201d, as the former sentence mentioned [SEP], does here the embedding denote [CLS] embedding?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "vKTY8jqFyT",
        "length": 319,
        "human_text": "paper_topic_and_main_contributions: This paper focuses on the clickbait detection task and collects a new Romanian clickbait dataset (low-resource languages) for future research. \nThe main contributions consist of collecting a new Romanian clickbait dataset and adopting contrastive learning into clickbait detection task.\n\nreasons_to_accept: The main contributions consist of collecting a new Romanian clickbait dataset and adopting contrastive learning into clickbait detection task.\n\nreasons_to_reject: 1. The description of the dataset is not specific enough, such as the domains, the average number of sentences of these articles, how many sentences are in the longest article, and how many sentences are in the shortest article. \n2. The data collection method, the annotation process lack of details, for example, what is the annotator agreement score? \n2. Do not describe the difference between their proposed method and Siamese networks. \n3. Do not clarify the difference between fake news detection, satirical news detection and clickbait detection. Maybe the author can explain it in the intro section.\n\nquestions_for_the_authors: A. (Line 200-202) Is this phenomenon announced by the website itself or by the author's study? If it is a study, please give an argument, otherwise I think this conclusion is a strong hypothesis lack of justification. \nB. The conclusion does not give insight and line332 mentioned that \"non-clickbait articles is fairly easy. However, clickbait articles are comparatively\",  is it possible that the data set is biased (4,460 non-clickbait samples and 3,453 clickbait samples)?\n\ntypos_grammar_style_and_presentation_improvements: line 278: for all i = ***, may be more understandable to use \"for all $i \\in [1,n]$\" (Latex)\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "hJSHfiA7Ao",
        "length": 737,
        "human_text": "paper_topic_and_main_contributions: This paper presents the authors' experience in Clickbait Detection, introducing RoCliCo, the first public corpus for Romanian clickbait detection. They train and evaluate four models and an ensemble of these models to establish competitive baselines for future research on their corpus. The authors propose a novel approach using a BERT model based on contrastive learning, which leverages cosine similarity between the title and content to detect clickbait articles. The empirical results indicate significant potential for future research on the Romanian clickbait detection task. The availability of RoCliCo opens up opportunities for collaboration and advancement in clickbait detection methods for the Romanian language.\n\nreasons_to_accept: - The process of creating the dataset is thoroughly explained.\n- A dataset of Clickbait in Romanian with comprehensive annotations holds potential value for researchers.\n- The authors conduct multiple experiments to validate the efficacy of both handcrafted and deep models as baseline methods on the dataset.\n- Reporting the performance of various commonly used models on this dataset serves as a valuable reference for future benchmarking efforts.\n\nreasons_to_reject: - Please provide detailed information on the qualifications of the annotators and the payment process for the labeling task to ensure transparency and reliability.\n- The rationale behind separating the news platforms between the training and test sets (as discussed in Section 3) should be clearly explained to justify the authors' approach.\n- Table 1 should be appropriately placed in the text, precisely where it is mentioned, to enhance readability and facilitate easy reference.\n- To demonstrate the dataset's utility, it is essential to compare it with popular datasets to highlight the unique dimensions it can contribute to various research areas.\n- While the authors have presented a comprehensive overview of the Clickbait Detection task, including additional information on the linguistic aspects of the task would be valuable. This addition would provide readers with a deeper understanding of the study's theoretical foundations and the methodologies used to achieve the results.\n- To ascertain the significance of the reported results, the authors should provide statistical analysis. This will offer readers a more accurate understanding of the effectiveness of the methods employed.\n- The manuscript lacks mention of hyperparameters or other model parameters. It is crucial to explain how these parameters are optimized to ensure reproducibility and enable others to apply the methodology effectively.\n\nquestions_for_the_authors: - Is there any related work for Clickbait Detection? How are their methods different from your work?\n- Please provide detailed information on the qualifications of the annotators and the payment process for the labeling task to ensure transparency and reliability.\n- The rationale behind separating the news platforms between the training and test sets (as discussed in Section 3) should be clearly explained to justify the authors' approach.\n- Table 1 should be appropriately placed in the text, precisely where it is mentioned, to enhance readability and facilitate easy reference.\n- To demonstrate the dataset's utility, it is essential to compare it with popular datasets to highlight the unique dimensions it can contribute to various research areas.\n- While the authors have presented a comprehensive overview of the Clickbait Detection task, including additional information on the linguistic aspects of the task would be valuable. This addition would provide readers with a deeper understanding of the study's theoretical foundations and the methodologies used to achieve the results.\n- To ascertain the significance of the reported results, the authors should provide statistical analysis. This will offer readers a more accurate understanding of the effectiveness of the methods employed.\n- The manuscript lacks mention of hyperparameters or other model parameters. It is crucial to explain how these parameters are optimized to ensure reproducibility and enable others to apply the methodology effectively.\n\ntypos_grammar_style_and_presentation_improvements: - To make the analysis of the results more transparent, authors should present their results in several metrics and as a 4-digit decimal (ex: 0.81xx).\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "iEFS0KB6hu",
        "length": 177,
        "human_text": "paper_topic_and_main_contributions: Gather qnd curate a click bait classification dataset in Romanian.\n\nreasons_to_accept: It's a novel dataset and has well established labeling, reliability metrics, and\n\nreasons_to_reject: For classification tasks it would be good to see a PR curve to evaluate overall performance rather than just mean metrics.\n\nquestions_for_the_authors: Why is it important to have this dataset in Romanian? How is it different than other languages?\n\nmissing_references: Other click bait datasets across languages and a side by side comparison.\n\ntypos_grammar_style_and_presentation_improvements: Some simple edits are necessary but nothing unusual.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "JGjbEFECoK",
        "length": 500,
        "human_text": "paper_topic_and_main_contributions: The authors present RoCliCo, a corpus that is a collection of over 7,900 news samples in Romanian that are manually annotated with clickbait and non-clickbait labels. It is the first publicly available corpus for clickbait detection in Romanian news, and it provides a valuable resource for researchers and practitioners to develop and evaluate machine learning models for this task. Experiments with a few machine learning methods to establish competitive baselines, which can serve as a starting point for future research on clickbait detection in Romanian.\n\nreasons_to_accept: This paper is really well-motivated and well-written.\nThe RoCliCo enriches the low-resource natural language corpora and clickbait detection.\nThe selected baseline models show that Romanian clickbait detection is a challenging task.\n\nreasons_to_reject: The number and type of baseline models in the Experiments section are too few, only 4.  The size of RoCliCo is relatively small since the majority of non-clickbait samples can be collected from some authoritative news website.\nThough Cohen\u2019s kappa coefficient among annotators is 0.73, the labeling process lack of cross-check, which can introduce biases and errors in the labeling results.\n\nquestions_for_the_authors: Can you provide more details on the criteria used for selecting the five Romanian news websites included in the corpus? Were there any specific reasons for choosing these websites over others?\nHow did you ensure the quality and consistency of the annotations provided by the volunteers? Did you conduct any inter-annotator agreement analysis or provide any guidelines for the annotators?\nAs the clickbait detection task is quite similar to the natural language inference task (NLI), why do the authors not pick some existing NLI models for comparison?\nSection 3: In the last paragraph, \u201c\u2026 the test data represents a more natural distribution, \u2026\u201d, what is \u201cnatural distribution\u201d? Is imbalance data the same as \u201cnatural distribution\u201d?\nSection 4: In the 1st paragraph, \u201c\u2026 based on part-of-speech, scores, and punctuation patterns.\u201d, what do the scores represent?\nSection 4: In the 2nd paragraph, \u201cEach input is first preprocessed by a tokenizer.\u201d, which tokenizer do the authors employ? NLTK, Gensim, or StanfordCoreNLP?\nSection 4: In the 2nd paragraph, \u201cThe resulting embeddings are passed as input to the neural network.\u201d, what neural network is chosen? MLP, CNN, or Transformer?\nSection 4: In the 3rd paragraph, \u201cThe generated Ro-BERT-based embeddings are passed through a dropout layer, a fully-connected layer, and a classification layer.\u201d, as the former sentence mentioned [SEP], does here the embedding denote [CLS] embedding?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "16_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_16_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7050666666666667,
      "max_similarity": 0.7183,
      "avg_coverage": 0.5508000000000001,
      "max_coverage": 0.7143
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 564,
      "avg_human_length": 320.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 11,
      "suggestions_count": 8
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "SAJc5m2bYe",
        "similarity": 0.7056,
        "coverage": 0.5714,
        "human_length": 334,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an approach to document-level sentiment classification task based on LLMs and in-context learning, using prompting with examples which enables reasoning to get correct labels. \nThe prompt asks the model to generate clue keywords related to positive or negative sentiments, and reasoning process that support the decision of labels, in addition to the final answer.  Also examples of (input, clues, reasoning and label) are added to the prompt for one-short and few-shot settings which can be automatically generated from the subset of training data. \nThe experimental results show the SotA scores in 4 out of 5 tasks, and they outperformed the supervised fine-tuned results.  Also this paper provides extensive discussion including ablation studies, alternative labels and consideration of low-resource scenario.\n\nreasons_to_accept: - Good design of prompt including examples.  The difference from the Chain-of-Thought method is clear.\n- The experimental results show the SotA scores in 4 out of 5 tasks, and they outperformed the supervised fine-tuned results.\n- The proposed method addresses the low-resource settings and domain adaptability.\n- The discussion of alternative labels is interesting.\n\nreasons_to_reject: - The notion of in-context learning has been already proposed.\n- The document-level classification task is too popular and simple and its application is limited.\n\nquestions_for_the_authors: A. The ablation study shows that it works even without the gold label in the example.  How does the examples help without gold labels?\n\ntypos_grammar_style_and_presentation_improvements: - Figure 2 should be placed on the top of the column.\n- Numbers in Table 1 and 2 are too small.  Since they are key results they should be displayed with larger letters.\n- CRAP->CARP in Table 2\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "5grt4Tp9Z6",
        "similarity": 0.6913,
        "coverage": 0.7143,
        "human_length": 239,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a framework to use LLMs for text classification task. It first guide the LLMs to extract the key clues from the input and use the clues to get the final decision. To improve the few-shot performance, authors also propose to train kNN to generate the examples.\n\nreasons_to_accept: The paper is overall written very well and is easy to follow. \nExperiments are thorough and complete. Using multiple methods, multiple tasks and ablation studies lead the generalizability of the paper's claims.\n\nreasons_to_reject: The authors should not claim their framework as \"annotation-free\" while using a kNN trained on supervised dataset.\n\nquestions_for_the_authors: Did you evaluate this method on some open-source LLMs, like Llama?\n\nmissing_references: Extract feature and feed into classifier: Zheng Tang, Mihai Surdeanu; It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers. Computational Linguistics 2023; 49 (1): 117\u2013156. doi: https://doi.org/10.1162/coli_a_00463\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "2oxRdK6IC3",
        "similarity": 0.7183,
        "coverage": 0.3667,
        "human_length": 389,
        "human_text": "paper_topic_and_main_contributions: This paper discusses tasks related to text classification through large models. The author first points out two reasons why large language models (LLMs) perform poorly in text classification tasks: a lack of strong reasoning capability and the performance limitations caused by window length restrictions. Subsequently, the author proposes the Clue And Reasoning Prompting (CARP) method. This method instructs large models on how to use superficial clues to enhance their reasoning ability during demonstrations. The most effective demonstrations are retrieved using kNN demonstration search. The author conducted experiments on five text classification datasets using text-devinci-003, and the results showed that this method performed well.\nContributions\uff1a 1. The author outlined the reasons why large models underperform in text classification, and introduced CARP to enhance their performance. \n2. Experiments were conducted on five commonly used text classification datasets.\n\nreasons_to_accept: 1. The writing is good and the method is easy to follow.\n\nreasons_to_reject: 1. The author mentions that large models perform poorly due to inadequate reasoning capability and performance constraints caused by window length limitations. However, the experimental results show that large models perform well (with an average accuracy of 90% under zero-shot) which makes it confusing. \n2. There is insufficient analysis and experimental support for the two reasons given. At the same time, there is no obvious answer to whether the strong enough reasoning capability proposed by the author is needed in these selected text classification datasets. \n3. There is a lack of novelty, and clues reasoning can be considered a natural extension of cot. Knn sampling is also something that has already been proposed in existing work. \n4. The method proposed by the author has limited improvement in performance.\n\nquestions_for_the_authors: 1. The experiment chose only one large model, which is text-devinci-003. Considering that ChatGPT has been released for eight months, why wasn't it considered in the experiment?  2. What do you think is the essential difference between your method and CoT?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "SAJc5m2bYe",
        "length": 334,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an approach to document-level sentiment classification task based on LLMs and in-context learning, using prompting with examples which enables reasoning to get correct labels. \nThe prompt asks the model to generate clue keywords related to positive or negative sentiments, and reasoning process that support the decision of labels, in addition to the final answer.  Also examples of (input, clues, reasoning and label) are added to the prompt for one-short and few-shot settings which can be automatically generated from the subset of training data. \nThe experimental results show the SotA scores in 4 out of 5 tasks, and they outperformed the supervised fine-tuned results.  Also this paper provides extensive discussion including ablation studies, alternative labels and consideration of low-resource scenario.\n\nreasons_to_accept: - Good design of prompt including examples.  The difference from the Chain-of-Thought method is clear.\n- The experimental results show the SotA scores in 4 out of 5 tasks, and they outperformed the supervised fine-tuned results.\n- The proposed method addresses the low-resource settings and domain adaptability.\n- The discussion of alternative labels is interesting.\n\nreasons_to_reject: - The notion of in-context learning has been already proposed.\n- The document-level classification task is too popular and simple and its application is limited.\n\nquestions_for_the_authors: A. The ablation study shows that it works even without the gold label in the example.  How does the examples help without gold labels?\n\ntypos_grammar_style_and_presentation_improvements: - Figure 2 should be placed on the top of the column.\n- Numbers in Table 1 and 2 are too small.  Since they are key results they should be displayed with larger letters.\n- CRAP->CARP in Table 2\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "5grt4Tp9Z6",
        "length": 239,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a framework to use LLMs for text classification task. It first guide the LLMs to extract the key clues from the input and use the clues to get the final decision. To improve the few-shot performance, authors also propose to train kNN to generate the examples.\n\nreasons_to_accept: The paper is overall written very well and is easy to follow. \nExperiments are thorough and complete. Using multiple methods, multiple tasks and ablation studies lead the generalizability of the paper's claims.\n\nreasons_to_reject: The authors should not claim their framework as \"annotation-free\" while using a kNN trained on supervised dataset.\n\nquestions_for_the_authors: Did you evaluate this method on some open-source LLMs, like Llama?\n\nmissing_references: Extract feature and feed into classifier: Zheng Tang, Mihai Surdeanu; It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers. Computational Linguistics 2023; 49 (1): 117\u2013156. doi: https://doi.org/10.1162/coli_a_00463\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "2oxRdK6IC3",
        "length": 389,
        "human_text": "paper_topic_and_main_contributions: This paper discusses tasks related to text classification through large models. The author first points out two reasons why large language models (LLMs) perform poorly in text classification tasks: a lack of strong reasoning capability and the performance limitations caused by window length restrictions. Subsequently, the author proposes the Clue And Reasoning Prompting (CARP) method. This method instructs large models on how to use superficial clues to enhance their reasoning ability during demonstrations. The most effective demonstrations are retrieved using kNN demonstration search. The author conducted experiments on five text classification datasets using text-devinci-003, and the results showed that this method performed well.\nContributions\uff1a 1. The author outlined the reasons why large models underperform in text classification, and introduced CARP to enhance their performance. \n2. Experiments were conducted on five commonly used text classification datasets.\n\nreasons_to_accept: 1. The writing is good and the method is easy to follow.\n\nreasons_to_reject: 1. The author mentions that large models perform poorly due to inadequate reasoning capability and performance constraints caused by window length limitations. However, the experimental results show that large models perform well (with an average accuracy of 90% under zero-shot) which makes it confusing. \n2. There is insufficient analysis and experimental support for the two reasons given. At the same time, there is no obvious answer to whether the strong enough reasoning capability proposed by the author is needed in these selected text classification datasets. \n3. There is a lack of novelty, and clues reasoning can be considered a natural extension of cot. Knn sampling is also something that has already been proposed in existing work. \n4. The method proposed by the author has limited improvement in performance.\n\nquestions_for_the_authors: 1. The experiment chose only one large model, which is text-devinci-003. Considering that ChatGPT has been released for eight months, why wasn't it considered in the experiment?  2. What do you think is the essential difference between your method and CoT?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "16_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_16_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7166333333333332,
      "max_similarity": 0.7282,
      "avg_coverage": 0.5,
      "max_coverage": 0.6429
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 381,
      "avg_human_length": 320.6666666666667
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 6,
      "suggestions_count": 4
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "SAJc5m2bYe",
        "similarity": 0.7167,
        "coverage": 0.5238,
        "human_length": 334,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an approach to document-level sentiment classification task based on LLMs and in-context learning, using prompting with examples which enables reasoning to get correct labels. \nThe prompt asks the model to generate clue keywords related to positive or negative sentiments, and reasoning process that support the decision of labels, in addition to the final answer.  Also examples of (input, clues, reasoning and label) are added to the prompt for one-short and few-shot settings which can be automatically generated from the subset of training data. \nThe experimental results show the SotA scores in 4 out of 5 tasks, and they outperformed the supervised fine-tuned results.  Also this paper provides extensive discussion including ablation studies, alternative labels and consideration of low-resource scenario.\n\nreasons_to_accept: - Good design of prompt including examples.  The difference from the Chain-of-Thought method is clear.\n- The experimental results show the SotA scores in 4 out of 5 tasks, and they outperformed the supervised fine-tuned results.\n- The proposed method addresses the low-resource settings and domain adaptability.\n- The discussion of alternative labels is interesting.\n\nreasons_to_reject: - The notion of in-context learning has been already proposed.\n- The document-level classification task is too popular and simple and its application is limited.\n\nquestions_for_the_authors: A. The ablation study shows that it works even without the gold label in the example.  How does the examples help without gold labels?\n\ntypos_grammar_style_and_presentation_improvements: - Figure 2 should be placed on the top of the column.\n- Numbers in Table 1 and 2 are too small.  Since they are key results they should be displayed with larger letters.\n- CRAP->CARP in Table 2\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "5grt4Tp9Z6",
        "similarity": 0.705,
        "coverage": 0.6429,
        "human_length": 239,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a framework to use LLMs for text classification task. It first guide the LLMs to extract the key clues from the input and use the clues to get the final decision. To improve the few-shot performance, authors also propose to train kNN to generate the examples.\n\nreasons_to_accept: The paper is overall written very well and is easy to follow. \nExperiments are thorough and complete. Using multiple methods, multiple tasks and ablation studies lead the generalizability of the paper's claims.\n\nreasons_to_reject: The authors should not claim their framework as \"annotation-free\" while using a kNN trained on supervised dataset.\n\nquestions_for_the_authors: Did you evaluate this method on some open-source LLMs, like Llama?\n\nmissing_references: Extract feature and feed into classifier: Zheng Tang, Mihai Surdeanu; It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers. Computational Linguistics 2023; 49 (1): 117\u2013156. doi: https://doi.org/10.1162/coli_a_00463\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "2oxRdK6IC3",
        "similarity": 0.7282,
        "coverage": 0.3333,
        "human_length": 389,
        "human_text": "paper_topic_and_main_contributions: This paper discusses tasks related to text classification through large models. The author first points out two reasons why large language models (LLMs) perform poorly in text classification tasks: a lack of strong reasoning capability and the performance limitations caused by window length restrictions. Subsequently, the author proposes the Clue And Reasoning Prompting (CARP) method. This method instructs large models on how to use superficial clues to enhance their reasoning ability during demonstrations. The most effective demonstrations are retrieved using kNN demonstration search. The author conducted experiments on five text classification datasets using text-devinci-003, and the results showed that this method performed well.\nContributions\uff1a 1. The author outlined the reasons why large models underperform in text classification, and introduced CARP to enhance their performance. \n2. Experiments were conducted on five commonly used text classification datasets.\n\nreasons_to_accept: 1. The writing is good and the method is easy to follow.\n\nreasons_to_reject: 1. The author mentions that large models perform poorly due to inadequate reasoning capability and performance constraints caused by window length limitations. However, the experimental results show that large models perform well (with an average accuracy of 90% under zero-shot) which makes it confusing. \n2. There is insufficient analysis and experimental support for the two reasons given. At the same time, there is no obvious answer to whether the strong enough reasoning capability proposed by the author is needed in these selected text classification datasets. \n3. There is a lack of novelty, and clues reasoning can be considered a natural extension of cot. Knn sampling is also something that has already been proposed in existing work. \n4. The method proposed by the author has limited improvement in performance.\n\nquestions_for_the_authors: 1. The experiment chose only one large model, which is text-devinci-003. Considering that ChatGPT has been released for eight months, why wasn't it considered in the experiment?  2. What do you think is the essential difference between your method and CoT?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "SAJc5m2bYe",
        "length": 334,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an approach to document-level sentiment classification task based on LLMs and in-context learning, using prompting with examples which enables reasoning to get correct labels. \nThe prompt asks the model to generate clue keywords related to positive or negative sentiments, and reasoning process that support the decision of labels, in addition to the final answer.  Also examples of (input, clues, reasoning and label) are added to the prompt for one-short and few-shot settings which can be automatically generated from the subset of training data. \nThe experimental results show the SotA scores in 4 out of 5 tasks, and they outperformed the supervised fine-tuned results.  Also this paper provides extensive discussion including ablation studies, alternative labels and consideration of low-resource scenario.\n\nreasons_to_accept: - Good design of prompt including examples.  The difference from the Chain-of-Thought method is clear.\n- The experimental results show the SotA scores in 4 out of 5 tasks, and they outperformed the supervised fine-tuned results.\n- The proposed method addresses the low-resource settings and domain adaptability.\n- The discussion of alternative labels is interesting.\n\nreasons_to_reject: - The notion of in-context learning has been already proposed.\n- The document-level classification task is too popular and simple and its application is limited.\n\nquestions_for_the_authors: A. The ablation study shows that it works even without the gold label in the example.  How does the examples help without gold labels?\n\ntypos_grammar_style_and_presentation_improvements: - Figure 2 should be placed on the top of the column.\n- Numbers in Table 1 and 2 are too small.  Since they are key results they should be displayed with larger letters.\n- CRAP->CARP in Table 2\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "5grt4Tp9Z6",
        "length": 239,
        "human_text": "paper_topic_and_main_contributions: This paper proposes a framework to use LLMs for text classification task. It first guide the LLMs to extract the key clues from the input and use the clues to get the final decision. To improve the few-shot performance, authors also propose to train kNN to generate the examples.\n\nreasons_to_accept: The paper is overall written very well and is easy to follow. \nExperiments are thorough and complete. Using multiple methods, multiple tasks and ablation studies lead the generalizability of the paper's claims.\n\nreasons_to_reject: The authors should not claim their framework as \"annotation-free\" while using a kNN trained on supervised dataset.\n\nquestions_for_the_authors: Did you evaluate this method on some open-source LLMs, like Llama?\n\nmissing_references: Extract feature and feed into classifier: Zheng Tang, Mihai Surdeanu; It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers. Computational Linguistics 2023; 49 (1): 117\u2013156. doi: https://doi.org/10.1162/coli_a_00463\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "2oxRdK6IC3",
        "length": 389,
        "human_text": "paper_topic_and_main_contributions: This paper discusses tasks related to text classification through large models. The author first points out two reasons why large language models (LLMs) perform poorly in text classification tasks: a lack of strong reasoning capability and the performance limitations caused by window length restrictions. Subsequently, the author proposes the Clue And Reasoning Prompting (CARP) method. This method instructs large models on how to use superficial clues to enhance their reasoning ability during demonstrations. The most effective demonstrations are retrieved using kNN demonstration search. The author conducted experiments on five text classification datasets using text-devinci-003, and the results showed that this method performed well.\nContributions\uff1a 1. The author outlined the reasons why large models underperform in text classification, and introduced CARP to enhance their performance. \n2. Experiments were conducted on five commonly used text classification datasets.\n\nreasons_to_accept: 1. The writing is good and the method is easy to follow.\n\nreasons_to_reject: 1. The author mentions that large models perform poorly due to inadequate reasoning capability and performance constraints caused by window length limitations. However, the experimental results show that large models perform well (with an average accuracy of 90% under zero-shot) which makes it confusing. \n2. There is insufficient analysis and experimental support for the two reasons given. At the same time, there is no obvious answer to whether the strong enough reasoning capability proposed by the author is needed in these selected text classification datasets. \n3. There is a lack of novelty, and clues reasoning can be considered a natural extension of cot. Knn sampling is also something that has already been proposed in existing work. \n4. The method proposed by the author has limited improvement in performance.\n\nquestions_for_the_authors: 1. The experiment chose only one large model, which is text-devinci-003. Considering that ChatGPT has been released for eight months, why wasn't it considered in the experiment?  2. What do you think is the essential difference between your method and CoT?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "211_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_211_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7151,
      "max_similarity": 0.7331,
      "avg_coverage": 0.5036666666666667,
      "max_coverage": 0.5652
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 496,
      "avg_human_length": 414.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 7,
      "suggestions_count": 4
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "XHyptIkTNP",
        "similarity": 0.7331,
        "coverage": 0.5652,
        "human_length": 294,
        "human_text": "paper_topic_and_main_contributions: This paper presents a framework, Prompt Optimization with Textual Gradients (ProTeGi), for automatic LLM prompt optimization. Starting from an initial prompt, it iteratively applies LLM with predefined meta-prompt over randomly sampled minibatch of examples to generate feedbacks as gradients. It then uses another meta-prompt with the initial prompt and gradients to produce improved prompts. Finally, it uses Bandit Selection algorithms to prune the size of candidate prompts (beam search). Empirical studies show the proposed method outperforms baselines like Monte Carlo search, reinforcement learning, and AutoGPT. Further studies also revealed the impact from beam search and bandit algorithms.\n\nreasons_to_accept: 1. The task discussed in this paper has high impact in both academic and industry. \n2. The paper presents a framework that is generally applicable to many LLM based methods. The analyses also shows interesting properties of the proposed method, which might lead to more followup studies. \n3. The paper is well-organized and easy to read.\n\nreasons_to_reject: Some important details are not clearly revealed in the paper. For example, it would be interesting to see what the feedback prompt and editing prompt are, and how they impact the whole framework.\n\nquestions_for_the_authors: 1. Table 1: it would also be good to show how beam size impact the performance. \n2. Line 322, although it states the log-likelihood does not help the proposed method, it is still be interesting to show how the log-likelihood change with iterations.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "aU6dK1o5Zp",
        "similarity": 0.7098,
        "coverage": 0.4643,
        "human_length": 603,
        "human_text": "paper_topic_and_main_contributions: - This paper proposes a new framework to generate, then edit, and then paraphrase and score new candidate prompts for large language models. It first generates reasoning for the wrong labeling for the model errors. This reasoning has been interpreted as textual gradients. Then it receives these reasons to perform edits over the initial prompt. Later, the candidate prompts are paraphrased to include more diverse alternatives. Throughout the search, the method keeps a beam of candidate prompts and evaluates them efficiently for the next beam iteration. It successfully uses USB bandits to evaluate the candidate prompts within a beam over a training dataset efficiently.\n\nreasons_to_accept: - The paper has a novel interpretation as the \"textual gradient\" and \"moving in the opposite direction of the textual gradient\" to make local edits in the semantic space. This has been explained well enough and the method clearly outperforms previous baselines on their 4 datasets.\n- As shown by the examples in the paper (Table 4), the generated prompts by this method are more understandable and contains more information about the expected context/output and outperforms the optimized prompts given another baselines.\n\nreasons_to_reject: - The proposed method to find better performing prompts is applicable only on truly large language models as they are capable of generating the textual gradients or reasoning steps for wrong generation by just prompting the LLM. The technique might not be useful for medium-size LMs such as T5-large or BERT-large, however previous discrete gradient-free prompt optimization techniques such as GrIPs or RLPrompt can be applied generally to any LM and they have been tested with medium-size LMs. It would be great to see how your concept of textual gradients can be extracted or applied from/to medium-size LMs.\n\nquestions_for_the_authors: - Is it OK to interpret your textual gradients as kind of chain-of-thoughts to generate new prompts using the $LLM_{\\delta}(p, g_i, e)$?\n- Can the authors expand the details of hyper-parameters for some of the baselines? The following paragraph (Line 300) in your paper seems a bit concerning regarding the experimental results: \"As the focus of this paper is non-parametric algorithms with broad applicability, we did not conduct any hyper-parameter search for the baseline or proposed algorithms, instead adopting default values and then using the same parameters throughout.\" \nYou are selecting 150 examples as your internal validation. Why is it not possible to tune the hyper-parameters of the baselines? What is the \"compelling\" reason to stay fully non-parametric while tuning few hyper-parameters might impact the results significantly?\n- What would be the performance of your models using just $LLM_{mc}$ without generating textual gradients + edits and just purely relying on another prompted LLM to rephrase the initial $P_0$?\n\ntypos_grammar_style_and_presentation_improvements: - Fix typo in Algorithm 1 $z$b, what is z?\n- The mention of GrIPS within an RL technique for prompt optimization is not correct! The technique is purely an edit based method and then selection of new candidate prompts on a search set based on the final task performance + entropy of predictions. Regarding the phrase chunking of GrIPS, it does not use pure NLTK! They use phrase chunking based on another CRF-based constituency parser. Please modify the description of the method correctly.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "mZyHDkEUvT",
        "similarity": 0.7024,
        "coverage": 0.4815,
        "human_length": 346,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an automatic prompt optimization method by iteratively prompting an LM to improve upon previous prompts. \nUsing a set of labeled examples, the method starts with an initial prompt and iteratively query the language model for new prompts when errors occur. An upper confidence bound (UCB)-based algorithm is proposed to select new prompts. \nThe authors evaluate their method on 4 benchmarks using GPT3.5 and show the efficacy of their method.\n\nreasons_to_accept: The proposed method provides a flexible way to optimize prompts in a gradient-free manner using discrete feedback. It's effective and potentially useful for many other tasks that require manual prompt engineering.\n\nreasons_to_reject: Gradient-free iterative prompt search has been explored in previous work (e.g., RLPrompt, GRIPS, TEMPERA). It is important to emphasize the novelty and contributions of this work and provide both qualitative and quantitative comparisons with previous work. However, such comparisons are missing from the paper. I.e., what kind of errors can be fixed by the proposed method but not by previous work? What are some example optimization sequences of GRIPS and the proposed method on the same inputs?\n\nquestions_for_the_authors: - Line 351, GRIPS (Prasad et al. 2022) does not use reinforcement learning. Thus it should not be posited in that paragraph.\nA. How does the proposed method compare to GRIPS in terms of performance?\nB. In Figure 4, the performance on Ethos and Sarcasm drops after ~4 optimization steps. What is the reason for this?\n\ntypos_grammar_style_and_presentation_improvements: 1. In Figure 2, where is $\\Delta$ mentioned in the caption? \n2. Line 183, engadge -> engage\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "XHyptIkTNP",
        "length": 294,
        "human_text": "paper_topic_and_main_contributions: This paper presents a framework, Prompt Optimization with Textual Gradients (ProTeGi), for automatic LLM prompt optimization. Starting from an initial prompt, it iteratively applies LLM with predefined meta-prompt over randomly sampled minibatch of examples to generate feedbacks as gradients. It then uses another meta-prompt with the initial prompt and gradients to produce improved prompts. Finally, it uses Bandit Selection algorithms to prune the size of candidate prompts (beam search). Empirical studies show the proposed method outperforms baselines like Monte Carlo search, reinforcement learning, and AutoGPT. Further studies also revealed the impact from beam search and bandit algorithms.\n\nreasons_to_accept: 1. The task discussed in this paper has high impact in both academic and industry. \n2. The paper presents a framework that is generally applicable to many LLM based methods. The analyses also shows interesting properties of the proposed method, which might lead to more followup studies. \n3. The paper is well-organized and easy to read.\n\nreasons_to_reject: Some important details are not clearly revealed in the paper. For example, it would be interesting to see what the feedback prompt and editing prompt are, and how they impact the whole framework.\n\nquestions_for_the_authors: 1. Table 1: it would also be good to show how beam size impact the performance. \n2. Line 322, although it states the log-likelihood does not help the proposed method, it is still be interesting to show how the log-likelihood change with iterations.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "aU6dK1o5Zp",
        "length": 603,
        "human_text": "paper_topic_and_main_contributions: - This paper proposes a new framework to generate, then edit, and then paraphrase and score new candidate prompts for large language models. It first generates reasoning for the wrong labeling for the model errors. This reasoning has been interpreted as textual gradients. Then it receives these reasons to perform edits over the initial prompt. Later, the candidate prompts are paraphrased to include more diverse alternatives. Throughout the search, the method keeps a beam of candidate prompts and evaluates them efficiently for the next beam iteration. It successfully uses USB bandits to evaluate the candidate prompts within a beam over a training dataset efficiently.\n\nreasons_to_accept: - The paper has a novel interpretation as the \"textual gradient\" and \"moving in the opposite direction of the textual gradient\" to make local edits in the semantic space. This has been explained well enough and the method clearly outperforms previous baselines on their 4 datasets.\n- As shown by the examples in the paper (Table 4), the generated prompts by this method are more understandable and contains more information about the expected context/output and outperforms the optimized prompts given another baselines.\n\nreasons_to_reject: - The proposed method to find better performing prompts is applicable only on truly large language models as they are capable of generating the textual gradients or reasoning steps for wrong generation by just prompting the LLM. The technique might not be useful for medium-size LMs such as T5-large or BERT-large, however previous discrete gradient-free prompt optimization techniques such as GrIPs or RLPrompt can be applied generally to any LM and they have been tested with medium-size LMs. It would be great to see how your concept of textual gradients can be extracted or applied from/to medium-size LMs.\n\nquestions_for_the_authors: - Is it OK to interpret your textual gradients as kind of chain-of-thoughts to generate new prompts using the $LLM_{\\delta}(p, g_i, e)$?\n- Can the authors expand the details of hyper-parameters for some of the baselines? The following paragraph (Line 300) in your paper seems a bit concerning regarding the experimental results: \"As the focus of this paper is non-parametric algorithms with broad applicability, we did not conduct any hyper-parameter search for the baseline or proposed algorithms, instead adopting default values and then using the same parameters throughout.\" \nYou are selecting 150 examples as your internal validation. Why is it not possible to tune the hyper-parameters of the baselines? What is the \"compelling\" reason to stay fully non-parametric while tuning few hyper-parameters might impact the results significantly?\n- What would be the performance of your models using just $LLM_{mc}$ without generating textual gradients + edits and just purely relying on another prompted LLM to rephrase the initial $P_0$?\n\ntypos_grammar_style_and_presentation_improvements: - Fix typo in Algorithm 1 $z$b, what is z?\n- The mention of GrIPS within an RL technique for prompt optimization is not correct! The technique is purely an edit based method and then selection of new candidate prompts on a search set based on the final task performance + entropy of predictions. Regarding the phrase chunking of GrIPS, it does not use pure NLTK! They use phrase chunking based on another CRF-based constituency parser. Please modify the description of the method correctly.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "mZyHDkEUvT",
        "length": 346,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an automatic prompt optimization method by iteratively prompting an LM to improve upon previous prompts. \nUsing a set of labeled examples, the method starts with an initial prompt and iteratively query the language model for new prompts when errors occur. An upper confidence bound (UCB)-based algorithm is proposed to select new prompts. \nThe authors evaluate their method on 4 benchmarks using GPT3.5 and show the efficacy of their method.\n\nreasons_to_accept: The proposed method provides a flexible way to optimize prompts in a gradient-free manner using discrete feedback. It's effective and potentially useful for many other tasks that require manual prompt engineering.\n\nreasons_to_reject: Gradient-free iterative prompt search has been explored in previous work (e.g., RLPrompt, GRIPS, TEMPERA). It is important to emphasize the novelty and contributions of this work and provide both qualitative and quantitative comparisons with previous work. However, such comparisons are missing from the paper. I.e., what kind of errors can be fixed by the proposed method but not by previous work? What are some example optimization sequences of GRIPS and the proposed method on the same inputs?\n\nquestions_for_the_authors: - Line 351, GRIPS (Prasad et al. 2022) does not use reinforcement learning. Thus it should not be posited in that paragraph.\nA. How does the proposed method compare to GRIPS in terms of performance?\nB. In Figure 4, the performance on Ethos and Sarcasm drops after ~4 optimization steps. What is the reason for this?\n\ntypos_grammar_style_and_presentation_improvements: 1. In Figure 2, where is $\\Delta$ mentioned in the caption? \n2. Line 183, engadge -> engage\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "211_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_211_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7186666666666666,
      "max_similarity": 0.733,
      "avg_coverage": 0.47469999999999996,
      "max_coverage": 0.4815
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 536,
      "avg_human_length": 414.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 3,
      "weaknesses_count": 7,
      "suggestions_count": 3
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "XHyptIkTNP",
        "similarity": 0.733,
        "coverage": 0.4783,
        "human_length": 294,
        "human_text": "paper_topic_and_main_contributions: This paper presents a framework, Prompt Optimization with Textual Gradients (ProTeGi), for automatic LLM prompt optimization. Starting from an initial prompt, it iteratively applies LLM with predefined meta-prompt over randomly sampled minibatch of examples to generate feedbacks as gradients. It then uses another meta-prompt with the initial prompt and gradients to produce improved prompts. Finally, it uses Bandit Selection algorithms to prune the size of candidate prompts (beam search). Empirical studies show the proposed method outperforms baselines like Monte Carlo search, reinforcement learning, and AutoGPT. Further studies also revealed the impact from beam search and bandit algorithms.\n\nreasons_to_accept: 1. The task discussed in this paper has high impact in both academic and industry. \n2. The paper presents a framework that is generally applicable to many LLM based methods. The analyses also shows interesting properties of the proposed method, which might lead to more followup studies. \n3. The paper is well-organized and easy to read.\n\nreasons_to_reject: Some important details are not clearly revealed in the paper. For example, it would be interesting to see what the feedback prompt and editing prompt are, and how they impact the whole framework.\n\nquestions_for_the_authors: 1. Table 1: it would also be good to show how beam size impact the performance. \n2. Line 322, although it states the log-likelihood does not help the proposed method, it is still be interesting to show how the log-likelihood change with iterations.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "aU6dK1o5Zp",
        "similarity": 0.7129,
        "coverage": 0.4643,
        "human_length": 603,
        "human_text": "paper_topic_and_main_contributions: - This paper proposes a new framework to generate, then edit, and then paraphrase and score new candidate prompts for large language models. It first generates reasoning for the wrong labeling for the model errors. This reasoning has been interpreted as textual gradients. Then it receives these reasons to perform edits over the initial prompt. Later, the candidate prompts are paraphrased to include more diverse alternatives. Throughout the search, the method keeps a beam of candidate prompts and evaluates them efficiently for the next beam iteration. It successfully uses USB bandits to evaluate the candidate prompts within a beam over a training dataset efficiently.\n\nreasons_to_accept: - The paper has a novel interpretation as the \"textual gradient\" and \"moving in the opposite direction of the textual gradient\" to make local edits in the semantic space. This has been explained well enough and the method clearly outperforms previous baselines on their 4 datasets.\n- As shown by the examples in the paper (Table 4), the generated prompts by this method are more understandable and contains more information about the expected context/output and outperforms the optimized prompts given another baselines.\n\nreasons_to_reject: - The proposed method to find better performing prompts is applicable only on truly large language models as they are capable of generating the textual gradients or reasoning steps for wrong generation by just prompting the LLM. The technique might not be useful for medium-size LMs such as T5-large or BERT-large, however previous discrete gradient-free prompt optimization techniques such as GrIPs or RLPrompt can be applied generally to any LM and they have been tested with medium-size LMs. It would be great to see how your concept of textual gradients can be extracted or applied from/to medium-size LMs.\n\nquestions_for_the_authors: - Is it OK to interpret your textual gradients as kind of chain-of-thoughts to generate new prompts using the $LLM_{\\delta}(p, g_i, e)$?\n- Can the authors expand the details of hyper-parameters for some of the baselines? The following paragraph (Line 300) in your paper seems a bit concerning regarding the experimental results: \"As the focus of this paper is non-parametric algorithms with broad applicability, we did not conduct any hyper-parameter search for the baseline or proposed algorithms, instead adopting default values and then using the same parameters throughout.\" \nYou are selecting 150 examples as your internal validation. Why is it not possible to tune the hyper-parameters of the baselines? What is the \"compelling\" reason to stay fully non-parametric while tuning few hyper-parameters might impact the results significantly?\n- What would be the performance of your models using just $LLM_{mc}$ without generating textual gradients + edits and just purely relying on another prompted LLM to rephrase the initial $P_0$?\n\ntypos_grammar_style_and_presentation_improvements: - Fix typo in Algorithm 1 $z$b, what is z?\n- The mention of GrIPS within an RL technique for prompt optimization is not correct! The technique is purely an edit based method and then selection of new candidate prompts on a search set based on the final task performance + entropy of predictions. Regarding the phrase chunking of GrIPS, it does not use pure NLTK! They use phrase chunking based on another CRF-based constituency parser. Please modify the description of the method correctly.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "mZyHDkEUvT",
        "similarity": 0.7101,
        "coverage": 0.4815,
        "human_length": 346,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an automatic prompt optimization method by iteratively prompting an LM to improve upon previous prompts. \nUsing a set of labeled examples, the method starts with an initial prompt and iteratively query the language model for new prompts when errors occur. An upper confidence bound (UCB)-based algorithm is proposed to select new prompts. \nThe authors evaluate their method on 4 benchmarks using GPT3.5 and show the efficacy of their method.\n\nreasons_to_accept: The proposed method provides a flexible way to optimize prompts in a gradient-free manner using discrete feedback. It's effective and potentially useful for many other tasks that require manual prompt engineering.\n\nreasons_to_reject: Gradient-free iterative prompt search has been explored in previous work (e.g., RLPrompt, GRIPS, TEMPERA). It is important to emphasize the novelty and contributions of this work and provide both qualitative and quantitative comparisons with previous work. However, such comparisons are missing from the paper. I.e., what kind of errors can be fixed by the proposed method but not by previous work? What are some example optimization sequences of GRIPS and the proposed method on the same inputs?\n\nquestions_for_the_authors: - Line 351, GRIPS (Prasad et al. 2022) does not use reinforcement learning. Thus it should not be posited in that paragraph.\nA. How does the proposed method compare to GRIPS in terms of performance?\nB. In Figure 4, the performance on Ethos and Sarcasm drops after ~4 optimization steps. What is the reason for this?\n\ntypos_grammar_style_and_presentation_improvements: 1. In Figure 2, where is $\\Delta$ mentioned in the caption? \n2. Line 183, engadge -> engage\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "XHyptIkTNP",
        "length": 294,
        "human_text": "paper_topic_and_main_contributions: This paper presents a framework, Prompt Optimization with Textual Gradients (ProTeGi), for automatic LLM prompt optimization. Starting from an initial prompt, it iteratively applies LLM with predefined meta-prompt over randomly sampled minibatch of examples to generate feedbacks as gradients. It then uses another meta-prompt with the initial prompt and gradients to produce improved prompts. Finally, it uses Bandit Selection algorithms to prune the size of candidate prompts (beam search). Empirical studies show the proposed method outperforms baselines like Monte Carlo search, reinforcement learning, and AutoGPT. Further studies also revealed the impact from beam search and bandit algorithms.\n\nreasons_to_accept: 1. The task discussed in this paper has high impact in both academic and industry. \n2. The paper presents a framework that is generally applicable to many LLM based methods. The analyses also shows interesting properties of the proposed method, which might lead to more followup studies. \n3. The paper is well-organized and easy to read.\n\nreasons_to_reject: Some important details are not clearly revealed in the paper. For example, it would be interesting to see what the feedback prompt and editing prompt are, and how they impact the whole framework.\n\nquestions_for_the_authors: 1. Table 1: it would also be good to show how beam size impact the performance. \n2. Line 322, although it states the log-likelihood does not help the proposed method, it is still be interesting to show how the log-likelihood change with iterations.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "aU6dK1o5Zp",
        "length": 603,
        "human_text": "paper_topic_and_main_contributions: - This paper proposes a new framework to generate, then edit, and then paraphrase and score new candidate prompts for large language models. It first generates reasoning for the wrong labeling for the model errors. This reasoning has been interpreted as textual gradients. Then it receives these reasons to perform edits over the initial prompt. Later, the candidate prompts are paraphrased to include more diverse alternatives. Throughout the search, the method keeps a beam of candidate prompts and evaluates them efficiently for the next beam iteration. It successfully uses USB bandits to evaluate the candidate prompts within a beam over a training dataset efficiently.\n\nreasons_to_accept: - The paper has a novel interpretation as the \"textual gradient\" and \"moving in the opposite direction of the textual gradient\" to make local edits in the semantic space. This has been explained well enough and the method clearly outperforms previous baselines on their 4 datasets.\n- As shown by the examples in the paper (Table 4), the generated prompts by this method are more understandable and contains more information about the expected context/output and outperforms the optimized prompts given another baselines.\n\nreasons_to_reject: - The proposed method to find better performing prompts is applicable only on truly large language models as they are capable of generating the textual gradients or reasoning steps for wrong generation by just prompting the LLM. The technique might not be useful for medium-size LMs such as T5-large or BERT-large, however previous discrete gradient-free prompt optimization techniques such as GrIPs or RLPrompt can be applied generally to any LM and they have been tested with medium-size LMs. It would be great to see how your concept of textual gradients can be extracted or applied from/to medium-size LMs.\n\nquestions_for_the_authors: - Is it OK to interpret your textual gradients as kind of chain-of-thoughts to generate new prompts using the $LLM_{\\delta}(p, g_i, e)$?\n- Can the authors expand the details of hyper-parameters for some of the baselines? The following paragraph (Line 300) in your paper seems a bit concerning regarding the experimental results: \"As the focus of this paper is non-parametric algorithms with broad applicability, we did not conduct any hyper-parameter search for the baseline or proposed algorithms, instead adopting default values and then using the same parameters throughout.\" \nYou are selecting 150 examples as your internal validation. Why is it not possible to tune the hyper-parameters of the baselines? What is the \"compelling\" reason to stay fully non-parametric while tuning few hyper-parameters might impact the results significantly?\n- What would be the performance of your models using just $LLM_{mc}$ without generating textual gradients + edits and just purely relying on another prompted LLM to rephrase the initial $P_0$?\n\ntypos_grammar_style_and_presentation_improvements: - Fix typo in Algorithm 1 $z$b, what is z?\n- The mention of GrIPS within an RL technique for prompt optimization is not correct! The technique is purely an edit based method and then selection of new candidate prompts on a search set based on the final task performance + entropy of predictions. Regarding the phrase chunking of GrIPS, it does not use pure NLTK! They use phrase chunking based on another CRF-based constituency parser. Please modify the description of the method correctly.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "mZyHDkEUvT",
        "length": 346,
        "human_text": "paper_topic_and_main_contributions: This paper proposes an automatic prompt optimization method by iteratively prompting an LM to improve upon previous prompts. \nUsing a set of labeled examples, the method starts with an initial prompt and iteratively query the language model for new prompts when errors occur. An upper confidence bound (UCB)-based algorithm is proposed to select new prompts. \nThe authors evaluate their method on 4 benchmarks using GPT3.5 and show the efficacy of their method.\n\nreasons_to_accept: The proposed method provides a flexible way to optimize prompts in a gradient-free manner using discrete feedback. It's effective and potentially useful for many other tasks that require manual prompt engineering.\n\nreasons_to_reject: Gradient-free iterative prompt search has been explored in previous work (e.g., RLPrompt, GRIPS, TEMPERA). It is important to emphasize the novelty and contributions of this work and provide both qualitative and quantitative comparisons with previous work. However, such comparisons are missing from the paper. I.e., what kind of errors can be fixed by the proposed method but not by previous work? What are some example optimization sequences of GRIPS and the proposed method on the same inputs?\n\nquestions_for_the_authors: - Line 351, GRIPS (Prasad et al. 2022) does not use reinforcement learning. Thus it should not be posited in that paragraph.\nA. How does the proposed method compare to GRIPS in terms of performance?\nB. In Figure 4, the performance on Ethos and Sarcasm drops after ~4 optimization steps. What is the reason for this?\n\ntypos_grammar_style_and_presentation_improvements: 1. In Figure 2, where is $\\Delta$ mentioned in the caption? \n2. Line 183, engadge -> engage\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "178_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_178_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7307,
      "max_similarity": 0.7485,
      "avg_coverage": 0.37873333333333337,
      "max_coverage": 0.5789
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 487,
      "avg_human_length": 772.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 10,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "A0DhPAObTg",
        "similarity": 0.7229,
        "coverage": 0.32,
        "human_length": 954,
        "human_text": "paper_topic_and_main_contributions: The paper is about the interpretability of NLP models, specifically in the domain of probing classifiers for linguistic properties. \nThe authors propose a new approach for designing an unsupervised probe, reminiscent of the probe proposed by Hewitt and Manning for recovering syntactic structure, but unsupervised. \nThe authors define such a probe (again, similar to the design of Hewitt and Manning, by trying to replicate a syntactic tree structure, defined by the depths of the nodes (words/tokens) in the graph) quite thoroughly, discussing the different properties and constraints on such tree, and how to achieve (approximate) it.\n*Main Contributions* - The authors propose a new unsupervised structural probe for analyzing the internal representations of neural language models - Using a metric defined for such a probe, the authors propose a method to improve fine-tuning.\n*Methodology* While my topology background is not very strong, I did my best to follow the different metrics, steps, and logical steps made in the paper, and it sounds reasonable (although I cannot guarantee that).\n*Evaluation* The paper conducts two main experiments:  1. Measuring their probe using BERT-large on different sentences (from different GLUE datasets) 2. Using their derived metric as an additional loss function during fine-tuning of the model on the linguistic acceptability dataset (CoLA).\nThe first experiment was unclear to me. I could not understand the results or the claims that the authors made. \nThe second experiment shows a minor improvement in performance and is not convincing. Moreover, the authors did not perform any statistical significance tests, which makes it hard to take the 1.5 difference very seriously. \nBesides these two experiments, there were no others. \nI don\u2019t understand this lack of comparison, given the wide range of relevant papers in the field (which they also cite). \nHow does their probe compare to others? E.g. Hewitt and Manning How does their probe compare to the \u201creal\u201d syntactic tree of a sentence (using an annotated treebank) *Writing* The paper was very hard to understand. \nIt makes very strong claims, which I often couldn\u2019t understand or interpret. It is very opaque at times. The results and their interpretation as a list of findings (written as fN.) and conclusions (written as cN.), open problems (written as sN.), and preferences (written as pN.) are also nonstandard. Such writing is not problematic per se, but it breaks the flow of the paper and reads as a list of findings, rather than a story, which is much harder to understand. Specific details are given in the \u201cpresentation improvements\u201d section. \nOverall recommendation: while I am generally excited about the idea and direction of the paper, I think the current state of writing (and experiments, to some degree) can be greatly improved. I strongly encourage the authors to resubmit the paper in an improved state.\n\nreasons_to_accept: - The paper provides an interesting formal explanation of probing, and specifically the purpose of unsupervised probing - The paper proposes a new unsupervised structural probe\n\nreasons_to_reject: - The paper\u2019s experimental setup is lacking comparison with previous work, and evaluation of the constructed probe - The paper\u2019s writing can be greatly improved\n\nmissing_references: Unsupervised Probing (also with syntactic structure)  - https://aclanthology.org/2020.acl-main.383.pdf Causal Probing - https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00359/98091/Amnesic-Probing-Behavioral-Explanation-with - https://direct.mit.edu/coli/article/47/2/333/98518\n\ntypos_grammar_style_and_presentation_improvements: - L.1 LMs are *not* expected to effectively map input text to vectors, this is perhaps an implicit requirement, but their main objective is to be able to predict the next word in a sequence - L.3 what are inherent relationships?\n- L.6 what are internal relations?\n- L.10 what do you mean by white-box?\n- L.12 what do you mean by inherent interpretability?\n- L.15 what do you mean by a novel line?\n- L.19 \u201cline of investigation\u201d sounds peculiar in this context - L.21 \u201cconducted measurements\u201d sounds peculiar - L.23 \u201cpropose a speculation\u201d - L.24 what is that \u201cworking mechanism\u201d?\n- L.28 what are submodules?\n- L.34 \u201calgebraic operation\u201d is very vague - L.37 what do you mean by \u201cwhat-box model\u201d in this context?\n- L.45 \u201cmetric computation process usually requires interpretability\u201d - strange phrasing, I cannot make sense of it - L.47 \u201cinherent interpretability of the source model is missing\u201d - strange phrasing, I cannot make sense of it - L.50 what do you mean by semiquantitative?\n- L.51 what are artificial hypotheses?\n- L.54 what\u2019s the connection here between contextual embeddings and static embeddings?\n- L.56 what does intuition have to do with interpretability?\n- L.62 what do you mean by \u201ccomplete word\u201d?\n- L.71 what are \u201ccomplete syntax relations\u201d?\n- L.76 \u201cexist other embeddings\u201d - I don\u2019t see the connection - L.79 why do we care about an upper bound?\n- L.117 how is your approach \u201cbeneficial for interpretability\u201d?\n- L.148 what are submodules?\n- L.181 what is \u201cModel Evidence Maximum\u201d?  - The presentation of the equations can also be improved to be more friendly (e.g. pdep -> p-dep) - Tables 1+2: they are not clear, and their captions are also very opaque.\n- L.425-434 (e1-e4) unclear experiments - L.444 strange formatting (f1.)\n- L.542-546 I don\u2019t understand this claim and why would we want these properties to be linearly encoded in the representations - L.607-625. This paragraph is very opaque\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "AFtSRJJ6dY",
        "similarity": 0.7485,
        "coverage": 0.2373,
        "human_length": 955,
        "human_text": "paper_topic_and_main_contributions: The paper is focused on designing a suite of metrics called the tree topological probe which is used for interpretability of pre-trained language models. Specifically, the paper examines BERT-large and uses their findings to enhance fine-tuning performance. The main contribution of the work is to provide the interpretability community with a self-supervised probe to examine (pre-trained) language models.\n\nreasons_to_accept: I like the general intuitions you have on the probe as detailed in section 3.1. The work has theoretical contributions and the framework overall could be beneficial to the wider NLP interpretability community.  The findings in sections 4.1 are interesting and reveal insights previously not available in the structural probe alone. However, it's really difficult to interpret how these findings could apply to the broader community as it is only performed on BERT-large. The subsequent results of fine-tuning layers using insights from 4.1 are convincing.\n\nreasons_to_reject: My main worries for the paper are: 1. The limited empirical evidence of the work given that the main experiments were only conducted on a single model. I think having two or three additional analyses of related models could really strengthen your work.\n2. There are a number of places where perturbations/sensitivity analysis could be done to reveal the robustness of the method/results. Calculating error bounds in Table 3, using multiple different datasets, different ways of approximating max(s_w).  3. The writing could benefit from including more motivation throughout the introduction and transition paragraphs. Why is it important study these phenomena? How would this benefit the larger community? I think there are compelling reasons here and the authors should seek to articulate them.  4. The ethics statement feels detached/removed from the rest of the paper. It's important for the ethics section to engage with the ethical harms and considerations of the presented theories and conducted experiments.\nDetails: There are a number of places where the authors are making assumptions without citations or sufficient motivation and reasoning. \ne.g., lines 052-059 --- it's not immediately clear to readers why the distinction between contextual and static embeddings is sufficient for researchers to trust a model's effectiveness. \" Researchers' intuitions\" is not defined/a poor motivation.\nThere is motivation provided in the introduction that is not answered throughout the paper e.g., lines 075-080. These are not central questions to the paper and would recommend cutting for space and overall tightening the writing of the paper.\nThere are also some claims that could be reworded with more hedges/citations as they are not obviously true statements e.g., lines 100-102. There is also a volume of follow-up work to Hewitt and Ethayarajh which should be cited here as well.\nThey key intuition in the self-supervised structural probe is a little hidden in the introduction: line 125 \"In the case where the internal constraints of the probed features are well defined, a probe that detects these features can naturally induce a probe that detects the internal constraints, which is self-supervised.\" This should be highlighted earlier in the introduction.\nA key challenge which you hint on line 227 is that there are multiple ways of constructing model M. It would be nice to see more evaluation here (ablation studies) to understand how sensitive the probe is to these chosen parameters.\nWriting suggestions:  It would be nice if you could define min(s_w) and max(s_w) at the same place in section 3.3 rather than defining max(s_w) when you start talking about enhancements. It would also be nice to intuitively describe what each of these sequences represent. From there, you can easily present equations (0), (10), and (12).  Having language like, \"since\" and \"similarly\" is not sufficient language to transition between your equations line 366. Why is it important to present the following equations? Why do we care about the properties of these sets? The motivation here is lost to the reader and could really strengthen your work.\nIn the case where Xssp(M) = Xessp(M), it would be interesting to know how often this happens. You could put a nice metric to how often the structural probe would miss insights only your probe can interpret. Could also consider moving content from lines 389-403 together with this initial definition (could also go in the appendix).\nLine 414, need some justification on why you are focused on BERT-large. Is it the most representative model? Currently as it reads, it's difficult to know if this only works on BERT-large or if there are unique properties to BERT-large that make this analysis rich. Do you have insights on what woudl happen with BERT-base/Roberta?\n\ntypos_grammar_style_and_presentation_improvements: This is a nit but equation (1) is really critical to the understanding of the intuition of your paper. It would be helpful to try and be very clear with the language leading up to it, sections 3.1 and 3.2. A simple figure could also help.\nLines 307 - 323 are dense to read. It might help to define and set up all the variables first, before jumping into equations. They currently read as sentences which is hard to parse.\nLine 382 could be moved to the appendix. Might be nice to have 367 as a separate section that is comparing and contrasting to the structural probe.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "etu0CTBIde",
        "similarity": 0.7207,
        "coverage": 0.5789,
        "human_length": 409,
        "human_text": "paper_topic_and_main_contributions: This paper rethinks the probing of PLM representations. \nBased on the motivation that the current probing methods lack an upper bound to help us understand the model capability more clearly, the authors propose a novel self-supervised probe: a tree-topological probe to probe the hierarchical structure learned by BERT. They theoretically justify the bounding relationship of the tree-topological probe and the structural probe and apply the probing method on BERT-large. Based on their probing results, they provide several findings and speculation, and the probing method can help find the submodules that need to be enhanced. They verify these findings by finetuning experiments, their probing method indeed can identify the submodules that need to (or cannot) be enhanced (by structural knowledge).\n\nreasons_to_accept: - The motivation is great, existing probing methods usually can only provide results like \"A is better than B\", and the results are often predictable. This paper proposes a probing method that can know \"Is A or B good enough?\", which can provide additional insights into the model's capability.\n- The paper provides a detailed theoretical analysis and a \"Rethinking process\" on how to induce the self-supervised probe.\n- They proposed the tree-topological probe that is \"upgraded\" from the structural probe, and the new probing method can provide an upper bound and has the potential to be a guideline for \"upgrading\" other probing methods.\n- Authors provide extensive analysis of the probing results and produce findings and speculations.\n- Authors verify their findings by a finetuning experiment, showing that their probing method is useful to identify the capability of each submodule, and enhancing these submodules by hierarchical information is useful. Which makes the whole paper self-consistent.\n\nreasons_to_reject: - Would be better by more experiments (as you mentioned, e2,3,4, and other PLMs)\n\nquestions_for_the_authors: - Is your method potentially applicable to LLMs?\n- Will the distribution approximation at L383 significantly affect the probing results?\n\ntypos_grammar_style_and_presentation_improvements: L274, given should be in \\text{given}, not in the math format\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "A0DhPAObTg",
        "length": 954,
        "human_text": "paper_topic_and_main_contributions: The paper is about the interpretability of NLP models, specifically in the domain of probing classifiers for linguistic properties. \nThe authors propose a new approach for designing an unsupervised probe, reminiscent of the probe proposed by Hewitt and Manning for recovering syntactic structure, but unsupervised. \nThe authors define such a probe (again, similar to the design of Hewitt and Manning, by trying to replicate a syntactic tree structure, defined by the depths of the nodes (words/tokens) in the graph) quite thoroughly, discussing the different properties and constraints on such tree, and how to achieve (approximate) it.\n*Main Contributions* - The authors propose a new unsupervised structural probe for analyzing the internal representations of neural language models - Using a metric defined for such a probe, the authors propose a method to improve fine-tuning.\n*Methodology* While my topology background is not very strong, I did my best to follow the different metrics, steps, and logical steps made in the paper, and it sounds reasonable (although I cannot guarantee that).\n*Evaluation* The paper conducts two main experiments:  1. Measuring their probe using BERT-large on different sentences (from different GLUE datasets) 2. Using their derived metric as an additional loss function during fine-tuning of the model on the linguistic acceptability dataset (CoLA).\nThe first experiment was unclear to me. I could not understand the results or the claims that the authors made. \nThe second experiment shows a minor improvement in performance and is not convincing. Moreover, the authors did not perform any statistical significance tests, which makes it hard to take the 1.5 difference very seriously. \nBesides these two experiments, there were no others. \nI don\u2019t understand this lack of comparison, given the wide range of relevant papers in the field (which they also cite). \nHow does their probe compare to others? E.g. Hewitt and Manning How does their probe compare to the \u201creal\u201d syntactic tree of a sentence (using an annotated treebank) *Writing* The paper was very hard to understand. \nIt makes very strong claims, which I often couldn\u2019t understand or interpret. It is very opaque at times. The results and their interpretation as a list of findings (written as fN.) and conclusions (written as cN.), open problems (written as sN.), and preferences (written as pN.) are also nonstandard. Such writing is not problematic per se, but it breaks the flow of the paper and reads as a list of findings, rather than a story, which is much harder to understand. Specific details are given in the \u201cpresentation improvements\u201d section. \nOverall recommendation: while I am generally excited about the idea and direction of the paper, I think the current state of writing (and experiments, to some degree) can be greatly improved. I strongly encourage the authors to resubmit the paper in an improved state.\n\nreasons_to_accept: - The paper provides an interesting formal explanation of probing, and specifically the purpose of unsupervised probing - The paper proposes a new unsupervised structural probe\n\nreasons_to_reject: - The paper\u2019s experimental setup is lacking comparison with previous work, and evaluation of the constructed probe - The paper\u2019s writing can be greatly improved\n\nmissing_references: Unsupervised Probing (also with syntactic structure)  - https://aclanthology.org/2020.acl-main.383.pdf Causal Probing - https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00359/98091/Amnesic-Probing-Behavioral-Explanation-with - https://direct.mit.edu/coli/article/47/2/333/98518\n\ntypos_grammar_style_and_presentation_improvements: - L.1 LMs are *not* expected to effectively map input text to vectors, this is perhaps an implicit requirement, but their main objective is to be able to predict the next word in a sequence - L.3 what are inherent relationships?\n- L.6 what are internal relations?\n- L.10 what do you mean by white-box?\n- L.12 what do you mean by inherent interpretability?\n- L.15 what do you mean by a novel line?\n- L.19 \u201cline of investigation\u201d sounds peculiar in this context - L.21 \u201cconducted measurements\u201d sounds peculiar - L.23 \u201cpropose a speculation\u201d - L.24 what is that \u201cworking mechanism\u201d?\n- L.28 what are submodules?\n- L.34 \u201calgebraic operation\u201d is very vague - L.37 what do you mean by \u201cwhat-box model\u201d in this context?\n- L.45 \u201cmetric computation process usually requires interpretability\u201d - strange phrasing, I cannot make sense of it - L.47 \u201cinherent interpretability of the source model is missing\u201d - strange phrasing, I cannot make sense of it - L.50 what do you mean by semiquantitative?\n- L.51 what are artificial hypotheses?\n- L.54 what\u2019s the connection here between contextual embeddings and static embeddings?\n- L.56 what does intuition have to do with interpretability?\n- L.62 what do you mean by \u201ccomplete word\u201d?\n- L.71 what are \u201ccomplete syntax relations\u201d?\n- L.76 \u201cexist other embeddings\u201d - I don\u2019t see the connection - L.79 why do we care about an upper bound?\n- L.117 how is your approach \u201cbeneficial for interpretability\u201d?\n- L.148 what are submodules?\n- L.181 what is \u201cModel Evidence Maximum\u201d?  - The presentation of the equations can also be improved to be more friendly (e.g. pdep -> p-dep) - Tables 1+2: they are not clear, and their captions are also very opaque.\n- L.425-434 (e1-e4) unclear experiments - L.444 strange formatting (f1.)\n- L.542-546 I don\u2019t understand this claim and why would we want these properties to be linearly encoded in the representations - L.607-625. This paragraph is very opaque\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "AFtSRJJ6dY",
        "length": 955,
        "human_text": "paper_topic_and_main_contributions: The paper is focused on designing a suite of metrics called the tree topological probe which is used for interpretability of pre-trained language models. Specifically, the paper examines BERT-large and uses their findings to enhance fine-tuning performance. The main contribution of the work is to provide the interpretability community with a self-supervised probe to examine (pre-trained) language models.\n\nreasons_to_accept: I like the general intuitions you have on the probe as detailed in section 3.1. The work has theoretical contributions and the framework overall could be beneficial to the wider NLP interpretability community.  The findings in sections 4.1 are interesting and reveal insights previously not available in the structural probe alone. However, it's really difficult to interpret how these findings could apply to the broader community as it is only performed on BERT-large. The subsequent results of fine-tuning layers using insights from 4.1 are convincing.\n\nreasons_to_reject: My main worries for the paper are: 1. The limited empirical evidence of the work given that the main experiments were only conducted on a single model. I think having two or three additional analyses of related models could really strengthen your work.\n2. There are a number of places where perturbations/sensitivity analysis could be done to reveal the robustness of the method/results. Calculating error bounds in Table 3, using multiple different datasets, different ways of approximating max(s_w).  3. The writing could benefit from including more motivation throughout the introduction and transition paragraphs. Why is it important study these phenomena? How would this benefit the larger community? I think there are compelling reasons here and the authors should seek to articulate them.  4. The ethics statement feels detached/removed from the rest of the paper. It's important for the ethics section to engage with the ethical harms and considerations of the presented theories and conducted experiments.\nDetails: There are a number of places where the authors are making assumptions without citations or sufficient motivation and reasoning. \ne.g., lines 052-059 --- it's not immediately clear to readers why the distinction between contextual and static embeddings is sufficient for researchers to trust a model's effectiveness. \" Researchers' intuitions\" is not defined/a poor motivation.\nThere is motivation provided in the introduction that is not answered throughout the paper e.g., lines 075-080. These are not central questions to the paper and would recommend cutting for space and overall tightening the writing of the paper.\nThere are also some claims that could be reworded with more hedges/citations as they are not obviously true statements e.g., lines 100-102. There is also a volume of follow-up work to Hewitt and Ethayarajh which should be cited here as well.\nThey key intuition in the self-supervised structural probe is a little hidden in the introduction: line 125 \"In the case where the internal constraints of the probed features are well defined, a probe that detects these features can naturally induce a probe that detects the internal constraints, which is self-supervised.\" This should be highlighted earlier in the introduction.\nA key challenge which you hint on line 227 is that there are multiple ways of constructing model M. It would be nice to see more evaluation here (ablation studies) to understand how sensitive the probe is to these chosen parameters.\nWriting suggestions:  It would be nice if you could define min(s_w) and max(s_w) at the same place in section 3.3 rather than defining max(s_w) when you start talking about enhancements. It would also be nice to intuitively describe what each of these sequences represent. From there, you can easily present equations (0), (10), and (12).  Having language like, \"since\" and \"similarly\" is not sufficient language to transition between your equations line 366. Why is it important to present the following equations? Why do we care about the properties of these sets? The motivation here is lost to the reader and could really strengthen your work.\nIn the case where Xssp(M) = Xessp(M), it would be interesting to know how often this happens. You could put a nice metric to how often the structural probe would miss insights only your probe can interpret. Could also consider moving content from lines 389-403 together with this initial definition (could also go in the appendix).\nLine 414, need some justification on why you are focused on BERT-large. Is it the most representative model? Currently as it reads, it's difficult to know if this only works on BERT-large or if there are unique properties to BERT-large that make this analysis rich. Do you have insights on what woudl happen with BERT-base/Roberta?\n\ntypos_grammar_style_and_presentation_improvements: This is a nit but equation (1) is really critical to the understanding of the intuition of your paper. It would be helpful to try and be very clear with the language leading up to it, sections 3.1 and 3.2. A simple figure could also help.\nLines 307 - 323 are dense to read. It might help to define and set up all the variables first, before jumping into equations. They currently read as sentences which is hard to parse.\nLine 382 could be moved to the appendix. Might be nice to have 367 as a separate section that is comparing and contrasting to the structural probe.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "etu0CTBIde",
        "length": 409,
        "human_text": "paper_topic_and_main_contributions: This paper rethinks the probing of PLM representations. \nBased on the motivation that the current probing methods lack an upper bound to help us understand the model capability more clearly, the authors propose a novel self-supervised probe: a tree-topological probe to probe the hierarchical structure learned by BERT. They theoretically justify the bounding relationship of the tree-topological probe and the structural probe and apply the probing method on BERT-large. Based on their probing results, they provide several findings and speculation, and the probing method can help find the submodules that need to be enhanced. They verify these findings by finetuning experiments, their probing method indeed can identify the submodules that need to (or cannot) be enhanced (by structural knowledge).\n\nreasons_to_accept: - The motivation is great, existing probing methods usually can only provide results like \"A is better than B\", and the results are often predictable. This paper proposes a probing method that can know \"Is A or B good enough?\", which can provide additional insights into the model's capability.\n- The paper provides a detailed theoretical analysis and a \"Rethinking process\" on how to induce the self-supervised probe.\n- They proposed the tree-topological probe that is \"upgraded\" from the structural probe, and the new probing method can provide an upper bound and has the potential to be a guideline for \"upgrading\" other probing methods.\n- Authors provide extensive analysis of the probing results and produce findings and speculations.\n- Authors verify their findings by a finetuning experiment, showing that their probing method is useful to identify the capability of each submodule, and enhancing these submodules by hierarchical information is useful. Which makes the whole paper self-consistent.\n\nreasons_to_reject: - Would be better by more experiments (as you mentioned, e2,3,4, and other PLMs)\n\nquestions_for_the_authors: - Is your method potentially applicable to LLMs?\n- Will the distribution approximation at L383 significantly affect the probing results?\n\ntypos_grammar_style_and_presentation_improvements: L274, given should be in \\text{given}, not in the math format\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "178_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_178_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7255666666666666,
      "max_similarity": 0.7392,
      "avg_coverage": 0.4389,
      "max_coverage": 0.6316
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 456,
      "avg_human_length": 772.6666666666666
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 10,
      "suggestions_count": 11
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "A0DhPAObTg",
        "similarity": 0.7182,
        "coverage": 0.38,
        "human_length": 954,
        "human_text": "paper_topic_and_main_contributions: The paper is about the interpretability of NLP models, specifically in the domain of probing classifiers for linguistic properties. \nThe authors propose a new approach for designing an unsupervised probe, reminiscent of the probe proposed by Hewitt and Manning for recovering syntactic structure, but unsupervised. \nThe authors define such a probe (again, similar to the design of Hewitt and Manning, by trying to replicate a syntactic tree structure, defined by the depths of the nodes (words/tokens) in the graph) quite thoroughly, discussing the different properties and constraints on such tree, and how to achieve (approximate) it.\n*Main Contributions* - The authors propose a new unsupervised structural probe for analyzing the internal representations of neural language models - Using a metric defined for such a probe, the authors propose a method to improve fine-tuning.\n*Methodology* While my topology background is not very strong, I did my best to follow the different metrics, steps, and logical steps made in the paper, and it sounds reasonable (although I cannot guarantee that).\n*Evaluation* The paper conducts two main experiments:  1. Measuring their probe using BERT-large on different sentences (from different GLUE datasets) 2. Using their derived metric as an additional loss function during fine-tuning of the model on the linguistic acceptability dataset (CoLA).\nThe first experiment was unclear to me. I could not understand the results or the claims that the authors made. \nThe second experiment shows a minor improvement in performance and is not convincing. Moreover, the authors did not perform any statistical significance tests, which makes it hard to take the 1.5 difference very seriously. \nBesides these two experiments, there were no others. \nI don\u2019t understand this lack of comparison, given the wide range of relevant papers in the field (which they also cite). \nHow does their probe compare to others? E.g. Hewitt and Manning How does their probe compare to the \u201creal\u201d syntactic tree of a sentence (using an annotated treebank) *Writing* The paper was very hard to understand. \nIt makes very strong claims, which I often couldn\u2019t understand or interpret. It is very opaque at times. The results and their interpretation as a list of findings (written as fN.) and conclusions (written as cN.), open problems (written as sN.), and preferences (written as pN.) are also nonstandard. Such writing is not problematic per se, but it breaks the flow of the paper and reads as a list of findings, rather than a story, which is much harder to understand. Specific details are given in the \u201cpresentation improvements\u201d section. \nOverall recommendation: while I am generally excited about the idea and direction of the paper, I think the current state of writing (and experiments, to some degree) can be greatly improved. I strongly encourage the authors to resubmit the paper in an improved state.\n\nreasons_to_accept: - The paper provides an interesting formal explanation of probing, and specifically the purpose of unsupervised probing - The paper proposes a new unsupervised structural probe\n\nreasons_to_reject: - The paper\u2019s experimental setup is lacking comparison with previous work, and evaluation of the constructed probe - The paper\u2019s writing can be greatly improved\n\nmissing_references: Unsupervised Probing (also with syntactic structure)  - https://aclanthology.org/2020.acl-main.383.pdf Causal Probing - https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00359/98091/Amnesic-Probing-Behavioral-Explanation-with - https://direct.mit.edu/coli/article/47/2/333/98518\n\ntypos_grammar_style_and_presentation_improvements: - L.1 LMs are *not* expected to effectively map input text to vectors, this is perhaps an implicit requirement, but their main objective is to be able to predict the next word in a sequence - L.3 what are inherent relationships?\n- L.6 what are internal relations?\n- L.10 what do you mean by white-box?\n- L.12 what do you mean by inherent interpretability?\n- L.15 what do you mean by a novel line?\n- L.19 \u201cline of investigation\u201d sounds peculiar in this context - L.21 \u201cconducted measurements\u201d sounds peculiar - L.23 \u201cpropose a speculation\u201d - L.24 what is that \u201cworking mechanism\u201d?\n- L.28 what are submodules?\n- L.34 \u201calgebraic operation\u201d is very vague - L.37 what do you mean by \u201cwhat-box model\u201d in this context?\n- L.45 \u201cmetric computation process usually requires interpretability\u201d - strange phrasing, I cannot make sense of it - L.47 \u201cinherent interpretability of the source model is missing\u201d - strange phrasing, I cannot make sense of it - L.50 what do you mean by semiquantitative?\n- L.51 what are artificial hypotheses?\n- L.54 what\u2019s the connection here between contextual embeddings and static embeddings?\n- L.56 what does intuition have to do with interpretability?\n- L.62 what do you mean by \u201ccomplete word\u201d?\n- L.71 what are \u201ccomplete syntax relations\u201d?\n- L.76 \u201cexist other embeddings\u201d - I don\u2019t see the connection - L.79 why do we care about an upper bound?\n- L.117 how is your approach \u201cbeneficial for interpretability\u201d?\n- L.148 what are submodules?\n- L.181 what is \u201cModel Evidence Maximum\u201d?  - The presentation of the equations can also be improved to be more friendly (e.g. pdep -> p-dep) - Tables 1+2: they are not clear, and their captions are also very opaque.\n- L.425-434 (e1-e4) unclear experiments - L.444 strange formatting (f1.)\n- L.542-546 I don\u2019t understand this claim and why would we want these properties to be linearly encoded in the representations - L.607-625. This paragraph is very opaque\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "AFtSRJJ6dY",
        "similarity": 0.7392,
        "coverage": 0.3051,
        "human_length": 955,
        "human_text": "paper_topic_and_main_contributions: The paper is focused on designing a suite of metrics called the tree topological probe which is used for interpretability of pre-trained language models. Specifically, the paper examines BERT-large and uses their findings to enhance fine-tuning performance. The main contribution of the work is to provide the interpretability community with a self-supervised probe to examine (pre-trained) language models.\n\nreasons_to_accept: I like the general intuitions you have on the probe as detailed in section 3.1. The work has theoretical contributions and the framework overall could be beneficial to the wider NLP interpretability community.  The findings in sections 4.1 are interesting and reveal insights previously not available in the structural probe alone. However, it's really difficult to interpret how these findings could apply to the broader community as it is only performed on BERT-large. The subsequent results of fine-tuning layers using insights from 4.1 are convincing.\n\nreasons_to_reject: My main worries for the paper are: 1. The limited empirical evidence of the work given that the main experiments were only conducted on a single model. I think having two or three additional analyses of related models could really strengthen your work.\n2. There are a number of places where perturbations/sensitivity analysis could be done to reveal the robustness of the method/results. Calculating error bounds in Table 3, using multiple different datasets, different ways of approximating max(s_w).  3. The writing could benefit from including more motivation throughout the introduction and transition paragraphs. Why is it important study these phenomena? How would this benefit the larger community? I think there are compelling reasons here and the authors should seek to articulate them.  4. The ethics statement feels detached/removed from the rest of the paper. It's important for the ethics section to engage with the ethical harms and considerations of the presented theories and conducted experiments.\nDetails: There are a number of places where the authors are making assumptions without citations or sufficient motivation and reasoning. \ne.g., lines 052-059 --- it's not immediately clear to readers why the distinction between contextual and static embeddings is sufficient for researchers to trust a model's effectiveness. \" Researchers' intuitions\" is not defined/a poor motivation.\nThere is motivation provided in the introduction that is not answered throughout the paper e.g., lines 075-080. These are not central questions to the paper and would recommend cutting for space and overall tightening the writing of the paper.\nThere are also some claims that could be reworded with more hedges/citations as they are not obviously true statements e.g., lines 100-102. There is also a volume of follow-up work to Hewitt and Ethayarajh which should be cited here as well.\nThey key intuition in the self-supervised structural probe is a little hidden in the introduction: line 125 \"In the case where the internal constraints of the probed features are well defined, a probe that detects these features can naturally induce a probe that detects the internal constraints, which is self-supervised.\" This should be highlighted earlier in the introduction.\nA key challenge which you hint on line 227 is that there are multiple ways of constructing model M. It would be nice to see more evaluation here (ablation studies) to understand how sensitive the probe is to these chosen parameters.\nWriting suggestions:  It would be nice if you could define min(s_w) and max(s_w) at the same place in section 3.3 rather than defining max(s_w) when you start talking about enhancements. It would also be nice to intuitively describe what each of these sequences represent. From there, you can easily present equations (0), (10), and (12).  Having language like, \"since\" and \"similarly\" is not sufficient language to transition between your equations line 366. Why is it important to present the following equations? Why do we care about the properties of these sets? The motivation here is lost to the reader and could really strengthen your work.\nIn the case where Xssp(M) = Xessp(M), it would be interesting to know how often this happens. You could put a nice metric to how often the structural probe would miss insights only your probe can interpret. Could also consider moving content from lines 389-403 together with this initial definition (could also go in the appendix).\nLine 414, need some justification on why you are focused on BERT-large. Is it the most representative model? Currently as it reads, it's difficult to know if this only works on BERT-large or if there are unique properties to BERT-large that make this analysis rich. Do you have insights on what woudl happen with BERT-base/Roberta?\n\ntypos_grammar_style_and_presentation_improvements: This is a nit but equation (1) is really critical to the understanding of the intuition of your paper. It would be helpful to try and be very clear with the language leading up to it, sections 3.1 and 3.2. A simple figure could also help.\nLines 307 - 323 are dense to read. It might help to define and set up all the variables first, before jumping into equations. They currently read as sentences which is hard to parse.\nLine 382 could be moved to the appendix. Might be nice to have 367 as a separate section that is comparing and contrasting to the structural probe.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "etu0CTBIde",
        "similarity": 0.7193,
        "coverage": 0.6316,
        "human_length": 409,
        "human_text": "paper_topic_and_main_contributions: This paper rethinks the probing of PLM representations. \nBased on the motivation that the current probing methods lack an upper bound to help us understand the model capability more clearly, the authors propose a novel self-supervised probe: a tree-topological probe to probe the hierarchical structure learned by BERT. They theoretically justify the bounding relationship of the tree-topological probe and the structural probe and apply the probing method on BERT-large. Based on their probing results, they provide several findings and speculation, and the probing method can help find the submodules that need to be enhanced. They verify these findings by finetuning experiments, their probing method indeed can identify the submodules that need to (or cannot) be enhanced (by structural knowledge).\n\nreasons_to_accept: - The motivation is great, existing probing methods usually can only provide results like \"A is better than B\", and the results are often predictable. This paper proposes a probing method that can know \"Is A or B good enough?\", which can provide additional insights into the model's capability.\n- The paper provides a detailed theoretical analysis and a \"Rethinking process\" on how to induce the self-supervised probe.\n- They proposed the tree-topological probe that is \"upgraded\" from the structural probe, and the new probing method can provide an upper bound and has the potential to be a guideline for \"upgrading\" other probing methods.\n- Authors provide extensive analysis of the probing results and produce findings and speculations.\n- Authors verify their findings by a finetuning experiment, showing that their probing method is useful to identify the capability of each submodule, and enhancing these submodules by hierarchical information is useful. Which makes the whole paper self-consistent.\n\nreasons_to_reject: - Would be better by more experiments (as you mentioned, e2,3,4, and other PLMs)\n\nquestions_for_the_authors: - Is your method potentially applicable to LLMs?\n- Will the distribution approximation at L383 significantly affect the probing results?\n\ntypos_grammar_style_and_presentation_improvements: L274, given should be in \\text{given}, not in the math format\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "A0DhPAObTg",
        "length": 954,
        "human_text": "paper_topic_and_main_contributions: The paper is about the interpretability of NLP models, specifically in the domain of probing classifiers for linguistic properties. \nThe authors propose a new approach for designing an unsupervised probe, reminiscent of the probe proposed by Hewitt and Manning for recovering syntactic structure, but unsupervised. \nThe authors define such a probe (again, similar to the design of Hewitt and Manning, by trying to replicate a syntactic tree structure, defined by the depths of the nodes (words/tokens) in the graph) quite thoroughly, discussing the different properties and constraints on such tree, and how to achieve (approximate) it.\n*Main Contributions* - The authors propose a new unsupervised structural probe for analyzing the internal representations of neural language models - Using a metric defined for such a probe, the authors propose a method to improve fine-tuning.\n*Methodology* While my topology background is not very strong, I did my best to follow the different metrics, steps, and logical steps made in the paper, and it sounds reasonable (although I cannot guarantee that).\n*Evaluation* The paper conducts two main experiments:  1. Measuring their probe using BERT-large on different sentences (from different GLUE datasets) 2. Using their derived metric as an additional loss function during fine-tuning of the model on the linguistic acceptability dataset (CoLA).\nThe first experiment was unclear to me. I could not understand the results or the claims that the authors made. \nThe second experiment shows a minor improvement in performance and is not convincing. Moreover, the authors did not perform any statistical significance tests, which makes it hard to take the 1.5 difference very seriously. \nBesides these two experiments, there were no others. \nI don\u2019t understand this lack of comparison, given the wide range of relevant papers in the field (which they also cite). \nHow does their probe compare to others? E.g. Hewitt and Manning How does their probe compare to the \u201creal\u201d syntactic tree of a sentence (using an annotated treebank) *Writing* The paper was very hard to understand. \nIt makes very strong claims, which I often couldn\u2019t understand or interpret. It is very opaque at times. The results and their interpretation as a list of findings (written as fN.) and conclusions (written as cN.), open problems (written as sN.), and preferences (written as pN.) are also nonstandard. Such writing is not problematic per se, but it breaks the flow of the paper and reads as a list of findings, rather than a story, which is much harder to understand. Specific details are given in the \u201cpresentation improvements\u201d section. \nOverall recommendation: while I am generally excited about the idea and direction of the paper, I think the current state of writing (and experiments, to some degree) can be greatly improved. I strongly encourage the authors to resubmit the paper in an improved state.\n\nreasons_to_accept: - The paper provides an interesting formal explanation of probing, and specifically the purpose of unsupervised probing - The paper proposes a new unsupervised structural probe\n\nreasons_to_reject: - The paper\u2019s experimental setup is lacking comparison with previous work, and evaluation of the constructed probe - The paper\u2019s writing can be greatly improved\n\nmissing_references: Unsupervised Probing (also with syntactic structure)  - https://aclanthology.org/2020.acl-main.383.pdf Causal Probing - https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00359/98091/Amnesic-Probing-Behavioral-Explanation-with - https://direct.mit.edu/coli/article/47/2/333/98518\n\ntypos_grammar_style_and_presentation_improvements: - L.1 LMs are *not* expected to effectively map input text to vectors, this is perhaps an implicit requirement, but their main objective is to be able to predict the next word in a sequence - L.3 what are inherent relationships?\n- L.6 what are internal relations?\n- L.10 what do you mean by white-box?\n- L.12 what do you mean by inherent interpretability?\n- L.15 what do you mean by a novel line?\n- L.19 \u201cline of investigation\u201d sounds peculiar in this context - L.21 \u201cconducted measurements\u201d sounds peculiar - L.23 \u201cpropose a speculation\u201d - L.24 what is that \u201cworking mechanism\u201d?\n- L.28 what are submodules?\n- L.34 \u201calgebraic operation\u201d is very vague - L.37 what do you mean by \u201cwhat-box model\u201d in this context?\n- L.45 \u201cmetric computation process usually requires interpretability\u201d - strange phrasing, I cannot make sense of it - L.47 \u201cinherent interpretability of the source model is missing\u201d - strange phrasing, I cannot make sense of it - L.50 what do you mean by semiquantitative?\n- L.51 what are artificial hypotheses?\n- L.54 what\u2019s the connection here between contextual embeddings and static embeddings?\n- L.56 what does intuition have to do with interpretability?\n- L.62 what do you mean by \u201ccomplete word\u201d?\n- L.71 what are \u201ccomplete syntax relations\u201d?\n- L.76 \u201cexist other embeddings\u201d - I don\u2019t see the connection - L.79 why do we care about an upper bound?\n- L.117 how is your approach \u201cbeneficial for interpretability\u201d?\n- L.148 what are submodules?\n- L.181 what is \u201cModel Evidence Maximum\u201d?  - The presentation of the equations can also be improved to be more friendly (e.g. pdep -> p-dep) - Tables 1+2: they are not clear, and their captions are also very opaque.\n- L.425-434 (e1-e4) unclear experiments - L.444 strange formatting (f1.)\n- L.542-546 I don\u2019t understand this claim and why would we want these properties to be linearly encoded in the representations - L.607-625. This paragraph is very opaque\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "AFtSRJJ6dY",
        "length": 955,
        "human_text": "paper_topic_and_main_contributions: The paper is focused on designing a suite of metrics called the tree topological probe which is used for interpretability of pre-trained language models. Specifically, the paper examines BERT-large and uses their findings to enhance fine-tuning performance. The main contribution of the work is to provide the interpretability community with a self-supervised probe to examine (pre-trained) language models.\n\nreasons_to_accept: I like the general intuitions you have on the probe as detailed in section 3.1. The work has theoretical contributions and the framework overall could be beneficial to the wider NLP interpretability community.  The findings in sections 4.1 are interesting and reveal insights previously not available in the structural probe alone. However, it's really difficult to interpret how these findings could apply to the broader community as it is only performed on BERT-large. The subsequent results of fine-tuning layers using insights from 4.1 are convincing.\n\nreasons_to_reject: My main worries for the paper are: 1. The limited empirical evidence of the work given that the main experiments were only conducted on a single model. I think having two or three additional analyses of related models could really strengthen your work.\n2. There are a number of places where perturbations/sensitivity analysis could be done to reveal the robustness of the method/results. Calculating error bounds in Table 3, using multiple different datasets, different ways of approximating max(s_w).  3. The writing could benefit from including more motivation throughout the introduction and transition paragraphs. Why is it important study these phenomena? How would this benefit the larger community? I think there are compelling reasons here and the authors should seek to articulate them.  4. The ethics statement feels detached/removed from the rest of the paper. It's important for the ethics section to engage with the ethical harms and considerations of the presented theories and conducted experiments.\nDetails: There are a number of places where the authors are making assumptions without citations or sufficient motivation and reasoning. \ne.g., lines 052-059 --- it's not immediately clear to readers why the distinction between contextual and static embeddings is sufficient for researchers to trust a model's effectiveness. \" Researchers' intuitions\" is not defined/a poor motivation.\nThere is motivation provided in the introduction that is not answered throughout the paper e.g., lines 075-080. These are not central questions to the paper and would recommend cutting for space and overall tightening the writing of the paper.\nThere are also some claims that could be reworded with more hedges/citations as they are not obviously true statements e.g., lines 100-102. There is also a volume of follow-up work to Hewitt and Ethayarajh which should be cited here as well.\nThey key intuition in the self-supervised structural probe is a little hidden in the introduction: line 125 \"In the case where the internal constraints of the probed features are well defined, a probe that detects these features can naturally induce a probe that detects the internal constraints, which is self-supervised.\" This should be highlighted earlier in the introduction.\nA key challenge which you hint on line 227 is that there are multiple ways of constructing model M. It would be nice to see more evaluation here (ablation studies) to understand how sensitive the probe is to these chosen parameters.\nWriting suggestions:  It would be nice if you could define min(s_w) and max(s_w) at the same place in section 3.3 rather than defining max(s_w) when you start talking about enhancements. It would also be nice to intuitively describe what each of these sequences represent. From there, you can easily present equations (0), (10), and (12).  Having language like, \"since\" and \"similarly\" is not sufficient language to transition between your equations line 366. Why is it important to present the following equations? Why do we care about the properties of these sets? The motivation here is lost to the reader and could really strengthen your work.\nIn the case where Xssp(M) = Xessp(M), it would be interesting to know how often this happens. You could put a nice metric to how often the structural probe would miss insights only your probe can interpret. Could also consider moving content from lines 389-403 together with this initial definition (could also go in the appendix).\nLine 414, need some justification on why you are focused on BERT-large. Is it the most representative model? Currently as it reads, it's difficult to know if this only works on BERT-large or if there are unique properties to BERT-large that make this analysis rich. Do you have insights on what woudl happen with BERT-base/Roberta?\n\ntypos_grammar_style_and_presentation_improvements: This is a nit but equation (1) is really critical to the understanding of the intuition of your paper. It would be helpful to try and be very clear with the language leading up to it, sections 3.1 and 3.2. A simple figure could also help.\nLines 307 - 323 are dense to read. It might help to define and set up all the variables first, before jumping into equations. They currently read as sentences which is hard to parse.\nLine 382 could be moved to the appendix. Might be nice to have 367 as a separate section that is comparing and contrasting to the structural probe.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "etu0CTBIde",
        "length": 409,
        "human_text": "paper_topic_and_main_contributions: This paper rethinks the probing of PLM representations. \nBased on the motivation that the current probing methods lack an upper bound to help us understand the model capability more clearly, the authors propose a novel self-supervised probe: a tree-topological probe to probe the hierarchical structure learned by BERT. They theoretically justify the bounding relationship of the tree-topological probe and the structural probe and apply the probing method on BERT-large. Based on their probing results, they provide several findings and speculation, and the probing method can help find the submodules that need to be enhanced. They verify these findings by finetuning experiments, their probing method indeed can identify the submodules that need to (or cannot) be enhanced (by structural knowledge).\n\nreasons_to_accept: - The motivation is great, existing probing methods usually can only provide results like \"A is better than B\", and the results are often predictable. This paper proposes a probing method that can know \"Is A or B good enough?\", which can provide additional insights into the model's capability.\n- The paper provides a detailed theoretical analysis and a \"Rethinking process\" on how to induce the self-supervised probe.\n- They proposed the tree-topological probe that is \"upgraded\" from the structural probe, and the new probing method can provide an upper bound and has the potential to be a guideline for \"upgrading\" other probing methods.\n- Authors provide extensive analysis of the probing results and produce findings and speculations.\n- Authors verify their findings by a finetuning experiment, showing that their probing method is useful to identify the capability of each submodule, and enhancing these submodules by hierarchical information is useful. Which makes the whole paper self-consistent.\n\nreasons_to_reject: - Would be better by more experiments (as you mentioned, e2,3,4, and other PLMs)\n\nquestions_for_the_authors: - Is your method potentially applicable to LLMs?\n- Will the distribution approximation at L383 significantly affect the probing results?\n\ntypos_grammar_style_and_presentation_improvements: L274, given should be in \\text{given}, not in the math format\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "117_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_117_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7478333333333333,
      "max_similarity": 0.7531,
      "avg_coverage": 0.4820333333333333,
      "max_coverage": 0.5333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 493,
      "avg_human_length": 346.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 7,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "jYwtGQBcSp",
        "similarity": 0.7398,
        "coverage": 0.5333,
        "human_length": 283,
        "human_text": "paper_topic_and_main_contributions: This paper tackles the zero-shot intent detection tasks and proposes a two-stage zero-shot bert adapters (Z-BERT-A), which first leverages a dependency parser to extract a set of potential intents, then uses NLI methods relying on Bert models to classify on the candidate classes. Experimental results show this method can outperform a wide variety of baselines in both known intents zero-shot classification and unseen intent discovery.\n\nreasons_to_accept: 1. This paper focus on important tasks of both known intents zero-shot classification and unseen intent discovery, and can leverages dependency parsers to enhance the intent generation process. \n2. Experimental results show the proposed methods are effective in zero-shot intent detection.\n\nreasons_to_reject: 1. This work is better suited as a demo track paper, rather than a regular long paper. \n2. The idea of using NLI to handle zero-shot learning tasks are quite common.\n\nquestions_for_the_authors: In section 4, the intent generation process generates the candidates novel class for intent classification, but I wonder for a set of unseen classes, how to normalize same intention with different utterances, and how to determine the total number of new intent classes?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "LzJ6Vg1zuU",
        "similarity": 0.7531,
        "coverage": 0.4583,
        "human_length": 440,
        "human_text": "paper_topic_and_main_contributions: The paper presents a technique for zero-shot intent classification. The authors make use of a BERT model finetuned on the NLI task and a dependency parser to discover new intents not seen before.\n\nreasons_to_accept: Creativity: The authors present a creative pipeline that combines several components to predict new intents in a zero-shot setting.\nExperiments: For many experiments, the authors show results for several different methods, comparing to a variety of LLMs.\n\nreasons_to_reject: Simplistic approach: The method presented in Algorithm 1 just extracts words from the sentence. If the intent word is not explicitly expressed in the sentence, this method will be incapable of generating the correct intent.\nLack of baseline in Table 4: The authors only present various settings for their model. I'm not familiar with this research area, so I have no idea if there are approaches in previously published work that outperform this method that the authors have left out.\nMarginal improvement in Table 4: The difference in results for each approach are very small, so the benefit of the proposed method does not seem large.\nInterpretability of remaining results: It's hard to compare the performance to the LLMs because they only use cosine distance. It's clear the model outperforms in semantic similarity (according to the semantic encoder models used), but for more trustworthy results, a small sample of human evaluations should be used as well to be sure that this method outperforms the LLMs in the zero-shot setting. Another option would be to modify the LLM experiment such that label F1 scores could be produced (use a verbalizer to map LLM output to intent classes).\n\nquestions_for_the_authors: 1. What is the frequency of examples in the dataset where the intent is explicitly mentioned in the sentence? If this is almost all of the cases, then my first reason to reject is not important. If there are a lot of examples without the intent mentioned, this method is fundamentally limited compared to LLMs which can generalize better than this approach (generate an intent without the intent being mentioned explicitly).\n2. Are there any baselines that you could compare to for zero-shot intent classification? If so, why didn't you include them in Table 4?\n3. What is the test set for Table 4?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "POnuqpw0mh",
        "similarity": 0.7506,
        "coverage": 0.4545,
        "human_length": 315,
        "human_text": "paper_topic_and_main_contributions: This paper proposed a method to do zero-shot intent classification, it can be applied to BERT-based transformer models. The method contains two stages, where for stage-1, the dependency parser is used to get potential intents and in stage-2 the zero-shot classification is performed for final output. Experiments are done on public datasets to verify the effectiveness of the proposed method.\n\nreasons_to_accept: The paper designed a method as a BERT adapter to handle the zero-shot intent discovery task. The model has been evaluated on two datasets and achieved state-of-the-art performance.\n\nreasons_to_reject: The contribution of the paper is not very clear, how does this method compare with other existing language model adapters. \nMore ablation study could be done to prove the effectiveness of components in the model architecture.\n\nquestions_for_the_authors: 1. How does this method compare with other existing language model adapters. \n2. What are the tunable parameters and what are the frozen parameters in the model? \n3. What is the size of the trainable parameters in the proposed method? \n4. The model has been used in English and Italian, can experiments be added to  one more language to better prove the multilingual ability?\n\ntypos_grammar_style_and_presentation_improvements: Here are some minor notes that the author may consider: 1. In line-184, building a pipeline [that is] able to handle unseen classes. \n2. As the proposed method is a two-stage pipeline, it could be better if stage-1 and stage-2 can be clearly illustrated in the pipeline figure.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "jYwtGQBcSp",
        "length": 283,
        "human_text": "paper_topic_and_main_contributions: This paper tackles the zero-shot intent detection tasks and proposes a two-stage zero-shot bert adapters (Z-BERT-A), which first leverages a dependency parser to extract a set of potential intents, then uses NLI methods relying on Bert models to classify on the candidate classes. Experimental results show this method can outperform a wide variety of baselines in both known intents zero-shot classification and unseen intent discovery.\n\nreasons_to_accept: 1. This paper focus on important tasks of both known intents zero-shot classification and unseen intent discovery, and can leverages dependency parsers to enhance the intent generation process. \n2. Experimental results show the proposed methods are effective in zero-shot intent detection.\n\nreasons_to_reject: 1. This work is better suited as a demo track paper, rather than a regular long paper. \n2. The idea of using NLI to handle zero-shot learning tasks are quite common.\n\nquestions_for_the_authors: In section 4, the intent generation process generates the candidates novel class for intent classification, but I wonder for a set of unseen classes, how to normalize same intention with different utterances, and how to determine the total number of new intent classes?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "LzJ6Vg1zuU",
        "length": 440,
        "human_text": "paper_topic_and_main_contributions: The paper presents a technique for zero-shot intent classification. The authors make use of a BERT model finetuned on the NLI task and a dependency parser to discover new intents not seen before.\n\nreasons_to_accept: Creativity: The authors present a creative pipeline that combines several components to predict new intents in a zero-shot setting.\nExperiments: For many experiments, the authors show results for several different methods, comparing to a variety of LLMs.\n\nreasons_to_reject: Simplistic approach: The method presented in Algorithm 1 just extracts words from the sentence. If the intent word is not explicitly expressed in the sentence, this method will be incapable of generating the correct intent.\nLack of baseline in Table 4: The authors only present various settings for their model. I'm not familiar with this research area, so I have no idea if there are approaches in previously published work that outperform this method that the authors have left out.\nMarginal improvement in Table 4: The difference in results for each approach are very small, so the benefit of the proposed method does not seem large.\nInterpretability of remaining results: It's hard to compare the performance to the LLMs because they only use cosine distance. It's clear the model outperforms in semantic similarity (according to the semantic encoder models used), but for more trustworthy results, a small sample of human evaluations should be used as well to be sure that this method outperforms the LLMs in the zero-shot setting. Another option would be to modify the LLM experiment such that label F1 scores could be produced (use a verbalizer to map LLM output to intent classes).\n\nquestions_for_the_authors: 1. What is the frequency of examples in the dataset where the intent is explicitly mentioned in the sentence? If this is almost all of the cases, then my first reason to reject is not important. If there are a lot of examples without the intent mentioned, this method is fundamentally limited compared to LLMs which can generalize better than this approach (generate an intent without the intent being mentioned explicitly).\n2. Are there any baselines that you could compare to for zero-shot intent classification? If so, why didn't you include them in Table 4?\n3. What is the test set for Table 4?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "POnuqpw0mh",
        "length": 315,
        "human_text": "paper_topic_and_main_contributions: This paper proposed a method to do zero-shot intent classification, it can be applied to BERT-based transformer models. The method contains two stages, where for stage-1, the dependency parser is used to get potential intents and in stage-2 the zero-shot classification is performed for final output. Experiments are done on public datasets to verify the effectiveness of the proposed method.\n\nreasons_to_accept: The paper designed a method as a BERT adapter to handle the zero-shot intent discovery task. The model has been evaluated on two datasets and achieved state-of-the-art performance.\n\nreasons_to_reject: The contribution of the paper is not very clear, how does this method compare with other existing language model adapters. \nMore ablation study could be done to prove the effectiveness of components in the model architecture.\n\nquestions_for_the_authors: 1. How does this method compare with other existing language model adapters. \n2. What are the tunable parameters and what are the frozen parameters in the model? \n3. What is the size of the trainable parameters in the proposed method? \n4. The model has been used in English and Italian, can experiments be added to  one more language to better prove the multilingual ability?\n\ntypos_grammar_style_and_presentation_improvements: Here are some minor notes that the author may consider: 1. In line-184, building a pipeline [that is] able to handle unseen classes. \n2. As the proposed method is a two-stage pipeline, it could be better if stage-1 and stage-2 can be clearly illustrated in the pipeline figure.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "117_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_117_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7403333333333334,
      "max_similarity": 0.7469,
      "avg_coverage": 0.49593333333333334,
      "max_coverage": 0.5333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 492,
      "avg_human_length": 346.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 7,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "jYwtGQBcSp",
        "similarity": 0.7296,
        "coverage": 0.5333,
        "human_length": 283,
        "human_text": "paper_topic_and_main_contributions: This paper tackles the zero-shot intent detection tasks and proposes a two-stage zero-shot bert adapters (Z-BERT-A), which first leverages a dependency parser to extract a set of potential intents, then uses NLI methods relying on Bert models to classify on the candidate classes. Experimental results show this method can outperform a wide variety of baselines in both known intents zero-shot classification and unseen intent discovery.\n\nreasons_to_accept: 1. This paper focus on important tasks of both known intents zero-shot classification and unseen intent discovery, and can leverages dependency parsers to enhance the intent generation process. \n2. Experimental results show the proposed methods are effective in zero-shot intent detection.\n\nreasons_to_reject: 1. This work is better suited as a demo track paper, rather than a regular long paper. \n2. The idea of using NLI to handle zero-shot learning tasks are quite common.\n\nquestions_for_the_authors: In section 4, the intent generation process generates the candidates novel class for intent classification, but I wonder for a set of unseen classes, how to normalize same intention with different utterances, and how to determine the total number of new intent classes?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "LzJ6Vg1zuU",
        "similarity": 0.7445,
        "coverage": 0.5,
        "human_length": 440,
        "human_text": "paper_topic_and_main_contributions: The paper presents a technique for zero-shot intent classification. The authors make use of a BERT model finetuned on the NLI task and a dependency parser to discover new intents not seen before.\n\nreasons_to_accept: Creativity: The authors present a creative pipeline that combines several components to predict new intents in a zero-shot setting.\nExperiments: For many experiments, the authors show results for several different methods, comparing to a variety of LLMs.\n\nreasons_to_reject: Simplistic approach: The method presented in Algorithm 1 just extracts words from the sentence. If the intent word is not explicitly expressed in the sentence, this method will be incapable of generating the correct intent.\nLack of baseline in Table 4: The authors only present various settings for their model. I'm not familiar with this research area, so I have no idea if there are approaches in previously published work that outperform this method that the authors have left out.\nMarginal improvement in Table 4: The difference in results for each approach are very small, so the benefit of the proposed method does not seem large.\nInterpretability of remaining results: It's hard to compare the performance to the LLMs because they only use cosine distance. It's clear the model outperforms in semantic similarity (according to the semantic encoder models used), but for more trustworthy results, a small sample of human evaluations should be used as well to be sure that this method outperforms the LLMs in the zero-shot setting. Another option would be to modify the LLM experiment such that label F1 scores could be produced (use a verbalizer to map LLM output to intent classes).\n\nquestions_for_the_authors: 1. What is the frequency of examples in the dataset where the intent is explicitly mentioned in the sentence? If this is almost all of the cases, then my first reason to reject is not important. If there are a lot of examples without the intent mentioned, this method is fundamentally limited compared to LLMs which can generalize better than this approach (generate an intent without the intent being mentioned explicitly).\n2. Are there any baselines that you could compare to for zero-shot intent classification? If so, why didn't you include them in Table 4?\n3. What is the test set for Table 4?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "POnuqpw0mh",
        "similarity": 0.7469,
        "coverage": 0.4545,
        "human_length": 315,
        "human_text": "paper_topic_and_main_contributions: This paper proposed a method to do zero-shot intent classification, it can be applied to BERT-based transformer models. The method contains two stages, where for stage-1, the dependency parser is used to get potential intents and in stage-2 the zero-shot classification is performed for final output. Experiments are done on public datasets to verify the effectiveness of the proposed method.\n\nreasons_to_accept: The paper designed a method as a BERT adapter to handle the zero-shot intent discovery task. The model has been evaluated on two datasets and achieved state-of-the-art performance.\n\nreasons_to_reject: The contribution of the paper is not very clear, how does this method compare with other existing language model adapters. \nMore ablation study could be done to prove the effectiveness of components in the model architecture.\n\nquestions_for_the_authors: 1. How does this method compare with other existing language model adapters. \n2. What are the tunable parameters and what are the frozen parameters in the model? \n3. What is the size of the trainable parameters in the proposed method? \n4. The model has been used in English and Italian, can experiments be added to  one more language to better prove the multilingual ability?\n\ntypos_grammar_style_and_presentation_improvements: Here are some minor notes that the author may consider: 1. In line-184, building a pipeline [that is] able to handle unseen classes. \n2. As the proposed method is a two-stage pipeline, it could be better if stage-1 and stage-2 can be clearly illustrated in the pipeline figure.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "jYwtGQBcSp",
        "length": 283,
        "human_text": "paper_topic_and_main_contributions: This paper tackles the zero-shot intent detection tasks and proposes a two-stage zero-shot bert adapters (Z-BERT-A), which first leverages a dependency parser to extract a set of potential intents, then uses NLI methods relying on Bert models to classify on the candidate classes. Experimental results show this method can outperform a wide variety of baselines in both known intents zero-shot classification and unseen intent discovery.\n\nreasons_to_accept: 1. This paper focus on important tasks of both known intents zero-shot classification and unseen intent discovery, and can leverages dependency parsers to enhance the intent generation process. \n2. Experimental results show the proposed methods are effective in zero-shot intent detection.\n\nreasons_to_reject: 1. This work is better suited as a demo track paper, rather than a regular long paper. \n2. The idea of using NLI to handle zero-shot learning tasks are quite common.\n\nquestions_for_the_authors: In section 4, the intent generation process generates the candidates novel class for intent classification, but I wonder for a set of unseen classes, how to normalize same intention with different utterances, and how to determine the total number of new intent classes?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "LzJ6Vg1zuU",
        "length": 440,
        "human_text": "paper_topic_and_main_contributions: The paper presents a technique for zero-shot intent classification. The authors make use of a BERT model finetuned on the NLI task and a dependency parser to discover new intents not seen before.\n\nreasons_to_accept: Creativity: The authors present a creative pipeline that combines several components to predict new intents in a zero-shot setting.\nExperiments: For many experiments, the authors show results for several different methods, comparing to a variety of LLMs.\n\nreasons_to_reject: Simplistic approach: The method presented in Algorithm 1 just extracts words from the sentence. If the intent word is not explicitly expressed in the sentence, this method will be incapable of generating the correct intent.\nLack of baseline in Table 4: The authors only present various settings for their model. I'm not familiar with this research area, so I have no idea if there are approaches in previously published work that outperform this method that the authors have left out.\nMarginal improvement in Table 4: The difference in results for each approach are very small, so the benefit of the proposed method does not seem large.\nInterpretability of remaining results: It's hard to compare the performance to the LLMs because they only use cosine distance. It's clear the model outperforms in semantic similarity (according to the semantic encoder models used), but for more trustworthy results, a small sample of human evaluations should be used as well to be sure that this method outperforms the LLMs in the zero-shot setting. Another option would be to modify the LLM experiment such that label F1 scores could be produced (use a verbalizer to map LLM output to intent classes).\n\nquestions_for_the_authors: 1. What is the frequency of examples in the dataset where the intent is explicitly mentioned in the sentence? If this is almost all of the cases, then my first reason to reject is not important. If there are a lot of examples without the intent mentioned, this method is fundamentally limited compared to LLMs which can generalize better than this approach (generate an intent without the intent being mentioned explicitly).\n2. Are there any baselines that you could compare to for zero-shot intent classification? If so, why didn't you include them in Table 4?\n3. What is the test set for Table 4?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "POnuqpw0mh",
        "length": 315,
        "human_text": "paper_topic_and_main_contributions: This paper proposed a method to do zero-shot intent classification, it can be applied to BERT-based transformer models. The method contains two stages, where for stage-1, the dependency parser is used to get potential intents and in stage-2 the zero-shot classification is performed for final output. Experiments are done on public datasets to verify the effectiveness of the proposed method.\n\nreasons_to_accept: The paper designed a method as a BERT adapter to handle the zero-shot intent discovery task. The model has been evaluated on two datasets and achieved state-of-the-art performance.\n\nreasons_to_reject: The contribution of the paper is not very clear, how does this method compare with other existing language model adapters. \nMore ablation study could be done to prove the effectiveness of components in the model architecture.\n\nquestions_for_the_authors: 1. How does this method compare with other existing language model adapters. \n2. What are the tunable parameters and what are the frozen parameters in the model? \n3. What is the size of the trainable parameters in the proposed method? \n4. The model has been used in English and Italian, can experiments be added to  one more language to better prove the multilingual ability?\n\ntypos_grammar_style_and_presentation_improvements: Here are some minor notes that the author may consider: 1. In line-184, building a pipeline [that is] able to handle unseen classes. \n2. As the proposed method is a two-stage pipeline, it could be better if stage-1 and stage-2 can be clearly illustrated in the pipeline figure.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "67_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_67_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7115666666666667,
      "max_similarity": 0.7206,
      "avg_coverage": 0.45386666666666664,
      "max_coverage": 0.4571
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 433,
      "avg_human_length": 374.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 6,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "1YsBYrLcTO",
        "similarity": 0.7123,
        "coverage": 0.4545,
        "human_length": 239,
        "human_text": "paper_topic_and_main_contributions: This research proposes an unsupervised pre-training method for news recommendation based on user behaviors. The authors introduce two tasks: user behavior masking to recover masked behaviors and user behavior generation to enhance user representations. Their approach outperforms existing methods in real-world news recommendation benchmarks.\n\nreasons_to_accept: 1. The problem studied in this paper is interesting and important, but rarely researched. Thus, this research has some novelty.\n2. The proposed pretraining methods, i.e., user behavior masking to recover masked behaviors and user behavior generation to enhance user representations, are reasonable, although they are standard techniques in language modeling.\n3. The experiments are extensive and solid. The improvement brought by the proposed method is significant.\n\nreasons_to_reject: 1. There are a few writing flaws. For example, there should be some punctuations after the equations.\n2. Besides the two MIND datasets, more datasets could be used in experiments.\n\nquestions_for_the_authors: 1. Why are  some methods mentioned in related works especially those for PLM-based news recommendation not compared in experiments?\n\nmissing_references: None\n\ntypos_grammar_style_and_presentation_improvements: There should be some punctuations after the equations.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "CZZmqI7wwN",
        "similarity": 0.7206,
        "coverage": 0.4571,
        "human_length": 653,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a pretraining model PUNR for news recommendation. PUNR is pretrained by two objectives of BERT-like masked behavior modeling and GPT-like autoregressive behavior generation. Comprehensive experiments show that PUNR beats several news recommendation baselines.\n\nreasons_to_accept: This paper explores user behavior pretraining, which is less studied by previous research. The idea is generally well stated and experiments are thorough.\n\nreasons_to_reject: 1. The major weakness of this paper is the overlap of pretraining and finetuning data scopes. In principle, pretraining and finetuning data should not be overlapped. Otherwise, data contamination [1] makes the pretrained model memorize the data and cannot be well generalized to various downstream tasks. The essence (or target) of PLM is to pretrain on large-scale unlabeled text corpus and finetune on various downstream tasks. For example, GPT-3 and T5 [2,3] are pretrained on Common Crawl, WebText and Wikipedia corpus and finetuned on downstream supervised tasks. Pretraining on task-specific data makes the model difficult to be adapted to other tasks. The authors pretrain the model on MIND and also finetune on MIND, which is irrational and should not be called pertaining (it can be regarded as typical training), unless the author tests the model on other datasets or tasks. Pretraining and finetuning on the same dataset MIND is not a good setting for PLM study. Especially note that MIND-small is a subset of MIND-large, the MIND-small test set is also included in pertaining MIND-large corpus.\n2. As the authors use relatively high-cost computation resources to \"pretrain\" the model of large parameters, advanced news recommendation baselines should be compared, including [4,5,6,7]. Fairly speaking, the chosen baselines are not updated to advanced news recommendation works of the recent two years.\n3. Equation 4 is not right where $t_{<i}$ is either missing or not self-consistent with the notation of $E(T)$. 4. Minor concern of mine: Masked user behavior modeling is intuitive. However, incorporating GPT-like user behavior generation by introducing an additional decoder seems redundant and unnecessary, though the authors claim with experiments that it can help to learn the representation of user vectors. Since the user behavior and its containing news articles are represented in a hierarchy, I also doubt that it is too difficult for the decoder to generate such hierarchical and complex user behaviors only based on a user representation vector. Considering that the user representation vector [CLS] is a one-dimension vector (not like the $n\\times d$ hidden states of Transformer or T5 encoder outputs), I doubt if this vector can generate the whole complex user behaviors.\n5. Initializing the generative decoder with the original BERT is irrational. Moreover, using BERT's architecture to model generation is not a good choice. BERT's architecture is designed and pretrained for Mask Language Modeling, and the authors revised it to do autoregressive Language Modeling (CLM in the paper). The pretraining gap between Mask Language Modeling and Language Modeling exists.\n[1] Data Contamination: From Memorization to Exploitation, on ACL-2022 [2] Language Models are Few-Shot Learners, on NIPS-2020 [3] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, on JMLR-2020 [4] HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation, on ACL-2021 [5] MINER: Multi-Interest Matching Network for News Recommendation, on ACL-2022 [6] MTRec: Multi-Task Learning over BERT for News Recommendation, on ACL-2022 [7] DIGAT: DIGAT: Modeling News Recommendation with Dual-Graph Interaction, on EMNLP-2022\n\nethical_concerns: No\n\ntypos_grammar_style_and_presentation_improvements: In line 005, 'works are mostly focused on' should be 'works mostly focus on'.\nThe terminology \"auto-regression\" should be \"auto-regressive\" which is standard used in our NLP community.\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "AqMwwF4NiN",
        "similarity": 0.7018,
        "coverage": 0.45,
        "human_length": 230,
        "human_text": "paper_topic_and_main_contributions: The authors propose an architecture that includes news and user representation using pre-training, which incorporate user behavior generation and user behavior masking tasks, to solve news recommendation.\n\nreasons_to_accept: 1. Experiments fully prove the effectiveness of the proposed method. \n2. The description is logical and easy to follow. \n3. The proposed architecture is straightforward and easy to understand.\n\nreasons_to_reject: 1. The idea of user behavior pre-training is very common in recommendation systems. \n2. According to the results in Table 2, the improvement of the model effect is gradual. Also, why not use UNBERT as a baseline for comparison, in Table 2?\n\nquestions_for_the_authors: 1. Can we look at case studies to see why this structure is more efficient than other baselines? \n2. What is the effect of word representations using random initialization?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "1YsBYrLcTO",
        "length": 239,
        "human_text": "paper_topic_and_main_contributions: This research proposes an unsupervised pre-training method for news recommendation based on user behaviors. The authors introduce two tasks: user behavior masking to recover masked behaviors and user behavior generation to enhance user representations. Their approach outperforms existing methods in real-world news recommendation benchmarks.\n\nreasons_to_accept: 1. The problem studied in this paper is interesting and important, but rarely researched. Thus, this research has some novelty.\n2. The proposed pretraining methods, i.e., user behavior masking to recover masked behaviors and user behavior generation to enhance user representations, are reasonable, although they are standard techniques in language modeling.\n3. The experiments are extensive and solid. The improvement brought by the proposed method is significant.\n\nreasons_to_reject: 1. There are a few writing flaws. For example, there should be some punctuations after the equations.\n2. Besides the two MIND datasets, more datasets could be used in experiments.\n\nquestions_for_the_authors: 1. Why are  some methods mentioned in related works especially those for PLM-based news recommendation not compared in experiments?\n\nmissing_references: None\n\ntypos_grammar_style_and_presentation_improvements: There should be some punctuations after the equations.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "CZZmqI7wwN",
        "length": 653,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a pretraining model PUNR for news recommendation. PUNR is pretrained by two objectives of BERT-like masked behavior modeling and GPT-like autoregressive behavior generation. Comprehensive experiments show that PUNR beats several news recommendation baselines.\n\nreasons_to_accept: This paper explores user behavior pretraining, which is less studied by previous research. The idea is generally well stated and experiments are thorough.\n\nreasons_to_reject: 1. The major weakness of this paper is the overlap of pretraining and finetuning data scopes. In principle, pretraining and finetuning data should not be overlapped. Otherwise, data contamination [1] makes the pretrained model memorize the data and cannot be well generalized to various downstream tasks. The essence (or target) of PLM is to pretrain on large-scale unlabeled text corpus and finetune on various downstream tasks. For example, GPT-3 and T5 [2,3] are pretrained on Common Crawl, WebText and Wikipedia corpus and finetuned on downstream supervised tasks. Pretraining on task-specific data makes the model difficult to be adapted to other tasks. The authors pretrain the model on MIND and also finetune on MIND, which is irrational and should not be called pertaining (it can be regarded as typical training), unless the author tests the model on other datasets or tasks. Pretraining and finetuning on the same dataset MIND is not a good setting for PLM study. Especially note that MIND-small is a subset of MIND-large, the MIND-small test set is also included in pertaining MIND-large corpus.\n2. As the authors use relatively high-cost computation resources to \"pretrain\" the model of large parameters, advanced news recommendation baselines should be compared, including [4,5,6,7]. Fairly speaking, the chosen baselines are not updated to advanced news recommendation works of the recent two years.\n3. Equation 4 is not right where $t_{<i}$ is either missing or not self-consistent with the notation of $E(T)$. 4. Minor concern of mine: Masked user behavior modeling is intuitive. However, incorporating GPT-like user behavior generation by introducing an additional decoder seems redundant and unnecessary, though the authors claim with experiments that it can help to learn the representation of user vectors. Since the user behavior and its containing news articles are represented in a hierarchy, I also doubt that it is too difficult for the decoder to generate such hierarchical and complex user behaviors only based on a user representation vector. Considering that the user representation vector [CLS] is a one-dimension vector (not like the $n\\times d$ hidden states of Transformer or T5 encoder outputs), I doubt if this vector can generate the whole complex user behaviors.\n5. Initializing the generative decoder with the original BERT is irrational. Moreover, using BERT's architecture to model generation is not a good choice. BERT's architecture is designed and pretrained for Mask Language Modeling, and the authors revised it to do autoregressive Language Modeling (CLM in the paper). The pretraining gap between Mask Language Modeling and Language Modeling exists.\n[1] Data Contamination: From Memorization to Exploitation, on ACL-2022 [2] Language Models are Few-Shot Learners, on NIPS-2020 [3] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, on JMLR-2020 [4] HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation, on ACL-2021 [5] MINER: Multi-Interest Matching Network for News Recommendation, on ACL-2022 [6] MTRec: Multi-Task Learning over BERT for News Recommendation, on ACL-2022 [7] DIGAT: DIGAT: Modeling News Recommendation with Dual-Graph Interaction, on EMNLP-2022\n\nethical_concerns: No\n\ntypos_grammar_style_and_presentation_improvements: In line 005, 'works are mostly focused on' should be 'works mostly focus on'.\nThe terminology \"auto-regression\" should be \"auto-regressive\" which is standard used in our NLP community.\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "AqMwwF4NiN",
        "length": 230,
        "human_text": "paper_topic_and_main_contributions: The authors propose an architecture that includes news and user representation using pre-training, which incorporate user behavior generation and user behavior masking tasks, to solve news recommendation.\n\nreasons_to_accept: 1. Experiments fully prove the effectiveness of the proposed method. \n2. The description is logical and easy to follow. \n3. The proposed architecture is straightforward and easy to understand.\n\nreasons_to_reject: 1. The idea of user behavior pre-training is very common in recommendation systems. \n2. According to the results in Table 2, the improvement of the model effect is gradual. Also, why not use UNBERT as a baseline for comparison, in Table 2?\n\nquestions_for_the_authors: 1. Can we look at case studies to see why this structure is more efficient than other baselines? \n2. What is the effect of word representations using random initialization?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "67_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_67_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7481333333333334,
      "max_similarity": 0.758,
      "avg_coverage": 0.32356666666666667,
      "max_coverage": 0.3636
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 254,
      "avg_human_length": 374.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 6,
      "weaknesses_count": 2,
      "suggestions_count": 1
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "1YsBYrLcTO",
        "similarity": 0.758,
        "coverage": 0.3636,
        "human_length": 239,
        "human_text": "paper_topic_and_main_contributions: This research proposes an unsupervised pre-training method for news recommendation based on user behaviors. The authors introduce two tasks: user behavior masking to recover masked behaviors and user behavior generation to enhance user representations. Their approach outperforms existing methods in real-world news recommendation benchmarks.\n\nreasons_to_accept: 1. The problem studied in this paper is interesting and important, but rarely researched. Thus, this research has some novelty.\n2. The proposed pretraining methods, i.e., user behavior masking to recover masked behaviors and user behavior generation to enhance user representations, are reasonable, although they are standard techniques in language modeling.\n3. The experiments are extensive and solid. The improvement brought by the proposed method is significant.\n\nreasons_to_reject: 1. There are a few writing flaws. For example, there should be some punctuations after the equations.\n2. Besides the two MIND datasets, more datasets could be used in experiments.\n\nquestions_for_the_authors: 1. Why are  some methods mentioned in related works especially those for PLM-based news recommendation not compared in experiments?\n\nmissing_references: None\n\ntypos_grammar_style_and_presentation_improvements: There should be some punctuations after the equations.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "CZZmqI7wwN",
        "similarity": 0.7469,
        "coverage": 0.2571,
        "human_length": 653,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a pretraining model PUNR for news recommendation. PUNR is pretrained by two objectives of BERT-like masked behavior modeling and GPT-like autoregressive behavior generation. Comprehensive experiments show that PUNR beats several news recommendation baselines.\n\nreasons_to_accept: This paper explores user behavior pretraining, which is less studied by previous research. The idea is generally well stated and experiments are thorough.\n\nreasons_to_reject: 1. The major weakness of this paper is the overlap of pretraining and finetuning data scopes. In principle, pretraining and finetuning data should not be overlapped. Otherwise, data contamination [1] makes the pretrained model memorize the data and cannot be well generalized to various downstream tasks. The essence (or target) of PLM is to pretrain on large-scale unlabeled text corpus and finetune on various downstream tasks. For example, GPT-3 and T5 [2,3] are pretrained on Common Crawl, WebText and Wikipedia corpus and finetuned on downstream supervised tasks. Pretraining on task-specific data makes the model difficult to be adapted to other tasks. The authors pretrain the model on MIND and also finetune on MIND, which is irrational and should not be called pertaining (it can be regarded as typical training), unless the author tests the model on other datasets or tasks. Pretraining and finetuning on the same dataset MIND is not a good setting for PLM study. Especially note that MIND-small is a subset of MIND-large, the MIND-small test set is also included in pertaining MIND-large corpus.\n2. As the authors use relatively high-cost computation resources to \"pretrain\" the model of large parameters, advanced news recommendation baselines should be compared, including [4,5,6,7]. Fairly speaking, the chosen baselines are not updated to advanced news recommendation works of the recent two years.\n3. Equation 4 is not right where $t_{<i}$ is either missing or not self-consistent with the notation of $E(T)$. 4. Minor concern of mine: Masked user behavior modeling is intuitive. However, incorporating GPT-like user behavior generation by introducing an additional decoder seems redundant and unnecessary, though the authors claim with experiments that it can help to learn the representation of user vectors. Since the user behavior and its containing news articles are represented in a hierarchy, I also doubt that it is too difficult for the decoder to generate such hierarchical and complex user behaviors only based on a user representation vector. Considering that the user representation vector [CLS] is a one-dimension vector (not like the $n\\times d$ hidden states of Transformer or T5 encoder outputs), I doubt if this vector can generate the whole complex user behaviors.\n5. Initializing the generative decoder with the original BERT is irrational. Moreover, using BERT's architecture to model generation is not a good choice. BERT's architecture is designed and pretrained for Mask Language Modeling, and the authors revised it to do autoregressive Language Modeling (CLM in the paper). The pretraining gap between Mask Language Modeling and Language Modeling exists.\n[1] Data Contamination: From Memorization to Exploitation, on ACL-2022 [2] Language Models are Few-Shot Learners, on NIPS-2020 [3] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, on JMLR-2020 [4] HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation, on ACL-2021 [5] MINER: Multi-Interest Matching Network for News Recommendation, on ACL-2022 [6] MTRec: Multi-Task Learning over BERT for News Recommendation, on ACL-2022 [7] DIGAT: DIGAT: Modeling News Recommendation with Dual-Graph Interaction, on EMNLP-2022\n\nethical_concerns: No\n\ntypos_grammar_style_and_presentation_improvements: In line 005, 'works are mostly focused on' should be 'works mostly focus on'.\nThe terminology \"auto-regression\" should be \"auto-regressive\" which is standard used in our NLP community.\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "AqMwwF4NiN",
        "similarity": 0.7395,
        "coverage": 0.35,
        "human_length": 230,
        "human_text": "paper_topic_and_main_contributions: The authors propose an architecture that includes news and user representation using pre-training, which incorporate user behavior generation and user behavior masking tasks, to solve news recommendation.\n\nreasons_to_accept: 1. Experiments fully prove the effectiveness of the proposed method. \n2. The description is logical and easy to follow. \n3. The proposed architecture is straightforward and easy to understand.\n\nreasons_to_reject: 1. The idea of user behavior pre-training is very common in recommendation systems. \n2. According to the results in Table 2, the improvement of the model effect is gradual. Also, why not use UNBERT as a baseline for comparison, in Table 2?\n\nquestions_for_the_authors: 1. Can we look at case studies to see why this structure is more efficient than other baselines? \n2. What is the effect of word representations using random initialization?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "1YsBYrLcTO",
        "length": 239,
        "human_text": "paper_topic_and_main_contributions: This research proposes an unsupervised pre-training method for news recommendation based on user behaviors. The authors introduce two tasks: user behavior masking to recover masked behaviors and user behavior generation to enhance user representations. Their approach outperforms existing methods in real-world news recommendation benchmarks.\n\nreasons_to_accept: 1. The problem studied in this paper is interesting and important, but rarely researched. Thus, this research has some novelty.\n2. The proposed pretraining methods, i.e., user behavior masking to recover masked behaviors and user behavior generation to enhance user representations, are reasonable, although they are standard techniques in language modeling.\n3. The experiments are extensive and solid. The improvement brought by the proposed method is significant.\n\nreasons_to_reject: 1. There are a few writing flaws. For example, there should be some punctuations after the equations.\n2. Besides the two MIND datasets, more datasets could be used in experiments.\n\nquestions_for_the_authors: 1. Why are  some methods mentioned in related works especially those for PLM-based news recommendation not compared in experiments?\n\nmissing_references: None\n\ntypos_grammar_style_and_presentation_improvements: There should be some punctuations after the equations.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "CZZmqI7wwN",
        "length": 653,
        "human_text": "paper_topic_and_main_contributions: This paper introduces a pretraining model PUNR for news recommendation. PUNR is pretrained by two objectives of BERT-like masked behavior modeling and GPT-like autoregressive behavior generation. Comprehensive experiments show that PUNR beats several news recommendation baselines.\n\nreasons_to_accept: This paper explores user behavior pretraining, which is less studied by previous research. The idea is generally well stated and experiments are thorough.\n\nreasons_to_reject: 1. The major weakness of this paper is the overlap of pretraining and finetuning data scopes. In principle, pretraining and finetuning data should not be overlapped. Otherwise, data contamination [1] makes the pretrained model memorize the data and cannot be well generalized to various downstream tasks. The essence (or target) of PLM is to pretrain on large-scale unlabeled text corpus and finetune on various downstream tasks. For example, GPT-3 and T5 [2,3] are pretrained on Common Crawl, WebText and Wikipedia corpus and finetuned on downstream supervised tasks. Pretraining on task-specific data makes the model difficult to be adapted to other tasks. The authors pretrain the model on MIND and also finetune on MIND, which is irrational and should not be called pertaining (it can be regarded as typical training), unless the author tests the model on other datasets or tasks. Pretraining and finetuning on the same dataset MIND is not a good setting for PLM study. Especially note that MIND-small is a subset of MIND-large, the MIND-small test set is also included in pertaining MIND-large corpus.\n2. As the authors use relatively high-cost computation resources to \"pretrain\" the model of large parameters, advanced news recommendation baselines should be compared, including [4,5,6,7]. Fairly speaking, the chosen baselines are not updated to advanced news recommendation works of the recent two years.\n3. Equation 4 is not right where $t_{<i}$ is either missing or not self-consistent with the notation of $E(T)$. 4. Minor concern of mine: Masked user behavior modeling is intuitive. However, incorporating GPT-like user behavior generation by introducing an additional decoder seems redundant and unnecessary, though the authors claim with experiments that it can help to learn the representation of user vectors. Since the user behavior and its containing news articles are represented in a hierarchy, I also doubt that it is too difficult for the decoder to generate such hierarchical and complex user behaviors only based on a user representation vector. Considering that the user representation vector [CLS] is a one-dimension vector (not like the $n\\times d$ hidden states of Transformer or T5 encoder outputs), I doubt if this vector can generate the whole complex user behaviors.\n5. Initializing the generative decoder with the original BERT is irrational. Moreover, using BERT's architecture to model generation is not a good choice. BERT's architecture is designed and pretrained for Mask Language Modeling, and the authors revised it to do autoregressive Language Modeling (CLM in the paper). The pretraining gap between Mask Language Modeling and Language Modeling exists.\n[1] Data Contamination: From Memorization to Exploitation, on ACL-2022 [2] Language Models are Few-Shot Learners, on NIPS-2020 [3] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, on JMLR-2020 [4] HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation, on ACL-2021 [5] MINER: Multi-Interest Matching Network for News Recommendation, on ACL-2022 [6] MTRec: Multi-Task Learning over BERT for News Recommendation, on ACL-2022 [7] DIGAT: DIGAT: Modeling News Recommendation with Dual-Graph Interaction, on EMNLP-2022\n\nethical_concerns: No\n\ntypos_grammar_style_and_presentation_improvements: In line 005, 'works are mostly focused on' should be 'works mostly focus on'.\nThe terminology \"auto-regression\" should be \"auto-regressive\" which is standard used in our NLP community.\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.",
        "has_content": true
      },
      {
        "reviewer_id": "AqMwwF4NiN",
        "length": 230,
        "human_text": "paper_topic_and_main_contributions: The authors propose an architecture that includes news and user representation using pre-training, which incorporate user behavior generation and user behavior masking tasks, to solve news recommendation.\n\nreasons_to_accept: 1. Experiments fully prove the effectiveness of the proposed method. \n2. The description is logical and easy to follow. \n3. The proposed architecture is straightforward and easy to understand.\n\nreasons_to_reject: 1. The idea of user behavior pre-training is very common in recommendation systems. \n2. According to the results in Table 2, the improvement of the model effect is gradual. Also, why not use UNBERT as a baseline for comparison, in Table 2?\n\nquestions_for_the_authors: 1. Can we look at case studies to see why this structure is more efficient than other baselines? \n2. What is the effect of word representations using random initialization?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "79_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_79_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7451333333333334,
      "max_similarity": 0.7625,
      "avg_coverage": 0.36916666666666664,
      "max_coverage": 0.4333
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 531,
      "avg_human_length": 558.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 9,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "ZZmz1EvXc0",
        "similarity": 0.7386,
        "coverage": 0.3333,
        "human_length": 745,
        "human_text": "paper_topic_and_main_contributions: Robustness of the neural NLP models have become an extremely important topic as these neural models are abundantly used in real world applications. Perturbation-based robustness study is popular among the deep learning researchers, specifically, in the computer vision community where the nature of perturbation is continuous.  Perturbation-based robustness study in NLP faces the challenge of handling discrete input leading to discrete perturbation. This paper aims at connecting discrete perturbation to continuous perturbation so that perturbation in the continuous domain is applicable in NLP. Primary contributions of the paper include 1) quantifying the correlation between discrete and continuous perturbation, 2) designing a regression-based model (viz, PerturbScore) to estimate the correlation.\n\nreasons_to_accept: 1. The paper studies one of the most important aspects of NLP research, i.e., measuring the robustness of the neural NLP models. \n2. To deal with imperceptibility of the discrete perturbations and combinatorial explosion in formulating those perturbations, the paper advocates for applying continuous perturbation. \n3. I really appreciate the two staged approach adopted by the authors: Firstly, mapping discrete perturbation to continuous perturbation with a set of non-trivial assumptions. This formalisation along with an optimization method the authors presented an elegant method for generating training examples each consisting of original input, discrete perturbation and equivalent continuous perturbation. Secondly, using those training instances, a regression model has been proposed to predict continuous prediction given a discrete perturbation. \n4. The authors performed extensive experiments to validate the claims and showcase the usefulness of the method\n\nreasons_to_reject: 1. Many claims are ambiguous and are not supported by strong logic. ( see questions) 2. Given the results, I have doubt in the generalizability of the proposed model.\n\nquestions_for_the_authors: 1. Page 4, Column 2: the sentence  \u2018..... obtain a data tuple [S, P(S), \\epsilon] , which is the correlation\u2019 is a bit ambiguous. Assuming \u2018correlation\u2019 refers  to \u2018\\epsioln\u2019, is it a correlation between the discrete and continuous perturbations? Or to me it seems that \u2018\\epsilon\u2019 is a continuous surrogate for the discrete perturbation P(S). Please clarify. This may be confused with another correlation measured in section 4.2 in the form of Kendal and Pearson Index. Also, the tuple [S, P(S), \\epsilon] is not a correlation.\n2. Page 6, Column 1: How does better resistance of models trained FreeLB method against discrete and continuous perturbation verify assumption 1 and ensure correlation between discrete and continuous perturbation? There seems to be a logical gap. Please clarify 3. Page 6: Column 1: Table 2 shows count of data tuples under different \\epsilon ranges. This shows most of the discrete distributions correspond to lower continuous perturbation value ranges. Some of the data tuples have been discarded? The claim is as a small proportion of data tuples have been discarded, the method is able to find the \u2018norm ball that satisfies Assumption 3\u2019. What is the criteria for discarding the data tuples?  If it is manually selected then size of the set of discarded tuples may vary. Also, it is apparent why discarding a small proportion of tuples leads to satisfaction of Assumption 3.  4. Page 6: Column 1: There is no evidence in the paper that \u201cTextfooler method generates more discrete perturbations \u2026\u2026 and corresponding continuous perturbations require larger norm balls. Please specify if these experimental observations have been explicitly presented in any part of the paper.\n5. The cross-dataset analysis in Appendix shows that there is no significant difference in the correlation between edit-distance and PerturbScorer. In many cases, the correlation is poorer in PerturbScorer. Though the situation improves when the datasets are combined.  How these results reflect on the generalizability of the model?\n\ntypos_grammar_style_and_presentation_improvements: 1. Use of mixed case styles: Textfooler, textfooler 2. Page 5, Column 1, Line 1: \u2018Specifically, the notation x used in line 7\u2026\u2019. Assuming line 7 refers to the line no 7 of Algorithm 1, no mention of \u2018x\u2019 is present.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "54DvMp7syH",
        "similarity": 0.7343,
        "coverage": 0.4333,
        "human_length": 434,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the research question: can we find a connection between discrete perturbation and continuous perturbation to study the robustness of neural NLP models? They approach this question by quantifying the connections with the norm-bound of gradient-based perturbations and training a regression model PerturbScorer to predict the strength of the connection (correlation). The resulting PerturbScorer can be useful for better understanding robust training methods of NLP models and transferring the robustness theories studied in the computer vision field to NLP.\n\nreasons_to_accept: 1. This paper looks into an interesting problem of finding a continuous perturbation as a proxy for discrete perturbation, which provides a new angle to study the robustness of NLP models through the lens of continuous space.  2. Experiment results of PerturbScorer look promising, indicating that the generated perturbation dataset and the trained model are of good quality.\n3. The method is carefully designed and well-explained in Sec 3.\n\nreasons_to_reject: 1. Limited numbers of models and datasets/tasks are studied in this paper. I hope to see similar results on more model architectures (including a simpler RNN model) and tasks to better support the generalization ability of PerturbScorer.\n2. I wish to see a concrete demonstration of how the PerturbScorer can be applied to broader scenarios such as studying the robustness of NLP models in continuous space, measuring sentence differences, etc.   3. The validity of the proposed method would be better supported by a theoretical proof of when the norm-bound would exist (the continuous perturbation effect is way bigger than discrete ones, line 342-345).\n\nquestions_for_the_authors: 1. The paper explores how to approximate a discrete perturbation with a continuous one. Is it possible to do the other way round, i.e. finding the corresponding discrete perturbation of a continuous one? This could make the two types of perturbations more connected.\n2. Do you use the same model (e.g. BERT) for perturbation dataset generation (sec 3.4) and perturbscoer (sec 3.5)? What would the correlation results be like if you use different types of models?\n\ntypos_grammar_style_and_presentation_improvements: The paper seems to miss a brief introduction to the FreeLB method appeared in Fig 1 & Table 2. Can you add a few sentences to explain it?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "eR4itEZVTh",
        "similarity": 0.7625,
        "coverage": 0.3409,
        "human_length": 495,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors study the problem of model robustness in NLP. Specifically, they seek to study the relationship between continuous and discrete perturbations in NLP models. The authors motivate this study by noting tha most robustness work in NLP focuses on discrete perturbations. However, this can be problematic as discrete perturbations are much more costly than their continuous counterparts. The authors first assume that there exists a connection between discrete and continuous perturbations, and using that design a method finds the proper norm bound between a continuous and discrete perturbation. They utilize this method to construct their own pertuber. They utilize this to conduct various studies comparing the two types of perturbations and the effect on model performance.\nPost-Rebuttal Update: I appreciate the detailed response. I find most of the responses satisfactory. I've raised my excitement from a 3 &rarr; 4\n\nreasons_to_accept: 1. The goal of this study is well-motivated and clear.\n2. The authors do a strong job of supporting the assumptions made in Section 2 through empirical study.  3. The empirical results are comprehensive. Multiple datasets and methods are used.\n\nreasons_to_reject: 1. More care can be taken in explaining the equation in Section 3. It was not obvious at first as to what exactly they should be measuring. An explanation given before or after each equation, explaining the purpose and intuition would be helpful.\n2. I don't find the results in presented in Section 4.4 to be very convincing. First, based on the results the authors state that the relationship between the discrete and continuous perturbations is weak. They conclude that discrete perturbations can't properly attack models. However, earlier they use Figure 1 to show that the continuous and discrete perturbations have a similar effect on model performance. It's hard to square these two assertions. Secondly, they note that PerturbScorer has a strong correlation with the continuous pertubations. I feel like this isn't surprising at all as PerturbScorer utilizes $\\epsilon$.\n\nquestions_for_the_authors: 1. What are the axes for the the plots in figure 1? I'd recommend including axis names in the future for better readability.\n2. Could you re-explain the significance of the findings from Section 4.4? See my comments in the weaknesses section.  3. Could you expound on some potential applications of this work? Ideally including a small section in the paper about this would be helpful.\n\ntypos_grammar_style_and_presentation_improvements: 1. I found it difficult to read Section 4. I'd recommend not utilizing so many sub sections (and sub-sub and so on...). However this may just be a personal taste.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "ZZmz1EvXc0",
        "length": 745,
        "human_text": "paper_topic_and_main_contributions: Robustness of the neural NLP models have become an extremely important topic as these neural models are abundantly used in real world applications. Perturbation-based robustness study is popular among the deep learning researchers, specifically, in the computer vision community where the nature of perturbation is continuous.  Perturbation-based robustness study in NLP faces the challenge of handling discrete input leading to discrete perturbation. This paper aims at connecting discrete perturbation to continuous perturbation so that perturbation in the continuous domain is applicable in NLP. Primary contributions of the paper include 1) quantifying the correlation between discrete and continuous perturbation, 2) designing a regression-based model (viz, PerturbScore) to estimate the correlation.\n\nreasons_to_accept: 1. The paper studies one of the most important aspects of NLP research, i.e., measuring the robustness of the neural NLP models. \n2. To deal with imperceptibility of the discrete perturbations and combinatorial explosion in formulating those perturbations, the paper advocates for applying continuous perturbation. \n3. I really appreciate the two staged approach adopted by the authors: Firstly, mapping discrete perturbation to continuous perturbation with a set of non-trivial assumptions. This formalisation along with an optimization method the authors presented an elegant method for generating training examples each consisting of original input, discrete perturbation and equivalent continuous perturbation. Secondly, using those training instances, a regression model has been proposed to predict continuous prediction given a discrete perturbation. \n4. The authors performed extensive experiments to validate the claims and showcase the usefulness of the method\n\nreasons_to_reject: 1. Many claims are ambiguous and are not supported by strong logic. ( see questions) 2. Given the results, I have doubt in the generalizability of the proposed model.\n\nquestions_for_the_authors: 1. Page 4, Column 2: the sentence  \u2018..... obtain a data tuple [S, P(S), \\epsilon] , which is the correlation\u2019 is a bit ambiguous. Assuming \u2018correlation\u2019 refers  to \u2018\\epsioln\u2019, is it a correlation between the discrete and continuous perturbations? Or to me it seems that \u2018\\epsilon\u2019 is a continuous surrogate for the discrete perturbation P(S). Please clarify. This may be confused with another correlation measured in section 4.2 in the form of Kendal and Pearson Index. Also, the tuple [S, P(S), \\epsilon] is not a correlation.\n2. Page 6, Column 1: How does better resistance of models trained FreeLB method against discrete and continuous perturbation verify assumption 1 and ensure correlation between discrete and continuous perturbation? There seems to be a logical gap. Please clarify 3. Page 6: Column 1: Table 2 shows count of data tuples under different \\epsilon ranges. This shows most of the discrete distributions correspond to lower continuous perturbation value ranges. Some of the data tuples have been discarded? The claim is as a small proportion of data tuples have been discarded, the method is able to find the \u2018norm ball that satisfies Assumption 3\u2019. What is the criteria for discarding the data tuples?  If it is manually selected then size of the set of discarded tuples may vary. Also, it is apparent why discarding a small proportion of tuples leads to satisfaction of Assumption 3.  4. Page 6: Column 1: There is no evidence in the paper that \u201cTextfooler method generates more discrete perturbations \u2026\u2026 and corresponding continuous perturbations require larger norm balls. Please specify if these experimental observations have been explicitly presented in any part of the paper.\n5. The cross-dataset analysis in Appendix shows that there is no significant difference in the correlation between edit-distance and PerturbScorer. In many cases, the correlation is poorer in PerturbScorer. Though the situation improves when the datasets are combined.  How these results reflect on the generalizability of the model?\n\ntypos_grammar_style_and_presentation_improvements: 1. Use of mixed case styles: Textfooler, textfooler 2. Page 5, Column 1, Line 1: \u2018Specifically, the notation x used in line 7\u2026\u2019. Assuming line 7 refers to the line no 7 of Algorithm 1, no mention of \u2018x\u2019 is present.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "54DvMp7syH",
        "length": 434,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the research question: can we find a connection between discrete perturbation and continuous perturbation to study the robustness of neural NLP models? They approach this question by quantifying the connections with the norm-bound of gradient-based perturbations and training a regression model PerturbScorer to predict the strength of the connection (correlation). The resulting PerturbScorer can be useful for better understanding robust training methods of NLP models and transferring the robustness theories studied in the computer vision field to NLP.\n\nreasons_to_accept: 1. This paper looks into an interesting problem of finding a continuous perturbation as a proxy for discrete perturbation, which provides a new angle to study the robustness of NLP models through the lens of continuous space.  2. Experiment results of PerturbScorer look promising, indicating that the generated perturbation dataset and the trained model are of good quality.\n3. The method is carefully designed and well-explained in Sec 3.\n\nreasons_to_reject: 1. Limited numbers of models and datasets/tasks are studied in this paper. I hope to see similar results on more model architectures (including a simpler RNN model) and tasks to better support the generalization ability of PerturbScorer.\n2. I wish to see a concrete demonstration of how the PerturbScorer can be applied to broader scenarios such as studying the robustness of NLP models in continuous space, measuring sentence differences, etc.   3. The validity of the proposed method would be better supported by a theoretical proof of when the norm-bound would exist (the continuous perturbation effect is way bigger than discrete ones, line 342-345).\n\nquestions_for_the_authors: 1. The paper explores how to approximate a discrete perturbation with a continuous one. Is it possible to do the other way round, i.e. finding the corresponding discrete perturbation of a continuous one? This could make the two types of perturbations more connected.\n2. Do you use the same model (e.g. BERT) for perturbation dataset generation (sec 3.4) and perturbscoer (sec 3.5)? What would the correlation results be like if you use different types of models?\n\ntypos_grammar_style_and_presentation_improvements: The paper seems to miss a brief introduction to the FreeLB method appeared in Fig 1 & Table 2. Can you add a few sentences to explain it?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "eR4itEZVTh",
        "length": 495,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors study the problem of model robustness in NLP. Specifically, they seek to study the relationship between continuous and discrete perturbations in NLP models. The authors motivate this study by noting tha most robustness work in NLP focuses on discrete perturbations. However, this can be problematic as discrete perturbations are much more costly than their continuous counterparts. The authors first assume that there exists a connection between discrete and continuous perturbations, and using that design a method finds the proper norm bound between a continuous and discrete perturbation. They utilize this method to construct their own pertuber. They utilize this to conduct various studies comparing the two types of perturbations and the effect on model performance.\nPost-Rebuttal Update: I appreciate the detailed response. I find most of the responses satisfactory. I've raised my excitement from a 3 &rarr; 4\n\nreasons_to_accept: 1. The goal of this study is well-motivated and clear.\n2. The authors do a strong job of supporting the assumptions made in Section 2 through empirical study.  3. The empirical results are comprehensive. Multiple datasets and methods are used.\n\nreasons_to_reject: 1. More care can be taken in explaining the equation in Section 3. It was not obvious at first as to what exactly they should be measuring. An explanation given before or after each equation, explaining the purpose and intuition would be helpful.\n2. I don't find the results in presented in Section 4.4 to be very convincing. First, based on the results the authors state that the relationship between the discrete and continuous perturbations is weak. They conclude that discrete perturbations can't properly attack models. However, earlier they use Figure 1 to show that the continuous and discrete perturbations have a similar effect on model performance. It's hard to square these two assertions. Secondly, they note that PerturbScorer has a strong correlation with the continuous pertubations. I feel like this isn't surprising at all as PerturbScorer utilizes $\\epsilon$.\n\nquestions_for_the_authors: 1. What are the axes for the the plots in figure 1? I'd recommend including axis names in the future for better readability.\n2. Could you re-explain the significance of the findings from Section 4.4? See my comments in the weaknesses section.  3. Could you expound on some potential applications of this work? Ideally including a small section in the paper about this would be helpful.\n\ntypos_grammar_style_and_presentation_improvements: 1. I found it difficult to read Section 4. I'd recommend not utilizing so many sub sections (and sub-sub and so on...). However this may just be a personal taste.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "79_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_79_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7504,
      "max_similarity": 0.7653,
      "avg_coverage": 0.37553333333333333,
      "max_coverage": 0.4667
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 626,
      "avg_human_length": 558.0
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 9,
      "suggestions_count": 10
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "ZZmz1EvXc0",
        "similarity": 0.7474,
        "coverage": 0.2963,
        "human_length": 745,
        "human_text": "paper_topic_and_main_contributions: Robustness of the neural NLP models have become an extremely important topic as these neural models are abundantly used in real world applications. Perturbation-based robustness study is popular among the deep learning researchers, specifically, in the computer vision community where the nature of perturbation is continuous.  Perturbation-based robustness study in NLP faces the challenge of handling discrete input leading to discrete perturbation. This paper aims at connecting discrete perturbation to continuous perturbation so that perturbation in the continuous domain is applicable in NLP. Primary contributions of the paper include 1) quantifying the correlation between discrete and continuous perturbation, 2) designing a regression-based model (viz, PerturbScore) to estimate the correlation.\n\nreasons_to_accept: 1. The paper studies one of the most important aspects of NLP research, i.e., measuring the robustness of the neural NLP models. \n2. To deal with imperceptibility of the discrete perturbations and combinatorial explosion in formulating those perturbations, the paper advocates for applying continuous perturbation. \n3. I really appreciate the two staged approach adopted by the authors: Firstly, mapping discrete perturbation to continuous perturbation with a set of non-trivial assumptions. This formalisation along with an optimization method the authors presented an elegant method for generating training examples each consisting of original input, discrete perturbation and equivalent continuous perturbation. Secondly, using those training instances, a regression model has been proposed to predict continuous prediction given a discrete perturbation. \n4. The authors performed extensive experiments to validate the claims and showcase the usefulness of the method\n\nreasons_to_reject: 1. Many claims are ambiguous and are not supported by strong logic. ( see questions) 2. Given the results, I have doubt in the generalizability of the proposed model.\n\nquestions_for_the_authors: 1. Page 4, Column 2: the sentence  \u2018..... obtain a data tuple [S, P(S), \\epsilon] , which is the correlation\u2019 is a bit ambiguous. Assuming \u2018correlation\u2019 refers  to \u2018\\epsioln\u2019, is it a correlation between the discrete and continuous perturbations? Or to me it seems that \u2018\\epsilon\u2019 is a continuous surrogate for the discrete perturbation P(S). Please clarify. This may be confused with another correlation measured in section 4.2 in the form of Kendal and Pearson Index. Also, the tuple [S, P(S), \\epsilon] is not a correlation.\n2. Page 6, Column 1: How does better resistance of models trained FreeLB method against discrete and continuous perturbation verify assumption 1 and ensure correlation between discrete and continuous perturbation? There seems to be a logical gap. Please clarify 3. Page 6: Column 1: Table 2 shows count of data tuples under different \\epsilon ranges. This shows most of the discrete distributions correspond to lower continuous perturbation value ranges. Some of the data tuples have been discarded? The claim is as a small proportion of data tuples have been discarded, the method is able to find the \u2018norm ball that satisfies Assumption 3\u2019. What is the criteria for discarding the data tuples?  If it is manually selected then size of the set of discarded tuples may vary. Also, it is apparent why discarding a small proportion of tuples leads to satisfaction of Assumption 3.  4. Page 6: Column 1: There is no evidence in the paper that \u201cTextfooler method generates more discrete perturbations \u2026\u2026 and corresponding continuous perturbations require larger norm balls. Please specify if these experimental observations have been explicitly presented in any part of the paper.\n5. The cross-dataset analysis in Appendix shows that there is no significant difference in the correlation between edit-distance and PerturbScorer. In many cases, the correlation is poorer in PerturbScorer. Though the situation improves when the datasets are combined.  How these results reflect on the generalizability of the model?\n\ntypos_grammar_style_and_presentation_improvements: 1. Use of mixed case styles: Textfooler, textfooler 2. Page 5, Column 1, Line 1: \u2018Specifically, the notation x used in line 7\u2026\u2019. Assuming line 7 refers to the line no 7 of Algorithm 1, no mention of \u2018x\u2019 is present.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "54DvMp7syH",
        "similarity": 0.7385,
        "coverage": 0.4667,
        "human_length": 434,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the research question: can we find a connection between discrete perturbation and continuous perturbation to study the robustness of neural NLP models? They approach this question by quantifying the connections with the norm-bound of gradient-based perturbations and training a regression model PerturbScorer to predict the strength of the connection (correlation). The resulting PerturbScorer can be useful for better understanding robust training methods of NLP models and transferring the robustness theories studied in the computer vision field to NLP.\n\nreasons_to_accept: 1. This paper looks into an interesting problem of finding a continuous perturbation as a proxy for discrete perturbation, which provides a new angle to study the robustness of NLP models through the lens of continuous space.  2. Experiment results of PerturbScorer look promising, indicating that the generated perturbation dataset and the trained model are of good quality.\n3. The method is carefully designed and well-explained in Sec 3.\n\nreasons_to_reject: 1. Limited numbers of models and datasets/tasks are studied in this paper. I hope to see similar results on more model architectures (including a simpler RNN model) and tasks to better support the generalization ability of PerturbScorer.\n2. I wish to see a concrete demonstration of how the PerturbScorer can be applied to broader scenarios such as studying the robustness of NLP models in continuous space, measuring sentence differences, etc.   3. The validity of the proposed method would be better supported by a theoretical proof of when the norm-bound would exist (the continuous perturbation effect is way bigger than discrete ones, line 342-345).\n\nquestions_for_the_authors: 1. The paper explores how to approximate a discrete perturbation with a continuous one. Is it possible to do the other way round, i.e. finding the corresponding discrete perturbation of a continuous one? This could make the two types of perturbations more connected.\n2. Do you use the same model (e.g. BERT) for perturbation dataset generation (sec 3.4) and perturbscoer (sec 3.5)? What would the correlation results be like if you use different types of models?\n\ntypos_grammar_style_and_presentation_improvements: The paper seems to miss a brief introduction to the FreeLB method appeared in Fig 1 & Table 2. Can you add a few sentences to explain it?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "eR4itEZVTh",
        "similarity": 0.7653,
        "coverage": 0.3636,
        "human_length": 495,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors study the problem of model robustness in NLP. Specifically, they seek to study the relationship between continuous and discrete perturbations in NLP models. The authors motivate this study by noting tha most robustness work in NLP focuses on discrete perturbations. However, this can be problematic as discrete perturbations are much more costly than their continuous counterparts. The authors first assume that there exists a connection between discrete and continuous perturbations, and using that design a method finds the proper norm bound between a continuous and discrete perturbation. They utilize this method to construct their own pertuber. They utilize this to conduct various studies comparing the two types of perturbations and the effect on model performance.\nPost-Rebuttal Update: I appreciate the detailed response. I find most of the responses satisfactory. I've raised my excitement from a 3 &rarr; 4\n\nreasons_to_accept: 1. The goal of this study is well-motivated and clear.\n2. The authors do a strong job of supporting the assumptions made in Section 2 through empirical study.  3. The empirical results are comprehensive. Multiple datasets and methods are used.\n\nreasons_to_reject: 1. More care can be taken in explaining the equation in Section 3. It was not obvious at first as to what exactly they should be measuring. An explanation given before or after each equation, explaining the purpose and intuition would be helpful.\n2. I don't find the results in presented in Section 4.4 to be very convincing. First, based on the results the authors state that the relationship between the discrete and continuous perturbations is weak. They conclude that discrete perturbations can't properly attack models. However, earlier they use Figure 1 to show that the continuous and discrete perturbations have a similar effect on model performance. It's hard to square these two assertions. Secondly, they note that PerturbScorer has a strong correlation with the continuous pertubations. I feel like this isn't surprising at all as PerturbScorer utilizes $\\epsilon$.\n\nquestions_for_the_authors: 1. What are the axes for the the plots in figure 1? I'd recommend including axis names in the future for better readability.\n2. Could you re-explain the significance of the findings from Section 4.4? See my comments in the weaknesses section.  3. Could you expound on some potential applications of this work? Ideally including a small section in the paper about this would be helpful.\n\ntypos_grammar_style_and_presentation_improvements: 1. I found it difficult to read Section 4. I'd recommend not utilizing so many sub sections (and sub-sub and so on...). However this may just be a personal taste.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "ZZmz1EvXc0",
        "length": 745,
        "human_text": "paper_topic_and_main_contributions: Robustness of the neural NLP models have become an extremely important topic as these neural models are abundantly used in real world applications. Perturbation-based robustness study is popular among the deep learning researchers, specifically, in the computer vision community where the nature of perturbation is continuous.  Perturbation-based robustness study in NLP faces the challenge of handling discrete input leading to discrete perturbation. This paper aims at connecting discrete perturbation to continuous perturbation so that perturbation in the continuous domain is applicable in NLP. Primary contributions of the paper include 1) quantifying the correlation between discrete and continuous perturbation, 2) designing a regression-based model (viz, PerturbScore) to estimate the correlation.\n\nreasons_to_accept: 1. The paper studies one of the most important aspects of NLP research, i.e., measuring the robustness of the neural NLP models. \n2. To deal with imperceptibility of the discrete perturbations and combinatorial explosion in formulating those perturbations, the paper advocates for applying continuous perturbation. \n3. I really appreciate the two staged approach adopted by the authors: Firstly, mapping discrete perturbation to continuous perturbation with a set of non-trivial assumptions. This formalisation along with an optimization method the authors presented an elegant method for generating training examples each consisting of original input, discrete perturbation and equivalent continuous perturbation. Secondly, using those training instances, a regression model has been proposed to predict continuous prediction given a discrete perturbation. \n4. The authors performed extensive experiments to validate the claims and showcase the usefulness of the method\n\nreasons_to_reject: 1. Many claims are ambiguous and are not supported by strong logic. ( see questions) 2. Given the results, I have doubt in the generalizability of the proposed model.\n\nquestions_for_the_authors: 1. Page 4, Column 2: the sentence  \u2018..... obtain a data tuple [S, P(S), \\epsilon] , which is the correlation\u2019 is a bit ambiguous. Assuming \u2018correlation\u2019 refers  to \u2018\\epsioln\u2019, is it a correlation between the discrete and continuous perturbations? Or to me it seems that \u2018\\epsilon\u2019 is a continuous surrogate for the discrete perturbation P(S). Please clarify. This may be confused with another correlation measured in section 4.2 in the form of Kendal and Pearson Index. Also, the tuple [S, P(S), \\epsilon] is not a correlation.\n2. Page 6, Column 1: How does better resistance of models trained FreeLB method against discrete and continuous perturbation verify assumption 1 and ensure correlation between discrete and continuous perturbation? There seems to be a logical gap. Please clarify 3. Page 6: Column 1: Table 2 shows count of data tuples under different \\epsilon ranges. This shows most of the discrete distributions correspond to lower continuous perturbation value ranges. Some of the data tuples have been discarded? The claim is as a small proportion of data tuples have been discarded, the method is able to find the \u2018norm ball that satisfies Assumption 3\u2019. What is the criteria for discarding the data tuples?  If it is manually selected then size of the set of discarded tuples may vary. Also, it is apparent why discarding a small proportion of tuples leads to satisfaction of Assumption 3.  4. Page 6: Column 1: There is no evidence in the paper that \u201cTextfooler method generates more discrete perturbations \u2026\u2026 and corresponding continuous perturbations require larger norm balls. Please specify if these experimental observations have been explicitly presented in any part of the paper.\n5. The cross-dataset analysis in Appendix shows that there is no significant difference in the correlation between edit-distance and PerturbScorer. In many cases, the correlation is poorer in PerturbScorer. Though the situation improves when the datasets are combined.  How these results reflect on the generalizability of the model?\n\ntypos_grammar_style_and_presentation_improvements: 1. Use of mixed case styles: Textfooler, textfooler 2. Page 5, Column 1, Line 1: \u2018Specifically, the notation x used in line 7\u2026\u2019. Assuming line 7 refers to the line no 7 of Algorithm 1, no mention of \u2018x\u2019 is present.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "54DvMp7syH",
        "length": 434,
        "human_text": "paper_topic_and_main_contributions: This paper addresses the research question: can we find a connection between discrete perturbation and continuous perturbation to study the robustness of neural NLP models? They approach this question by quantifying the connections with the norm-bound of gradient-based perturbations and training a regression model PerturbScorer to predict the strength of the connection (correlation). The resulting PerturbScorer can be useful for better understanding robust training methods of NLP models and transferring the robustness theories studied in the computer vision field to NLP.\n\nreasons_to_accept: 1. This paper looks into an interesting problem of finding a continuous perturbation as a proxy for discrete perturbation, which provides a new angle to study the robustness of NLP models through the lens of continuous space.  2. Experiment results of PerturbScorer look promising, indicating that the generated perturbation dataset and the trained model are of good quality.\n3. The method is carefully designed and well-explained in Sec 3.\n\nreasons_to_reject: 1. Limited numbers of models and datasets/tasks are studied in this paper. I hope to see similar results on more model architectures (including a simpler RNN model) and tasks to better support the generalization ability of PerturbScorer.\n2. I wish to see a concrete demonstration of how the PerturbScorer can be applied to broader scenarios such as studying the robustness of NLP models in continuous space, measuring sentence differences, etc.   3. The validity of the proposed method would be better supported by a theoretical proof of when the norm-bound would exist (the continuous perturbation effect is way bigger than discrete ones, line 342-345).\n\nquestions_for_the_authors: 1. The paper explores how to approximate a discrete perturbation with a continuous one. Is it possible to do the other way round, i.e. finding the corresponding discrete perturbation of a continuous one? This could make the two types of perturbations more connected.\n2. Do you use the same model (e.g. BERT) for perturbation dataset generation (sec 3.4) and perturbscoer (sec 3.5)? What would the correlation results be like if you use different types of models?\n\ntypos_grammar_style_and_presentation_improvements: The paper seems to miss a brief introduction to the FreeLB method appeared in Fig 1 & Table 2. Can you add a few sentences to explain it?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "eR4itEZVTh",
        "length": 495,
        "human_text": "paper_topic_and_main_contributions: In this paper, the authors study the problem of model robustness in NLP. Specifically, they seek to study the relationship between continuous and discrete perturbations in NLP models. The authors motivate this study by noting tha most robustness work in NLP focuses on discrete perturbations. However, this can be problematic as discrete perturbations are much more costly than their continuous counterparts. The authors first assume that there exists a connection between discrete and continuous perturbations, and using that design a method finds the proper norm bound between a continuous and discrete perturbation. They utilize this method to construct their own pertuber. They utilize this to conduct various studies comparing the two types of perturbations and the effect on model performance.\nPost-Rebuttal Update: I appreciate the detailed response. I find most of the responses satisfactory. I've raised my excitement from a 3 &rarr; 4\n\nreasons_to_accept: 1. The goal of this study is well-motivated and clear.\n2. The authors do a strong job of supporting the assumptions made in Section 2 through empirical study.  3. The empirical results are comprehensive. Multiple datasets and methods are used.\n\nreasons_to_reject: 1. More care can be taken in explaining the equation in Section 3. It was not obvious at first as to what exactly they should be measuring. An explanation given before or after each equation, explaining the purpose and intuition would be helpful.\n2. I don't find the results in presented in Section 4.4 to be very convincing. First, based on the results the authors state that the relationship between the discrete and continuous perturbations is weak. They conclude that discrete perturbations can't properly attack models. However, earlier they use Figure 1 to show that the continuous and discrete perturbations have a similar effect on model performance. It's hard to square these two assertions. Secondly, they note that PerturbScorer has a strong correlation with the continuous pertubations. I feel like this isn't surprising at all as PerturbScorer utilizes $\\epsilon$.\n\nquestions_for_the_authors: 1. What are the axes for the the plots in figure 1? I'd recommend including axis names in the future for better readability.\n2. Could you re-explain the significance of the findings from Section 4.4? See my comments in the weaknesses section.  3. Could you expound on some potential applications of this work? Ideally including a small section in the paper about this would be helpful.\n\ntypos_grammar_style_and_presentation_improvements: 1. I found it difficult to read Section 4. I'd recommend not utilizing so many sub sections (and sub-sub and so on...). However this may just be a personal taste.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "132_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_132_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7066333333333334,
      "max_similarity": 0.7124,
      "avg_coverage": 0.5022000000000001,
      "max_coverage": 0.5714
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 584,
      "avg_human_length": 309.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 7
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "BYtwUIEEVU",
        "similarity": 0.7124,
        "coverage": 0.4615,
        "human_length": 333,
        "human_text": "paper_topic_and_main_contributions: This paper studied the problem of dense retrieval. It aims at improving the document embedding for clustering as well as accelerate the retrieval process. To this end, the authors proposed a hybrid inverted index which combines the advantage of dense retrieval and lexical matching. The authors also proposed learning techniques for term and cluster selection with a joint optimization solution. Experiments are conducted on several popular datasets and the results look promising.\n\nreasons_to_accept: - The research topic is important.\n- The paper is well written.\n- The proposed techniques are solid.\n\nreasons_to_reject: - Some latest related works from the field of high dimensional similarity search are ignored such as [1]. They have been proved to have better performance than graph based methods such as HNSW compared here.\n- The comparison with some inverted list based sparse retrieval methods are missing. There are some works about compression over inverted lists and perfrom similarity search on them such as [2] [3]. They could also be extended as baseline methods. Since they are optimized, they should perform better than the selected sparse method here.\n- There is no result about space overhead.\n\nquestions_for_the_authors: What is the space overhead or memory consumption of propose techniques?\n\nmissing_references: [1] MQH: Locality Sensitive Hashing on Multi-level Quantization Errors for Point-to-Hyperplane Distances. PVLDB 2022. https://www.vldb.org/pvldb/vol16/p864-lu.pdf [2] Highly Efficient String Similarity Search and Join over Compressed Indexes. ICDE 2022. https://ieeexplore.ieee.org/document/9835221 [3] MILC: Inverted List Compression in Memory. VLDB 2017. http://www.vldb.org/pvldb/vol10/p853-wang.pdf\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "yH8CEQtwVL",
        "similarity": 0.7033,
        "coverage": 0.4737,
        "human_length": 229,
        "human_text": "paper_topic_and_main_contributions: This paper deals with the retrieval quality of dense retrieval and proposes a Hybrid Inverted Index that uses the embedding clusters and salient terms collaboratively to accelerate dense retrieval. It achieves lossless retrieval quality with competitive efficiency across a variety of index settings. The source code is provided.\n\nreasons_to_accept: 1. The idea to combine embedding clusters and salient terms is interesting. \n2. The paper is well organized. The description of the method design and implementation is clear and easy to follow. \n3. The experimental results prove the effectiveness and efficiency of the proposed method on two popular benchmark datasets.\n\nreasons_to_reject: 1. In the experiments, only the query latency is given. The setup latency should also be given. \n2. The usage of \u201clossless retrieval quality\u201d is easy to be confusing.\n\ntypos_grammar_style_and_presentation_improvements: There are some typos such as \u201cIVF-Flat and DistillIVF-Flat achieves recall\u201d ([pdf](zotero://open-pdf/library/items/WJ33MJVQ?page=2)) , \u201c2) Natural Questions (Kwiatkowski et al., 2019).\u201d\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "7szchI4gAO",
        "similarity": 0.7042,
        "coverage": 0.5714,
        "human_length": 366,
        "human_text": "paper_topic_and_main_contributions: The manuscript describes a hybrid lexical and semantic inverted index for dense retrieval. The lexical component provides speed and precision due to terms; the semantic component allows to connect concepts beyond words. The semantic component works by clustering documents and creating meta-words that can be used to accumulate entries in the posting lists of the index. The approach includes a score function and a strategy to optimize lexical and semantic \"words\" selection.\n\nreasons_to_accept: - The hybrid inverted index that it is fast and precise.\n- It provides a way to combine lexical and semantic scores smoothly - It introduces a way to distill the data to become smaller, and faster, while maintaining precise\n\nreasons_to_reject: Methodology weaknesses: It is not clear if they are seeing the same information as the hybrid index. If they are seeing the same, it is fair and it is a matter of speed the comparison, if they are seeing differences it is a matter of the score but speed is not necessarily comparable, at least as how the manuscript presents results as compared with IVFPQ and HNSW.\n\nquestions_for_the_authors: A. I recommend measuring with brute force and the lexical+semantic score proposed, or some variation, to know how much precision/recall is missing due to the index approximation.\nB. Please mention what kind of input receives each ANN and what kind of similarity/metric is used in each case. \n  C. Discuss the implications of using different representations for different ANN.\nD. HNSW and IVFPQ are sensitive to their hyper-parameters. Did you optimize their hyper-parameters?\n\ntypos_grammar_style_and_presentation_improvements: Table 1, Fig 3, Fig 4, Line 145. Natual -> Natural\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "BYtwUIEEVU",
        "length": 333,
        "human_text": "paper_topic_and_main_contributions: This paper studied the problem of dense retrieval. It aims at improving the document embedding for clustering as well as accelerate the retrieval process. To this end, the authors proposed a hybrid inverted index which combines the advantage of dense retrieval and lexical matching. The authors also proposed learning techniques for term and cluster selection with a joint optimization solution. Experiments are conducted on several popular datasets and the results look promising.\n\nreasons_to_accept: - The research topic is important.\n- The paper is well written.\n- The proposed techniques are solid.\n\nreasons_to_reject: - Some latest related works from the field of high dimensional similarity search are ignored such as [1]. They have been proved to have better performance than graph based methods such as HNSW compared here.\n- The comparison with some inverted list based sparse retrieval methods are missing. There are some works about compression over inverted lists and perfrom similarity search on them such as [2] [3]. They could also be extended as baseline methods. Since they are optimized, they should perform better than the selected sparse method here.\n- There is no result about space overhead.\n\nquestions_for_the_authors: What is the space overhead or memory consumption of propose techniques?\n\nmissing_references: [1] MQH: Locality Sensitive Hashing on Multi-level Quantization Errors for Point-to-Hyperplane Distances. PVLDB 2022. https://www.vldb.org/pvldb/vol16/p864-lu.pdf [2] Highly Efficient String Similarity Search and Join over Compressed Indexes. ICDE 2022. https://ieeexplore.ieee.org/document/9835221 [3] MILC: Inverted List Compression in Memory. VLDB 2017. http://www.vldb.org/pvldb/vol10/p853-wang.pdf\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "yH8CEQtwVL",
        "length": 229,
        "human_text": "paper_topic_and_main_contributions: This paper deals with the retrieval quality of dense retrieval and proposes a Hybrid Inverted Index that uses the embedding clusters and salient terms collaboratively to accelerate dense retrieval. It achieves lossless retrieval quality with competitive efficiency across a variety of index settings. The source code is provided.\n\nreasons_to_accept: 1. The idea to combine embedding clusters and salient terms is interesting. \n2. The paper is well organized. The description of the method design and implementation is clear and easy to follow. \n3. The experimental results prove the effectiveness and efficiency of the proposed method on two popular benchmark datasets.\n\nreasons_to_reject: 1. In the experiments, only the query latency is given. The setup latency should also be given. \n2. The usage of \u201clossless retrieval quality\u201d is easy to be confusing.\n\ntypos_grammar_style_and_presentation_improvements: There are some typos such as \u201cIVF-Flat and DistillIVF-Flat achieves recall\u201d ([pdf](zotero://open-pdf/library/items/WJ33MJVQ?page=2)) , \u201c2) Natural Questions (Kwiatkowski et al., 2019).\u201d\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "7szchI4gAO",
        "length": 366,
        "human_text": "paper_topic_and_main_contributions: The manuscript describes a hybrid lexical and semantic inverted index for dense retrieval. The lexical component provides speed and precision due to terms; the semantic component allows to connect concepts beyond words. The semantic component works by clustering documents and creating meta-words that can be used to accumulate entries in the posting lists of the index. The approach includes a score function and a strategy to optimize lexical and semantic \"words\" selection.\n\nreasons_to_accept: - The hybrid inverted index that it is fast and precise.\n- It provides a way to combine lexical and semantic scores smoothly - It introduces a way to distill the data to become smaller, and faster, while maintaining precise\n\nreasons_to_reject: Methodology weaknesses: It is not clear if they are seeing the same information as the hybrid index. If they are seeing the same, it is fair and it is a matter of speed the comparison, if they are seeing differences it is a matter of the score but speed is not necessarily comparable, at least as how the manuscript presents results as compared with IVFPQ and HNSW.\n\nquestions_for_the_authors: A. I recommend measuring with brute force and the lexical+semantic score proposed, or some variation, to know how much precision/recall is missing due to the index approximation.\nB. Please mention what kind of input receives each ANN and what kind of similarity/metric is used in each case. \n  C. Discuss the implications of using different representations for different ANN.\nD. HNSW and IVFPQ are sensitive to their hyper-parameters. Did you optimize their hyper-parameters?\n\ntypos_grammar_style_and_presentation_improvements: Table 1, Fig 3, Fig 4, Line 145. Natual -> Natural\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "132_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_132_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7051,
      "max_similarity": 0.7138,
      "avg_coverage": 0.5484333333333333,
      "max_coverage": 0.619
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 497,
      "avg_human_length": 309.3333333333333
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 8,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "BYtwUIEEVU",
        "similarity": 0.7138,
        "coverage": 0.5,
        "human_length": 333,
        "human_text": "paper_topic_and_main_contributions: This paper studied the problem of dense retrieval. It aims at improving the document embedding for clustering as well as accelerate the retrieval process. To this end, the authors proposed a hybrid inverted index which combines the advantage of dense retrieval and lexical matching. The authors also proposed learning techniques for term and cluster selection with a joint optimization solution. Experiments are conducted on several popular datasets and the results look promising.\n\nreasons_to_accept: - The research topic is important.\n- The paper is well written.\n- The proposed techniques are solid.\n\nreasons_to_reject: - Some latest related works from the field of high dimensional similarity search are ignored such as [1]. They have been proved to have better performance than graph based methods such as HNSW compared here.\n- The comparison with some inverted list based sparse retrieval methods are missing. There are some works about compression over inverted lists and perfrom similarity search on them such as [2] [3]. They could also be extended as baseline methods. Since they are optimized, they should perform better than the selected sparse method here.\n- There is no result about space overhead.\n\nquestions_for_the_authors: What is the space overhead or memory consumption of propose techniques?\n\nmissing_references: [1] MQH: Locality Sensitive Hashing on Multi-level Quantization Errors for Point-to-Hyperplane Distances. PVLDB 2022. https://www.vldb.org/pvldb/vol16/p864-lu.pdf [2] Highly Efficient String Similarity Search and Join over Compressed Indexes. ICDE 2022. https://ieeexplore.ieee.org/document/9835221 [3] MILC: Inverted List Compression in Memory. VLDB 2017. http://www.vldb.org/pvldb/vol10/p853-wang.pdf\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "yH8CEQtwVL",
        "similarity": 0.7032,
        "coverage": 0.5263,
        "human_length": 229,
        "human_text": "paper_topic_and_main_contributions: This paper deals with the retrieval quality of dense retrieval and proposes a Hybrid Inverted Index that uses the embedding clusters and salient terms collaboratively to accelerate dense retrieval. It achieves lossless retrieval quality with competitive efficiency across a variety of index settings. The source code is provided.\n\nreasons_to_accept: 1. The idea to combine embedding clusters and salient terms is interesting. \n2. The paper is well organized. The description of the method design and implementation is clear and easy to follow. \n3. The experimental results prove the effectiveness and efficiency of the proposed method on two popular benchmark datasets.\n\nreasons_to_reject: 1. In the experiments, only the query latency is given. The setup latency should also be given. \n2. The usage of \u201clossless retrieval quality\u201d is easy to be confusing.\n\ntypos_grammar_style_and_presentation_improvements: There are some typos such as \u201cIVF-Flat and DistillIVF-Flat achieves recall\u201d ([pdf](zotero://open-pdf/library/items/WJ33MJVQ?page=2)) , \u201c2) Natural Questions (Kwiatkowski et al., 2019).\u201d\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "7szchI4gAO",
        "similarity": 0.6983,
        "coverage": 0.619,
        "human_length": 366,
        "human_text": "paper_topic_and_main_contributions: The manuscript describes a hybrid lexical and semantic inverted index for dense retrieval. The lexical component provides speed and precision due to terms; the semantic component allows to connect concepts beyond words. The semantic component works by clustering documents and creating meta-words that can be used to accumulate entries in the posting lists of the index. The approach includes a score function and a strategy to optimize lexical and semantic \"words\" selection.\n\nreasons_to_accept: - The hybrid inverted index that it is fast and precise.\n- It provides a way to combine lexical and semantic scores smoothly - It introduces a way to distill the data to become smaller, and faster, while maintaining precise\n\nreasons_to_reject: Methodology weaknesses: It is not clear if they are seeing the same information as the hybrid index. If they are seeing the same, it is fair and it is a matter of speed the comparison, if they are seeing differences it is a matter of the score but speed is not necessarily comparable, at least as how the manuscript presents results as compared with IVFPQ and HNSW.\n\nquestions_for_the_authors: A. I recommend measuring with brute force and the lexical+semantic score proposed, or some variation, to know how much precision/recall is missing due to the index approximation.\nB. Please mention what kind of input receives each ANN and what kind of similarity/metric is used in each case. \n  C. Discuss the implications of using different representations for different ANN.\nD. HNSW and IVFPQ are sensitive to their hyper-parameters. Did you optimize their hyper-parameters?\n\ntypos_grammar_style_and_presentation_improvements: Table 1, Fig 3, Fig 4, Line 145. Natual -> Natural\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "BYtwUIEEVU",
        "length": 333,
        "human_text": "paper_topic_and_main_contributions: This paper studied the problem of dense retrieval. It aims at improving the document embedding for clustering as well as accelerate the retrieval process. To this end, the authors proposed a hybrid inverted index which combines the advantage of dense retrieval and lexical matching. The authors also proposed learning techniques for term and cluster selection with a joint optimization solution. Experiments are conducted on several popular datasets and the results look promising.\n\nreasons_to_accept: - The research topic is important.\n- The paper is well written.\n- The proposed techniques are solid.\n\nreasons_to_reject: - Some latest related works from the field of high dimensional similarity search are ignored such as [1]. They have been proved to have better performance than graph based methods such as HNSW compared here.\n- The comparison with some inverted list based sparse retrieval methods are missing. There are some works about compression over inverted lists and perfrom similarity search on them such as [2] [3]. They could also be extended as baseline methods. Since they are optimized, they should perform better than the selected sparse method here.\n- There is no result about space overhead.\n\nquestions_for_the_authors: What is the space overhead or memory consumption of propose techniques?\n\nmissing_references: [1] MQH: Locality Sensitive Hashing on Multi-level Quantization Errors for Point-to-Hyperplane Distances. PVLDB 2022. https://www.vldb.org/pvldb/vol16/p864-lu.pdf [2] Highly Efficient String Similarity Search and Join over Compressed Indexes. ICDE 2022. https://ieeexplore.ieee.org/document/9835221 [3] MILC: Inverted List Compression in Memory. VLDB 2017. http://www.vldb.org/pvldb/vol10/p853-wang.pdf\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "yH8CEQtwVL",
        "length": 229,
        "human_text": "paper_topic_and_main_contributions: This paper deals with the retrieval quality of dense retrieval and proposes a Hybrid Inverted Index that uses the embedding clusters and salient terms collaboratively to accelerate dense retrieval. It achieves lossless retrieval quality with competitive efficiency across a variety of index settings. The source code is provided.\n\nreasons_to_accept: 1. The idea to combine embedding clusters and salient terms is interesting. \n2. The paper is well organized. The description of the method design and implementation is clear and easy to follow. \n3. The experimental results prove the effectiveness and efficiency of the proposed method on two popular benchmark datasets.\n\nreasons_to_reject: 1. In the experiments, only the query latency is given. The setup latency should also be given. \n2. The usage of \u201clossless retrieval quality\u201d is easy to be confusing.\n\ntypos_grammar_style_and_presentation_improvements: There are some typos such as \u201cIVF-Flat and DistillIVF-Flat achieves recall\u201d ([pdf](zotero://open-pdf/library/items/WJ33MJVQ?page=2)) , \u201c2) Natural Questions (Kwiatkowski et al., 2019).\u201d\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 5: Could easily reproduce the results.",
        "has_content": true
      },
      {
        "reviewer_id": "7szchI4gAO",
        "length": 366,
        "human_text": "paper_topic_and_main_contributions: The manuscript describes a hybrid lexical and semantic inverted index for dense retrieval. The lexical component provides speed and precision due to terms; the semantic component allows to connect concepts beyond words. The semantic component works by clustering documents and creating meta-words that can be used to accumulate entries in the posting lists of the index. The approach includes a score function and a strategy to optimize lexical and semantic \"words\" selection.\n\nreasons_to_accept: - The hybrid inverted index that it is fast and precise.\n- It provides a way to combine lexical and semantic scores smoothly - It introduces a way to distill the data to become smaller, and faster, while maintaining precise\n\nreasons_to_reject: Methodology weaknesses: It is not clear if they are seeing the same information as the hybrid index. If they are seeing the same, it is fair and it is a matter of speed the comparison, if they are seeing differences it is a matter of the score but speed is not necessarily comparable, at least as how the manuscript presents results as compared with IVFPQ and HNSW.\n\nquestions_for_the_authors: A. I recommend measuring with brute force and the lexical+semantic score proposed, or some variation, to know how much precision/recall is missing due to the index approximation.\nB. Please mention what kind of input receives each ANN and what kind of similarity/metric is used in each case. \n  C. Discuss the implications of using different representations for different ANN.\nD. HNSW and IVFPQ are sensitive to their hyper-parameters. Did you optimize their hyper-parameters?\n\ntypos_grammar_style_and_presentation_improvements: Table 1, Fig 3, Fig 4, Line 145. Natual -> Natural\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "19_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_19_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7437333333333332,
      "max_similarity": 0.7565,
      "avg_coverage": 0.32316666666666666,
      "max_coverage": 0.4286
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 385,
      "avg_human_length": 794.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 6,
      "suggestions_count": 6
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "s1wsuqNeS1",
        "similarity": 0.7564,
        "coverage": 0.2,
        "human_length": 832,
        "human_text": "paper_topic_and_main_contributions: This paper is centrally interested in structured analogical reasoning in large language models. Existing work has focused on word analogies. This work suggests that results from that paradigm overestimate the overlap between neural models and human cognition. They introduce a task of analogical structure abduction that evaluates whether models can systemically associate the components of an analogy. The results suggest that models often miss the underlying relations, and thus, differ from humans. The main contributions are: i) a benchmark called ScAR and ii) a evaluation of LLMs on this task highlighting the need for future work to improve models.\n\nreasons_to_accept: The paper is well motivated and its aim clear. The benchmark will be of use to community and appears to address a gap in existing resources. The experiments are reasonably thorough and expose limitations with existing models, while suggesting ways they might be improved.\n\nreasons_to_reject: The problem identified by the paper is interesting and well articulated. There are a few methodological issues that weaken the rating (included under questions for the authors). Additionally, the linkage to human cognition (which appears to be a central aim of the paper) is a bit unclear. It is stated that the Structure Mapping Theory (SMT) has been proposed as a mechanism for analogical reasoning. Other times, it is stated that SMT suggests that new knowledge is gained by system level analogies. Put another way, there are two uses of SMT in the paper, (i) as the account for analogical reasoning broadly construed, and (ii) as the account for system level analogical reasoning (a more narrow form of analogy). If (ii) is the correct interpretation, then it\u2019s possible models can do simple word analogies in a human-like fashion without the ability to do system level analogies. The lack of clarity around these possibilities makes it difficult to interpret what these results say about comparisons between models and humans.  Ultimately, the paper proposes a potentially useful benchmark for evaluating the underlying structural reasoning abilities of large-language models. However, some issues with the methodology and the underlying arguments motivating the conclusion depress my rating.\n\nquestions_for_the_authors: A. Can you elaborate on the reasonableness of using the E-KAR dataset heavily in this work? The main concern is that it is a dataset taken from China\u2019s Civil Service Examinations. The description provided in Appendix A suggests that \u201ccultural knowledge\u201d is part of the exam. Is it reasonable to expect that a model trained largely on English data and evaluated on English data will be able to answer these questions? I have in mind work like Santurkar et al. (2023). Whose Opinions Do Language Models Reflect. https://arxiv.org/abs/2303.17548., which demonstrate that models encode only some groups well.  B. Why was GPT-4 used so extensively in the creation of the dataset (e.g., for the explanation generation)? Wouldn\u2019t higher quality data come from having the annotators do all of this (they are already reading through them after all)?  C. In the Ethics Statement it is stated that \u201cas described in our paper, all annotators are compensated\u201d, where is this stated in the body of the paper? It would be helpful if more details about the annotators (e.g., what are the 5 annotators backgrounds on lines 229-231) and their task were included (e.g., the instructions they were provided). This would facilitate both replication and extension of this work.  D. Technique for embeddings was unclear in section 2. It is stated that the method proposed by Ushio et al. (2021) was used (line 139). However, that paper used cosine similarity in evaluating embeddings, as far as I can tell. Moreover, it is stated that the winning answer candidate was determined with the marginal likelihood biased perplexity measure which uses probability of the token in context, not the embedding. Could this method be clarified in the paper?   MINOR: E. In section 4, a few more models were evaluated. Why weren\u2019t these models included in the other experiments?  F. The role of RSI in the paper is a bit unclear. Is it part of the new proposed benchmark as well? Or is it just used as an additional source of evidence that models fail to capture the underlying structural relations in an analogy (similar to how the ability of models to fully associate the set of concepts in the benchmark evidences they systematically understand the relations)?\n\ntypos_grammar_style_and_presentation_improvements: Figure 3d is used to argue that models are more robust in the CoT prompting scenario. The inclusion of error bars would facilitate more informed comparisons between the variation in a given condition and across conditions.  Consistent y-axis bounds in Figures 3, 5, and 6 would facilitate comparisons.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "hzWaFpE9ty",
        "similarity": 0.7565,
        "coverage": 0.3409,
        "human_length": 1095,
        "human_text": "paper_topic_and_main_contributions: This paper is about analogical reasoning in language models supported by structure abduction. Its contributions are as follows: 1. The authors present a study on past analogical reasoning benchmarks to show that while LLMs can achieve high performance on simpler word analogy benchmarks, this is not always supported by an understanding of the underlying structure for the analogy (unlike humans). This motivates them to create a stronger evaluation for analogical reasoning that considers structure abduction jointly with making analogies.\n2. The authors create a benchmark called Scientific Analogical Reasoning (ScAR) which provides 400 analogies across systems in 13 different scientific and other domains, including 1600 possible mappings between concepts within systems. They additionally collect background knowledge and explanations to support the analogies in the dataset, making it a possibly valuable and high-quality resource for studying analogy-making in LLMs.\n3. The authors evaluate contemporary LLMs, both open-source and closed API-based, on ScAR under several prompting settings. Further, they perform analysis on the impact of various LLM-based aspects in the experiment and the domain. Lastly, they apply earlier word embedding-based methods for analogy as a reference, and perform a preliminary study on open analogical structure abduction, where concepts from the two analogous systems are not provided directly, and rather must be retrieved from a large list.\nWhile the paper does not present new methods for tackling analogical reasoning with LLMs and perhaps lacks some consideration of the impact of LLM pre-training data on this problem, it still constitutes a substantial contribution that could be valuable to the NLP community.\n\nreasons_to_accept: **Strength A:** The paper compiles a (seemingly) high-quality benchmark dataset of interesting analogies across scientific and other domains, including consistent details about the underlying structure of analogies, and background knowledge and explanations for analogies. This resource could be useful to others in the community.\n**Strength B:** The creation of the benchmark is justified by a preliminary study on how word analogy tasks do not actually represent analogical reasoning capabilities. I have not seen another work demonstrate this before, and this can be motivation for the community to look at analogical reasoning more holistically than prior work has.\n**Strength C:** The analysis of benchmark results is thorough and fairly comprehensive, covering a lot of variations and details on the prompt engineering/LLM interface side, as well as including a thoughtful study on how analogies can be made across an open domain of concepts (which I believe is the hardest challenge in analogical reasoning). The authors also explore earlier representation/embedding-based approaches to analogy. It feels like the authors have considered a lot of questions in performing these experiments.\n\nreasons_to_reject: **Weakness A:** This paper doesn\u2019t propose any new modeling techniques or methods to apply LLMs to analogy tasks. This causes the main benchmark results on the paper to not be very inspiring, as they don\u2019t show much significant variation across existing models and prompting techniques that are explored.\n**Weakness B:** The paper does not rigorously explore the impact of pre-training data on LLMs\u2019 analogical reasoning. In natural domains like this that may be largely supported by memorization of training data, some analysis for this is important.\n\nquestions_for_the_authors: **Question A:** To what extent can contamination of LLM pre-training data impact the observed abilities of LLMs in analogical reasoning? For example, I can Google about the aperture of a camera and pupil of a human eye and find plenty of web content talking about how they\u2019re analogous. Does the LLM gain this capability just by memorizing the pre-training data?  I would be curious to see how thoroughly the system understands common systems or concepts across examples from ScAR - if the system can correctly answer the *aperture -> pupil* example, but can\u2019t understand some other example about *aperture* that\u2019s less common, this may suggest a lack of deep understanding of the concept of *aperture.* Figure 4 gives some hints about this possibility across domains, but doing such an analysis could also be a valuable addition to the work! * As a side note, this is one reason why Mitchell (2021) listed later in my review argues that we shouldn\u2019t focus on natural language analogy problems, but I think this issue can also be explored by more thorough evaluations.*\n\nmissing_references: ACL 2023 has at least one paper on LLMs and analogical reasoning, which looks at abstract analogical reasoning: https://aclanthology.org/2023.acl-long.109/  This paper by Melanie Mitchell would be important to cite: https://arxiv.org/abs/2102.10717\n\ntypos_grammar_style_and_presentation_improvements: **Organization/presentation suggestions:** L091: If accepted, you may want to use the extra space to break this paragraph out into a numbered list - may make it easier to read. There are some other paragraphs where you do this too, and I would recommend the same.\nSection 3.4 presents a formal problem definition, which might make more sense to introduce before your dataset. This would help give the reader a foundation on what task you\u2019re looking at, and then the data collection would be easier to understand and map each step to components of the task. I\u2019m not totally sure - would leave it up to you.\nI\u2019m not sure if it\u2019s fair to characterize Webb et al. (2022) as just looking at analogy generation with word analogies or simple sentence analogies as your Section 5 says around L529. This paper looks at abstract language-based analogy tasks and uses them to study how well LLMs can perform analogical reasoning. I might suggest adding a separate paragraph in Section 5 looking at abstract analogical reasoning rather than lumping it into the paragraph about analogies between situations. You may also think about organizing Section 5 based on how these different works focus on different parts of analogy-making, i.e., representation, mapping, and evaluation, and how your work contrasts from it.\n**Formatting/typo corrections:** L036, L046, L055, L060, L090, L110, L135, L344-348\u2026: putting two sets of parentheses next to each other or nesting parentheses is improper format and a bit confusing to read. You can use \\citealp in LaTeX to combine citations with other content in parentheses, or find another way to remove one set of parentheses.\nL124: missing a space after period Did you use ChatGPT or GPT-4 to revise background information about systems? The paper says ChatGPT, but Table 10 says GPT-4.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "yuHFUXJjjQ",
        "similarity": 0.7183,
        "coverage": 0.4286,
        "human_length": 456,
        "human_text": "paper_topic_and_main_contributions: The paper proposes to move beyond word analogies for LLMs and to enable \"analogical structure abduction\".  To this end a new data set is constructed based on the efforts of human annotators paid a local minimum wage, wherever local may be and however much that works out to be.  The results are mixed on a closed list of analogies, quite bad on an open list.  The main contributions are a move towards going beyond word analogies and the creation of a new data set.\n\nreasons_to_accept: The topic of the paper is interesting given that, as the authors point out, analogy is a very basic human skill that is applied across many different domains. It is as such interesting to extend LLMs capabilities to include analogical reasoning.\n\nreasons_to_reject: Within linguistics, language change based on analogy is pervasive as is language acquisition based on rule formation via analogy.  Analogy is also a powerful tool for story-telling, writing and argumentation.  Yet the authors chose none of these domains for their experiments.  Rather, they construct an artificial task of a slightly more complex relationship between words that looking at word similarity in that the relationship involves pairs of two words rather than a single pair of words.  It is not clear how this really reflects the human process of analogy or how it is more generally of interest beyond this rather artificially constructed sets of relations between pairs of words.  This is also presumably why the attempts at having LLMs work with the open list of \"analogies\" do so badly: the task is artificially defined and does not tap into higher principles/generalizations that the LLMs might have learned.\n\nquestions_for_the_authors: It is not clear to me how the classic \"queen is to woman as king is man\" is really an instance of analogy since basically only the similarity of two vectors to one another  is being compared: how is that really analogical reasoning?   Do the authors equate semantic similarity with analogy?  This point needs to be brought out more strongly in the paper.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: The annotation was done by human annotators at \"local\" minimum wage.  It is not said where those annotators are from or under what conditions they are performing the annotation.  This concern was addressed sufficiently by the authors.   They used undergraduates who were trained in this particular task.\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "s1wsuqNeS1",
        "length": 832,
        "human_text": "paper_topic_and_main_contributions: This paper is centrally interested in structured analogical reasoning in large language models. Existing work has focused on word analogies. This work suggests that results from that paradigm overestimate the overlap between neural models and human cognition. They introduce a task of analogical structure abduction that evaluates whether models can systemically associate the components of an analogy. The results suggest that models often miss the underlying relations, and thus, differ from humans. The main contributions are: i) a benchmark called ScAR and ii) a evaluation of LLMs on this task highlighting the need for future work to improve models.\n\nreasons_to_accept: The paper is well motivated and its aim clear. The benchmark will be of use to community and appears to address a gap in existing resources. The experiments are reasonably thorough and expose limitations with existing models, while suggesting ways they might be improved.\n\nreasons_to_reject: The problem identified by the paper is interesting and well articulated. There are a few methodological issues that weaken the rating (included under questions for the authors). Additionally, the linkage to human cognition (which appears to be a central aim of the paper) is a bit unclear. It is stated that the Structure Mapping Theory (SMT) has been proposed as a mechanism for analogical reasoning. Other times, it is stated that SMT suggests that new knowledge is gained by system level analogies. Put another way, there are two uses of SMT in the paper, (i) as the account for analogical reasoning broadly construed, and (ii) as the account for system level analogical reasoning (a more narrow form of analogy). If (ii) is the correct interpretation, then it\u2019s possible models can do simple word analogies in a human-like fashion without the ability to do system level analogies. The lack of clarity around these possibilities makes it difficult to interpret what these results say about comparisons between models and humans.  Ultimately, the paper proposes a potentially useful benchmark for evaluating the underlying structural reasoning abilities of large-language models. However, some issues with the methodology and the underlying arguments motivating the conclusion depress my rating.\n\nquestions_for_the_authors: A. Can you elaborate on the reasonableness of using the E-KAR dataset heavily in this work? The main concern is that it is a dataset taken from China\u2019s Civil Service Examinations. The description provided in Appendix A suggests that \u201ccultural knowledge\u201d is part of the exam. Is it reasonable to expect that a model trained largely on English data and evaluated on English data will be able to answer these questions? I have in mind work like Santurkar et al. (2023). Whose Opinions Do Language Models Reflect. https://arxiv.org/abs/2303.17548., which demonstrate that models encode only some groups well.  B. Why was GPT-4 used so extensively in the creation of the dataset (e.g., for the explanation generation)? Wouldn\u2019t higher quality data come from having the annotators do all of this (they are already reading through them after all)?  C. In the Ethics Statement it is stated that \u201cas described in our paper, all annotators are compensated\u201d, where is this stated in the body of the paper? It would be helpful if more details about the annotators (e.g., what are the 5 annotators backgrounds on lines 229-231) and their task were included (e.g., the instructions they were provided). This would facilitate both replication and extension of this work.  D. Technique for embeddings was unclear in section 2. It is stated that the method proposed by Ushio et al. (2021) was used (line 139). However, that paper used cosine similarity in evaluating embeddings, as far as I can tell. Moreover, it is stated that the winning answer candidate was determined with the marginal likelihood biased perplexity measure which uses probability of the token in context, not the embedding. Could this method be clarified in the paper?   MINOR: E. In section 4, a few more models were evaluated. Why weren\u2019t these models included in the other experiments?  F. The role of RSI in the paper is a bit unclear. Is it part of the new proposed benchmark as well? Or is it just used as an additional source of evidence that models fail to capture the underlying structural relations in an analogy (similar to how the ability of models to fully associate the set of concepts in the benchmark evidences they systematically understand the relations)?\n\ntypos_grammar_style_and_presentation_improvements: Figure 3d is used to argue that models are more robust in the CoT prompting scenario. The inclusion of error bars would facilitate more informed comparisons between the variation in a given condition and across conditions.  Consistent y-axis bounds in Figures 3, 5, and 6 would facilitate comparisons.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "hzWaFpE9ty",
        "length": 1095,
        "human_text": "paper_topic_and_main_contributions: This paper is about analogical reasoning in language models supported by structure abduction. Its contributions are as follows: 1. The authors present a study on past analogical reasoning benchmarks to show that while LLMs can achieve high performance on simpler word analogy benchmarks, this is not always supported by an understanding of the underlying structure for the analogy (unlike humans). This motivates them to create a stronger evaluation for analogical reasoning that considers structure abduction jointly with making analogies.\n2. The authors create a benchmark called Scientific Analogical Reasoning (ScAR) which provides 400 analogies across systems in 13 different scientific and other domains, including 1600 possible mappings between concepts within systems. They additionally collect background knowledge and explanations to support the analogies in the dataset, making it a possibly valuable and high-quality resource for studying analogy-making in LLMs.\n3. The authors evaluate contemporary LLMs, both open-source and closed API-based, on ScAR under several prompting settings. Further, they perform analysis on the impact of various LLM-based aspects in the experiment and the domain. Lastly, they apply earlier word embedding-based methods for analogy as a reference, and perform a preliminary study on open analogical structure abduction, where concepts from the two analogous systems are not provided directly, and rather must be retrieved from a large list.\nWhile the paper does not present new methods for tackling analogical reasoning with LLMs and perhaps lacks some consideration of the impact of LLM pre-training data on this problem, it still constitutes a substantial contribution that could be valuable to the NLP community.\n\nreasons_to_accept: **Strength A:** The paper compiles a (seemingly) high-quality benchmark dataset of interesting analogies across scientific and other domains, including consistent details about the underlying structure of analogies, and background knowledge and explanations for analogies. This resource could be useful to others in the community.\n**Strength B:** The creation of the benchmark is justified by a preliminary study on how word analogy tasks do not actually represent analogical reasoning capabilities. I have not seen another work demonstrate this before, and this can be motivation for the community to look at analogical reasoning more holistically than prior work has.\n**Strength C:** The analysis of benchmark results is thorough and fairly comprehensive, covering a lot of variations and details on the prompt engineering/LLM interface side, as well as including a thoughtful study on how analogies can be made across an open domain of concepts (which I believe is the hardest challenge in analogical reasoning). The authors also explore earlier representation/embedding-based approaches to analogy. It feels like the authors have considered a lot of questions in performing these experiments.\n\nreasons_to_reject: **Weakness A:** This paper doesn\u2019t propose any new modeling techniques or methods to apply LLMs to analogy tasks. This causes the main benchmark results on the paper to not be very inspiring, as they don\u2019t show much significant variation across existing models and prompting techniques that are explored.\n**Weakness B:** The paper does not rigorously explore the impact of pre-training data on LLMs\u2019 analogical reasoning. In natural domains like this that may be largely supported by memorization of training data, some analysis for this is important.\n\nquestions_for_the_authors: **Question A:** To what extent can contamination of LLM pre-training data impact the observed abilities of LLMs in analogical reasoning? For example, I can Google about the aperture of a camera and pupil of a human eye and find plenty of web content talking about how they\u2019re analogous. Does the LLM gain this capability just by memorizing the pre-training data?  I would be curious to see how thoroughly the system understands common systems or concepts across examples from ScAR - if the system can correctly answer the *aperture -> pupil* example, but can\u2019t understand some other example about *aperture* that\u2019s less common, this may suggest a lack of deep understanding of the concept of *aperture.* Figure 4 gives some hints about this possibility across domains, but doing such an analysis could also be a valuable addition to the work! * As a side note, this is one reason why Mitchell (2021) listed later in my review argues that we shouldn\u2019t focus on natural language analogy problems, but I think this issue can also be explored by more thorough evaluations.*\n\nmissing_references: ACL 2023 has at least one paper on LLMs and analogical reasoning, which looks at abstract analogical reasoning: https://aclanthology.org/2023.acl-long.109/  This paper by Melanie Mitchell would be important to cite: https://arxiv.org/abs/2102.10717\n\ntypos_grammar_style_and_presentation_improvements: **Organization/presentation suggestions:** L091: If accepted, you may want to use the extra space to break this paragraph out into a numbered list - may make it easier to read. There are some other paragraphs where you do this too, and I would recommend the same.\nSection 3.4 presents a formal problem definition, which might make more sense to introduce before your dataset. This would help give the reader a foundation on what task you\u2019re looking at, and then the data collection would be easier to understand and map each step to components of the task. I\u2019m not totally sure - would leave it up to you.\nI\u2019m not sure if it\u2019s fair to characterize Webb et al. (2022) as just looking at analogy generation with word analogies or simple sentence analogies as your Section 5 says around L529. This paper looks at abstract language-based analogy tasks and uses them to study how well LLMs can perform analogical reasoning. I might suggest adding a separate paragraph in Section 5 looking at abstract analogical reasoning rather than lumping it into the paragraph about analogies between situations. You may also think about organizing Section 5 based on how these different works focus on different parts of analogy-making, i.e., representation, mapping, and evaluation, and how your work contrasts from it.\n**Formatting/typo corrections:** L036, L046, L055, L060, L090, L110, L135, L344-348\u2026: putting two sets of parentheses next to each other or nesting parentheses is improper format and a bit confusing to read. You can use \\citealp in LaTeX to combine citations with other content in parentheses, or find another way to remove one set of parentheses.\nL124: missing a space after period Did you use ChatGPT or GPT-4 to revise background information about systems? The paper says ChatGPT, but Table 10 says GPT-4.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "yuHFUXJjjQ",
        "length": 456,
        "human_text": "paper_topic_and_main_contributions: The paper proposes to move beyond word analogies for LLMs and to enable \"analogical structure abduction\".  To this end a new data set is constructed based on the efforts of human annotators paid a local minimum wage, wherever local may be and however much that works out to be.  The results are mixed on a closed list of analogies, quite bad on an open list.  The main contributions are a move towards going beyond word analogies and the creation of a new data set.\n\nreasons_to_accept: The topic of the paper is interesting given that, as the authors point out, analogy is a very basic human skill that is applied across many different domains. It is as such interesting to extend LLMs capabilities to include analogical reasoning.\n\nreasons_to_reject: Within linguistics, language change based on analogy is pervasive as is language acquisition based on rule formation via analogy.  Analogy is also a powerful tool for story-telling, writing and argumentation.  Yet the authors chose none of these domains for their experiments.  Rather, they construct an artificial task of a slightly more complex relationship between words that looking at word similarity in that the relationship involves pairs of two words rather than a single pair of words.  It is not clear how this really reflects the human process of analogy or how it is more generally of interest beyond this rather artificially constructed sets of relations between pairs of words.  This is also presumably why the attempts at having LLMs work with the open list of \"analogies\" do so badly: the task is artificially defined and does not tap into higher principles/generalizations that the LLMs might have learned.\n\nquestions_for_the_authors: It is not clear to me how the classic \"queen is to woman as king is man\" is really an instance of analogy since basically only the similarity of two vectors to one another  is being compared: how is that really analogical reasoning?   Do the authors equate semantic similarity with analogy?  This point needs to be brought out more strongly in the paper.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: The annotation was done by human annotators at \"local\" minimum wage.  It is not said where those annotators are from or under what conditions they are performing the annotation.  This concern was addressed sufficiently by the authors.   They used undergraduates who were trained in this particular task.\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "19_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_19_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 3,
    "summary_metrics": {
      "avg_similarity": 0.7411666666666666,
      "max_similarity": 0.7504,
      "avg_coverage": 0.3156,
      "max_coverage": 0.4286
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 370,
      "avg_human_length": 794.3333333333334
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 5,
      "weaknesses_count": 6,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "s1wsuqNeS1",
        "similarity": 0.7458,
        "coverage": 0.2,
        "human_length": 832,
        "human_text": "paper_topic_and_main_contributions: This paper is centrally interested in structured analogical reasoning in large language models. Existing work has focused on word analogies. This work suggests that results from that paradigm overestimate the overlap between neural models and human cognition. They introduce a task of analogical structure abduction that evaluates whether models can systemically associate the components of an analogy. The results suggest that models often miss the underlying relations, and thus, differ from humans. The main contributions are: i) a benchmark called ScAR and ii) a evaluation of LLMs on this task highlighting the need for future work to improve models.\n\nreasons_to_accept: The paper is well motivated and its aim clear. The benchmark will be of use to community and appears to address a gap in existing resources. The experiments are reasonably thorough and expose limitations with existing models, while suggesting ways they might be improved.\n\nreasons_to_reject: The problem identified by the paper is interesting and well articulated. There are a few methodological issues that weaken the rating (included under questions for the authors). Additionally, the linkage to human cognition (which appears to be a central aim of the paper) is a bit unclear. It is stated that the Structure Mapping Theory (SMT) has been proposed as a mechanism for analogical reasoning. Other times, it is stated that SMT suggests that new knowledge is gained by system level analogies. Put another way, there are two uses of SMT in the paper, (i) as the account for analogical reasoning broadly construed, and (ii) as the account for system level analogical reasoning (a more narrow form of analogy). If (ii) is the correct interpretation, then it\u2019s possible models can do simple word analogies in a human-like fashion without the ability to do system level analogies. The lack of clarity around these possibilities makes it difficult to interpret what these results say about comparisons between models and humans.  Ultimately, the paper proposes a potentially useful benchmark for evaluating the underlying structural reasoning abilities of large-language models. However, some issues with the methodology and the underlying arguments motivating the conclusion depress my rating.\n\nquestions_for_the_authors: A. Can you elaborate on the reasonableness of using the E-KAR dataset heavily in this work? The main concern is that it is a dataset taken from China\u2019s Civil Service Examinations. The description provided in Appendix A suggests that \u201ccultural knowledge\u201d is part of the exam. Is it reasonable to expect that a model trained largely on English data and evaluated on English data will be able to answer these questions? I have in mind work like Santurkar et al. (2023). Whose Opinions Do Language Models Reflect. https://arxiv.org/abs/2303.17548., which demonstrate that models encode only some groups well.  B. Why was GPT-4 used so extensively in the creation of the dataset (e.g., for the explanation generation)? Wouldn\u2019t higher quality data come from having the annotators do all of this (they are already reading through them after all)?  C. In the Ethics Statement it is stated that \u201cas described in our paper, all annotators are compensated\u201d, where is this stated in the body of the paper? It would be helpful if more details about the annotators (e.g., what are the 5 annotators backgrounds on lines 229-231) and their task were included (e.g., the instructions they were provided). This would facilitate both replication and extension of this work.  D. Technique for embeddings was unclear in section 2. It is stated that the method proposed by Ushio et al. (2021) was used (line 139). However, that paper used cosine similarity in evaluating embeddings, as far as I can tell. Moreover, it is stated that the winning answer candidate was determined with the marginal likelihood biased perplexity measure which uses probability of the token in context, not the embedding. Could this method be clarified in the paper?   MINOR: E. In section 4, a few more models were evaluated. Why weren\u2019t these models included in the other experiments?  F. The role of RSI in the paper is a bit unclear. Is it part of the new proposed benchmark as well? Or is it just used as an additional source of evidence that models fail to capture the underlying structural relations in an analogy (similar to how the ability of models to fully associate the set of concepts in the benchmark evidences they systematically understand the relations)?\n\ntypos_grammar_style_and_presentation_improvements: Figure 3d is used to argue that models are more robust in the CoT prompting scenario. The inclusion of error bars would facilitate more informed comparisons between the variation in a given condition and across conditions.  Consistent y-axis bounds in Figures 3, 5, and 6 would facilitate comparisons.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "hzWaFpE9ty",
        "similarity": 0.7504,
        "coverage": 0.3182,
        "human_length": 1095,
        "human_text": "paper_topic_and_main_contributions: This paper is about analogical reasoning in language models supported by structure abduction. Its contributions are as follows: 1. The authors present a study on past analogical reasoning benchmarks to show that while LLMs can achieve high performance on simpler word analogy benchmarks, this is not always supported by an understanding of the underlying structure for the analogy (unlike humans). This motivates them to create a stronger evaluation for analogical reasoning that considers structure abduction jointly with making analogies.\n2. The authors create a benchmark called Scientific Analogical Reasoning (ScAR) which provides 400 analogies across systems in 13 different scientific and other domains, including 1600 possible mappings between concepts within systems. They additionally collect background knowledge and explanations to support the analogies in the dataset, making it a possibly valuable and high-quality resource for studying analogy-making in LLMs.\n3. The authors evaluate contemporary LLMs, both open-source and closed API-based, on ScAR under several prompting settings. Further, they perform analysis on the impact of various LLM-based aspects in the experiment and the domain. Lastly, they apply earlier word embedding-based methods for analogy as a reference, and perform a preliminary study on open analogical structure abduction, where concepts from the two analogous systems are not provided directly, and rather must be retrieved from a large list.\nWhile the paper does not present new methods for tackling analogical reasoning with LLMs and perhaps lacks some consideration of the impact of LLM pre-training data on this problem, it still constitutes a substantial contribution that could be valuable to the NLP community.\n\nreasons_to_accept: **Strength A:** The paper compiles a (seemingly) high-quality benchmark dataset of interesting analogies across scientific and other domains, including consistent details about the underlying structure of analogies, and background knowledge and explanations for analogies. This resource could be useful to others in the community.\n**Strength B:** The creation of the benchmark is justified by a preliminary study on how word analogy tasks do not actually represent analogical reasoning capabilities. I have not seen another work demonstrate this before, and this can be motivation for the community to look at analogical reasoning more holistically than prior work has.\n**Strength C:** The analysis of benchmark results is thorough and fairly comprehensive, covering a lot of variations and details on the prompt engineering/LLM interface side, as well as including a thoughtful study on how analogies can be made across an open domain of concepts (which I believe is the hardest challenge in analogical reasoning). The authors also explore earlier representation/embedding-based approaches to analogy. It feels like the authors have considered a lot of questions in performing these experiments.\n\nreasons_to_reject: **Weakness A:** This paper doesn\u2019t propose any new modeling techniques or methods to apply LLMs to analogy tasks. This causes the main benchmark results on the paper to not be very inspiring, as they don\u2019t show much significant variation across existing models and prompting techniques that are explored.\n**Weakness B:** The paper does not rigorously explore the impact of pre-training data on LLMs\u2019 analogical reasoning. In natural domains like this that may be largely supported by memorization of training data, some analysis for this is important.\n\nquestions_for_the_authors: **Question A:** To what extent can contamination of LLM pre-training data impact the observed abilities of LLMs in analogical reasoning? For example, I can Google about the aperture of a camera and pupil of a human eye and find plenty of web content talking about how they\u2019re analogous. Does the LLM gain this capability just by memorizing the pre-training data?  I would be curious to see how thoroughly the system understands common systems or concepts across examples from ScAR - if the system can correctly answer the *aperture -> pupil* example, but can\u2019t understand some other example about *aperture* that\u2019s less common, this may suggest a lack of deep understanding of the concept of *aperture.* Figure 4 gives some hints about this possibility across domains, but doing such an analysis could also be a valuable addition to the work! * As a side note, this is one reason why Mitchell (2021) listed later in my review argues that we shouldn\u2019t focus on natural language analogy problems, but I think this issue can also be explored by more thorough evaluations.*\n\nmissing_references: ACL 2023 has at least one paper on LLMs and analogical reasoning, which looks at abstract analogical reasoning: https://aclanthology.org/2023.acl-long.109/  This paper by Melanie Mitchell would be important to cite: https://arxiv.org/abs/2102.10717\n\ntypos_grammar_style_and_presentation_improvements: **Organization/presentation suggestions:** L091: If accepted, you may want to use the extra space to break this paragraph out into a numbered list - may make it easier to read. There are some other paragraphs where you do this too, and I would recommend the same.\nSection 3.4 presents a formal problem definition, which might make more sense to introduce before your dataset. This would help give the reader a foundation on what task you\u2019re looking at, and then the data collection would be easier to understand and map each step to components of the task. I\u2019m not totally sure - would leave it up to you.\nI\u2019m not sure if it\u2019s fair to characterize Webb et al. (2022) as just looking at analogy generation with word analogies or simple sentence analogies as your Section 5 says around L529. This paper looks at abstract language-based analogy tasks and uses them to study how well LLMs can perform analogical reasoning. I might suggest adding a separate paragraph in Section 5 looking at abstract analogical reasoning rather than lumping it into the paragraph about analogies between situations. You may also think about organizing Section 5 based on how these different works focus on different parts of analogy-making, i.e., representation, mapping, and evaluation, and how your work contrasts from it.\n**Formatting/typo corrections:** L036, L046, L055, L060, L090, L110, L135, L344-348\u2026: putting two sets of parentheses next to each other or nesting parentheses is improper format and a bit confusing to read. You can use \\citealp in LaTeX to combine citations with other content in parentheses, or find another way to remove one set of parentheses.\nL124: missing a space after period Did you use ChatGPT or GPT-4 to revise background information about systems? The paper says ChatGPT, but Table 10 says GPT-4.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "yuHFUXJjjQ",
        "similarity": 0.7273,
        "coverage": 0.4286,
        "human_length": 456,
        "human_text": "paper_topic_and_main_contributions: The paper proposes to move beyond word analogies for LLMs and to enable \"analogical structure abduction\".  To this end a new data set is constructed based on the efforts of human annotators paid a local minimum wage, wherever local may be and however much that works out to be.  The results are mixed on a closed list of analogies, quite bad on an open list.  The main contributions are a move towards going beyond word analogies and the creation of a new data set.\n\nreasons_to_accept: The topic of the paper is interesting given that, as the authors point out, analogy is a very basic human skill that is applied across many different domains. It is as such interesting to extend LLMs capabilities to include analogical reasoning.\n\nreasons_to_reject: Within linguistics, language change based on analogy is pervasive as is language acquisition based on rule formation via analogy.  Analogy is also a powerful tool for story-telling, writing and argumentation.  Yet the authors chose none of these domains for their experiments.  Rather, they construct an artificial task of a slightly more complex relationship between words that looking at word similarity in that the relationship involves pairs of two words rather than a single pair of words.  It is not clear how this really reflects the human process of analogy or how it is more generally of interest beyond this rather artificially constructed sets of relations between pairs of words.  This is also presumably why the attempts at having LLMs work with the open list of \"analogies\" do so badly: the task is artificially defined and does not tap into higher principles/generalizations that the LLMs might have learned.\n\nquestions_for_the_authors: It is not clear to me how the classic \"queen is to woman as king is man\" is really an instance of analogy since basically only the similarity of two vectors to one another  is being compared: how is that really analogical reasoning?   Do the authors equate semantic similarity with analogy?  This point needs to be brought out more strongly in the paper.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: The annotation was done by human annotators at \"local\" minimum wage.  It is not said where those annotators are from or under what conditions they are performing the annotation.  This concern was addressed sufficiently by the authors.   They used undergraduates who were trained in this particular task.\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "s1wsuqNeS1",
        "length": 832,
        "human_text": "paper_topic_and_main_contributions: This paper is centrally interested in structured analogical reasoning in large language models. Existing work has focused on word analogies. This work suggests that results from that paradigm overestimate the overlap between neural models and human cognition. They introduce a task of analogical structure abduction that evaluates whether models can systemically associate the components of an analogy. The results suggest that models often miss the underlying relations, and thus, differ from humans. The main contributions are: i) a benchmark called ScAR and ii) a evaluation of LLMs on this task highlighting the need for future work to improve models.\n\nreasons_to_accept: The paper is well motivated and its aim clear. The benchmark will be of use to community and appears to address a gap in existing resources. The experiments are reasonably thorough and expose limitations with existing models, while suggesting ways they might be improved.\n\nreasons_to_reject: The problem identified by the paper is interesting and well articulated. There are a few methodological issues that weaken the rating (included under questions for the authors). Additionally, the linkage to human cognition (which appears to be a central aim of the paper) is a bit unclear. It is stated that the Structure Mapping Theory (SMT) has been proposed as a mechanism for analogical reasoning. Other times, it is stated that SMT suggests that new knowledge is gained by system level analogies. Put another way, there are two uses of SMT in the paper, (i) as the account for analogical reasoning broadly construed, and (ii) as the account for system level analogical reasoning (a more narrow form of analogy). If (ii) is the correct interpretation, then it\u2019s possible models can do simple word analogies in a human-like fashion without the ability to do system level analogies. The lack of clarity around these possibilities makes it difficult to interpret what these results say about comparisons between models and humans.  Ultimately, the paper proposes a potentially useful benchmark for evaluating the underlying structural reasoning abilities of large-language models. However, some issues with the methodology and the underlying arguments motivating the conclusion depress my rating.\n\nquestions_for_the_authors: A. Can you elaborate on the reasonableness of using the E-KAR dataset heavily in this work? The main concern is that it is a dataset taken from China\u2019s Civil Service Examinations. The description provided in Appendix A suggests that \u201ccultural knowledge\u201d is part of the exam. Is it reasonable to expect that a model trained largely on English data and evaluated on English data will be able to answer these questions? I have in mind work like Santurkar et al. (2023). Whose Opinions Do Language Models Reflect. https://arxiv.org/abs/2303.17548., which demonstrate that models encode only some groups well.  B. Why was GPT-4 used so extensively in the creation of the dataset (e.g., for the explanation generation)? Wouldn\u2019t higher quality data come from having the annotators do all of this (they are already reading through them after all)?  C. In the Ethics Statement it is stated that \u201cas described in our paper, all annotators are compensated\u201d, where is this stated in the body of the paper? It would be helpful if more details about the annotators (e.g., what are the 5 annotators backgrounds on lines 229-231) and their task were included (e.g., the instructions they were provided). This would facilitate both replication and extension of this work.  D. Technique for embeddings was unclear in section 2. It is stated that the method proposed by Ushio et al. (2021) was used (line 139). However, that paper used cosine similarity in evaluating embeddings, as far as I can tell. Moreover, it is stated that the winning answer candidate was determined with the marginal likelihood biased perplexity measure which uses probability of the token in context, not the embedding. Could this method be clarified in the paper?   MINOR: E. In section 4, a few more models were evaluated. Why weren\u2019t these models included in the other experiments?  F. The role of RSI in the paper is a bit unclear. Is it part of the new proposed benchmark as well? Or is it just used as an additional source of evidence that models fail to capture the underlying structural relations in an analogy (similar to how the ability of models to fully associate the set of concepts in the benchmark evidences they systematically understand the relations)?\n\ntypos_grammar_style_and_presentation_improvements: Figure 3d is used to argue that models are more robust in the CoT prompting scenario. The inclusion of error bars would facilitate more informed comparisons between the variation in a given condition and across conditions.  Consistent y-axis bounds in Figures 3, 5, and 6 would facilitate comparisons.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "hzWaFpE9ty",
        "length": 1095,
        "human_text": "paper_topic_and_main_contributions: This paper is about analogical reasoning in language models supported by structure abduction. Its contributions are as follows: 1. The authors present a study on past analogical reasoning benchmarks to show that while LLMs can achieve high performance on simpler word analogy benchmarks, this is not always supported by an understanding of the underlying structure for the analogy (unlike humans). This motivates them to create a stronger evaluation for analogical reasoning that considers structure abduction jointly with making analogies.\n2. The authors create a benchmark called Scientific Analogical Reasoning (ScAR) which provides 400 analogies across systems in 13 different scientific and other domains, including 1600 possible mappings between concepts within systems. They additionally collect background knowledge and explanations to support the analogies in the dataset, making it a possibly valuable and high-quality resource for studying analogy-making in LLMs.\n3. The authors evaluate contemporary LLMs, both open-source and closed API-based, on ScAR under several prompting settings. Further, they perform analysis on the impact of various LLM-based aspects in the experiment and the domain. Lastly, they apply earlier word embedding-based methods for analogy as a reference, and perform a preliminary study on open analogical structure abduction, where concepts from the two analogous systems are not provided directly, and rather must be retrieved from a large list.\nWhile the paper does not present new methods for tackling analogical reasoning with LLMs and perhaps lacks some consideration of the impact of LLM pre-training data on this problem, it still constitutes a substantial contribution that could be valuable to the NLP community.\n\nreasons_to_accept: **Strength A:** The paper compiles a (seemingly) high-quality benchmark dataset of interesting analogies across scientific and other domains, including consistent details about the underlying structure of analogies, and background knowledge and explanations for analogies. This resource could be useful to others in the community.\n**Strength B:** The creation of the benchmark is justified by a preliminary study on how word analogy tasks do not actually represent analogical reasoning capabilities. I have not seen another work demonstrate this before, and this can be motivation for the community to look at analogical reasoning more holistically than prior work has.\n**Strength C:** The analysis of benchmark results is thorough and fairly comprehensive, covering a lot of variations and details on the prompt engineering/LLM interface side, as well as including a thoughtful study on how analogies can be made across an open domain of concepts (which I believe is the hardest challenge in analogical reasoning). The authors also explore earlier representation/embedding-based approaches to analogy. It feels like the authors have considered a lot of questions in performing these experiments.\n\nreasons_to_reject: **Weakness A:** This paper doesn\u2019t propose any new modeling techniques or methods to apply LLMs to analogy tasks. This causes the main benchmark results on the paper to not be very inspiring, as they don\u2019t show much significant variation across existing models and prompting techniques that are explored.\n**Weakness B:** The paper does not rigorously explore the impact of pre-training data on LLMs\u2019 analogical reasoning. In natural domains like this that may be largely supported by memorization of training data, some analysis for this is important.\n\nquestions_for_the_authors: **Question A:** To what extent can contamination of LLM pre-training data impact the observed abilities of LLMs in analogical reasoning? For example, I can Google about the aperture of a camera and pupil of a human eye and find plenty of web content talking about how they\u2019re analogous. Does the LLM gain this capability just by memorizing the pre-training data?  I would be curious to see how thoroughly the system understands common systems or concepts across examples from ScAR - if the system can correctly answer the *aperture -> pupil* example, but can\u2019t understand some other example about *aperture* that\u2019s less common, this may suggest a lack of deep understanding of the concept of *aperture.* Figure 4 gives some hints about this possibility across domains, but doing such an analysis could also be a valuable addition to the work! * As a side note, this is one reason why Mitchell (2021) listed later in my review argues that we shouldn\u2019t focus on natural language analogy problems, but I think this issue can also be explored by more thorough evaluations.*\n\nmissing_references: ACL 2023 has at least one paper on LLMs and analogical reasoning, which looks at abstract analogical reasoning: https://aclanthology.org/2023.acl-long.109/  This paper by Melanie Mitchell would be important to cite: https://arxiv.org/abs/2102.10717\n\ntypos_grammar_style_and_presentation_improvements: **Organization/presentation suggestions:** L091: If accepted, you may want to use the extra space to break this paragraph out into a numbered list - may make it easier to read. There are some other paragraphs where you do this too, and I would recommend the same.\nSection 3.4 presents a formal problem definition, which might make more sense to introduce before your dataset. This would help give the reader a foundation on what task you\u2019re looking at, and then the data collection would be easier to understand and map each step to components of the task. I\u2019m not totally sure - would leave it up to you.\nI\u2019m not sure if it\u2019s fair to characterize Webb et al. (2022) as just looking at analogy generation with word analogies or simple sentence analogies as your Section 5 says around L529. This paper looks at abstract language-based analogy tasks and uses them to study how well LLMs can perform analogical reasoning. I might suggest adding a separate paragraph in Section 5 looking at abstract analogical reasoning rather than lumping it into the paragraph about analogies between situations. You may also think about organizing Section 5 based on how these different works focus on different parts of analogy-making, i.e., representation, mapping, and evaluation, and how your work contrasts from it.\n**Formatting/typo corrections:** L036, L046, L055, L060, L090, L110, L135, L344-348\u2026: putting two sets of parentheses next to each other or nesting parentheses is improper format and a bit confusing to read. You can use \\citealp in LaTeX to combine citations with other content in parentheses, or find another way to remove one set of parentheses.\nL124: missing a space after period Did you use ChatGPT or GPT-4 to revise background information about systems? The paper says ChatGPT, but Table 10 says GPT-4.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "yuHFUXJjjQ",
        "length": 456,
        "human_text": "paper_topic_and_main_contributions: The paper proposes to move beyond word analogies for LLMs and to enable \"analogical structure abduction\".  To this end a new data set is constructed based on the efforts of human annotators paid a local minimum wage, wherever local may be and however much that works out to be.  The results are mixed on a closed list of analogies, quite bad on an open list.  The main contributions are a move towards going beyond word analogies and the creation of a new data set.\n\nreasons_to_accept: The topic of the paper is interesting given that, as the authors point out, analogy is a very basic human skill that is applied across many different domains. It is as such interesting to extend LLMs capabilities to include analogical reasoning.\n\nreasons_to_reject: Within linguistics, language change based on analogy is pervasive as is language acquisition based on rule formation via analogy.  Analogy is also a powerful tool for story-telling, writing and argumentation.  Yet the authors chose none of these domains for their experiments.  Rather, they construct an artificial task of a slightly more complex relationship between words that looking at word similarity in that the relationship involves pairs of two words rather than a single pair of words.  It is not clear how this really reflects the human process of analogy or how it is more generally of interest beyond this rather artificially constructed sets of relations between pairs of words.  This is also presumably why the attempts at having LLMs work with the open list of \"analogies\" do so badly: the task is artificially defined and does not tap into higher principles/generalizations that the LLMs might have learned.\n\nquestions_for_the_authors: It is not clear to me how the classic \"queen is to woman as king is man\" is really an instance of analogy since basically only the similarity of two vectors to one another  is being compared: how is that really analogical reasoning?   Do the authors equate semantic similarity with analogy?  This point needs to be brought out more strongly in the paper.\n\nethical_concerns: No\n\njustification_for_ethical_concerns: The annotation was done by human annotators at \"local\" minimum wage.  It is not said where those annotators are from or under what conditions they are performing the annotation.  This concern was addressed sufficiently by the authors.   They used undergraduates who were trained in this particular task.\n\nScores:\n\n  soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems\n\n  excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "200_original",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_200_gemini-2.0-flash-lite_default/review_original.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.716225,
      "max_similarity": 0.7343,
      "avg_coverage": 0.437925,
      "max_coverage": 0.5
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 331,
      "avg_human_length": 353.75
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 5,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "tYYYmxvc7F",
        "similarity": 0.7343,
        "coverage": 0.5,
        "human_length": 476,
        "human_text": "paper_topic_and_main_contributions: This paper marks the introduction of ALLIES, an innovative approach that leverages a beam search strategy for response generation in open-domain question answering. With a unique extension strategy and dynamic pruning technique, ALLIES capably tackles the critical limitations of narrow information coverage and low fault tolerance associated with large language models. Demonstrated on popular benchmarks, ALLIES excels, significantly outperforming existing methods.\nOverall, the contribution of the paper is substantial, leading me to lean towards accepting it.\n\nreasons_to_accept: - The paper effectively motivates its work by addressing the prevalent challenges of narrow information coverage and low fault tolerance encountered in existing methods utilizing LLMs for query responses.\n- The proposed ALLIES method is refreshingly innovative, employing an extension strategy for expansive information coverage and utilizing a dynamic pruning technique to maintain top-tier responses, thereby fostering the model's capacity to accommodate inaccuracies and alternative answers. This approach enhances response precision and reliability.\n- The paper successfully demonstrates the superiority of ALLIES over established methods through comprehensive experiments on zero-shot open-domain QA benchmarks, including NQ, TriviaQA, and WebQ, exhibiting marked improvements (+10 on NQ and WebQ).\n\nreasons_to_reject: - There is a noticeable increase in computational costs due to the proposed method's iterative nature, encompassing the retrieval and generation process.\n- While the paper illustrates its limitations in the introduction, the authors fail to provide a comprehensive comparison between ALLIES and existing methods, leaving the contribution of this work less well-defined.\n- Despite the use of a single prompt in the paper, the sensitivity of the model to variations in the prompt was not evaluated, leaving an important aspect of the method unexplored.\n\nquestions_for_the_authors: - Concerning the issue of narrow information coverage, how do the authors view the relevance of their work in the context of query expansion [1]?\n- In relation to the low fault tolerance, how do the authors view the relevance of their work with self-consistency [2]?\n- Have the authors experimented with varying prompts at each step to assess prompt robustness?\n- In Figure 3, the paper presents the sensitivity of the beam size and depth. Could the authors provide their insights as to why an increase in depth leads to diminished performance?\n[1] Generation-augmented retrieval for open-domain question answering. ACL 2021 [2] Self-consistency improves chain of thought reasoning in language models. ICLR 2023\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "7rMg6rbslD",
        "similarity": 0.724,
        "coverage": 0.36,
        "human_length": 280,
        "human_text": "paper_topic_and_main_contributions: The paper introduces Allies, a novel method designed to address the limitations of using large language models (LLMs) for complex tasks. Allies leverages LLMs to generate related queries iteratively, enabling iterative reasoning and expanding the scope of the original query to capture hidden knowledge. The proposed method is evaluated in zero-shot open-domain question answering and outperforms other baselines on benchmark datasets.\n\nreasons_to_accept: 1. The paper identifies issues with stacking LLM queries and proposes a new framework to optimize iterative calling of LLM APIs. \n2. Allies presents a robust method for achieving prompt augmentation, enhancing the model's performance. \n3. Experimental results demonstrate the superior performance of Allies on open book QA benchmarks. \n4. The paper introduces a valuable approach for addressing the limitations of using LLMs for complex tasks.\n\nreasons_to_reject: 1. The limitations of beam depths are not thoroughly explored. Further investigation is needed to understand how longer reasoning chains affect performance. \n2. The potential impact of larger beam sizes could be explored further.\n\nquestions_for_the_authors: 1. How do beam depths larger than 3 impact the results? It would be beneficial to provide specific examples illustrating the effects of longer reasoning chains. \n2. With a larger beam size, can we expect the generation of more noisy or greater numbers of augmented queries?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "RP8ax6rQpo",
        "similarity": 0.7121,
        "coverage": 0.4211,
        "human_length": 358,
        "human_text": "paper_topic_and_main_contributions: For open domain question answering, this paper employed an extension strategy to broaden the scope of the original question by generating multiple relevant questions, which allows the LLM to develop a more comprehensive understanding of the complex question by examining its individual components. Throughout the iterative process, a dynamic pruning technique was utilized to narrow down the options, retaining only the top answers at each step.\n\nreasons_to_accept: The testing results appear favorable.\n\nreasons_to_reject: The major contributions do not exhibit significant impact.  One instance is the utilization of beam search, which is not a new technique. Therefore, it is crucial to identify the main contribution of using beam search in this study. Additionally, it is important to elucidate the distinctions between the interactive and iterative process and the chain of thought approach.\n\nquestions_for_the_authors: Question A: What are the distinctions between the interactive and iterative process depicted in Figure 1 and the algorithm based on a chain of thought?\nQuestion B: While beam search is not a novel technique, what is the main contribution of its utilization in this paper?\nQuestion C: The authors state that InstructGPT is employed, indicating LLM alignment with human feedbacks. How is beam search implemented on the top InstructGPT?\nQuestion D: Does ALLIES provide an answer based solely on the top-1 retrieved evidence?\nQuestion E: The authors mention scoring the response using the LLM. How is the score qualified or evaluated?\n\ntypos_grammar_style_and_presentation_improvements: It is hard to follow the \"Algorithm 1 The process of generating the response to a given query using ALLIES\" on page 4.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "jrL3lg1lMn",
        "similarity": 0.6945,
        "coverage": 0.4706,
        "human_length": 301,
        "human_text": "paper_topic_and_main_contributions: The paper introduces an iterative approach to enhance the retrieval-augmented generation (RAG) pipeline through query augmentation. \nThe method involves leveraging Language Models (LLMs) to generate additional related queries in an iterative manner, and then scoring the output using both the query and the retrieved evidence. \nThe effectiveness of the proposed approach is demonstrated through a comparison with the retrieve-then-read and generate-then-read pipelines.\n\nreasons_to_accept: Iterative query augmentation with scoring function can improve the RAG pipeline significantly.\n\nreasons_to_reject: While the paper demonstrates its effectiveness, there are some minor concerns that I would like to address: 1. Lack of experiment details:  - Providing explicit details regarding the corpus or external sources employed for retrieval purposes would be helpful - Table 4 could benefit from further explanation to improve understanding. \n    - It seems that the API time for GENREAD should be listed as 2 instead of 1. \n    - It is unclear which specific example was utilized to generate the results presented in Table 4. Does the \"retrieval&summary\" category for ALLIES indicate that the process was repeated five times?\n2. Unclear why WebQ results show performance decline with additional context which is contrast behavior with previous findings.\n\nquestions_for_the_authors: 1. What is the corpus for the retrieval?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "tYYYmxvc7F",
        "length": 476,
        "human_text": "paper_topic_and_main_contributions: This paper marks the introduction of ALLIES, an innovative approach that leverages a beam search strategy for response generation in open-domain question answering. With a unique extension strategy and dynamic pruning technique, ALLIES capably tackles the critical limitations of narrow information coverage and low fault tolerance associated with large language models. Demonstrated on popular benchmarks, ALLIES excels, significantly outperforming existing methods.\nOverall, the contribution of the paper is substantial, leading me to lean towards accepting it.\n\nreasons_to_accept: - The paper effectively motivates its work by addressing the prevalent challenges of narrow information coverage and low fault tolerance encountered in existing methods utilizing LLMs for query responses.\n- The proposed ALLIES method is refreshingly innovative, employing an extension strategy for expansive information coverage and utilizing a dynamic pruning technique to maintain top-tier responses, thereby fostering the model's capacity to accommodate inaccuracies and alternative answers. This approach enhances response precision and reliability.\n- The paper successfully demonstrates the superiority of ALLIES over established methods through comprehensive experiments on zero-shot open-domain QA benchmarks, including NQ, TriviaQA, and WebQ, exhibiting marked improvements (+10 on NQ and WebQ).\n\nreasons_to_reject: - There is a noticeable increase in computational costs due to the proposed method's iterative nature, encompassing the retrieval and generation process.\n- While the paper illustrates its limitations in the introduction, the authors fail to provide a comprehensive comparison between ALLIES and existing methods, leaving the contribution of this work less well-defined.\n- Despite the use of a single prompt in the paper, the sensitivity of the model to variations in the prompt was not evaluated, leaving an important aspect of the method unexplored.\n\nquestions_for_the_authors: - Concerning the issue of narrow information coverage, how do the authors view the relevance of their work in the context of query expansion [1]?\n- In relation to the low fault tolerance, how do the authors view the relevance of their work with self-consistency [2]?\n- Have the authors experimented with varying prompts at each step to assess prompt robustness?\n- In Figure 3, the paper presents the sensitivity of the beam size and depth. Could the authors provide their insights as to why an increase in depth leads to diminished performance?\n[1] Generation-augmented retrieval for open-domain question answering. ACL 2021 [2] Self-consistency improves chain of thought reasoning in language models. ICLR 2023\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "7rMg6rbslD",
        "length": 280,
        "human_text": "paper_topic_and_main_contributions: The paper introduces Allies, a novel method designed to address the limitations of using large language models (LLMs) for complex tasks. Allies leverages LLMs to generate related queries iteratively, enabling iterative reasoning and expanding the scope of the original query to capture hidden knowledge. The proposed method is evaluated in zero-shot open-domain question answering and outperforms other baselines on benchmark datasets.\n\nreasons_to_accept: 1. The paper identifies issues with stacking LLM queries and proposes a new framework to optimize iterative calling of LLM APIs. \n2. Allies presents a robust method for achieving prompt augmentation, enhancing the model's performance. \n3. Experimental results demonstrate the superior performance of Allies on open book QA benchmarks. \n4. The paper introduces a valuable approach for addressing the limitations of using LLMs for complex tasks.\n\nreasons_to_reject: 1. The limitations of beam depths are not thoroughly explored. Further investigation is needed to understand how longer reasoning chains affect performance. \n2. The potential impact of larger beam sizes could be explored further.\n\nquestions_for_the_authors: 1. How do beam depths larger than 3 impact the results? It would be beneficial to provide specific examples illustrating the effects of longer reasoning chains. \n2. With a larger beam size, can we expect the generation of more noisy or greater numbers of augmented queries?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "RP8ax6rQpo",
        "length": 358,
        "human_text": "paper_topic_and_main_contributions: For open domain question answering, this paper employed an extension strategy to broaden the scope of the original question by generating multiple relevant questions, which allows the LLM to develop a more comprehensive understanding of the complex question by examining its individual components. Throughout the iterative process, a dynamic pruning technique was utilized to narrow down the options, retaining only the top answers at each step.\n\nreasons_to_accept: The testing results appear favorable.\n\nreasons_to_reject: The major contributions do not exhibit significant impact.  One instance is the utilization of beam search, which is not a new technique. Therefore, it is crucial to identify the main contribution of using beam search in this study. Additionally, it is important to elucidate the distinctions between the interactive and iterative process and the chain of thought approach.\n\nquestions_for_the_authors: Question A: What are the distinctions between the interactive and iterative process depicted in Figure 1 and the algorithm based on a chain of thought?\nQuestion B: While beam search is not a novel technique, what is the main contribution of its utilization in this paper?\nQuestion C: The authors state that InstructGPT is employed, indicating LLM alignment with human feedbacks. How is beam search implemented on the top InstructGPT?\nQuestion D: Does ALLIES provide an answer based solely on the top-1 retrieved evidence?\nQuestion E: The authors mention scoring the response using the LLM. How is the score qualified or evaluated?\n\ntypos_grammar_style_and_presentation_improvements: It is hard to follow the \"Algorithm 1 The process of generating the response to a given query using ALLIES\" on page 4.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "jrL3lg1lMn",
        "length": 301,
        "human_text": "paper_topic_and_main_contributions: The paper introduces an iterative approach to enhance the retrieval-augmented generation (RAG) pipeline through query augmentation. \nThe method involves leveraging Language Models (LLMs) to generate additional related queries in an iterative manner, and then scoring the output using both the query and the retrieved evidence. \nThe effectiveness of the proposed approach is demonstrated through a comparison with the retrieve-then-read and generate-then-read pipelines.\n\nreasons_to_accept: Iterative query augmentation with scoring function can improve the RAG pipeline significantly.\n\nreasons_to_reject: While the paper demonstrates its effectiveness, there are some minor concerns that I would like to address: 1. Lack of experiment details:  - Providing explicit details regarding the corpus or external sources employed for retrieval purposes would be helpful - Table 4 could benefit from further explanation to improve understanding. \n    - It seems that the API time for GENREAD should be listed as 2 instead of 1. \n    - It is unclear which specific example was utilized to generate the results presented in Table 4. Does the \"retrieval&summary\" category for ALLIES indicate that the process was repeated five times?\n2. Unclear why WebQ results show performance decline with additional context which is contrast behavior with previous findings.\n\nquestions_for_the_authors: 1. What is the corpus for the retrieval?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  },
  {
    "paper_id": "200_updated",
    "generated_review_path": "evaluation/results/runs/gemini-2.0-flash-lite/paper_200_gemini-2.0-flash-lite_default/review_updated.json",
    "human_reviews_count": 4,
    "summary_metrics": {
      "avg_similarity": 0.71395,
      "max_similarity": 0.7368,
      "avg_coverage": 0.474,
      "max_coverage": 0.5455
    },
    "content_metrics": {
      "genericity_score": 0.0
    },
    "length_metrics": {
      "generated_length": 349,
      "avg_human_length": 353.75
    },
    "structural_metrics": {
      "has_summary": true,
      "has_strengths": true,
      "has_weaknesses": true,
      "has_suggestions": true
    },
    "content_count_metrics": {
      "strengths_count": 4,
      "weaknesses_count": 5,
      "suggestions_count": 5
    },
    "similarity_details": [
      {
        "human_review_index": 0,
        "reviewer_id": "tYYYmxvc7F",
        "similarity": 0.7368,
        "coverage": 0.5455,
        "human_length": 476,
        "human_text": "paper_topic_and_main_contributions: This paper marks the introduction of ALLIES, an innovative approach that leverages a beam search strategy for response generation in open-domain question answering. With a unique extension strategy and dynamic pruning technique, ALLIES capably tackles the critical limitations of narrow information coverage and low fault tolerance associated with large language models. Demonstrated on popular benchmarks, ALLIES excels, significantly outperforming existing methods.\nOverall, the contribution of the paper is substantial, leading me to lean towards accepting it.\n\nreasons_to_accept: - The paper effectively motivates its work by addressing the prevalent challenges of narrow information coverage and low fault tolerance encountered in existing methods utilizing LLMs for query responses.\n- The proposed ALLIES method is refreshingly innovative, employing an extension strategy for expansive information coverage and utilizing a dynamic pruning technique to maintain top-tier responses, thereby fostering the model's capacity to accommodate inaccuracies and alternative answers. This approach enhances response precision and reliability.\n- The paper successfully demonstrates the superiority of ALLIES over established methods through comprehensive experiments on zero-shot open-domain QA benchmarks, including NQ, TriviaQA, and WebQ, exhibiting marked improvements (+10 on NQ and WebQ).\n\nreasons_to_reject: - There is a noticeable increase in computational costs due to the proposed method's iterative nature, encompassing the retrieval and generation process.\n- While the paper illustrates its limitations in the introduction, the authors fail to provide a comprehensive comparison between ALLIES and existing methods, leaving the contribution of this work less well-defined.\n- Despite the use of a single prompt in the paper, the sensitivity of the model to variations in the prompt was not evaluated, leaving an important aspect of the method unexplored.\n\nquestions_for_the_authors: - Concerning the issue of narrow information coverage, how do the authors view the relevance of their work in the context of query expansion [1]?\n- In relation to the low fault tolerance, how do the authors view the relevance of their work with self-consistency [2]?\n- Have the authors experimented with varying prompts at each step to assess prompt robustness?\n- In Figure 3, the paper presents the sensitivity of the beam size and depth. Could the authors provide their insights as to why an increase in depth leads to diminished performance?\n[1] Generation-augmented retrieval for open-domain question answering. ACL 2021 [2] Self-consistency improves chain of thought reasoning in language models. ICLR 2023\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 1,
        "reviewer_id": "7rMg6rbslD",
        "similarity": 0.7141,
        "coverage": 0.4,
        "human_length": 280,
        "human_text": "paper_topic_and_main_contributions: The paper introduces Allies, a novel method designed to address the limitations of using large language models (LLMs) for complex tasks. Allies leverages LLMs to generate related queries iteratively, enabling iterative reasoning and expanding the scope of the original query to capture hidden knowledge. The proposed method is evaluated in zero-shot open-domain question answering and outperforms other baselines on benchmark datasets.\n\nreasons_to_accept: 1. The paper identifies issues with stacking LLM queries and proposes a new framework to optimize iterative calling of LLM APIs. \n2. Allies presents a robust method for achieving prompt augmentation, enhancing the model's performance. \n3. Experimental results demonstrate the superior performance of Allies on open book QA benchmarks. \n4. The paper introduces a valuable approach for addressing the limitations of using LLMs for complex tasks.\n\nreasons_to_reject: 1. The limitations of beam depths are not thoroughly explored. Further investigation is needed to understand how longer reasoning chains affect performance. \n2. The potential impact of larger beam sizes could be explored further.\n\nquestions_for_the_authors: 1. How do beam depths larger than 3 impact the results? It would be beneficial to provide specific examples illustrating the effects of longer reasoning chains. \n2. With a larger beam size, can we expect the generation of more noisy or greater numbers of augmented queries?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
      },
      {
        "human_review_index": 2,
        "reviewer_id": "RP8ax6rQpo",
        "similarity": 0.7124,
        "coverage": 0.4211,
        "human_length": 358,
        "human_text": "paper_topic_and_main_contributions: For open domain question answering, this paper employed an extension strategy to broaden the scope of the original question by generating multiple relevant questions, which allows the LLM to develop a more comprehensive understanding of the complex question by examining its individual components. Throughout the iterative process, a dynamic pruning technique was utilized to narrow down the options, retaining only the top answers at each step.\n\nreasons_to_accept: The testing results appear favorable.\n\nreasons_to_reject: The major contributions do not exhibit significant impact.  One instance is the utilization of beam search, which is not a new technique. Therefore, it is crucial to identify the main contribution of using beam search in this study. Additionally, it is important to elucidate the distinctions between the interactive and iterative process and the chain of thought approach.\n\nquestions_for_the_authors: Question A: What are the distinctions between the interactive and iterative process depicted in Figure 1 and the algorithm based on a chain of thought?\nQuestion B: While beam search is not a novel technique, what is the main contribution of its utilization in this paper?\nQuestion C: The authors state that InstructGPT is employed, indicating LLM alignment with human feedbacks. How is beam search implemented on the top InstructGPT?\nQuestion D: Does ALLIES provide an answer based solely on the top-1 retrieved evidence?\nQuestion E: The authors mention scoring the response using the LLM. How is the score qualified or evaluated?\n\ntypos_grammar_style_and_presentation_improvements: It is hard to follow the \"Algorithm 1 The process of generating the response to a given query using ALLIES\" on page 4.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      },
      {
        "human_review_index": 3,
        "reviewer_id": "jrL3lg1lMn",
        "similarity": 0.6925,
        "coverage": 0.5294,
        "human_length": 301,
        "human_text": "paper_topic_and_main_contributions: The paper introduces an iterative approach to enhance the retrieval-augmented generation (RAG) pipeline through query augmentation. \nThe method involves leveraging Language Models (LLMs) to generate additional related queries in an iterative manner, and then scoring the output using both the query and the retrieved evidence. \nThe effectiveness of the proposed approach is demonstrated through a comparison with the retrieve-then-read and generate-then-read pipelines.\n\nreasons_to_accept: Iterative query augmentation with scoring function can improve the RAG pipeline significantly.\n\nreasons_to_reject: While the paper demonstrates its effectiveness, there are some minor concerns that I would like to address: 1. Lack of experiment details:  - Providing explicit details regarding the corpus or external sources employed for retrieval purposes would be helpful - Table 4 could benefit from further explanation to improve understanding. \n    - It seems that the API time for GENREAD should be listed as 2 instead of 1. \n    - It is unclear which specific example was utilized to generate the results presented in Table 4. Does the \"retrieval&summary\" category for ALLIES indicate that the process was repeated five times?\n2. Unclear why WebQ results show performance decline with additional context which is contrast behavior with previous findings.\n\nquestions_for_the_authors: 1. What is the corpus for the retrieval?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
      }
    ],
    "human_reviews_summary": [
      {
        "reviewer_id": "tYYYmxvc7F",
        "length": 476,
        "human_text": "paper_topic_and_main_contributions: This paper marks the introduction of ALLIES, an innovative approach that leverages a beam search strategy for response generation in open-domain question answering. With a unique extension strategy and dynamic pruning technique, ALLIES capably tackles the critical limitations of narrow information coverage and low fault tolerance associated with large language models. Demonstrated on popular benchmarks, ALLIES excels, significantly outperforming existing methods.\nOverall, the contribution of the paper is substantial, leading me to lean towards accepting it.\n\nreasons_to_accept: - The paper effectively motivates its work by addressing the prevalent challenges of narrow information coverage and low fault tolerance encountered in existing methods utilizing LLMs for query responses.\n- The proposed ALLIES method is refreshingly innovative, employing an extension strategy for expansive information coverage and utilizing a dynamic pruning technique to maintain top-tier responses, thereby fostering the model's capacity to accommodate inaccuracies and alternative answers. This approach enhances response precision and reliability.\n- The paper successfully demonstrates the superiority of ALLIES over established methods through comprehensive experiments on zero-shot open-domain QA benchmarks, including NQ, TriviaQA, and WebQ, exhibiting marked improvements (+10 on NQ and WebQ).\n\nreasons_to_reject: - There is a noticeable increase in computational costs due to the proposed method's iterative nature, encompassing the retrieval and generation process.\n- While the paper illustrates its limitations in the introduction, the authors fail to provide a comprehensive comparison between ALLIES and existing methods, leaving the contribution of this work less well-defined.\n- Despite the use of a single prompt in the paper, the sensitivity of the model to variations in the prompt was not evaluated, leaving an important aspect of the method unexplored.\n\nquestions_for_the_authors: - Concerning the issue of narrow information coverage, how do the authors view the relevance of their work in the context of query expansion [1]?\n- In relation to the low fault tolerance, how do the authors view the relevance of their work with self-consistency [2]?\n- Have the authors experimented with varying prompts at each step to assess prompt robustness?\n- In Figure 3, the paper presents the sensitivity of the beam size and depth. Could the authors provide their insights as to why an increase in depth leads to diminished performance?\n[1] Generation-augmented retrieval for open-domain question answering. ACL 2021 [2] Self-consistency improves chain of thought reasoning in language models. ICLR 2023\n\nethical_concerns: No\n\nScores:\n\n  soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "7rMg6rbslD",
        "length": 280,
        "human_text": "paper_topic_and_main_contributions: The paper introduces Allies, a novel method designed to address the limitations of using large language models (LLMs) for complex tasks. Allies leverages LLMs to generate related queries iteratively, enabling iterative reasoning and expanding the scope of the original query to capture hidden knowledge. The proposed method is evaluated in zero-shot open-domain question answering and outperforms other baselines on benchmark datasets.\n\nreasons_to_accept: 1. The paper identifies issues with stacking LLM queries and proposes a new framework to optimize iterative calling of LLM APIs. \n2. Allies presents a robust method for achieving prompt augmentation, enhancing the model's performance. \n3. Experimental results demonstrate the superior performance of Allies on open book QA benchmarks. \n4. The paper introduces a valuable approach for addressing the limitations of using LLMs for complex tasks.\n\nreasons_to_reject: 1. The limitations of beam depths are not thoroughly explored. Further investigation is needed to understand how longer reasoning chains affect performance. \n2. The potential impact of larger beam sizes could be explored further.\n\nquestions_for_the_authors: 1. How do beam depths larger than 3 impact the results? It would be beneficial to provide specific examples illustrating the effects of longer reasoning chains. \n2. With a larger beam size, can we expect the generation of more noisy or greater numbers of augmented queries?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.\n\n  reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.",
        "has_content": true
      },
      {
        "reviewer_id": "RP8ax6rQpo",
        "length": 358,
        "human_text": "paper_topic_and_main_contributions: For open domain question answering, this paper employed an extension strategy to broaden the scope of the original question by generating multiple relevant questions, which allows the LLM to develop a more comprehensive understanding of the complex question by examining its individual components. Throughout the iterative process, a dynamic pruning technique was utilized to narrow down the options, retaining only the top answers at each step.\n\nreasons_to_accept: The testing results appear favorable.\n\nreasons_to_reject: The major contributions do not exhibit significant impact.  One instance is the utilization of beam search, which is not a new technique. Therefore, it is crucial to identify the main contribution of using beam search in this study. Additionally, it is important to elucidate the distinctions between the interactive and iterative process and the chain of thought approach.\n\nquestions_for_the_authors: Question A: What are the distinctions between the interactive and iterative process depicted in Figure 1 and the algorithm based on a chain of thought?\nQuestion B: While beam search is not a novel technique, what is the main contribution of its utilization in this paper?\nQuestion C: The authors state that InstructGPT is employed, indicating LLM alignment with human feedbacks. How is beam search implemented on the top InstructGPT?\nQuestion D: Does ALLIES provide an answer based solely on the top-1 retrieved evidence?\nQuestion E: The authors mention scoring the response using the LLM. How is the score qualified or evaluated?\n\ntypos_grammar_style_and_presentation_improvements: It is hard to follow the \"Algorithm 1 The process of generating the response to a given query using ALLIES\" on page 4.\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      },
      {
        "reviewer_id": "jrL3lg1lMn",
        "length": 301,
        "human_text": "paper_topic_and_main_contributions: The paper introduces an iterative approach to enhance the retrieval-augmented generation (RAG) pipeline through query augmentation. \nThe method involves leveraging Language Models (LLMs) to generate additional related queries in an iterative manner, and then scoring the output using both the query and the retrieved evidence. \nThe effectiveness of the proposed approach is demonstrated through a comparison with the retrieve-then-read and generate-then-read pipelines.\n\nreasons_to_accept: Iterative query augmentation with scoring function can improve the RAG pipeline significantly.\n\nreasons_to_reject: While the paper demonstrates its effectiveness, there are some minor concerns that I would like to address: 1. Lack of experiment details:  - Providing explicit details regarding the corpus or external sources employed for retrieval purposes would be helpful - Table 4 could benefit from further explanation to improve understanding. \n    - It seems that the API time for GENREAD should be listed as 2 instead of 1. \n    - It is unclear which specific example was utilized to generate the results presented in Table 4. Does the \"retrieval&summary\" category for ALLIES indicate that the process was repeated five times?\n2. Unclear why WebQ results show performance decline with additional context which is contrast behavior with previous findings.\n\nquestions_for_the_authors: 1. What is the corpus for the retrieval?\n\nethical_concerns: No\n\nScores:\n\n  soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n\n  excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n\n  reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.",
        "has_content": true
      }
    ]
  }
]