[
  {
    "rebuttal": "We thank the reviewer for their insightful and detailed feedback. We address each point below:\n\n**Weaknesses:**\n\n*   **Lack of clear baselines:** We respectfully disagree. Section 4.3, \"Implementations,\" explicitly details the baselines used: FT, PT, RAG, SimCTG, and GPT-3.5. We also state the backbone models (GPT-2 and T5). The architectures of these baselines are standard in the literature, and we cite the relevant papers. We believe the reviewer may have overlooked this section.\n\n*   **Lack of detail on implementation and novelty:** We partially addressed this. The Introduction highlights instruction tuning, knowledge expansion (hybrid prompting with discrete and neural components), and rationale generation as key features. Section 3 provides detailed explanations of these techniques. We will enhance the Introduction to more clearly differentiate our approach from existing methods. We will add a concise definition of 'instruction tuning' in the introduction and expand on the 'explicit supervision for generating grounding rationales' in the method section, detailing the specific approach.\n\n*   **Overclaiming impact on trial failure:** We acknowledge this. While our results demonstrate improved accuracy and human preference, we did not provide direct evidence of impact on real-world trial outcomes. We will revise Section 5 to temper our claims and focus on the potential benefits. We will also add a Broader Impact section with mitigation strategies.\n\n*   **Limitations and ethical implications:** We partially addressed this. Section 5 discusses data quality limitations. We will expand this section to include a more detailed analysis of limitations related to unexpected side effects and ethical considerations, including potential biases, misuse, and the importance of human oversight. We will also provide details on the data sources used for training the LLM, including information on data collection and storage practices. We will clarify the intended use of the model's outputs, including whether they are intended for commercial use, and specify the applicable licensing and usage terms.\n\n*   **Figure and Table clarity:** We acknowledge this and will address it. We will ensure all axes are clearly labeled with units where applicable. We will improve the color contrast and legend clarity in Figure 4. We will include standard deviations or confidence intervals in Tables 2, 3, and 4 to show the variability of the results. Table 1 will be updated to include more details about the data.\n\n*   **Lack of discussion of misuse:** We acknowledge this and will add a section on the ethical considerations of using LLMs in clinical trial design, including the potential for misuse and the importance of human oversight.\n\n**Suggestions:**\n\n*   **GPT-3.5 comparison:** We will provide more specifics on the evaluation setup and the rationale behind the human evaluation. (Sec 4)\n\n*   **Instruction prompting and knowledge expansion details:** We will expand on the 'instruction prompting' and 'knowledge expansion' techniques, and how they differ from existing methods. (Introduction)\n\n*   **Evidence of real-world impact:** We will revise Section 5 to temper our claims and focus on the potential benefits.\n\n*   **Detailed analysis of limitations:** We will conduct a more detailed analysis of the limitations related to data quality and unexpected side effects, potentially including sensitivity analyses or error analysis. (Sec 5)\n\n*   **Error bars and confidence intervals:** We will add error bars to the plots in Figure 2 and other relevant figures to show confidence intervals. (Fig 2, Fig 4, Tables 2, 3, 4)\n\n*   **Ethical considerations:** We will add a section on the ethical considerations of using LLMs in clinical trial design, including the potential for misuse and the importance of human oversight. (Sec 5)\n\n*   **Notation table:** We will add a notation table or a section explaining the symbols and variables used in the method. (Methods ยง3)"
  }
]