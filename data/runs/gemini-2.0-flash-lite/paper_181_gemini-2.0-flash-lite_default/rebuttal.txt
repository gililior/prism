[
  {
    "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback. We appreciate the time taken to thoroughly review our paper and provide constructive criticism. We address each point below:\n\n**Weakness 1: Lack of Dataset Details and Overstated Generality**\nWe acknowledge the reviewer's concern regarding the level of detail on datasets and the generality of our claims. While we provide dataset statistics in Table 1 and detailed descriptions in Section 2.1, we agree that further clarification is needed. We will expand the dataset descriptions in Section 2.1 to include more specific details, such as the source of the data, the specific preprocessing steps, and the size of the training, validation, and test sets. We will also revise the conclusion in Section 7 to be more precise about the scope of our findings, explicitly stating the tasks and datasets on which our method was evaluated.\n\n**Weakness 2: Delta Compared to Existing Methods**\nWe believe the reviewer's concern regarding the delta compared to existing methods is partially addressed. We discuss related work in Section 6, including existing self-influence calculation methods and AutoCL approaches. However, we agree that a more direct comparison with other data cleaning methods, particularly in terms of downstream task performance, is needed. We will add a new section to Section 4 and Section 5 that directly compares our method with a state-of-the-art data cleaning method on the same datasets and tasks. This will provide a clearer understanding of the improvements offered by our approach.\n\n**Weakness 3: Lack of Experimental Environment Details**\nThe reviewer is correct that Section 2.3 lacks details on the experimental environment. We will add a sentence to Section 2.3 specifying the random seeds used for all experiments and will include details on the hardware and software environment (e.g., the specific versions of the libraries used) to enhance reproducibility. We will also provide a link to our code repository, which will include the scripts and the environment file (e.g., conda environment.yml or Dockerfile) as suggested.\n\n**Weakness 4: Lack of Statistical Information**\nWe acknowledge the lack of statistical information, such as confidence intervals and p-values, in our current presentation. We will address this by including standard deviations or confidence intervals for all reported metrics and adding p-values to indicate the statistical significance of the differences between different methods or settings, especially in Table 3. We will also clarify the number of trials or repetitions for each experiment.\n\n**Weakness 5: Clarification of 'Outliers' and Influence Functions**\nWe agree with the reviewer that the term 'outliers' could be clarified further. We will expand the definitions in the Abstract and Section 1 to provide specific examples of each type of outlier (label noise, out-of-distribution, ambiguous, difficult-to-learn). We will also add a more detailed explanation of Influence Functions (IFs) and self-influence scores in Section 1.1, especially for readers unfamiliar with the concept.\n\n**Weakness 6: Lack of Ethical Considerations**\nWe acknowledge the reviewer's concern regarding the lack of information on dataset licensing, consent, privacy, and usage terms, as well as the ethical risks associated with the datasets. We will add a new section detailing the datasets used, their licenses, terms of use, and any relevant consent information. We will also address potential privacy concerns and ethical risks associated with the data, including potential biases and fairness implications.\n\n**Weakness 7: Missing Comparison with Specific Methods**\nWe acknowledge the reviewer's point regarding the lack of direct comparison with Nguyen et al. (2020) and Swayamdipta et al. (2020). We will conduct a head-to-head comparison with these methods on the same datasets used in the current paper (machine translation, question answering, and text classification). We will evaluate the performance on both in-distribution and out-of-distribution test sets to assess generalization. We will also include an ablation study comparing the proposed method with a baseline that uses confidence scores or label noise detection for data cleaning.\n\n**Suggestions:**\nWe have addressed the suggestions in the responses to the weaknesses above. We will provide more details on the datasets and tasks, include ablation studies, specify the random seeds and report variance, include ethical considerations, provide concrete examples, and conduct head-to-head comparisons.\n\nWe believe that these revisions will significantly improve the clarity, rigor, and impact of our paper. We are grateful for the reviewer's guidance and look forward to incorporating their feedback."
  }
]