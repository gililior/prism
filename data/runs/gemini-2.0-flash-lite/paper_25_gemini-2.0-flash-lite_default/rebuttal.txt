[
  {
    "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback. We appreciate the effort taken to thoroughly evaluate our work. We address each point below:\n\n**Weaknesses:**\n\n*   **Lack of direct comparison with relevant prior work:** The reviewer is correct that a direct comparison with the most relevant prior work is crucial. We believe the paper *does* address this, though perhaps not as explicitly as the reviewer desires. We compare our results to Salesky et al. (2021) in Table 1 and throughout Section 3, using their TED-7 dataset and architecture as a baseline. We also directly compare to the best previous results on the TED-59 dataset, including those of Jin and Xiong (2022) in Section 3.2. We will strengthen this comparison in the revised version by explicitly quantifying the improvements over Salesky et al. (2021) in the abstract and introduction.\n\n*   **Overclaiming benefits and lack of detailed analysis:** We respectfully disagree that we overclaim benefits. We carefully frame our findings, highlighting both improvements and limitations. Section 4.1 and Figure 6 provide a detailed analysis of the impact of different scripts and data representation, correlating performance improvements with script data size. We will expand this analysis in the revised version by including additional metrics and statistical tests to further clarify the relationship between script coverage and performance.\n\n*   **Missing seeds, variance, and environment details:** The reviewer is correct. We did not explicitly state the seeds used or report variance. We will rectify this in the revised version by reporting the seeds used for all experiments, including the variance across multiple runs, and providing details about the software and hardware environment used for the experiments. (Section 2, Section 3.1, Table 7)\n\n*   **Lack of clear axis labels and units:** The reviewer is correct. We will ensure all figures have clear axis labels and units in the revised version. (Fig 3)\n\n*   **Missing statistical measures in Table 1:** The reviewer is correct. We will specify the statistical measures used to determine the average metric scores and include information on variance or confidence intervals in Table 1 and other relevant tables in the revised version. (Table 1)\n\n*   **Lack of clear definition and notation:** The reviewer is correct. We will add a concise definition of 'pixel representations' early in the introduction and include a notation table or section to define all symbols used in the revised version. (Abstract, ยง1, Sec 2.1)\n\n*   **Lack of discussion on misuse, fairness, and societal impacts:** We acknowledge this gap. We will add a section on potential risks and misuse, including mitigation strategies, a discussion of fairness considerations and potential biases, along with mitigation strategies, and expand the discussion of broader societal impacts, including positive and negative consequences in the revised version. (Insufficient evidence)\n\n*   **Missing comparison with transliteration and vocabulary expansion:** We acknowledge this is a relevant point. We will add a discussion of transliteration-based approaches and vocabulary expansion methods in the related work section. We will also conduct experiments comparing our approach with vocabulary expansion methods on a shared set of languages and scripts to quantify the performance difference in the revised version. (Sec 4.1)\n\n*   **Lack of computational cost analysis:** We acknowledge this gap. We will include a section in the paper that analyzes the computational efficiency of the proposed method compared to subword embeddings and other baselines, including training and inference time in the revised version. (Sec X)\n\n**Suggestions:**\n\n*   **Include direct comparison with Rust et al. (2023) or Salesky et al. (2021):** As mentioned above, we already compare to Salesky et al. (2021) and will strengthen this comparison in the revised version.\n\n*   **Provide more details on architectures and training methodologies:** We believe we already provide sufficient detail. Section 2.3 describes the model architecture, and Section 2.1 and Appendix C detail the rendering process. We will clarify these sections further in the revised version.\n\n*   **Conduct more detailed experiments on script and data representation:** We will expand our analysis as described above.\n\n*   **Provide a link to the code repository:** We plan to release our code upon publication and will include a link to the code repository in the revised version. (GitHub link placeholder)\n\n*   **Report seeds, variance, and environment details:** We will incorporate these details as described above.\n\n*   **Add error bars and statistical significance indicators:** We will incorporate these in the revised version.\n\n*   **Provide explicit statements about dataset licenses and usage restrictions:** We will add these statements in the revised version. (Insufficient evidence)\n\n*   **Provide a concise definition of 'pixel representations' and notation:** We will incorporate these as described above.\n\n*   **Incorporate a figure or diagram to illustrate the architecture:** We will add a figure or diagram to illustrate the architecture of the pixel-based multilingual translation model, clarifying how pixel representations are integrated in the revised version. (ยง2)\n\n*   **Add a section on potential risks and misuse, fairness, and societal impacts:** We will incorporate these as described above.\n\n*   **Conduct experiments comparing with transliteration and vocabulary expansion:** We will incorporate these as described above.\n\n*   **Include a section on computational efficiency:** We will incorporate these as described above.\n\nWe believe these revisions will significantly improve the clarity and impact of our work. Thank you again for your valuable feedback."
  }
]