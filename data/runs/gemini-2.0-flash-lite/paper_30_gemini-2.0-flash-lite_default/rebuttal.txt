[
  {
    "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback. We appreciate the time taken to thoroughly review our paper and provide constructive criticism. We address each point below:\n\n**Weaknesses:**\n\n*   **Overclaiming self-correction (Section 5):** We respectfully disagree. While we acknowledge the need for further investigation, our results in Table 2 and Figure 2, particularly the improvement scores and the Grad-CAM visualizations, provide evidence of ReVisE's self-correcting capabilities. Table 2 demonstrates consistent performance gains on samples initially misinterpreted by the base model. Figure 2 visually illustrates how ReVisE refines the model's attention over iterations, guiding it towards more relevant image regions. We will clarify the limitations of the self-correction claim in the conclusion.\n\n*   **Lack of detailed comparison with relevant baselines (Related Work):** We believe the reviewer overlooked our extensive comparison in Section 4.1 and Table 1. We compare our method with several state-of-the-art models, including e-UG, PJ-X, FME, RTV, QA-only, NLX-GPT, and OFA-X MT, which are all relevant baselines. We will add a sentence in the related work to highlight the low-data regime of our approach.\n\n*   **Computational environment (Section 4):** We acknowledge this omission. We will add a section in Appendix B detailing the hardware (GPU type, memory) and software (operating system, CUDA version, PyTorch version) used for training and evaluation.\n\n*   **Seeds for experiments (Section 4):** We acknowledge this omission. We will include the random seeds used for all experiments and report the standard deviation of the results across multiple runs with different seeds in the next version.\n\n*   **Statistical information and readability (Tables 1, 3, 5, 6, 7):** We agree and will include standard deviations and/or confidence intervals in all tables. We will also add p-values to indicate the statistical significance of the performance differences and ensure consistent formatting.\n\n*   **Definition of 'step' and 'iteration' (Abstract, Intro ยง1):** The reviewer appears to have overlooked the detailed explanation in Section 3.2 and Algorithm 1. We will add a sentence in the abstract and introduction to clarify the iterative nature of ReVisE. Each 'step' or 'iteration' involves generating an explanation based on the previous one, recomputing visual features, and refining the answer. The 'preceding sentence' (explanation from the previous step) is tokenized, embedded, and concatenated with the K queries to guide the QFormer in generating the next-step explanation. The pseudo-code in Algorithm 1 provides a clear algorithmic description.\n\n*   **Notation table (Methods ยง2):** We acknowledge this omission and will include a notation table in Appendix E to define all variables and symbols.\n\n*   **Comparison with VLMO [1] and A-OKVQA [2] (Related Work):** We will add a sentence in the related work to mention VLMO and A-OKVQA.\n\n*   **Figure 3 axis labels (Fig 3):** We agree and will add clear axis labels, units, and a legend to Figure 3.\n\n*   **Dataset license and usage restrictions:** We will add explicit license and consent statements for datasets used in Appendix E.\n\n*   **Potential misuse or failure modes:** We will add a discussion of potential misuse and failure modes in the conclusion.\n\n**Suggestions:**\n\n*   **Address failure cases (Section 5):** We will expand the discussion of failure cases in Section 4.5 and the conclusion, as suggested.\n\n*   **Head-to-head comparison with iterative/low-data methods (Sec 4.1):** We believe our existing comparisons in Section 4.1 and Table 1 already address this suggestion. We will highlight the iterative and low-data aspects more explicitly.\n\n*   **Ablation studies (Sec 4.2):** We believe the ablation studies in Section 4.5 already demonstrate the contribution of each component. We will clarify this in the text.\n\n*   **Seeds and variance (Section 4):** We will incorporate this suggestion.\n\n*   **Code repository (GitHub link placeholder):** We will provide a link to the code repository, including the training and evaluation scripts, and a requirements file in the next version.\n\n*   **Hardware/software details (Section 4):** We will incorporate this suggestion.\n\n*   **Dataset licenses:** We will incorporate this suggestion.\n\n*   **Figure 3 labels (Fig 3):** We will incorporate this suggestion.\n\n*   **Statistical information in tables (Tables 1-8):** We will incorporate this suggestion.\n\n*   **Detailed explanation of the iterative process (Methods ยง2):** We believe the current description in Section 3.2 and Algorithm 1 is already detailed. We will clarify the role of the 'preceding sentence' and how visual features are computed and integrated with the text input.\n\n*   **Notation table (Appendix E):** We will incorporate this suggestion.\n\n*   **Clarify the role of the 'preceding sentence' (Intro ยง1):** We will incorporate this suggestion.\n\n*   **Broader Impact section (Conclusion):** We will incorporate this suggestion.\n\n*   **Experiment with VLMO [1] and A-OKVQA [2] (Sec 4.1, VCR and VQAX datasets, Sec 4.1, A-OKVQA):** We will add a sentence in the related work to mention VLMO and A-OKVQA. We will add a sentence in the conclusion to mention the future work of comparing ReVisE with VLMO and A-OKVQA.\n\nWe are confident that these revisions will significantly improve the clarity and impact of our paper. Thank you again for your valuable feedback."
  }
]