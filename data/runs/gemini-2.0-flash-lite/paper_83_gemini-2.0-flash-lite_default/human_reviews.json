[
  {
    "rid": "clbWUYlaw3",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper looks at stereotypes in Stable Diffusion (text-to-image), with a focus on gender and nationality/continental identity. The research revealed that when generating images of a 'person', the model disproportionately skews towards males and individuals from Europe/North America, presenting an implicit bias. A troubling pattern of sexualization of women, especially Latin American, Mexican, Egyptian, and Indian women, was also identified. The authors establish these patterns using pairwise CLIP-cosine similarity and manual examination across 138 prompts.",
      "reasons_to_accept": "Years ago there was important work in linguistics and related fields to establish that, in English, \"generic man\" and \"generic he\" (standing in for 'humanity') was not neutral, this paper is in a similar vein but works on images/representations. What does \"a person\" look like? I think many researcher will be interested in their use of CLIP-cosine similarity and disagreement with this method feels likely to be fruitful for everyone involved.",
      "reasons_to_reject": "I guess if I'm looking for risks, I could say \"Oh no, the authors are going to flash a lot of overly sexualized pictures of women at the audience\" but uh, the care the authors have around this make me feel like that's not a real risk here.  I think the authors could probably be clearer about the drawbacks of picking cosine similarity. They do mention that all sorts of stuff might be present in the image that humans would see but that aren't picked up by CLIP. They also mention the problem of bias already known in CLIP but I think this could be slowed down and built out to show readers that the logic isn't circular.  The obvious alternative or addition would seem to be a big annotation project...that would be great to verify the method but to be clear I don't see that as being required to shore this up (though it should be someone's follow-on research).  Something more within the authors' power/scope could be to demonstrate the images/descriptions of pairs that are especially high. The aggregate stats are meant to zoom out but it can be useful to give more granular examples. In particular, showing cross-category highly similar pairs and within-category highly dissimilar pairs may help readers understand what the embeddings are and aren't doing.",
      "questions_for_the_authors": "1) If you could direct the engineering team behind Stable Diffusion to address these problems, where would you have them put their effort?\nFor example, I assume one part of the problem is the training data, LAION-5B likely includes plenty of sexualized images of women but even the labeling of non-offensive images is likely to have a subtle POV that reflects social structures. That is, many white Americans giving a caption wouldn't think to write \"a white person waving goodbye\".\nYou may think of that as a morass and prefer to direct the engineers to sampling\u2014note when users have typed text that seems to be fairly generic and make sure that the model doesn't reproduce biases by, say, adding in other terms at random (\"a person waving goodbye\" will be given some probability of x gender being selected, some probability of y ethnicity/country).  2) Relatedly, what are the uses of Stable Diffusion that you are most worried about? For example, is it marketing folks generating images for  their websites/emails/ads? Individuals generating memes? Something else? If you could solve the bias you're detecting for one and only one use case, which would it be and why?\n--- Whether or not your answers to these questions make it into the paper (I hope they do), I think this discussion among yourselves will clarify how you see the situation and judge interventions to deal with the problems you're detecting.  You have written \"Our findings have worrisome implications on exacerbating societal tendencies of the Western stereotype, and designers should consider how their datasets and design choices lead up to such results.\" I think that is well put but to say \"think carefully\" without saying \"we recommend X\" is to put a lot more onus on those designers and if you have a way of helping them, that feels like it is a helpful part of harm reduction. And if other researchers disagree, that seems like a valuable thing for the field to wrestle with.",
      "typos_grammar_style_and_presentation_improvements": "I feel torn about this part of your Ethics Statement: \"Though our finding of the stereotypical definition of personhood being a Western, light-skinned man can amplify societal problems where people of nonbinary gender, especially transgender individuals, are considered inhuman abominations by conservative peoples (Roen, 2002), we do not claim this as a finding.\"\nPart of me feels like you are helping readers see a very stark and dehumanizing reality. But more of me feels like you could probably convey this without saying \"inhuman abominations\".",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  },
  {
    "rid": "KuF6HlGd2a",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper conducts a diagnostic study to uncover stereotypes in Stable Diffusion. The stereotypical definition of personhood corresponds closely to Western, light-skinned men. Sexualization of women, mostly Latin Amerin was also common.",
      "reasons_to_accept": "1. A comprehensive diagnostic study was conducted to reveal stereotypes in Stable Diffusion. \n2. The paper is well written.",
      "reasons_to_reject": "1. The findings are not novel compared to [1], which finds that \"diffusion models over-represent the portion of their latent space associated with whiteness and masculinity across target attributes.\"\n[1] Luccioni, A. S., Akiki, C., Mitchell, M., & Jernite, Y. (2023). Stable bias: Analyzing societal representations in diffusion models. arXiv preprint arXiv:2303.11408.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",
      "excitement": "3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "yo74GpioI8",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper analyzes the stereotypical definitions of personhood in Stable Diffusion, a text-to-image generator. They tested 138 prompts for front-facing photos of people of different genders and continental/national identities and compared the results to reveal two stereotypes - gender and continental/national identities. They found that Stable Diffusion's definition of personhood corresponded closely to Western, light-skinned men, threatening to erase historically marginalized groups. Additionally, they found a pattern of sexualization of women, perpetuating the Western stereotype of fetishizing women of color. These findings suggest the need for more careful use of these tools and improvements in developing fair generators.",
      "reasons_to_accept": "This paper provides several exciting findings in text-to-image generations: (1) there exists stereotypes of 'person' for Stable Diffusion when no other information about gender is provided, skews male and ignores nonbinary genders; (2) based on my understanding, this paper is the pioneer to discuss the stereotypes in contexts of national/continental identities; (3) they uncovered the patterns of sexualization of woman, which extends to the findings from previous work. These results can help researchers build new methods to resolve fairness issues and alarm the models' reliability when practitioners want to apply them to real-world applications.",
      "reasons_to_reject": "1. As mentioned in line 360, CLIP-embeddings are to be biased; I would question whether the higher cosine similarity between \u2018person\u2019 and \u2018man\u2019 may come from the biased evaluation metric. The paper will be sound if experiments evaluate the inherent biases in CLIP-embeddings and confirm that the biases will not have a dominant effect on the results are done. Moreover, I would like to see human evaluation beyond the cosine similarity to support the findings in the paper. \n2. As mentioned in line 289, the countries chosen in the paper are the top five most populated countries; I would question the selection here to be unfair. I would recommend selecting more countries regarding population size: large, medium, and small, to conduct more comprehensive experiments. \n3. As mentioned in line 655, models like Fair Diffusion/Safe Latent Diffusion have been developed to improve the quality of Stable Diffusion generation in terms of social stereotypes. It would be interesting and important to see similar experiments conducted in this paper applied to debiasing text-to-image generators. I believe these experimental results will provide strong evidence to support the arguments in this paper since the debiasing method has been already developed and it is necessary to see whether the arguments mentioned in this paper have been resolved entirely or partially or not by the methods. \n4. The overall reason is although this paper provides many qualitative discussions, it lacks quantitative explanations to support the arguments. It is crucial to include human evaluation to balance the drawback of biased evaluation metrics (CLIP-embeddings.) Moreover, it is impressive to learn the findings from this paper, but thinking about how to reduce/mitigate the stereotypes might also be noteworthy.",
      "questions_for_the_authors": "1. The images presented in Figure 1 seem cherry-picked to some extent. I would like to see more statistics on these generated images. I think it is possible to randomly sample the number of pictures and apply human evaluation to label the details. \n2. Would it be possible to provide images of examples of nonbinary gender? \n3. In Table 5, for Bangladesh and Ghana, cosine similarity scores of nonbinary gender are higher than \u2018man\u2019 or \u2018woman\u2019. Could you provide more explanations on these results? Similarly, in Table 4, many cosine similarity scores are very close among different columns. These results make me question whether the huge difference in Table 3 comes from pre-existing bias in CLIP-embeddings. \n4. I would like to see the statistical significance of these quantitative results.",
      "typos_grammar_style_and_presentation_improvements": "1. The paper could be much appreciated if proofread by an English native speaker. Many repetitions exist in several sections, making the overall paper look wordy. \n2. Captions are not self-contained, which makes the reader hard to understand without referring to the main context. Besides, it would be appreciated if the authors could improve the presentation of Tables 3, 4, and 5 since the current format can easily confuse the reader and hinder understanding.",
      "ethical_concerns": "Yes",
      "justification_for_ethical_concerns": "In section 4.3, which discusses the sexualization of non-western women, I think the authors need to revise the content, for example, avoiding using 'sexy' in Table 2 and other places since this word usage might be inappropriate for specific groups."
    },
    "scores": {
      "soundness": "2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems",
      "excitement": "3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  }
]