[
  {
    "rebuttal": "We thank the reviewer for their thorough and insightful feedback. We address each point below:\n\n**Weaknesses:**\n\n*   **Lack of Details (LLM API, Hyperparameters, Datasets):** The reviewer is correct that more detail is needed. We partially addressed this in the original submission. We used the Azure OpenAI LLM API service with gpt-3.5-turbo (January 2023 version) for all experiments unless otherwise stated (Section 3.2). We also explicitly state the hyperparameters used for the ProTeGi algorithm (minibatch size, beam size, optimization steps, etc.) in Section 3.2. We also provide the datasets used in Section 3.1. We will include a table in the appendix listing the exact versions of the LLM APIs used and the specific dataset splits. We will also include a more detailed description of the dataset preprocessing steps.\n\n*   **Overclaiming and Statistical Measures:** We respectfully disagree that the paper overclaims the benefits of ProTeGi. We present the average F1 score across three trials and report the standard error in Table 5. We will add standard deviations to Tables 1, 2, and 3 to better quantify the variability of the results. We also provide a more detailed discussion of the variance of the optimization process in the Appendix. The claim that RLHF-tuned models dramatically outperform GPT-3 is supported by the results in Table 3. We will clarify the phrasing to emphasize that this is an observation based on our experimental results.\n\n*   **Seeds and Figure Clarity:** The reviewer is correct that we did not explicitly mention the use of seeds. We will add a statement in Section 3.2 that we used random seeds for all experiments and report the specific seeds used in the appendix. We will also add axis labels and units to Figure 4.\n\n*   **Code Availability, Data Privacy, API Terms, Consent, and Usage Terms:** We acknowledge the lack of information on code availability, data privacy, API terms, consent, and usage terms. We are unable to release the code at this time due to proprietary considerations. We will add a section to the conclusion discussing the privacy measures taken to protect user data and the usage terms for the algorithm. We will also include a statement about our adherence to LLM API terms of service.\n\n*   **Non-Standard 'Gradient' and Metric Function Definition:** We acknowledge that the term 'gradient' is used in a non-standard way. We provide a detailed explanation of the textual 'gradient' generation process in Section 2.1 and the prompts used in Appendix 1.1. We will clarify the definition of the metric function 'm' in Section 2 and the abstract.\n\n*   **Misuse Cases, Failure Modes, Fairness, and Societal Impacts:** We acknowledge the lack of discussion on potential misuse cases, failure modes, fairness considerations, and broader societal impacts. We will add a Broader Impact section to the conclusion with mitigation strategies for potential misuse and bias.\n\n*   **Comparison with Parameter-Efficient Prompt Tuning and LLM-Based Feedback:** We will add a discussion of parameter-efficient prompt tuning methods (Lester et al., 2021) and methods that use LLM-based feedback in the Related Work section. We will also add a comparison with a baseline that uses LLM-based feedback.\n\n**Suggestions:**\n\n*   **Code, Seeds, and Variance:** We will add the random seeds used for all experiments and the variance of the results as described above.\n\n*   **Head-to-Head Comparison with Lester et al. (2021) and LLM-Based Feedback Baseline:** We will add a head-to-head comparison with Lester et al. (2021) and an LLM-based feedback baseline as suggested.\n\n*   **Detailed Explanation of 'Textual Gradient' and Notation:** We provide a detailed explanation of the 'textual gradient' generation process in Section 2.1 and the prompts used in Appendix 1.1. We will clarify the notation used in the paper and include standard deviations or confidence intervals to quantify the variability of the results in all tables.\n\n*   **Broader Impact Section:** We will add a Broader Impact section as described above.\n\nWe believe these revisions will significantly improve the clarity and impact of our paper."
  }
]