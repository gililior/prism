[
  {
    "kind": "summary",
    "text": "The paper introduces ALLIES, a method for open-domain question answering. The authors claim ALLIES outperforms baselines and is generalizable. Human evaluation supports the scoring function's accuracy. Limitations include high computational cost and prompt dependency.",
    "grounding": "Sections 5.4, 5.8, 6",
    "facet": "summary"
  },
  {
    "kind": "strength",
    "text": "ALLIES outperforms baselines in zero-shot open-domain question answering.",
    "grounding": "Table 1, Section 5.4",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "strength",
    "text": "Human evaluation shows 93% alignment of generated scores with requirements.",
    "grounding": "Section 5.8",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "weakness",
    "text": "Claim that ALLIES is more generalizable is not fully supported, as the paper only mentions it relies less on result parsing, but does not provide evidence of its generalizability to other applications.",
    "grounding": "Section 5.4",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "weakness",
    "text": "The paper overclaims the impact of the retriever choice on model performance. While the choice of retriever has a significant impact, the paper does not provide enough evidence to support this claim.",
    "grounding": "Section 5.4",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "suggestion",
    "text": "Provide more evidence to support the claim of generalizability by testing ALLIES on different applications.",
    "grounding": "Section 5.4",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "suggestion",
    "text": "Conduct experiments to compare ALLIES with and without the iterative query generation to isolate the impact of this component.",
    "grounding": "Section 6",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "questions",
    "text": "What specific applications are considered for future work to demonstrate generalizability?",
    "grounding": "Section 6",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "questions",
    "text": "How does the computational cost of ALLIES compare to the baselines?",
    "grounding": "Section 6",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "limitations",
    "text": "The authors acknowledge the limitations of ALLIES, including high computational cost and prompt dependency.",
    "grounding": "Section 6",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "N/A",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "summary",
    "text": "The paper presents ALLIES, an approach for Open-Domain Question Answering (ODQA). The paper details the methodology, experimental settings, and evaluation metrics. The paper lacks information on seeds and variance.",
    "grounding": "Sections 4, 5",
    "facet": "reproducibility"
  },
  {
    "kind": "strength",
    "text": "The paper describes the experimental setting, including datasets (NQ, TriviaQA, WebQ) and evaluation metrics (EM, F1).",
    "grounding": "Section 5.1",
    "facet": "reproducibility"
  },
  {
    "kind": "weakness",
    "text": "The paper does not report the seeds used for the experiments, making it difficult to reproduce the results. Variance across multiple runs is not discussed.",
    "grounding": "All sections",
    "facet": "reproducibility"
  },
  {
    "kind": "weakness",
    "text": "The paper does not provide information on the computational resources used for the experiments.",
    "grounding": "All sections",
    "facet": "reproducibility"
  },
  {
    "kind": "suggestion",
    "text": "Provide the code, including the prompts used for the LLMs (Appendix C), and the environment (e.g., Dockerfile, Conda environment file) to ensure reproducibility.",
    "grounding": "All sections",
    "facet": "reproducibility"
  },
  {
    "kind": "suggestion",
    "text": "Specify the random seeds used for all experiments and report the variance of the results across multiple runs.",
    "grounding": "Section 5",
    "facet": "reproducibility"
  },
  {
    "kind": "suggestion",
    "text": "Include details about the computational resources (e.g., GPU type, memory) used for the experiments.",
    "grounding": "Section 5",
    "facet": "reproducibility"
  },
  {
    "kind": "question",
    "text": "Are the prompts used for the LLMs available?",
    "grounding": "Appendix C",
    "facet": "reproducibility"
  },
  {
    "kind": "question",
    "text": "Will the authors provide the code and the environment to reproduce the results?",
    "grounding": "All sections",
    "facet": "reproducibility"
  },
  {
    "kind": "question",
    "text": "Are the authors able to provide the compute resources used?",
    "grounding": "Section 5",
    "facet": "reproducibility"
  },
  {
    "kind": "limitation",
    "text": "The lack of code, environment details, and seed information hinders reproducibility.",
    "grounding": "All sections",
    "facet": "reproducibility"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "N/A",
    "facet": "ethics"
  },
  {
    "kind": "rating",
    "text": "Needs Improvement",
    "grounding": "All sections",
    "facet": "reproducibility"
  },
  {
    "kind": "summary",
    "text": "The paper introduces ALLIES, a novel approach for open-domain question answering that uses a beam search strategy to generate responses. It addresses limitations of existing methods by extending the original question to multiple relevant questions and employing a dynamic pruning technique. The method is evaluated on several benchmarks, demonstrating improved performance over baselines.",
    "grounding": "Intro",
    "facet": null
  },
  {
    "kind": "strength",
    "text": "The paper clearly identifies limitations in existing methods (narrow information coverage and low fault tolerance) and proposes a solution (ALLIES) to address them. (Intro)",
    "grounding": "Intro",
    "facet": null
  },
  {
    "kind": "strength",
    "text": "The proposed method, ALLIES, is well-explained, and the iterative process is easy to follow. (Intro, Figure 1)",
    "grounding": "Intro, Figure 1",
    "facet": null
  },
  {
    "kind": "weakness",
    "text": "The paper lacks a detailed comparison with the most relevant baselines. The delta between ALLIES and the closest performing baseline is not clearly articulated.",
    "grounding": "Sec 4.1",
    "facet": null
  },
  {
    "kind": "weakness",
    "text": "The novelty hinges on the specific implementation of the beam search strategy and the query extension method. The paper does not provide enough details to assess the novelty of these components.",
    "grounding": "Method",
    "facet": null
  },
  {
    "kind": "suggestion",
    "text": "Include a detailed ablation study to analyze the impact of each component of ALLIES (query extension, dynamic pruning, scoring).",
    "grounding": "Method",
    "facet": null
  },
  {
    "kind": "suggestion",
    "text": "Provide a head-to-head comparison with a strong, contemporaneous baseline, such as a method that also uses iterative refinement or query generation.",
    "grounding": "Sec 4.1",
    "facet": null
  },
  {
    "kind": "questions",
    "text": "How does the computational cost of ALLIES compare to the baselines, and is it scalable?",
    "grounding": "Sec 4.1",
    "facet": null
  },
  {
    "kind": "questions",
    "text": "What is the sensitivity of ALLIES to the choice of LLM and the scoring threshold?",
    "grounding": "Method",
    "facet": null
  },
  {
    "kind": "questions",
    "text": "Are there failure cases or limitations of ALLIES that are not discussed in the paper?",
    "grounding": "Method",
    "facet": null
  },
  {
    "kind": "limitations",
    "text": "The novelty of the approach depends on the specific implementation details of the query extension and beam search strategy, which are not fully elaborated.",
    "grounding": "Method",
    "facet": null
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "n/a",
    "facet": null
  },
  {
    "kind": "provisional_rating",
    "text": "3",
    "grounding": "n/a",
    "facet": null
  },
  {
    "kind": "weakness",
    "text": "The paper does not specify the licenses of the datasets used (NQ, TriviaQA, WebQ).",
    "grounding": "Insufficient evidence",
    "facet": "dataset_licensing"
  },
  {
    "kind": "weakness",
    "text": "The paper does not address consent or privacy considerations related to the datasets.",
    "grounding": "Insufficient evidence",
    "facet": "consent_privacy"
  },
  {
    "kind": "weakness",
    "text": "The paper does not mention the usage terms of the datasets.",
    "grounding": "Insufficient evidence",
    "facet": "usage_terms"
  },
  {
    "kind": "summary",
    "text": "The figures provide examples and performance comparisons, but some lack clarity in axes and legends.",
    "grounding": "Figures 1-3",
    "facet": "figures"
  },
  {
    "kind": "strength",
    "text": "Figure 1 provides a clear example of the ALLIES method in action.",
    "grounding": "Fig 1",
    "facet": "figures"
  },
  {
    "kind": "weakness",
    "text": "Figure 3 lacks clear axis labels and units, making it difficult to interpret the performance comparison.",
    "grounding": "Fig 3",
    "facet": "figures"
  },
  {
    "kind": "suggestion",
    "text": "Add a legend to Figure 2 to explain the different components of the ALLIES process. Add error bars to Figure 3 to show confidence intervals.",
    "grounding": "Fig 2, Fig 3",
    "facet": "figures"
  },
  {
    "kind": "questions",
    "text": "What specific hyperparameters are being compared in Figure 3? What are the units of the performance metric in Figure 3? What does the abstract process in Figure 2 entail?",
    "grounding": "Fig 2, Fig 3",
    "facet": "figures"
  },
  {
    "kind": "limitations",
    "text": "The visualizations are limited to illustrating the ALLIES method and comparing performance, without providing detailed insights into the underlying mechanisms.",
    "grounding": "All figures",
    "facet": "figures"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "N/A",
    "facet": "figures"
  },
  {
    "kind": "summary",
    "text": "The tables present results from zero-shot experiments, ablation studies, and other analyses related to the proposed method. The quality of the tables varies, with some lacking crucial statistical information.",
    "grounding": "Tables 1-8",
    "facet": "tables"
  },
  {
    "kind": "strength",
    "text": "The presence of standard deviation in some tables indicates an effort to provide more complete statistical information.",
    "grounding": "Table 2",
    "facet": "tables"
  },
  {
    "kind": "weakness",
    "text": "Several tables are missing critical statistical information such as confidence intervals, p-values, or clear indications of statistical significance.",
    "grounding": "Table 1, Table 3, Table 4, Table 7",
    "facet": "tables"
  },
  {
    "kind": "suggestion",
    "text": "Include p-values or other statistical significance indicators to compare the performance of different methods and configurations. Clearly define all acronyms and abbreviations used in the table headers and content. Provide more detailed descriptions of the datasets used, including the number of samples and the evaluation metrics.",
    "grounding": "Tables 1-8",
    "facet": "tables"
  },
  {
    "kind": "questions",
    "text": "What statistical tests were used to determine the significance of the results presented in the tables? How were the datasets split for the ablation studies and other experiments? What are the specific evaluation metrics used for each dataset? Are the hyper-parameters in Table 6 optimized, and if so, how?",
    "grounding": "All tables",
    "facet": "tables"
  },
  {
    "kind": "limitations",
    "text": "The conclusions drawn from the tables are limited by the scope of the datasets and the specific experimental setups. The generalizability of the findings to other domains or tasks is not fully addressed.",
    "grounding": "All tables",
    "facet": "tables"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "N/A",
    "facet": "tables"
  },
  {
    "kind": "summary",
    "text": "The paper introduces ALLIES, a novel approach for open-domain question answering that uses a beam search strategy to generate responses. It addresses limitations in existing methods related to information coverage and fault tolerance. The method is evaluated on several benchmarks, demonstrating improved performance compared to baselines.",
    "grounding": null,
    "facet": "organization"
  },
  {
    "kind": "strength",
    "text": "The introduction clearly outlines the problem, existing limitations, and the proposed solution (ALLIES). The structure is easy to follow.",
    "grounding": "Introduction",
    "facet": "organization"
  },
  {
    "kind": "strength",
    "text": "The advantages of the method are clearly stated and well-explained.",
    "grounding": "Advantages section",
    "facet": "clarity_presentation"
  },
  {
    "kind": "weakness",
    "text": "The term 'query-evidence pair' is used without a clear definition. What constitutes a 'query-evidence pair' needs to be explicitly defined.",
    "grounding": "Introduction, Figure 1",
    "facet": "clarity_presentation"
  },
  {
    "kind": "weakness",
    "text": "The paper lacks a dedicated section for notation. Symbols like 'B' (in 'top B answers') should be defined in a notation table.",
    "grounding": "Advantages section",
    "facet": "organization"
  },
  {
    "kind": "suggestion",
    "text": "Provide a more detailed explanation of the beam search strategy as it applies to ALLIES. Clarify how the 'score' is calculated and the predefined threshold is determined.",
    "grounding": "Methods section",
    "facet": "clarity_presentation"
  },
  {
    "kind": "suggestion",
    "text": "Include a table of notation to define all symbols used in the paper (e.g., B, score, threshold).",
    "grounding": "Throughout the paper",
    "facet": "organization"
  },
  {
    "kind": "questions",
    "text": "1. How is the initial query-evidence pair constructed? 2. What specific LLMs are used in the experiments? 3. What is the computational cost of ALLIES compared to the baselines? 4. How sensitive is ALLIES to the choice of the threshold?",
    "grounding": null,
    "facet": "questions"
  },
  {
    "kind": "limitations",
    "text": "The paper's description of the method is not detailed enough to allow for complete reproduction. Specifics of the LLM calls, scoring mechanism, and threshold selection are missing.",
    "grounding": null,
    "facet": "reproduction"
  },
  {
    "kind": "ethics_flag",
    "text": "No ethical concerns are apparent in the provided text.",
    "grounding": null,
    "facet": "ethics"
  },
  {
    "kind": "ratings",
    "text": "Organization: 4/5, Clarity: 3/5, Terminology: 3/5",
    "grounding": null,
    "facet": "ratings"
  }
]