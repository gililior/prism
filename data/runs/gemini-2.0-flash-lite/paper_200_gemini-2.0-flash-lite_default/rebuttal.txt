[
  {
    "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback. We appreciate the time and effort invested in reviewing our paper. We address each point below:\n\n**Weaknesses:**\n\n*   **Weakness 1: Generalizability and Retriever Choice (Section 5.4).** The reviewer questions the generalizability claim and the impact of retriever choice. We respectfully disagree with the assessment that the impact of retriever choice is overclaimed. Section 5.4 clearly states, \"Among the methods that utilize a retriever, the choice of the retriever has a significant impact on the model's performance. This indicates that the quality of the retrieved documents plays a crucial role in determining the overall system performance.\" This highlights the importance of the retriever. Regarding generalizability, we acknowledge that our experiments are limited to ODQA. We will address this in the future work section by explicitly stating that further evaluation on diverse tasks is needed to fully validate the generalizability of ALLIES.\n\n*   **Weakness 2: Reproducibility (All sections).** The reviewer points out the lack of details for reproducibility. We acknowledge this is a valid concern. While we provide implementation details in Section 5.2, we will add the following for the next version: (1) We will specify the random seeds used for all experiments in Section 5.2. (2) We will report the variance of the results across multiple runs by including standard deviations in Table 1 and other relevant tables. (3) We will include a dedicated section in the Appendix detailing the computational resources used, including the specific GPU and the API calls.\n\n*   **Weakness 3: Comparison with Baselines (Sec 4.1).** The reviewer requests a more detailed comparison with relevant baselines. We believe Section 5.3 already provides a comprehensive comparison, including baselines from different categories (retriever-based, retriever-free, and our implementations). We will clarify in Section 4.1 that our method is designed for zero-shot ODQA and that the baselines are chosen accordingly. We will also add a sentence to highlight the key differences between ALLIES and the baselines in the introduction.\n\n*   **Weakness 4: Missing Information in Figures and Tables (Fig 3, Table 1, Table 3, Table 4, Table 7).** We acknowledge the need for improvement in figures and tables. We will add axis labels and units to Figure 3, as suggested. We will also include p-values or other statistical significance indicators in Tables 1, 3, 4, and 7 to enhance the clarity of our results. We will add a legend to Figure 2.\n\n*   **Weakness 5: Definition of 'query-evidence pair' and Notation (Introduction, Figure 1, Advantages section).** The reviewer points out the lack of a clear definition for 'query-evidence pair' and a dedicated notation section. We agree that this could lead to confusion. We will add a dedicated section for notation immediately after the introduction to define all symbols used in the paper. We will also explicitly define 'query-evidence pair' in the introduction and throughout the paper, clarifying that it represents the query and the retrieved evidence.\n\n**Suggestions:**\n\n*   **Suggestion 1: Testing on Different Applications (Section 5.4).** We acknowledge this suggestion and will incorporate it into our future work section, as mentioned above.\n\n*   **Suggestion 2: Specify Random Seeds, Variance, and Computational Resources (Section 5).** We will implement this suggestion as described in our response to Weakness 2.\n\n*   **Suggestion 3: Detailed Ablation Study and Head-to-Head Comparison (Method, Sec 4.1).** We believe that Section 5.5 already provides a detailed ablation study. We will clarify this in the response to the reviewer. We will also add a head-to-head comparison with a strong, contemporaneous baseline in the next version.\n\n*   **Suggestion 4: Add Legend, Error Bars, and Statistical Significance (Fig 2, Fig 3, Tables 1-8).** We will implement this suggestion as described in our response to Weakness 4.\n\n*   **Suggestion 5: Explanation of Beam Search, Score, and Threshold (Methods section, Throughout the paper).** We will provide a more detailed explanation of the beam search strategy in Section 3, clarifying the Ask, Answer, Retrieve, and Score functions. We will also clarify the calculation of the 'score' and the predefined threshold in Section 3.4. We believe that the prompts in Appendix C already provide sufficient information for the reader to understand the scoring function. We will add a sentence to clarify this in Section 4.4."
  }
]