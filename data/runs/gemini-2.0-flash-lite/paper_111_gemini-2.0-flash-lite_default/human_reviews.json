[
  {
    "rid": "QPRPCShLCt",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "this work finetuned a multilingual self-supervised speech model using labels generated from off-the-shelf unsupervised word-level speech segmentation systems, namely DPDP, VG-HuBERT, and DP-Parse, for improved word segmentation performance. \nSignificant performance gain is observed for all three types of pseudo-labels. In particular, using pseudo labels from DP-Parse, finetuning XLS-R brings an improvement of 130%.  This work advanced the state-of-the-art of unsupervised word-level speech segmentation by a large margin, which is important for bridging the gap between text-based and speech-based language systems. This work leads to a deeper understanding of multilingual self-supervised speech models",
      "reasons_to_accept": "1. the fact that simple finetuning on pseudo labels via the general cross-entropy loss can bring such significant gain is very interestingly, which reveals interesting properties of the pretrained self-supervised speech models.\n2. this work examined using pseudo labels generated by a range of very different state-of-the-art unsupervised systems, namely DPDP, VG-HuBERT, and DP-Parse, and observed universal improvement. This indicate the robustness of the proposed approach.\n3. Training one model on all languages leads to better performance than training language specific models. Although this has been observed in large scale speech models (e.g. OpenAI's Whisper), it's the first time I observe this phenomenon in small scale studies (the total amount of data is below 80 hours.)\n4. a few tricks has being proposed to (potentially) make their approach works better, namely augmentation, smoothing, loss selection, peak detection. These tricks are not novel, but the usage is new and they make sense intuitively. These tricks could be valuable for researchers working on speech segmentation",
      "reasons_to_reject": "Although significant improvements are shown, more explanations are desired:  1. with regard to the impressive performance zero-shot DP-Parse in Table 1, why is zero-shot working so well? What kind of words are being predicted? Or is there a pattern?; \n2. what leads to the discrepancy between using pseudo-labels from DPDP, VG-HuBERT, and DP-Parse",
      "questions_for_the_authors": "A. I'd like to see some ablation studies on how different tricks affect the performance (augmentation, smoothing, loss selection, peak detection) B. When using the time stretch augmentation, does the word boundary label also get stretched? \nC. How many iterations of self-training are needed before the model performance start to decrease?",
      "missing_references": "Citations to two relevant papers are missing: [1] and [2]. But it's worth mentioning that [2] is published within 3 months of this paper.  Both two papers and this work use pseudo-labels to finetune self-supervised speech models for speech segmentation.\n[1] focused on unsupervised phoneme segmentation, by finetuning w2v2 and hubert on pseudo labels generated by off-the-shelf unsup phoneme segmentation models.\n[2] showed that using the features of the SSL model itself to produce pseudo label can already bring significant improvement.\n[1] L. Strgar and D. Harwath, \"Phoneme Segmentation Using Self-Supervised Speech Models,\" 2022 IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar, 2023, pp. 1067-1073, doi: 10.1109/SLT54892.2023.10022827.\n[2] T. S. Fuchs and Y. Hoshen, \"Unsupervised Word Segmentation Using Temporal Gradient Pseudo-Labels,\" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10095363.",
      "typos_grammar_style_and_presentation_improvements": "line 228:  32Go GPU -> 32GB GPU line 257: Germand -> German",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "5: Could easily reproduce the results."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work."
    }
  },
  {
    "rid": "2d9hcEjXjY",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper proposes to finetune the XLS-R model in an \"unsupervised\" way to get better performance on word boundary detection. It compares several systems using their methods.",
      "reasons_to_accept": "Exploring unsupervised ways to perform word segmentation in speech is an interesting direction",
      "reasons_to_reject": "1. I wonder if this is unsupervised, as it still receives supervision signals from a trained system. Training that system requires supervised labels, and it is uncertain where we can get those labels if the experiments are unsupervised. \n2. This paper does not compare to any force-alignment systems. The task, in my understanding, can be achieved reasonably easily using force alignment with a GMM-based system. \n3. Why is the system trained on Gold reference achieved 100.0 token-F1? So is that system already perfect? What on earth is the evaluation metric? How is it computed?",
      "questions_for_the_authors": "See RR. \nAlso, I am not convinced why through the described training method, we can get those improvements in token-F1. We did not get any further supervision signal, and there are no new mechanisms introduced. Can you explain why we get that improvement clearly?",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "60X75uO91q",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper proposes an unsupervised speech segmentation system by fine-tuning the XLS-R speech representation model. Through an iterative training strategy, starting with an off-the-shelf speech segmentation system, the finetuned model shows improved results on the speech segmentation downstream task.",
      "reasons_to_accept": "The idea of the proposed method is simple yet yields good results. By fine-tuning the added random feed-forward layer, the paper demonstrates that the pre-trained speech SSL system can be effective for the downstream task of speech segmentation. Additionally, the paper utilizes speech from different languages, which provides a valuable exploration into the understanding of SSL speech models on multilingual speech segmentation tasks.",
      "reasons_to_reject": "The proposed method does not convincingly demonstrate its generalization and novelty, mostly showing the effectiveness of fine-tuning a certain speech SSL model on speech segmentation tasks. In addition to the main approach, there are many extra tricks used during training and evaluation, such as post-processing, loss sample selection, and data augmentation steps. It would be clearer to add some analysis or ablation study to convince the readers and show the distinct contributions of these different steps.",
      "questions_for_the_authors": "A. The proposed method is simple but efficient, however, the generalization of the fine-tuning approach is not convincing, as shown in Table 1 that the performance of DPDP and Hubert based ones are worse than the baseline for Mandarin. The discrepancy between different SSL speech models should be investigated to improve the quality of this work. For instance, the authors could dive deep to compare the precision and recall, the effectiveness of data augmentation, etc.\nB. Is the method robust to the accuracy/performance of the initial pseud-label from an off-the-shell speech segmentation system?",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",
      "excitement": "2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  }
]