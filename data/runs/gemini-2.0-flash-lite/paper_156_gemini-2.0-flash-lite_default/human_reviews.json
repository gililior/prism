[
  {
    "rid": "rLgi8qsXPs",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper presents a new dataset of machine translation hallucination and omission errors, including sentence-level and token-level annotations for eighteen translation directions involving nine languages. They evaluate multiple hallucination and omission detection methods on the dataset, finding that existing detection methods perform much better on high-resource language pairs. Sequence log-probability is best for sentence-level hallucination detection, and token attribution methods are best for omission detection or token-level detection.",
      "reasons_to_accept": "The dataset is a useful resource for future work on hallucinations and omissions in machine translation systems. It is particularly interesting to see the differences in existing detection methods' performance between low and high resource language pairs.",
      "reasons_to_reject": "The main text is longer than 8 pages.\nThe dataset only contains hallucination and coverage errors for the 600M distilled NLLB model.\nThe limitations section is quite short; it could mention other limitations, e.g. ambiguity in the definition of hallucinations and omissions, difficulties in word-level annotation (e.g. depending on different languages' morphology), focusing on pairs to/from English, etc.",
      "questions_for_the_authors": "A: How were translators recruited, particularly for the low-resource languages?\nB: How might function words and languages' morphology affect token-level hallucination/omission annotations and evaluation scores? E.g. words like \"to\" and \"the\" behave very differently across languages (e.g. sometimes as affixes to content words, or sometimes as standalone words). In many cases, it seems unclear how to annotate function words given the provided hallucination/omission guidelines. This would affect token-level detection scores.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  },
  {
    "rid": "5vHGcemQA4",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper is about developing a new dataset for detecting hallucinations and omissions in machine translation. The dataset is quite general; it contains 18 low and high resourced language pair directions. The dataset is quite useful, as it provides a way for MT systems to be trained to detect such pathological translation mistakes. The paper shows how existing system which were developed for detecting such pathologies, perform on the new dataset, with interesting results.",
      "reasons_to_accept": "This paper should be accepted for EMNLP as it provides a very useful dataset for the research community, which was not available before. I find the annotation guidelines clear and simple. However, I do think more annotation examples should have been provided. The existing examples in Figure 3 are probably not enough.",
      "reasons_to_reject": "1. The paper should be accepted. However, it does go beyond the 8 pages requirement (the Conclusions section crosses over to the 9th page), which is not allowed in this setting. \n2. The translations annotated in this project, were all generated by the same model. I recommend adding another MT model to make sure the data is not biased.",
      "questions_for_the_authors": "I am not sure I totally understand what is the background of the annotators. The authors do say something about it, but I think it should be discussed with more details.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work."
    }
  },
  {
    "rid": "yRNyuRRslE",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper presents a multilingual dataset and benchmark for the detection of hallucinations and omissions in machine translation. The dataset consists of sentence-level and token-level manual annotations for 18 translation directions providing a valuable novel and comprehensive benchmark. The paper also includes results from known detection methods and also proposes a new metric for the reliable detection of omissions.",
      "reasons_to_accept": "This is a very much needed dataset that will enable systematic tests of detection methods of hallucinations in machine translations. The experiments also show that a good language coverage is necessary to draw conclusions that generalize. The manual annotation effort is important to avoid artificial settings that do not reflect the real nature of the problem.\nThe paper is very well written, clearly presented and the dataset and detection methods are clearly described and discussed. The analyses and discussion contributes valuable information to this important topic and the resources are reusable and very valuable.",
      "reasons_to_reject": "There is almost no information about the annotation process and the work of the annotators. I would like to see some discussions about the difficulty of the annotation work. I can imagine that there i a lot of variation in opinions on hallucination and I would like to know how the authors dealt with discrepancies and disagreements.",
      "questions_for_the_authors": "Did you measure annotation agreement and did you consider to resolve or keep annotation variation? \nThe choice of NLLB 600: selecting the smallest model, is this because of inference costs or better chance of finding omissions and hallucinations? \nHow did you combine the 3 sampling strategies? Did you take equal amounts from each of them in the final dataset? Is it interesting to look at the impact of sampling strategy somehow?",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  }
]