[
  {
    "kind": "strength",
    "text": "The paper introduces a novel metric, the compositionality gap, to evaluate the ability of language models to perform compositional reasoning. (Intro)",
    "grounding": "Intro",
    "facet": "originality"
  },
  {
    "kind": "strength",
    "text": "The paper proposes a new method, self-ask, for improving compositional reasoning in language models. (Sec 1)",
    "grounding": "Sec 1",
    "facet": "originality"
  },
  {
    "kind": "weakness",
    "text": "The paper does not provide a detailed comparison of self-ask with other prompting techniques, such as chain-of-thought, on the same datasets. (Sec 2)",
    "grounding": "Sec 2",
    "facet": "comparative evidence"
  },
  {
    "kind": "weakness",
    "text": "The paper's analysis of the compositionality gap in relation to model size is limited by the lack of publicly shared details about the training data or model architecture for ChatGPT and GPT-4. (Sec 2)",
    "grounding": "Sec 2",
    "facet": "positioning"
  },
  {
    "kind": "suggestion",
    "text": "Conduct a head-to-head comparison of self-ask with chain-of-thought and other prompting methods on the Compositional Celebrities dataset, providing detailed performance metrics.",
    "grounding": "Sec 2",
    "facet": "comparative evidence"
  },
  {
    "kind": "suggestion",
    "text": "Evaluate self-ask on a broader range of compositional reasoning tasks and datasets to demonstrate its generalizability.",
    "grounding": "Sec 2",
    "facet": "comparative evidence"
  },
  {
    "kind": "question",
    "text": "How does the performance of self-ask compare to other prompting techniques when using a search engine?",
    "grounding": "Sec 2",
    "facet": "comparative evidence"
  },
  {
    "kind": "question",
    "text": "What are the limitations of the Compositional Celebrities dataset, and how might they affect the evaluation of the compositionality gap?",
    "grounding": "Sec 1",
    "facet": "originality"
  },
  {
    "kind": "question",
    "text": "How does the choice of search engine impact the performance of self-ask?",
    "grounding": "Sec 2",
    "facet": "comparative evidence"
  },
  {
    "kind": "limitation",
    "text": "The novelty of self-ask hinges on the effectiveness of the prompting strategy and the availability of a reliable search engine.",
    "grounding": "Sec 2",
    "facet": "originality"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": null,
    "facet": "ethics"
  },
  {
    "kind": "summary",
    "text": "The paper presents experiments on question answering using language models. The authors compare their methods against baselines on multiple datasets. The paper provides details on the prompts used and the experimental setup.",
    "grounding": "Sections 3.4, 3.5",
    "facet": "reproducibility"
  },
  {
    "kind": "strength",
    "text": "The paper provides details on the prompts used for each method in the appendix.",
    "grounding": "Appendix Tables 8-13",
    "facet": "reproducibility"
  },
  {
    "kind": "weakness",
    "text": "The paper does not mention the use of seeds, making it difficult to reproduce the results.",
    "grounding": "Section 3.4, 3.5",
    "facet": "reproducibility"
  },
  {
    "kind": "weakness",
    "text": "The paper does not provide information on the variance of the results.",
    "grounding": "Section 3.4, 3.5",
    "facet": "reproducibility"
  },
  {
    "kind": "suggestion",
    "text": "Provide the code used for the experiments, including the prompts and the implementation of the methods. Include a requirements.txt or equivalent for environment setup.",
    "grounding": "Section 3.4, 3.5",
    "facet": "reproducibility"
  },
  {
    "kind": "suggestion",
    "text": "Specify the random seeds used for all experiments and report the variance of the results.",
    "grounding": "Section 3.4, 3.5",
    "facet": "reproducibility"
  },
  {
    "kind": "question",
    "text": "Are the datasets used publicly available, or can they be provided?",
    "grounding": "Section 3.4, 3.5",
    "facet": "data_availability"
  },
  {
    "kind": "question",
    "text": "What is the compute infrastructure used for the experiments?",
    "grounding": "Section 3.4, 3.5",
    "facet": "environment_reproducibility"
  },
  {
    "kind": "limitation",
    "text": "The paper lacks information on the computational resources used, making it difficult to assess the feasibility of reproducing the experiments.",
    "grounding": "Section 3.4, 3.5",
    "facet": "reproducibility"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "Section 3.4, 3.5",
    "facet": "ethics"
  },
  {
    "kind": "summary",
    "text": "The paper introduces a self-ask method to improve compositional question answering. Experiments compare self-ask with chain of thought and search-based baselines across multiple datasets. The results show improvements in accuracy and speed.",
    "grounding": null,
    "facet": "summary"
  },
  {
    "kind": "strength",
    "text": "Self-ask improves over chain of thought on 2WikiMultiHopQA and Musique, and by a large margin on Bamboogle (11% absolute).",
    "grounding": "Sec 3.6, Table 1",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "strength",
    "text": "Integrating a search engine into self-ask further improves performance on all datasets.",
    "grounding": "Sec 3.6",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "strength",
    "text": "Self-ask achieves similar or better performance while running more than 30% faster than least-to-most.",
    "grounding": "Table 2",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "weakness",
    "text": "The paper claims that the varied nature of Bamboogle makes it harder for chain of thought to decompose questions. This is a hypothesis, not a proven fact.",
    "grounding": "Sec 3.6",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "weakness",
    "text": "The paper overclaims the impact of the search engine integration, stating improvements 'sometimes by as much as 10% (absolute)'. The exact datasets and conditions for this improvement are not specified.",
    "grounding": "Sec 3.6",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "suggestion",
    "text": "Conduct an ablation study to isolate the impact of each component of the self-ask method (e.g., explicit decomposition, search engine integration).",
    "grounding": "Sec 3.6, 5",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "suggestion",
    "text": "Provide more details on the datasets used, including statistics on question complexity and the types of reasoning required.",
    "grounding": "Sec 3.4, 3.6",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "suggestion",
    "text": "Analyze the types of errors made by each method to better understand their strengths and weaknesses.",
    "grounding": "Sec 3.6",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "question",
    "text": "What are the specific prompts used for each method, and how do they influence the results?",
    "grounding": null,
    "facet": "questions"
  },
  {
    "kind": "question",
    "text": "What are the limitations of the search engine used, and how might these limitations affect the performance of the search engine integration?",
    "grounding": null,
    "facet": "questions"
  },
  {
    "kind": "question",
    "text": "How does the choice of LM (Davinci-002) affect the generalizability of the results?",
    "grounding": null,
    "facet": "questions"
  },
  {
    "kind": "limitations",
    "text": "The authors acknowledge the limitations of their method by discussing the datasets and the specific modifications made to the chain of thought implementation.",
    "grounding": "Sec 3.4, 3.6",
    "facet": "limitations"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": null,
    "facet": "ethics"
  },
  {
    "kind": "summary",
    "text": "The figures present results of different prompting methods and model performance. The figures are generally readable, but some lack clear labels and could benefit from additional details.",
    "grounding": "Figures 4, 5, 6",
    "facet": "figures"
  },
  {
    "kind": "strength",
    "text": "Figure 5 effectively visualizes the relationship between sub-question answer confidence and the probability of answering the compositional question correctly.",
    "grounding": "Figure 5",
    "facet": "figures"
  },
  {
    "kind": "weakness",
    "text": "Figure 6 lacks clear axis labels and units, making it difficult to interpret the results.",
    "grounding": "Figure 6",
    "facet": "figures"
  },
  {
    "kind": "suggestion",
    "text": "For Figure 5, consider adding a y-axis label to clarify what is being measured (e.g., 'Probability of Answering Compositional Question Correctly').",
    "grounding": "Figure 5",
    "facet": "figures"
  },
  {
    "kind": "questions",
    "text": "What specific datasets are used to generate the results in Figure 6?",
    "grounding": "Figure 6",
    "facet": "figures"
  },
  {
    "kind": "limitations",
    "text": "The visualizations primarily focus on overall performance metrics, without providing detailed insights into the reasoning process.",
    "grounding": "Figures 5, 6",
    "facet": "figures"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "N/A",
    "facet": "figures"
  },
  {
    "kind": "summary",
    "text": "The tables present experimental results comparing different prompting methods on various datasets. The tables generally report accuracy and, in some cases, speed. However, the tables lack detailed statistical analysis, such as confidence intervals or p-values, which limits the ability to assess the significance of the observed differences.",
    "grounding": "Tables 1, 2, 14, 15",
    "facet": "tables"
  },
  {
    "kind": "strength",
    "text": "Table 2 includes both accuracy and speed metrics, providing a more complete picture of the performance of the methods. The inclusion of multiple metrics (accuracy and speed) is a strength.",
    "grounding": "Table 2",
    "facet": "tables"
  },
  {
    "kind": "weakness",
    "text": "Tables 1 and 14 lack crucial statistical information such as standard deviations or confidence intervals. Table 15 is qualitative, presenting examples rather than quantitative results. The headers in some tables could be more descriptive.",
    "grounding": "Tables 1, 14, 15",
    "facet": "tables"
  },
  {
    "kind": "suggestion",
    "text": "Include standard deviations or confidence intervals for the accuracy and speed metrics in Table 2. Add p-values to Table 1 to indicate the statistical significance of the differences between the methods. For Table 15, consider quantifying the rate of full-sentence answers for each method to provide a more objective comparison.",
    "grounding": "Tables 1, 2, 15",
    "facet": "tables"
  },
  {
    "kind": "questions",
    "text": "What statistical tests were used to determine the significance of the differences in accuracy between the methods in Table 1 and Table 2? What is the variance or standard deviation of the accuracy results reported in Table 1? How were the datasets split for training and evaluation?",
    "grounding": "Tables 1, 2",
    "facet": "tables"
  },
  {
    "kind": "limitations",
    "text": "The conclusions drawn from the tables are limited by the specific datasets used (Bamboogle, 2WikiMultiHopQA, Musique, CC). Generalizability to other datasets or tasks is not directly addressed.",
    "grounding": "Tables 1, 2, 14, 15",
    "facet": "tables"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "N/A",
    "facet": "tables"
  },
  {
    "kind": "summary",
    "text": "The paper investigates the compositional reasoning abilities of language models (LMs). It introduces the 'compositionality gap' to measure the difference between answering sub-problems correctly and generating the overall solution in multi-hop question answering. The authors find that the compositionality gap does not decrease with increasing model size in the GPT-3 family. They then demonstrate how elicitive prompting and a new method called 'self-ask' can narrow this gap. The paper also introduces a new dataset, Compositional Celebrities (CC), and applies the methods to existing datasets.",
    "grounding": "Abstract, Introduction",
    "facet": "organization"
  },
  {
    "kind": "strength",
    "text": "The paper clearly defines the 'compositionality gap' and its significance.",
    "grounding": "Abstract, Introduction",
    "facet": "clarity_terminology"
  },
  {
    "kind": "strength",
    "text": "The introduction provides a good overview of the problem and the paper's contributions.",
    "grounding": "Introduction",
    "facet": "clarity_organization"
  },
  {
    "kind": "weakness",
    "text": "The definition of 'elicitive prompting' could be more precise. What specific techniques are included?",
    "grounding": "Abstract, Introduction",
    "facet": "clarity_terminology"
  },
  {
    "kind": "weakness",
    "text": "The paper lacks a dedicated section for notation, making it difficult to follow the mathematical expressions and terminology.",
    "grounding": "Throughout the paper",
    "facet": "clarity_organization"
  },
  {
    "kind": "suggestion",
    "text": "Provide a more detailed explanation of the 'self-ask' method, including pseudocode or a clear algorithmic description.",
    "grounding": "Abstract, Introduction",
    "facet": "clarity_presentation"
  },
  {
    "kind": "suggestion",
    "text": "Include a table of notation to clarify the symbols and abbreviations used throughout the paper.",
    "grounding": "Throughout the paper",
    "facet": "clarity_presentation"
  },
  {
    "kind": "suggestion",
    "text": "Expand on the limitations of the study, such as the potential for dataset bias or the generalizability of the findings to other LM architectures.",
    "grounding": "Discussion/Conclusion",
    "facet": "clarity_presentation"
  },
  {
    "kind": "questions",
    "text": "1.  What specific prompting techniques are included under 'elicitive prompting'?\n2.  How does the self-ask method compare to other prompting techniques in terms of computational cost and efficiency?\n3.  What are the specific templates used for generating the Compositional Celebrities dataset?\n4.  What are the limitations of the Bamboogle dataset?\n5.  How do the results generalize to other model families beyond GPT-3?",
    "grounding": "Throughout the paper",
    "facet": "questions"
  },
  {
    "kind": "limitations",
    "text": "The lack of detailed information on the self-ask method and the datasets used may hinder reproducibility.",
    "grounding": "Methods, Datasets",
    "facet": "reproduction"
  },
  {
    "kind": "ethics_flag",
    "text": "No ethical concerns identified.",
    "grounding": "N/A",
    "facet": "ethics"
  },
  {
    "kind": "ratings",
    "text": "Clarity: 3/5, Organization: 4/5, Terminology: 3/5",
    "grounding": "Overall",
    "facet": "ratings"
  },
  {
    "kind": "weakness",
    "text": "The paper does not address potential biases in the dataset or model outputs.",
    "grounding": "Insufficient evidence",
    "facet": "fairness"
  },
  {
    "kind": "weakness",
    "text": "The paper does not discuss the potential for the model to be used to generate misleading or harmful content.",
    "grounding": "Insufficient evidence",
    "facet": "risks"
  },
  {
    "kind": "weakness",
    "text": "The paper does not discuss the broader societal impacts of the research.",
    "grounding": "Insufficient evidence",
    "facet": "broader_impacts"
  },
  {
    "kind": "suggestion",
    "text": "Include a section on potential biases in the dataset and model outputs, and how these biases might affect the model's performance or societal impact.",
    "grounding": "Insufficient evidence",
    "facet": "fairness"
  },
  {
    "kind": "suggestion",
    "text": "Add a discussion of the potential for the model to be used to generate misleading or harmful content, and propose mitigation strategies.",
    "grounding": "Insufficient evidence",
    "facet": "risks"
  },
  {
    "kind": "suggestion",
    "text": "Explore the broader impacts of the research, including its potential benefits and drawbacks for society.",
    "grounding": "Insufficient evidence",
    "facet": "broader_impacts"
  },
  {
    "kind": "strength",
    "text": "The paper introduces the concept of the compositionality gap and demonstrates its existence in large language models (LLMs), which is a novel contribution. The paper also proposes a new method, self-ask, that improves on chain-of-thought prompting, which is a significant advancement.",
    "grounding": "Intro",
    "facet": "novelty"
  },
  {
    "kind": "weakness",
    "text": "The paper does not explicitly compare its self-ask method with the methods presented in [1] and [2]. While the paper mentions that the self-ask method is faster and more scalable than manually breaking down complex questions, a direct comparison in terms of performance on the same tasks would strengthen the claims.",
    "grounding": "Related Work",
    "facet": "comparison"
  },
  {
    "kind": "suggestion",
    "text": "Conduct experiments comparing the performance of self-ask with the methods in [1] and [2] on the same multi-hop question answering tasks. This comparison should include the compositionality gap metric to evaluate the effectiveness of each method in addressing the issue.",
    "grounding": "Sec 4.1",
    "facet": "experiment"
  },
  {
    "kind": "weakness",
    "text": "The paper does not fully address the limitations of the self-ask method. For example, it does not discuss the potential for the model to generate incorrect follow-up questions or how to mitigate this issue.",
    "grounding": "Sec 4.1",
    "facet": "limitations"
  },
  {
    "kind": "suggestion",
    "text": "Include a discussion on the potential limitations of the self-ask method, such as the generation of incorrect follow-up questions. Propose potential solutions or future research directions to address these limitations.",
    "grounding": "Sec 5",
    "facet": "future_work"
  }
]