[
  {
    "rebuttal": "We thank the reviewer for their careful reading and insightful feedback. We address each point below:\n\n**Weaknesses:**\n\n*   **Comparison with other prompting techniques (Sec 2):** The reviewer requests a more detailed comparison of self-ask with other prompting techniques. We believe this is already addressed in the paper. Section 3.4 and Tables 1 and 2 directly compare self-ask with direct prompting and chain-of-thought on three datasets (Bamboogle, 2WikiMultiHopQA, and Musique). We show that self-ask outperforms chain-of-thought on Bamboogle by a significant margin (11% absolute) and achieves comparable or better performance on the other datasets. We also provide the prompts used for each method in the Appendix (Tables 8-13). We respectfully disagree that this comparison is insufficient.\n\n*   **Limitations due to lack of ChatGPT/GPT-4 details (Sec 2):** We acknowledge the limitations regarding the analysis of ChatGPT and GPT-4. As stated in Section 1, \"There are no publicly shared details about the training data or model architecture for ChatGPT and GPT-4 (Rogers et al., 2023), so we can not analyze their impact on our finding that the compositionality gap does not narrow with scale.\" We have clearly stated the limitations imposed by the lack of information about these models.\n\n*   **Seeds and Variance (Section 3.4, 3.5):** The reviewer is correct that we did not explicitly mention the use of seeds or provide information on the variance of the results. This is an important point, and we will address it in the next version. We will specify the random seeds used for all experiments and report the variance (e.g., standard deviations or confidence intervals) of the results. We will also provide the code used for the experiments, including the prompts and the implementation of the methods.\n\n*   **Overclaiming impact of search engine (Sec 3.6):** We believe the impact of the search engine integration is accurately presented. Section 3.6 states, \"Integrating the search engine into self-ask further improves performance on all datasets, sometimes by as much as 10% (absolute).\" We specify the datasets (Bamboogle, 2WikiMultiHopQA, and Musique) and the conditions (Davinci-002) in Section 3.4 and Table 1. We will clarify the exact datasets and conditions in the next version.\n\n*   **Figure 6 labels:** We acknowledge the lack of clear axis labels and units in Figure 6. We will add these labels to improve clarity.\n\n*   **Statistical information in tables (Tables 1, 14, 15):** We agree that Tables 1 and 14 would benefit from statistical information. We will include standard deviations or confidence intervals for the accuracy and speed metrics in Table 2. We will also add p-values to Table 1 to indicate the statistical significance of the differences between the methods. For Table 15, we will quantify the rate of full-sentence answers for each method to provide a more objective comparison.\n\n*   **Definition of 'elicitive prompting' (Abstract, Introduction):** We believe the definition of 'elicitive prompting' is sufficiently clear. We define it in Section 1 as prompts that \"lets the model 'talk things through' before answering.\" We will consider adding more detail to the definition in the introduction.\n\n*   **Notation (Throughout the paper):** We acknowledge the lack of a dedicated section for notation. We will include a table of notation to clarify the symbols and abbreviations used throughout the paper.\n\n*   **Dataset bias, societal impact, and harmful content:** We acknowledge the importance of addressing potential biases, societal impacts, and the generation of harmful content. We will add a section discussing these aspects and potential mitigation strategies in the next version.\n\n*   **Comparison with methods in [1] and [2] (Related Work):** The reviewer requests a direct comparison with methods in [1] and [2]. We will add a comparison of self-ask with the methods in [1] and [2] on the same multi-hop question answering tasks, including the compositionality gap metric.\n\n*   **Limitations of self-ask (Sec 4.1):** We agree that the potential for incorrect follow-up questions is a limitation of self-ask. We will include a discussion on this limitation and propose potential solutions or future research directions in the next version.\n\n**Suggestions:**\n\n*   **Head-to-head comparison on Compositional Celebrities:** We will consider adding a head-to-head comparison on the Compositional Celebrities dataset, but we believe the current comparisons on other datasets are sufficient to demonstrate the effectiveness of self-ask.\n\n*   **Ablation study:** We will consider conducting an ablation study to isolate the impact of each component of the self-ask method.\n\n*   **Dataset details:** We believe the dataset details are sufficiently described in Section 2 and Appendix A.2. We will consider adding more statistics on question complexity and the types of reasoning required.\n\n*   **Figure 5 y-axis label:** We will add a y-axis label to Figure 5.\n\n*   **Pseudocode/algorithmic description:** We will consider adding pseudocode or a clear algorithmic description of the self-ask method.\n\nWe appreciate the reviewer's thoroughness and will incorporate these changes to improve the paper."
  }
]