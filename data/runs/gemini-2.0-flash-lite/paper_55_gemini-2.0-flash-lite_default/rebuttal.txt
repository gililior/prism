[
  {
    "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback. We appreciate the time and effort invested in reviewing our paper. We address each point below:\n\n**Weaknesses:**\n\n*   **Lack of Statistical Analysis:** We acknowledge the lack of standard deviations, confidence intervals, and p-values in Tables 3, 4, 5, and 7. We agree that this limits the statistical rigor of our claims. *In the next version, we will include standard deviations and, where appropriate, p-values to indicate statistical significance.* We will also report the seeds used and the variance across multiple runs to improve reproducibility.\n\n*   **Reproducibility Concerns:** The reviewer correctly points out the lack of information regarding seed usage, environment details, and variance. *We will address this by including the seeds used for all experiments and providing a more detailed description of the experimental environment, including software versions and hardware specifications. We will also provide a link to a public repository (e.g., GitHub) containing our code and a Dockerfile to facilitate reproducibility.* We will also report the variance across multiple runs.\n\n*   **Generalizability of Findings:** We acknowledge that our conclusions are based on a limited set of GLUE tasks. *We will expand our experiments to include a wider range of tasks and datasets to better assess the generalizability of our findings.* We will also include a discussion of the limitations of our current evaluation.\n\n*   **Evidence for CoLA Improvement:** The reviewer questions the evidence supporting our claim that the improvement in the CoLA task is due to capturing coordination structures. While we provide qualitative analysis and cite relevant literature (Warstadt and Bowman, 2019), we agree that more direct evidence is needed. *In the next version, we will conduct a more detailed analysis of the specific syntactic features learned by the models and how they contribute to the performance improvement on different tasks. This will include an ablation study to isolate the impact of coordination structures.*\n\n*   **Figure 2 and Terminology:** The reviewer points out the lack of axis labels, units, and a clear legend in Figure 2, as well as the absence of a clear definition for 'additional syntactic training.' We agree that these are valid concerns. *We will add axis labels and units to Figure 2 and provide a clear legend explaining the different lines/colors. We will also include a concise definition of 'additional syntactic training' early in the introduction (Section 1) and provide a table of notation to define all symbols and acronyms used in the methods section (Section 2, Table 1).* The term 'additional syntactic training' is defined in the introduction (Section 1) as embedding syntactic knowledge into a model while preserving the original semantic knowledge.\n\n*   **Notation and Methods:** We acknowledge the lack of a clear explanation of the notation used, especially in the methods section. *We will include a table of notation to define all symbols and acronyms used in the methods section.*\n\n*   **Misuse, Fairness, and Societal Impacts:** We agree that the paper lacks a detailed analysis of potential misuse scenarios, fairness considerations, and broader societal impacts. *We will add a section in the paper that explicitly addresses potential misuse cases, such as generating biased or harmful content, and propose mitigation strategies. We will also include a discussion on fairness, including an analysis of potential biases in the training data and the model's outputs. We will expand the ethics statement to include a more detailed discussion of the potential societal impacts of the research, including both positive and negative consequences.*\n\n*   **Dataset Licensing:** We acknowledge the lack of explicit license and consent statements for datasets used. *We will add explicit license and consent statements for datasets used.*\n\n**Suggestions:**\n\n*   **Head-to-Head Comparison:** While we did not include a direct head-to-head comparison with a contemporaneous method, we believe our work provides valuable insights into the impact of different syntactic tasks and optimization functions. *We will consider including a head-to-head comparison with a relevant contemporaneous method in the next version.*\n\n*   **Code, Data, and Environment Details:** We plan to provide code, data, and environment details (e.g., Dockerfile, Conda environment) to enhance reproducibility. (GitHub/CodeOcean/Zenodo)\n\n*   **Detailed Analysis of Syntactic Features:** We plan to provide a more detailed analysis of the specific syntactic features learned by the models and how they contribute to the performance improvement on different tasks. (Sec 6)\n\n*   **Axis Labels, Units, and Statistical Significance:** We will add axis labels and units to figures, and provide a clear legend explaining the different lines/colors. Include standard deviations or confidence intervals to indicate the variability of the results. Add p-values to indicate statistical significance. Clarify headers to improve readability. (Figure 2, Tables 3, 4, 5, 6, 7)\n\n*   **Definition of 'Additional Syntactic Training' and Notation Table:** We will provide a concise definition of 'additional syntactic training' early in the introduction and include a table of notation to define all symbols and acronyms used in the methods section. (Section 1, Section 2, Table 1)\n\n*   **Misuse, Fairness, and Societal Impacts:** We will include a section in the paper that explicitly addresses potential misuse cases, such as generating biased or harmful content, and propose mitigation strategies. Add a discussion on fairness, including an analysis of potential biases in the training data and the model's outputs. Expand the ethics statement to include a more detailed discussion of the potential societal impacts of the research, including both positive and negative consequences. (Insufficient evidence)"
  }
]