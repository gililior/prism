[
  {
    "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback. We appreciate the time taken to thoroughly review our paper and provide constructive criticism. We address each point below:\n\n**Weaknesses:**\n\n*   **Weakness 1:** The paper claims that decomposing complex claims is valuable, but this is not fully substantiated experimentally, and the generalizability to open-domain settings is overclaimed. (Sec 7, Sec 5)\n    *   **Response:** We acknowledge that the experimental validation of claim decomposition's impact could be strengthened. While Section 4 does show improvements in entailment classification using Claim-Split, we agree that more direct experimentation is needed. We will add a new experiment in the next version. We will also add a discussion of the limitations of our current open-domain claims in Section 7, and add a discussion of how the dataset could be extended to open-domain settings.\n\n*   **Weakness 2:** The paper lacks a clear comparison with the closest baseline, and the delta between WICE and existing datasets/methods is not clearly quantified. (Related Work, Sec 4)\n    *   **Response:** We respectfully disagree. Section 4 and the related work section do discuss and compare WICE to existing datasets. We compare our results to those of Laban et al. (2022) and Schuster et al. (2021) in Table 4, and we discuss the differences between WICE and FEVER and VitaminC in Section 2.5 and Table 3. We will clarify these comparisons further in the revised manuscript.\n\n*   **Weakness 3:** The evaluation section needs more details, and the performance gap is not quantified. (Sec 4)\n    *   **Response:** We partially addressed this. Section 3.1 and Table 4 quantify the performance gap between models and human performance. We will clarify the evaluation metrics used in the results section and provide more details on the specific models evaluated.\n\n*   **Weakness 4:** Some figures and tables lack clarity. (Figures 3, 6, 7, Table 1, Table 4)\n    *   **Response:** We will revise Figures 3, 6, and 7 to include more descriptive axis labels and legends. We will add standard deviations or confidence intervals to Table 1 and ensure all tables have clear headers and descriptions.\n\n*   **Weakness 5:** The paper does not explicitly state the license or address consent and privacy. (Insufficient evidence)\n    *   **Response:** We acknowledge this omission. We will add a section to the paper explicitly stating the dataset's license, usage terms, and addressing consent and privacy considerations related to the use of Wikipedia data and GPT-3.5.\n\n*   **Weakness 6:** The term 'Claim-Split' is introduced without a clear definition, and the paper lacks a dedicated section for notation. (Intro ยง1, Methods ยง2.1)\n    *   **Response:** We will provide a more detailed explanation of the Claim-Split method in the introduction and methods sections, including the prompt used with GPT-3.5. We will also include a table of notation to define all symbols and abbreviations used.\n\n*   **Weakness 7:** No discussion of potential misuse or failure modes. (Insufficient evidence)\n    *   **Response:** We will add a section to the paper discussing potential misuse and failure modes of the dataset and the Claim-Split method.\n\n**Suggestions:**\n\n*   **Suggestion 1:** Conduct experiments on the impact of claim decomposition and in open-domain settings. (Sec 7, Sec 5)\n    *   **Response:** We will add a new experiment in the next version to evaluate the impact of claim decomposition on entailment prediction performance. We will also add a discussion of the limitations of our current open-domain claims in Section 7, and add a discussion of how the dataset could be extended to open-domain settings.\n\n*   **Suggestion 2:** Include a head-to-head comparison with a state-of-the-art document-level entailment model. (Sec 4)\n    *   **Response:** We will include a head-to-head comparison with a state-of-the-art document-level entailment model in the next version.\n\n*   **Suggestion 3:** Quantify the performance gap and clarify the evaluation metrics. (Sec 4, Results)\n    *   **Response:** We will clarify the evaluation metrics used in the results section and provide more details on the specific models evaluated.\n\n*   **Suggestion 4:** Add more descriptive axis labels and legends, and include standard deviations or confidence intervals. (Figures 3, 6, 7, Table 1, Table 4)\n    *   **Response:** We will revise Figures 3, 6, and 7 to include more descriptive axis labels and legends. We will add standard deviations or confidence intervals to Table 1 and ensure all tables have clear headers and descriptions.\n\n*   **Suggestion 5:** Include a clear statement about the dataset's license, usage terms, and any relevant consent or privacy considerations. (Insufficient evidence)\n    *   **Response:** We will add a section to the paper explicitly stating the dataset's license, usage terms, and addressing consent and privacy considerations related to the use of Wikipedia data and GPT-3.5.\n\n*   **Suggestion 6:** Provide a more detailed explanation of the Claim-Split method and include a table of notation. (Intro ยง1, Methods ยง2.1, Results)\n    *   **Response:** We will provide a more detailed explanation of the Claim-Split method in the introduction and methods sections, including the prompt used with GPT-3.5. We will also include a table of notation to define all symbols and abbreviations used.\n\n*   **Suggestion 7:** Add a Broader Impact section. (Conclusion)\n    *   **Response:** We will add a Broader Impact section with mitigation strategies in the conclusion.\n\n*   **Suggestion 8:** Conduct experiments comparing the performance of models trained on WICE with models trained on FEVER and VitaminC. (Related Work)\n    *   **Response:** We will add experiments comparing the performance of models trained on WICE with models trained on FEVER and VitaminC in the next version."
  }
]