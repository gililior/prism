[
  {
    "rid": "pXpYHwQuz7",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper investigates how to simulate a specific person with large language models (LLMs). Different from previous works that prompt existing LLM, the authors construct the experience of that person and then fine-tune an LLM on the experience dataset. The experience consists of (1) character experience generated by another LLM based on the profile, and (2) protective experience that shows the person has no idea about unrelated knowledge. Evaluated by GPT 3.5, the trained model performs better than Alpaca, Vicuna, and is comparable with ChatGPT, in several aspects such as memorizing the character profile and reducing hallucination.",
      "reasons_to_accept": "1. This paper proposes an experience reconstruction method to extend a character's profile to many detailed scenes that mimic interactions between the target character and other characters. The experience can provide more information for character simulation. This method may inspire future work on character data construction. \n2. This paper proposes constructing and using protective experience to reduce hallucination during role-playing. Trained on protective experience, the model can pretend to be ignorant of the knowledge that the character does not know. The character hallucination problem is important, and this work gives some insight.",
      "reasons_to_reject": "1. Lack of important details. It's unclear which LLM did the authors use for scene extraction and experience generation (both character experience and protective experience). Does the selection of this model important? Does the generated experience faithful and without hallucination? How many scenes are there in the protective experience for training? For evaluation, how are the questions/topics selected? \n2. Lack of ablation study to show the superiority of the proposed experience construction method. I think adding a comparison between models fine-tuned on different kinds of character data helps.",
      "questions_for_the_authors": "See above",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  },
  {
    "rid": "LdCxndmrbd",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper presents a novel approach for training trainable agents to simulate specific individuals by providing them with profiles and experiences. The authors propose a framework that includes experience reconstruction, experience upload, and protective experiences. They evaluate the trained agents through interviews and compare them with baseline models. The results show that the trainable agents outperform the baselines in terms of personality, memorization, hallucination, and stability. The paper provides valuable insights into building better character simulacra.",
      "reasons_to_accept": "(1) The paper introduces a novel approach for training trainable agents to simulate specific individuals, which is a step closer to character simulacra. \n(2) The framework proposed by the authors, including experience reconstruction, experience upload, and protective experiences, is well-structured and practical. \n(3) The evaluation process, including interviews and AI-based judging, provides comprehensive insights into the performance of the trained agents.",
      "reasons_to_reject": "(1) The paper lacks implementation details, such as the specific architecture and training settings used for the trainable agents. \n(2) The evaluation could benefit from comparisons with more baselines and a larger number of interview questions. \n(3) The paper could provide more in-depth discussions on the limitations and potential ethical concerns of using trainable agents.",
      "questions_for_the_authors": "(1) Could you please provide more details about the implementation of the trainable agents, such as the specific architecture and training settings used? \n(2) Have you considered comparing the trained agents with more baselines, such as other instruction-tuned models or models trained with different methodologies? \n(3) Can you discuss the potential ethical concerns of using trainable agents and how you plan to address them?",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",
      "excitement": "2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",
      "reproducibility": "2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  },
  {
    "rid": "WNkPFljv4L",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper introduces an innovative pipeline designed to construct trainable agents on LLMs that can convincingly emulate specific historical figures, such as Beethoven. The pipeline commences with the collection of comprehensive profiles for the chosen characters. These profiles are then utilized to prompt the LLMs to generate detailed scenes and experiences that are characteristic of these figures. Subsequently, a specialized LLM is fine-tuned based on these character-specific experiences, resulting in a convincing simulacrum of the character. Experimental evaluations, conducted using ChatGPT as a benchmark, demonstrate that the performance of these character simulacra is comparable to ChatGPT.",
      "reasons_to_accept": "1. The paper introduces a novel pipeline for creating trainable agents on LLMs that can emulate specific characters. This approach is innovative and could have wide-ranging applications in various fields, such as virtual assistants, education, and entertainment.\n2. This paper presents a thorough process that covers everything from gathering character profiles to refining LLMs using those profiles. This meticulous method could serve as a guide for other scholars in the same field.\n3. The paper also discusses potential flaws and areas for improvement in the proposed approach, indicating a thorough and thoughtful analysis.",
      "reasons_to_reject": "1. While the Protective Experience method is used to reduce hallucination issues, there is a possibility that it may result in bias or incorrect information being incorporated into the agent. This is a matter of concern for future usage.\n2. Including a human evaluation could potentially increase its credibility.",
      "questions_for_the_authors": "1. In your experiments with RLHF-trained or distilled from RLHF models such as ChatGPT and Vicuna, have you noticed them generating safe responses like \"As an AI, I can't...\"? If so, how do you handle these situations in your evaluation?\n2. Do you notice a preference for ChatGPT agents evaluated by humans since you use GPT as the evaluator?\n3. Do you observe hallucinations in scenes? If so, how do you prevent them?",
      "missing_references": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization - Kim et al.",
      "typos_grammar_style_and_presentation_improvements": "L447 OpenAI",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  }
]