{
  "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback. We appreciate the time and effort invested in reviewing our paper. We address each point below:\n\n**Weaknesses:**\n\n*   **Lack of information on seeds, environment, and reproducibility:** The reviewer is correct that this information is crucial. While we provide some details, we acknowledge that the paper lacks explicit specification of random seeds and detailed environment information. We will include the specific random seeds used for all experiments in the revised version. We will also provide a more detailed description of the hardware and software environment, including specific versions of dependencies, to enhance reproducibility. (Sec 6, Sec 7, Sec 8)\n\n*   **Lack of detailed comparison with concurrent work:** We believe the paper already addresses this to some extent. Section 2, \"Related Work,\" discusses concurrent work, including Stanford Alpaca. We compare Baize's performance with Alpaca and Vicuna in Figure 3 and Table 7. However, we agree that a more in-depth head-to-head comparison, evaluating performance on standard benchmarks, would be beneficial. We will incorporate this in the revised version.\n\n*   **Novelty of SDF and lack of detailed explanation:** We acknowledge that the explanation of SDF could be more detailed. We will expand the description of SDF in Section 4, including a more thorough explanation of the process and its advantages. We will also add a dedicated subsection to highlight the novelty and provide more comparative evidence.\n\n*   **Dataset license and usage restrictions:** The reviewer is correct. We will explicitly state the dataset licenses and usage restrictions in the revised version, clarifying that the fine-tuning corpus is under CC-BY-NC 4.0 and the model weights are for research use only, as stated in Section 7.\n\n*   **Figure 3 and Table deficiencies:** We will add axis labels and a legend to Figure 3. We acknowledge the lack of quantitative analysis in Tables 1, 4, 5, 6, 8, and 9. While these tables are primarily for qualitative illustration, we will consider adding a quantitative evaluation of the responses where feasible, perhaps using metrics like response length or GPT-4's scoring.\n\n*   **Lack of clear definition of \"parameter-efficient tuning\":** We believe this is partially addressed in Section 4, where we describe the LoRA method. However, we will add a more explicit definition of parameter-efficient tuning in the introduction and abstract, clarifying the general concept and the specific techniques used.\n\n*   **Lack of analysis of potential biases and misuse:** We acknowledge this is a critical area. We will add a dedicated section discussing potential biases in the generated corpus and the model, as well as potential misuse scenarios and mitigation strategies. We will also expand the discussion in Section 7, \"Safety and Access Control.\"\n\n**Suggestions:**\n\n*   **Code, data, and training scripts availability:** We plan to release the code, data, and training scripts to facilitate reproducibility. We will provide a link to a public repository in the final version.\n\n*   **Specify random seeds and report variance:** As mentioned above, we will include the random seeds and report the variance of the results.\n\n*   **Head-to-head comparison with Alpaca and other models:** We will incorporate this as described above.\n\n*   **Ablation studies for SDF and corpus quality analysis:** We will conduct an ablation study to isolate the impact of SDF and analyze the quality of the generated chat corpus. We will also compare the performance of Baize against Alpaca-LoRA on a common evaluation dataset.\n\n*   **Add explicit license and consent statements and labels to Figure 3:** We will implement these suggestions.\n\n*   **Include standard deviations or confidence intervals and quantitative evaluation:** We will consider adding standard deviations or confidence intervals to Table 7 and quantitative evaluation to Tables 4, 5, 6, 8, and 9.\n\n*   **More detailed explanation of parameter-efficient tuning and SDF:** We will implement these suggestions.\n\n*   **Include a section on potential biases and mitigation strategies and misuse scenarios:** We will implement these suggestions.\n\n*   **Ablation study and comparison with Alpaca-LoRA:** We will implement these suggestions.\n\nWe believe that addressing these points will significantly improve the clarity, rigor, and impact of our paper. Thank you again for your valuable feedback."
}