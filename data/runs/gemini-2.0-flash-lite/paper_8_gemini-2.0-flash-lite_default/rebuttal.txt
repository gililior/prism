[
  {
    "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback, which has helped us identify areas for improvement in our paper. We address each point below:\n\n**Weaknesses:**\n\n*   **Reproducibility (Lack of random seeds, computational environment, and variance):** We acknowledge this is a crucial aspect. While we did not explicitly state the random seeds, computational environment, and variance in the original submission, we will include them in the revised version. We will provide the random seeds used for all experiments in the Appendix and report the standard deviation of the results across multiple runs with different seeds. We will also include details of the computational environment (hardware, software, and libraries) used for the experiments, ideally with a Dockerfile or environment.yml file in the Appendix. \n\n*   **Substantiation of Outperforming Unsupervised Methods:** We respectfully disagree that our claims are unsubstantiated. Table 1 directly compares our CMKT with the unsupervised method DSCNet (Liu et al., 2022c), showing improvements across multiple IoU thresholds on both datasets. We will strengthen this by including a head-to-head comparison with DSCNet, providing more detailed quantitative results, and clarifying the comparison in Section 4.1 and 4.5.\n\n*   **Architectural/Algorithmic Novelty and Knowledge Transfer:** We believe the reviewer may have overlooked the details of our approach. We explicitly state our novelty lies in transferring cross-modal knowledge from other tasks to unsupervised TSG. The specific knowledge being transferred is detailed in Section 3.2 (appearance knowledge from Image-Noun) and Section 3.3 (action knowledge from Video-Verb). The mechanism of transfer is described in Section 3.4, where we use cosine similarity to search for relevant knowledge and then re-represent the query features using the collected knowledge. We will clarify these points further in the revised version.\n\n*   **Statistical Analysis (Lack of confidence intervals, p-values, and standard deviations):** We acknowledge this limitation. We will include standard deviations and p-values in Tables 1-6 to provide a more rigorous statistical analysis of our results.\n\n*   **Figure 3 Clarity:** We agree that Figure 3 can be improved. We will add clear axis labels (e.g., \"Parameter Value\" and \"Performance Metric\") and specify the units where applicable. We will also add a legend to improve clarity.\n\n*   **Definition of 'Appearance Knowledge' and 'Action Knowledge':** We will add a dedicated subsection in Section 3.1 to formally define the terms 'appearance knowledge' and 'action knowledge' and clarify their role within the CMKT network. The reviewer may have overlooked the descriptions in the Abstract, Introduction, and Figure 1(b), which provide initial context.\n\n*   **Notation Table:** We acknowledge the lack of a notation table. We will add a notation table to define all symbols and abbreviations used in the equations and throughout the paper in the revised version.\n\n*   **Comparison with Weakly-Supervised and Unsupervised Methods [1] and [2]:** We will add a comparison with the methods in [1] and [2] on the same dataset in the revised version.\n\n**Suggestions:**\n\n*   We will incorporate the suggestions regarding reproducibility, statistical analysis, and figure clarity as described above.\n\n*   **Detailed Ablation Study:** We already provide a detailed ablation study in Table 2, analyzing the contribution of each component. We will further strengthen this by ablating the appearance and action knowledge transfer modules separately to analyze their individual contributions to the overall performance.\n\n*   **Copy-Paste Approach Explanation:** We will provide a more detailed explanation of the 'copy-paste approach' in Section 3.3.\n\n*   **Broader Impact Section:** We will add a Broader Impact section with mitigation strategies in the revised version.\n\nWe believe these revisions will significantly improve the clarity, rigor, and impact of our paper. Thank you again for your valuable feedback."
  }
]