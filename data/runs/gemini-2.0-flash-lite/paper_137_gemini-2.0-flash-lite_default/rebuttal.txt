[
  {
    "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback. We appreciate the time and effort invested in reviewing our paper. We address each point below:\n\n**Weaknesses:**\n\n*   **Lack of details on datasets, translation, and seeds (Sec 5):** We acknowledge the need for enhanced reproducibility. While Section 3.1 and Appendix B provide details on the English datasets and the translation process (Microsoft Azure), we agree that Section 5 could be more explicit about the specific datasets used for finetuning and the seeds. We will add a new subsection in Section 5 detailing the exact datasets used for finetuning (including links to their open-source repositories), the parameters of the translation service, and the random seeds used for all experiments. We will also include the software environment and hardware used for training and evaluation in the Appendix F. The reviewer may have overlooked the details in Section 3.1 and Appendix B, which already describe the English datasets and translation service.\n\n*   **Insufficient details on discriminative metric selection (Related Work):** We believe the Related Work section provides sufficient rationale. We selected PoE and FineD-Eval as representative discriminative metrics because they are SoTA and well-cited in the open-domain dialogue evaluation literature. We will add a sentence in Section 2 clarifying this selection rationale, explicitly mentioning their SoTA status and citation count. The reviewer may have overlooked the fact that we explicitly state that we selected SoTA BERT-based metrics in Section 4.\n\n*   **Missing axis labels, legends, and statistical measures (Figures & Tables):** We acknowledge this weakness and will address it in the revised version. We will add axis labels with units to all figures (Figures 1, 2, and 3). We will include clear legends to identify the different models or metrics. We will also include standard deviations and confidence intervals for all reported scores in Tables 2, 4, 5, 15, 16, and 17. We will add p-values to indicate statistical significance of performance differences where appropriate. The reviewer is correct that these additions will significantly improve the clarity and rigor of our presentation.\n\n*   **Lack of discussion on risks, fairness, and societal impacts:** We agree that this is an important omission. We will add a new section to the end of the paper (before the Ethics Statement) dedicated to discussing potential risks, fairness considerations, and broader societal impacts. This section will address the potential for misuse of dialogue evaluation metrics, bias, and the positive impacts on online conversations. We will also include mitigation strategies.\n\n*   **Clarification of 'discriminative' and 'generative' categories (Intro ยง1):** We believe the definitions are clear in the Introduction. Discriminative metrics are defined as those trained to distinguish between good and bad responses, while generative metrics directly assess the quality of generated text. We will add a sentence in Section 1 to further clarify these definitions, perhaps using different wording to avoid any confusion. The reviewer may have overlooked the definitions provided in Section 1.\n\n*   **Missing head-to-head comparison with a top cited baseline (Related Work):** We respectfully disagree. Our paper already includes comparisons with ChatGPT, which is a top-cited baseline in the field. We will add a sentence in Section 2 explicitly stating that ChatGPT is a top-cited baseline. The reviewer may have overlooked the comparisons with ChatGPT throughout the paper, especially in Section 6.\n\n**Suggestions:**\n\n*   **Detailed description of synthetic multilingual dialogue data (Sec 5):** We will expand the description of the synthetic multilingual dialogue data in Section 5, including details on its creation process. This will involve clarifying how the synthetic data was generated and its characteristics. This is related to our response to the first weakness.\n\n*   **Report seeds and variance (Section 5):** We will implement this suggestion, as described in our response to the first weakness. This will significantly improve the reproducibility of our work.\n\n*   **Add axis labels, legends, and statistical measures:** We will implement this suggestion, as described in our response to the third weakness.\n\n*   **Include a section on potential risks, fairness, and societal impacts:** We will implement this suggestion, as described in our response to the fourth weakness.\n\n*   **Add ablation isolating the delta vs. cited method (Sec 4.1):** We are not sure what the reviewer means by \"delta vs. cited method\". We are not aware of any cited method that is directly comparable to our work. We will clarify this point in Section 4.1.\n\nWe believe that these revisions will significantly improve the clarity, rigor, and impact of our paper. Thank you again for your valuable feedback."
  }
]