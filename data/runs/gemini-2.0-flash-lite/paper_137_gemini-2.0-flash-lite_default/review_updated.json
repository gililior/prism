{
  "summary": "This paper introduces xDial-Eval, a multilingual dialogue evaluation benchmark, and assesses the performance of various discriminative and generative metrics. The paper presents a valuable contribution to multilingual dialogue evaluation. The authors have addressed some of the weaknesses raised in the initial review, particularly regarding reproducibility and statistical rigor. However, some concerns remain regarding the clarity of the methodology and the depth of the discussion on risks and societal impacts.",
  "strengths": [
    {
      "kind": "strength",
      "text": "The paper introduces xDial-Eval, a multilingual open-domain dialogue evaluation benchmark, addressing the limited focus on languages other than English in automatic dialogue evaluation research. (Intro) [originality]",
      "grounding": "Intro",
      "facet": "originality"
    },
    {
      "kind": "strength",
      "text": "The paper details the LLMs used, including their backbones and instruction-tuned variants, which aids reproducibility. (Sec 5) [methods]",
      "grounding": "Sec 5",
      "facet": "methods"
    },
    {
      "kind": "strength",
      "text": "The figures generally support the claims made in the results section. (Figures 1, 2, 3) [figures]",
      "grounding": "Figures 1, 2, 3",
      "facet": "figures"
    },
    {
      "kind": "strength",
      "text": "Tables 5 and 17 highlight the best scores for each language, which aids in quick comparison. The use of bolding and symbols to denote specific model characteristics is helpful. (Tables 5, 17) [tables]",
      "grounding": "Tables 5, 17",
      "facet": "tables"
    },
    {
      "kind": "strength",
      "text": "The introduction clearly outlines the problem, the proposed solution, and the paper's contributions. (Intro §1) [clarity_presentation]",
      "grounding": "Intro §1",
      "facet": "clarity_presentation"
    },
    {
      "kind": "strength",
      "text": "The paper clearly distinguishes contributions and method differences from closely related approaches. (Intro §1.2) [related_work]",
      "grounding": "Intro §1.2",
      "facet": "related_work"
    }
  ],
  "weaknesses": [
    {
      "kind": "weakness",
      "text": "The paper lacks details on the specific datasets used for finetuning, hindering reproducibility. (Sec 5) [reproducibility]",
      "grounding": "Sec 5",
      "facet": "reproducibility"
    },
    {
      "kind": "weakness",
      "text": "The paper does not provide sufficient details on the selection of existing discriminative metrics and the rationale behind their choice for comparison. (Related Work) [relation to prior work]",
      "grounding": "Related Work",
      "facet": "relation to prior work"
    },
    {
      "kind": "weakness",
      "text": "The axes in the figures lack clear labels and units, and legends are missing or unclear. Several tables lack standard statistical measures such as standard deviation, confidence intervals, and p-values, making it difficult to assess the significance of the results. (Figures 1, 2, 3, Tables 1, 3, 6-14) [tables]",
      "grounding": "Figures 1, 2, 3, Tables 1, 3, 6-14",
      "facet": "tables"
    },
    {
      "kind": "weakness",
      "text": "The paper lacks a discussion of potential risks, fairness considerations, and broader societal impacts associated with the proposed evaluation metrics. (Insufficient evidence) [risks]",
      "grounding": "Insufficient evidence",
      "facet": "risks"
    },
    {
      "kind": "weakness",
      "text": "The definition of 'discriminative category' and 'generative category' could be clarified. (Intro §1) [terminology]",
      "grounding": "Intro §1",
      "facet": "terminology"
    }
  ],
  "suggestions": [
    {
      "kind": "suggestion",
      "text": "Provide a more detailed description of the synthetic multilingual dialogue data, including its creation process and characteristics. (Sec 5) [methods]",
      "grounding": "Sec 5",
      "facet": "methods"
    }
  ],
  "scores": null,
  "overall": null,
  "confidence": null
}