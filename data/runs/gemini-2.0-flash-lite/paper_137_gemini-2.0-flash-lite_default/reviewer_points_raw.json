[
  {
    "kind": "summary",
    "text": "The paper investigates multilingual dialogue evaluation using LLMs, exploring zero-shot prompting and finetuning strategies. It examines crosslingual generalization, the impact of instruction tuning, and ensemble methods.",
    "grounding": "Introduction and Sec 5",
    "facet": "methods"
  },
  {
    "kind": "strength",
    "text": "The paper details the LLMs used, including their backbones and instruction-tuned variants, which aids reproducibility.",
    "grounding": "Sec 5",
    "facet": "methods"
  },
  {
    "kind": "weakness",
    "text": "The paper lacks details on the specific datasets used for finetuning, making it difficult to assess reproducibility.",
    "grounding": "Sec 5",
    "facet": "methods"
  },
  {
    "kind": "weakness",
    "text": "The ensemble method is described as a simple arithmetic mean, but the rationale and potential benefits are not thoroughly discussed.",
    "grounding": "Sec 5",
    "facet": "methods"
  },
  {
    "kind": "suggestion",
    "text": "Provide a more detailed description of the synthetic multilingual dialogue data, including its creation process and characteristics.",
    "grounding": "Sec 5",
    "facet": "methods"
  },
  {
    "kind": "suggestion",
    "text": "Compare the performance of the ensemble method with other ensemble techniques, such as weighted averaging or stacking.",
    "grounding": "Sec 5",
    "facet": "methods"
  },
  {
    "kind": "question",
    "text": "Are the random seeds used for finetuning and evaluation reported? If yes, it would increase the score.",
    "grounding": "Sec 5",
    "facet": "reproducibility"
  },
  {
    "kind": "question",
    "text": "What is the variance of the evaluation metrics? Reporting standard deviations or confidence intervals would increase the score.",
    "grounding": "Sec 5",
    "facet": "analysis"
  },
  {
    "kind": "question",
    "text": "Are the hyperparameter settings for finetuning the LLMs available? If yes, it would increase the score.",
    "grounding": "Sec 5",
    "facet": "reproducibility"
  },
  {
    "kind": "limitations",
    "text": "Yes. The paper does not provide enough information to fully reproduce the experiments.",
    "grounding": "Sec 5",
    "facet": "reproducibility"
  },
  {
    "kind": "ethics_flag",
    "text": "No.",
    "grounding": "N/A",
    "facet": "ethics"
  },
  {
    "kind": "strength",
    "text": "The paper introduces xDial-Eval, a multilingual open-domain dialogue evaluation benchmark, addressing the limited focus on languages other than English in automatic dialogue evaluation research.",
    "grounding": "Intro",
    "facet": "originality"
  },
  {
    "kind": "strength",
    "text": "The paper comprehensively assesses the performance of both discriminative and generative metrics on the proposed benchmark, providing a thorough analysis of their capabilities.",
    "grounding": "Intro",
    "facet": "positioning"
  },
  {
    "kind": "weakness",
    "text": "The paper lacks specific details on the selection of existing discriminative metrics and the rationale behind their choice for comparison. It is unclear how representative the selected methods are.",
    "grounding": "Related Work",
    "facet": "relation to prior work"
  },
  {
    "kind": "weakness",
    "text": "The paper does not provide sufficient details on the translation process of the English datasets into other languages, which could impact the quality and comparability of the benchmark.",
    "grounding": "Intro",
    "facet": "originality"
  },
  {
    "kind": "suggestion",
    "text": "Include a detailed comparison of the proposed benchmark with existing multilingual dialogue evaluation datasets, highlighting the key differences in terms of size, language coverage, and annotation quality.",
    "grounding": "Intro",
    "facet": "relation to prior work"
  },
  {
    "kind": "suggestion",
    "text": "Provide a more in-depth analysis of the performance of different LLMs, including a comparison of their performance across different languages and dialogue types.",
    "grounding": "Sec 3.2",
    "facet": "comparative evidence"
  },
  {
    "kind": "question",
    "text": "How does the performance of the proposed metric ensemble compare to the state-of-the-art methods on other multilingual dialogue evaluation benchmarks?",
    "grounding": null,
    "facet": "originality"
  },
  {
    "kind": "question",
    "text": "What is the impact of the quality of machine translation on the performance of the evaluated metrics?",
    "grounding": null,
    "facet": "originality"
  },
  {
    "kind": "question",
    "text": "Are the human annotations for the different languages of comparable quality?",
    "grounding": null,
    "facet": "originality"
  },
  {
    "kind": "limitations",
    "text": "The novelty of the work hinges on the quality and representativeness of the xDial-Eval benchmark and the effectiveness of the proposed metric ensemble.",
    "grounding": null,
    "facet": "originality"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": null,
    "facet": "ethics"
  },
  {
    "kind": "provisional_rating",
    "text": "3",
    "grounding": null,
    "facet": "overall"
  },
  {
    "kind": "summary",
    "text": "The paper investigates multilingual dialogue evaluation using LLMs, exploring zero-shot prompting and finetuning strategies. It examines crosslingual generalization, the impact of instruction tuning, and ensemble methods. The paper lacks details on seeds and variance.",
    "grounding": "Sections 3, 4, 5",
    "facet": "reproducibility"
  },
  {
    "kind": "strength",
    "text": "The paper mentions the use of specific LLM models (PoE, Fined-Eval, Alpaca-7B, Phoenix-7B, LLaMA-2-7B, Baichuan-2-7B, BLOOM-7B) and datasets (xDial-Eval).",
    "grounding": "Section 5, Table 6",
    "facet": "reproducibility"
  },
  {
    "kind": "weakness",
    "text": "The paper does not report the seeds used for training and evaluation, making it impossible to assess the variance of the results.",
    "grounding": "Section 5",
    "facet": "reproducibility"
  },
  {
    "kind": "weakness",
    "text": "The paper does not provide details on the computational resources used, such as the number of GPUs, memory, and runtime.",
    "grounding": "Section 5, Appendix F",
    "facet": "reproducibility"
  },
  {
    "kind": "suggestion",
    "text": "Provide a link to the code repository with the training and evaluation scripts. Include a requirements.txt or a similar file to specify the environment.",
    "grounding": "Code repository placeholder",
    "facet": "reproducibility"
  },
  {
    "kind": "suggestion",
    "text": "Report the seeds used for all experiments and conduct multiple runs to report the variance (e.g., standard deviation) of the results.",
    "grounding": "Section 5",
    "facet": "reproducibility"
  },
  {
    "kind": "suggestion",
    "text": "Specify the hardware and software environment used for training and evaluation (e.g., GPU type, CUDA version, Python version, and library versions).",
    "grounding": "Section 5, Appendix F",
    "facet": "reproducibility"
  },
  {
    "kind": "question",
    "text": "Are the synthetic multilingual dialogue data and the xDial-Eval data publicly available or will they be released?",
    "grounding": "Section 5, Table 6",
    "facet": "reproducibility"
  },
  {
    "kind": "question",
    "text": "Will the authors provide the prompt template used for translation quality evaluation with GPT-4?",
    "grounding": "Table 6",
    "facet": "reproducibility"
  },
  {
    "kind": "limitations",
    "text": "The review is limited by the information provided in the text. Details on compute resources, specific hyperparameters, and the availability of data and code are missing.",
    "grounding": "Paper text",
    "facet": "reproducibility"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "N/A",
    "facet": "ethics"
  },
  {
    "kind": "summary",
    "text": "The figures present correlation results for different models and datasets. The clarity and effectiveness of the figures need improvement.",
    "grounding": "Figures 1-3",
    "facet": "figures"
  },
  {
    "kind": "strength",
    "text": "The figures appear to support the claims made in the results section.",
    "grounding": "Figures 1, 2, 3",
    "facet": "figures"
  },
  {
    "kind": "weakness",
    "text": "The axes in the figures lack clear labels and units. Legends are missing or unclear.",
    "grounding": "Figures 1, 2, 3",
    "facet": "figures"
  },
  {
    "kind": "suggestion",
    "text": "Add axis labels with units to all figures. Include a clear legend to identify the different models or metrics.",
    "grounding": "Figures 1, 2, 3",
    "facet": "figures"
  },
  {
    "kind": "questions",
    "text": "What do the different colors or line styles represent in each visualization? Are the Pearson correlations statistically significant?",
    "grounding": "Figures 1, 2, 3",
    "facet": "figures"
  },
  {
    "kind": "limitations",
    "text": "The visualizations are limited to Pearson correlations and do not provide insights into the underlying data distributions.",
    "grounding": "All figures",
    "facet": "figures"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "N/A",
    "facet": "figures"
  },
  {
    "kind": "weakness",
    "text": "Dataset licensing and usage restrictions not stated.",
    "grounding": "Insufficient evidence",
    "facet": "ethics_licensing"
  },
  {
    "kind": "suggestion",
    "text": "Add explicit license and consent statements for datasets used.",
    "grounding": "Insufficient evidence",
    "facet": "ethics_licensing"
  },
  {
    "kind": "summary",
    "text": "The tables present a variety of information, including dataset statistics, evaluation results, and prompt templates. The quality varies, with some tables lacking crucial statistical information.",
    "grounding": "Tables 1-17",
    "facet": "tables"
  },
  {
    "kind": "strength",
    "text": "Table 5 and 17 highlight the best scores for each language, which aids in quick comparison. The use of bolding and symbols to denote specific model characteristics is helpful.",
    "grounding": "Tables 5, 17",
    "facet": "tables"
  },
  {
    "kind": "weakness",
    "text": "Several tables lack standard statistical measures such as standard deviation, confidence intervals, and p-values, making it difficult to assess the significance of the results. Some tables also lack clear headers and descriptions.",
    "grounding": "Tables 1, 3, 6-14",
    "facet": "tables"
  },
  {
    "kind": "suggestion",
    "text": "Include standard deviations and confidence intervals for all reported scores. Add p-values to indicate statistical significance of performance differences. Ensure all tables have clear and descriptive headers and captions.",
    "grounding": "Tables 2, 4, 5, 15-17",
    "facet": "tables"
  },
  {
    "kind": "questions",
    "text": "1. What statistical tests were used to determine the significance of the differences in performance between the models? 2. Are the datasets used in the tables representative of the broader domain? 3. Why are Hindi and Arabic data excluded from Tables 7 and 8?",
    "grounding": "All tables",
    "facet": "tables"
  },
  {
    "kind": "limitations",
    "text": "The conclusions drawn from the tables are limited by the specific datasets and models used in the experiments. Generalizability to other datasets and models is not directly addressed.",
    "grounding": "All tables",
    "facet": "tables"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "N/A",
    "facet": "tables"
  },
  {
    "kind": "strength",
    "text": "Paper is generally well organized with clear sectioning.",
    "grounding": "Intro \u00a71",
    "facet": "organization"
  },
  {
    "kind": "strength",
    "text": "The introduction clearly outlines the problem, the proposed solution, and the paper's contributions.",
    "grounding": "Intro \u00a71",
    "facet": "clarity_presentation"
  },
  {
    "kind": "weakness",
    "text": "The definition of 'discriminative category' and 'generative category' could be clarified. Are these referring to the two paradigms of automatic evaluation (reference-based and reference-free)?",
    "grounding": "Intro \u00a71",
    "facet": "terminology"
  },
  {
    "kind": "weakness",
    "text": "The paper could benefit from a more detailed explanation of the xDial-Eval benchmark construction process, including the specific translation methods used.",
    "grounding": "Intro \u00a71",
    "facet": "clarity_presentation"
  },
  {
    "kind": "suggestion",
    "text": "Define acronyms (e.g., SoTA) upon first use.",
    "grounding": "Intro \u00a71",
    "facet": "clarity_presentation"
  },
  {
    "kind": "suggestion",
    "text": "Provide a table summarizing the datasets used in xDial-Eval, including their sizes and languages.",
    "grounding": "Intro \u00a71",
    "facet": "organization"
  },
  {
    "kind": "question",
    "text": "What specific machine translation models were used to translate the English data?",
    "grounding": "Intro \u00a71",
    "facet": "clarity_presentation"
  },
  {
    "kind": "question",
    "text": "How does the proposed metric ensemble work? What specific methods are used?",
    "grounding": "Intro \u00a71",
    "facet": "clarity_presentation"
  },
  {
    "kind": "question",
    "text": "What are the limitations of the xDial-Eval benchmark?",
    "grounding": "Intro \u00a71",
    "facet": "clarity_presentation"
  },
  {
    "kind": "limitations",
    "text": "The paper's reproducibility depends on the availability of the xDial-Eval benchmark and the specific LLMs used.",
    "grounding": "All",
    "facet": "reproducibility"
  },
  {
    "kind": "ethics_flag",
    "text": "None",
    "grounding": "All",
    "facet": "ethics"
  },
  {
    "kind": "rating",
    "text": "Overall, the paper is well-structured and addresses an important problem. However, some definitions and details could be clarified to improve understanding.",
    "grounding": "All",
    "facet": "overall"
  },
  {
    "kind": "weakness",
    "text": "The paper does not explicitly address potential risks associated with the use of the proposed evaluation metrics, such as the generation of biased or misleading content. There is no discussion of how the metrics might be misused or the potential negative consequences of their application.",
    "grounding": "Insufficient evidence",
    "facet": "risks"
  },
  {
    "kind": "weakness",
    "text": "The paper does not discuss fairness considerations, such as ensuring that the evaluation metrics are not biased towards any particular language or culture. The potential for bias in the evaluation metrics could lead to unfair or discriminatory outcomes.",
    "grounding": "Insufficient evidence",
    "facet": "fairness"
  },
  {
    "kind": "weakness",
    "text": "The paper lacks a dedicated discussion of the broader societal impacts of the research. While the research has the potential to improve the quality of online conversations and interactions, the paper does not explore these potential benefits or the potential for negative consequences.",
    "grounding": "Insufficient evidence",
    "facet": "broader_impacts"
  },
  {
    "kind": "suggestion",
    "text": "Include a section on potential risks, such as the misuse of dialogue evaluation metrics for generating biased or misleading content. Discuss mitigation strategies.",
    "grounding": "Insufficient evidence",
    "facet": "risks"
  },
  {
    "kind": "suggestion",
    "text": "Address fairness considerations, such as ensuring that the evaluation metrics are not biased towards any particular language or culture. Discuss mitigation strategies.",
    "grounding": "Insufficient evidence",
    "facet": "fairness"
  },
  {
    "kind": "suggestion",
    "text": "Address the broader societal impacts of the research, such as the potential for dialogue evaluation to be used to improve the quality of online conversations and interactions. Discuss mitigation strategies.",
    "grounding": "Insufficient evidence",
    "facet": "broader_impacts"
  },
  {
    "kind": "strength",
    "text": "Clearly distinguishes contributions and method differences from closely related approach [2].",
    "grounding": "Intro \u00a71.2",
    "facet": "related_work"
  },
  {
    "kind": "weakness",
    "text": "Missing head-to-head comparison with a top cited baseline.",
    "grounding": "Related Work",
    "facet": "related_work"
  },
  {
    "kind": "suggestion",
    "text": "Add ablation isolating the delta vs. cited method [1] (what the new module adds).",
    "grounding": "Sec 4.1",
    "facet": "related_work"
  }
]