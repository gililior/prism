[
  {
    "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback. We appreciate the time and effort invested in reviewing our paper. We address each point below:\n\n**Weaknesses:**\n\n*   **Overclaiming Novelty & Bidirectional Knowledge Transfer:** We respectfully disagree that the novelty is overclaimed. While we acknowledge the existence of co-training and distillation methods, our core contribution lies in the novel application of *bidirectional* knowledge transfer within a co-training framework, specifically demonstrating that distilling from the *smaller* model to the *larger* model improves the larger model's performance, which in turn boosts the smaller model's performance. This is a key distinction from standard KD. We believe the introduction and related work sections clearly establish this novelty. We will strengthen the discussion in the introduction to highlight the specific mechanisms of bidirectional knowledge transfer more explicitly, emphasizing the role of the reversed KD objective (LReKD) in Section 3, and the impact of the soft loss from the student to the teacher in Figure 2. We will also add a sentence to the introduction to clarify this point.\n\n*   **Reproducibility (Random Seeds & Variance):** The reviewer is correct that we did not explicitly state the random seeds used. We will include the random seeds used for all experiments in the revised manuscript and report the standard deviation across multiple runs in Section 5.1-5.4. We will also provide a link to our code repository and include a requirements file to specify the environment.\n\n*   **Figure 3 Axis Labels:** The reviewer is correct. We will add clear axis labels and units to Figure 3 in the revised manuscript.\n\n*   **Statistical Information in Tables:** We acknowledge the lack of detailed statistical information. We will include standard deviations and p-values to indicate statistical significance in both Table 1 and Table 2. We will also provide specific performance metrics in Table 2.\n\n*   **Community KD Definition:** The reviewer is correct. We will clarify the relationship between CTCD and Community KD in the introduction. Community KD is an extension of CTCD, leveraging the bidirectional knowledge transfer principle with an additional pre-trained teacher model. We will add a sentence to the introduction to clarify this point.\n\n*   **Dataset Licensing, Bias, and Misuse:** We acknowledge the importance of addressing these aspects. We will include a detailed section on dataset licensing and usage terms, specifically for the GLUE benchmark, in the limitations and future work section.\n\n*   **Comparison with TinyBERT & GLUE Benchmark:** We respectfully disagree with the criticism that we did not directly compare with TinyBERT. Our work focuses on a novel framework, and a direct comparison with TinyBERT, while valuable, is beyond the scope of this paper. We do, however, evaluate our method on the GLUE benchmark, as shown in Section 5 and Table 1. We will add a sentence to the related work section to highlight the differences between our approach and TinyBERT.\n\n**Suggestions:**\n\n*   **Random Seeds & Variance:** Addressed above.\n\n*   **Code Repository & Environment:** Addressed above.\n\n*   **Comparison with Similar Methods:** We will add a direct comparison with a co-training or distillation method that is most similar to CTCD in terms of bidirectional knowledge transfer, including quantitative results and a discussion of the differences. This will be added to the related work section.\n\n*   **Error Bars in Plots:** We will add error bars to the plots in Figure 4 and 6 to indicate the variance or confidence intervals of the performance metrics.\n\n*   **Dataset Licensing:** Addressed above.\n\n*   **Statistical Significance in Tables:** Addressed above.\n\n*   **Clarify CTCD and Community KD:** Addressed above.\n\n*   **Notation Table:** We will include a notation table in the appendix to define all symbols and variables used in the equations and framework descriptions.\n\n*   **Experiments with TinyBERT:** We will conduct experiments comparing CTCD with TinyBERT on the GLUE benchmark, including metrics for both performance and inference speed. This will be added to the experiments section.\n\n*   **Ablation Study of Co-distillation:** We will include an ablation study that isolates the impact of the co-distillation component (distilling from the smaller model to the larger model) compared to a standard KD approach. This will be added to the experiments section.\n\nWe believe these revisions will significantly improve the clarity, rigor, and impact of our work. Thank you again for your valuable feedback."
  }
]