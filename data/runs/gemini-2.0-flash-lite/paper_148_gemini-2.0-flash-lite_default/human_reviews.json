[
  {
    "rid": "1Sjfzf8mvP",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "In this article, authors propose to project the representation of a piece of text obtained with a deep language model into a \u201cconcept\u201d space (more precisely, computing cosine similarity between the piece of text representation and each concept representation) for improving LLM interpretability. In the experiments, these concepts are built using Wikipedia categories, for which selection depends on the required level of granularity. The method is evaluated on  7 common datasets with both human and automatic evaluations.",
      "reasons_to_accept": "Simple yet effective method The approach is carefully evaluated, both with human in the loop and automatic/semi-automatic protocols. \nThe method can be adapted to many application domains, as one can tailor the concept space to the dataset at hand.",
      "reasons_to_reject": "The article lacks a comparison with competitors, that are well presented on the related work section. \nThe article lacks an analysis of the impact of the chosen concepts (even on the granularity).",
      "questions_for_the_authors": "Concepts representations are not contextualized. What about ambiguous context? The method proposed in 2.3 might circumvent this issue, but it should be properly tested. \nThis approach is related with topic modeling, in the specific case of fixed topic. How this approach competes against these supervised topic modeling approaches ? \nIf there exist some correlation in the concept space (concepts are not orthogonal enough in term of semantic, e.g. in the case of colinear concept's representations) then a lot of the initial information will be lost (contrary to what is stated in 3.3). The approach seems not to lose that much on classification task, but how the chosen concepts impact the performance of the obtained representation should be carefully evaluated.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  },
  {
    "rid": "I9abwbfJmA",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper proposes an approach for transforming the otherwise opaque dimensions of contextualized sentence embeddings into directly interpretable categories of an ontology and evaluate them in various topic classification tasks.   The proposed approach (dubbed CES) turns some concept inventory into text, transforms them into the representation space of a sentence encoder, finally constructs a transformation matrix of these vectorized concept vectors that maps the latent vectorial representations of the encoder into such a space in which the individual dimensions are directly tied to one of the concepts from the concept inventory.   The idea is simple and sound, even though the concepts seem to be converted into the representation space somewhat naively.   The representations with interpretable dimensions are evaluated in 7 different document classification tasks.   One limitation of the work is that it only works for document classification.   The quality of the transformed representations is assessed is via measuring the prediction similarity of such random forest classifiers that use either the transformed or the original representations as document features.   As the goal of the transformation was to create interpretable representations, using a more transluent classifier (other than a random forest with 100 classifiers) would be more adequate.   Could the prediction similarity be due to the large number of classifiers used in the ensemble of classifiers?   Besides the agreement rate of the classifiers relying on the original and the interpretable features, their accuracy is equally important, which results are delegated to the appendix.   It seems that the use of the transformed representations degrade classification performance noticeably.   Some amount of performance drop is fine in exchange for more interpretable representations, but this kind of tradeoff should be made explicit and clearly articulated.   The understandability of the representations from the perspective of document classification is assessed by humans as well as in an automated way.   Related to the automated evaluation of the understandability of the transformed representations, the alternative baseline approaches that assign meaning to each dimension of the original latent space are too naive, i.e., the proposed CES approach has access to C*, the concept inventory tailored for the given task, whereas the alternative approach uses either a vocabulary of the most frequent words or a concept inventory that does not enjoy the benefit of being adjusted to the domain in question.   This way, it remains unknown to what extent CES performs better due to the more useful vocabulary or the actual approach itself.   When performing the automated undestandability assessment, based on the figures in Table 4, only 20 documents per dataset was used.   Since the automated assessment is not constrained by human labor, this kind of comparison might have been performed over more documents.",
      "reasons_to_accept": "Developing interpretable (document) representations is important topic, for which a simple approach is provided.",
      "reasons_to_reject": "The proposed approach only works for document classification, the applied baseline is too simple and the comparison with other existing approaches is lacking.   The classification performance drops quite noticeably compared to the use of the original (yet uninterpretable) representations.",
      "typos_grammar_style_and_presentation_improvements": "The statement in the related work section that sparsification only works for static embeddings is not true, see e.g. [1,2,3].   [3], being an extension of (Senel et al., 2018) for the contextual case could serve as a stronger baseline as it is also capable of assigning human interpretable labels to the dimensions of transformed latent representations.\n[1] Berend, G\u00e1bor. [\" Sparsity makes sense: Word sense disambiguation using sparse contextualized word representations.\"](https://aclanthology.org/2020.emnlp-main.683/)   [2] Yun, Zeyu, et al. [\"Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors.\"](https://aclanthology.org/2021.deelio-1.1/)   [3] Ficsor, Tam\u00e1s, and G\u00e1bor Berend. [\" Changing the Basis of Contextual Representations with Explicit Semantics.\" ]( https://aclanthology.org/2021.acl-srw.25/)",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work."
    }
  },
  {
    "rid": "T0h4ObI82F",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The authors propose to make embedding space more interpretable with respect to the model that produced them by conceptualizing the embedding space. The concept(ual) space is supposed to be more comprehensible for humans. \nThey motivate their work by the need for debugging embedding models (in this case LLMs) and for detecting biases, as well as for explaining decision made by systems incorporating LMs. \nThey propose an algorithm for conceptualization as well as a new evaluation technique to test the conceptualization. \nAs concept database, they use a Wikipedia graph. For labeling edges between concepts, they calculate sibling scores based on the similarity of the sibling concepts to detect \"is-a\" relations. \nSince concepts often are hierarchical and not all down-stream tasks might need all concepts, the conceptual space can have different granularity, depending on how \"deep\" a concept can branch out (or how many children concepts are available) and what is needed. \nThe authors accomplish this by adapting the concept space iteratively to the input text.\nFor evaluating their method, they use both human and LLM judgements.\nThe authors' main contribution is a new variant of how to make embedding spaces more interpretable, together with an interesting evaluation.",
      "reasons_to_accept": "- The work is well motivated and set into context with related approaches.\n- It is definitely relevant for research in embedding spaces (wrt biases, model decisions, etc.)\n- I like the idea of having different granularity for the concept spaces.\n- It seems that the proposed method works well and it is certainly an interesting take on embedding space interpretation, but I am not 100% certain I understood everything (see questions).",
      "reasons_to_reject": "- The method needs some clarification -- see questions.\n- The evaluation is interesting, but at least the results using a classifier trained on the concept space are not super convincing, since the agreement scores are not very high. When comparing humans vs. LLM ratings, the test set is very small.",
      "questions_for_the_authors": "General: - Why do we need the conceptual space in the first place? Is it not \"enough\" or maybe even equivalent to having close (i.e. highly similar) concepts in L, which describe the text vector t in L?\nAbstract: - The first sentence in the abstract is confusing to me: One of the \"main methods\" for interpreting a text is to embed it? Not really, is it? It's one of the main methods to process natural language using computers etc., but for interpreting a text, humans do not map it to vectors -- probably to concepts, though!?\nSection 1: - The embeddings of small LMs like skip-gram or GloVe were also not interpretable -- did you consider these as well? ( referring to ll. 36) - ll. 49: by \"understanding\" the embedding space, we might get some insights on those layers, but not into the rest of the model.\n2.2: - l. 181: |C\u00b9| = 37: if I understand correctly, this means that in the first level of the hierarchy, there are only 37 concepts in total? Or am I missing something -- it seems a rather small number for top-level concepts in Wikipedia to me.\n- I am not 100% certain I understand the \"selective refinement\". What does \"the concept with the largest weight\" mean exactly?\n- How is \"removeP\" determined? Is it simply a parameter you set at the beginning?\n2.1: - Can you please provide a URL to the \"Wikipedia category directed graph\" 3.:\n- For the function tau, you use the string representation of the concept, correct? Might this induce some confusion for the embedding model, since there is no context for these concepts?\n3.1: - The 10 sentences you use as context, are they preceding the sentence under consideration? How are they connected to the sentence except that they are from the same article?\n3.2: - The Kappa coefficient is \"relatively high\" on some of the datasets, but also quite low on others, e.g., 20 news group, Ohsumed. Also, the difference in accuracy on 20 news group is pretty large, especially when compared to the other datasets. Do you have an explanation for that?\n- Maybe I oversaw it -- please mention that all datasets are in English.\n3.3.2: - Please add to the table captions that the scores represent \"raw agreement\".\n4.2: - The plots in Figure 2 are really interesting! I just wonder what they actually mean -- the concept of \"weapons\" with input \"government\" in GPT-2 gets stronger with higher layers -- why? What was the context for \"government\"?",
      "missing_references": "- a citation for DMA?\nThese references are not necessarily missing, but maybe they could give some further ideas since they are based on similar idea(s), i.e., \"conceptualization of embeddings\": - Koc et al., 2018: Imparting Interpretability to Word Embeddings while Preserving Semantic Structure - Sommerauer & Fokkens, 2018: Firearms and Tigers are Dangerous, Kitchen Knives and Zebras are Not: Testing whether Word Embeddings Can Tell - Molino et al., 2019: Parallax: Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae",
      "typos_grammar_style_and_presentation_improvements": "The paper is well structured and written.\nTypos: - l. 76: \"that latent dimensionS correspond ..\" / \"that A latent dimension corresponds ..\" Style: - The conceptual space C and the set of concepts C are hard to differentiate, maybe use CS for conceptual space?",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  }
]