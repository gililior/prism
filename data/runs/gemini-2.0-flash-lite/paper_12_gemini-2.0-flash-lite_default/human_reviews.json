[
  {
    "rid": "8Bg7T1CXO6",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper explores the hypothesis that there exists, somewhere in a large language model, a generalizable representation of the truth or falsity of a statement. The authors use a small special-purpose network to train a classifier that can convert the outputs of any hidden layer into an estimate of the truthfulness of the statement. The performance of classifiers of this type leads the authors to conclude that the generalized representation is indeed present.\nIn service of the hypothesis, the authors create a small corpus of examples, spanning several domains. This will be a resource for replications and future studies.",
      "reasons_to_accept": "This is a very interesting and important question. The experiments are generally well-designed and informative. \nThe organization and narrative of the paper is excellent, making very clear what was done and why, with good examples.",
      "reasons_to_reject": "There are significant numbers of small writing errors that do not impede comprehension or readability. A careful rewrite by an author who did not do much of the initial writing would fix this. These are obvious errors that would be caught by an on-line style and grammar checker.\nThe authors test five layers. This is a multiple compatsions setting, so statistics aiming to show that the result is real should use some kind of correction for multiple comparisons.  It is also worth, for safety, running a simple control in which the true/false labels are randomly permuted, so that each sentence is associated with the truth value of a randomly chosen counterpart. If all is well, we will find that the model is unable to predict the permuted labels with any accuracy.  This will help to establish beyond a reasonable doubt that the observed performance is due to the ability to detect truth, not to some strange effect of the behavior of large capacity models capable of learning almost anything. Possibly, doing this test is overly paranoid, but since it is cheap to do and the result is high-impact, why not?\nI don't like the use of the anthropomorphic term \"lying\" in the title and intro. It's just about OK, because of the very good discussion of why the model might produce false statements even though both another LLM and SAPLMA can tell that they are false. But I would still prefer a more moderate phrasing in the title.",
      "typos_grammar_style_and_presentation_improvements": "see commenta bove",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "teW6R1nwYi",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper proposes a method called SAPLMA to detect whether statements generated by large language models (LLMs) like GPT are true or false. The key idea is to use the hidden layer activations of an LLM and classify whether the LLM \"believes\" the statement is true/false. The authors create a dataset of true/false factual statements across 6 topics, train a classifier on 5 sets and test on the hold-out set. This ensures the classifier learns to extract the LLM's internal signal of truth/falsehood rather than topic-specific knowledge.\nExperiments with OPT-6.7B showed that SAPLMA achieves ~70% accuracy on held-out topics, significantly outperforming 1) few-shot prompting of the LLM itself 2) classifiers trained on BERT embeddings. The 20th hidden layer works best. SAPLMA also gets 63% accuracy on statements generated by the LLM, again outperforming baselines.",
      "reasons_to_accept": "- Addresses an important issue - false information generated confidently by large LLMs. Proposes a practical method to detect such false statements.\n- Thorough experiments with different LLM layer activations, different topics, and human-labeled LLM-generated statements. Compares well to baselines.",
      "reasons_to_reject": "- I don't know why the author claimed that \"we expect that the BERT embeddings will perform very poorly, as they focus on the semantics of the statement rather than on whether a statement is true or false. \" In my opinion, the classifier trained on OPT-175B and the classifier trained on BERT should be serving exactly the same function. The only difference is the model size, BERT is relatively small, while OPT-6.7B is more powerful. It's weird to say that \"BERT embedding focus on the semantics, not true/falsehood\". This should be your conclusion/explanation after you see the experiment results, not your claim before doing the experiment. Please explain if I misunderstand your claim here.\n- As you didn't consider inputting any grounded documents when examining the truthfulness of the model, a better way is that you should only construct the examples using the pretraining data of OPT-6.7B. You showed that the final accuracy is around 70%, but we cannot know whether the remaining 30% accuracy gap is caused by #1) OPT-6.7B has not been trained on some of the new knowledge existing in your new dataset, so it's unsure (lying) about its prediction, even if the input sentence is truthful #2) the classifier is just not good enough. If you can construct a dataset in this way, we can leave out reason #1 and only focus on reason #2.\n- No experiments on larger models like GPT-175B or LLaMA-65B. Unclear if findings transfer to more capable LLMs.\n- The classifier architecture used seems arbitrary. No tuning of hyperparameters. More analysis is needed on optimal architectures.",
      "questions_for_the_authors": "- A. Can you explain why you \"expect that the BERT embeddings will perform very poorly, as they focus on the semantics of the statement rather than on whether a statement is true or false.\" ? It's unclear for me.\n- B. Can you provide the experiment results for larger models like OPT-175B?\n- C. Can you provide some ablation study or analysis to show that your classifier architecture is optimal?",
      "missing_references": "N/A",
      "typos_grammar_style_and_presentation_improvements": "N/A",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "5: Could easily reproduce the results."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "lC8li2PUPs",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper shows evidence for the encoding of the factuality-related information in the latent representations of LMs, even when they generate nonfactual text, providing evidence for a disparity between the content of the representation space and the nature of the generated text.",
      "reasons_to_accept": "The paper addresses an important question -- hallucinations in LM generation and ways to identify them. The key result is nontrivial, showing that models encode the truthfulness of an answer regardless of the text they actually generate as a continuation of the prompt. The leave-on-out design of the probe is nice and is showing good generalization to different domains (although it would be nice to *also* report numbers on the same domain).",
      "reasons_to_reject": "- Testing on a single dataset and a single model. At the very least, the results should be replicated on different OPT models and on different factual datasets.\n- Several design decisions are not clearly motivated - see questions for the authors. Particularly, it is not clear whether the GPT-generated portion of the data was validated.\n- Assuming the data is balanced, the accuracy scores reported are low. This puts into questions the claims that models \"know\" when the statement is true.\n- It is not clear whether the information extracted by the probe is being used by the model (in the causal sense). It would be nice to include linear probes and perform linear erasure / amnesic probing experiments.",
      "questions_for_the_authors": "- The methods section reads \"We used a reliable source that included a table with several properties for each instance\". What is this source?\n- Did you verify the scientific-facts portion of the data, generated by ChatGPT?\n- The paper reports that trying to prompt GPT directly to decide whether the statement is true of false yielded random accuracy. This is a very surprising - is that true for all categories in the data? Did you try to look at the probabilities of the actual true and false answers, rather than at the probabilities of the tokens \"true\" and \"false\"?",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems",
      "excitement": "3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  },
  {
    "rid": "TANNXBr2Bw",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper describes a method to get a model to identify the veracity of its own generations at a low additional cost. It shows that with a simple classifier on top of certain layers of a model's output hidden states, the veracity prediction of model generations can be learned with a modest amount of training data, and is generalizable to unseen domains.",
      "reasons_to_accept": "The problem targeted by this paper, the hallucination issue and improving trust in LLMs, is one of great importance to the deployment of LLMs. The proposed method shows promising performance with lightweight architecture, experiments are thorough in the different conditions, generalization from artificial training data to actual model generations is also covered.\nIn the analysis of why a model would generate false claims even when they are able to discern them from true ones, the first reason discussed is also inspiring.",
      "reasons_to_reject": "The method is only evaluated on a dataset that the authors created with the paper, where the exact sources of information for these datasets, as well as the construction details (e.g. how the tables are turned into sentences, etc.), are not mentioned (lines 324-325) While the authors compared with a BERT classifier and few-shot self-correction, the pool of baselines feels a bit thin, with a lack of external baselines tackling the same problem (e.g. the missing reference listed below).\nThe scope of the paper is limited to simple factoid sentences, where the veracity of factoid claims embedded in more complex utterances, or the veracity of the more complex utterances themselves are not covered.\nA number of presentation improvements could be made, for instance, the second reason-for-generation failure in lines 105-111 is not obvious, need further support.",
      "questions_for_the_authors": "I have a few questions to the authors: A. Regarding BERT classifiers and the strategy of using generation probabilities, what would the results be, if they are posed a simplified task of comparing pairs of true / false statements? ( Given the construction method of the dataset, the true and false statements should be in pairs) This would serve as a measure to the information these perplexity/pseudo-perplexity values have on the veracity of statements, if the length/frequency factors etc. could be marginalized.\nB. Given the difference in accuracy values with the change in thresholds, it feels it would ultimately be better to report AUC values instead of these accuracy values, for the AUC values are a more comprehensive measure of discriminative power along the spectrum of thresholds. Thoughts?\nC. Considering future work, is it possible for the LM to tell at which tokens did its own outputs begin to derail?",
      "missing_references": "The following paper also tackles the problem of having a LM critique on its own predictions: Language Models (Mostly) Know What They Know (https://arxiv.org/abs/2207.05221). While it mostly concerns the problem of calibration of logit likelihoods, the two papers are of similar spirits and the methods may serve as additional baselines.",
      "typos_grammar_style_and_presentation_improvements": "lines 018-020: repetitive to lines 008-010; line 118: capital M in Model; line 205: multilingual line 222: text line 267: LM/LLM, not LMM",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "2: Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
    }
  }
]