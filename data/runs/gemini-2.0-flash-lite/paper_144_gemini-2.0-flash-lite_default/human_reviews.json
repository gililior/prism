[
  {
    "rid": "du7kQQQCpK",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper addresses the problem of evaluating the factuality of long-form text that is generated by large language models.\nThe authors propose FACTSCORE, a new scoring mechanism that takes into consideration each of atomic facts in the generated text. \nFACTSCORE enables more fine-grained evaluation and comparison of the models.\nSince a straightforward way of annotating data by human annotators is time-consuming and costly, the authors also propose an automatic approach of approximately estimating the value of FACTSCORE by using other models for retrieval and prediction of whether or not each atomic fact is supported by the pre-defined knowledge source.\nThorough analysis of the estimation for 12 different LMs indicates some interesting findings on correlation between factual accuracy and the model size, training data and instruction tuning, etc.",
      "reasons_to_accept": "- FACTSCORE is a novel formulation of evaluation of factuality and addresses the problem of existing sentence-level evaluation methods.\n- Though most of the details are in the Appendix, the paper is very well organized and each section is clearly explained.",
      "reasons_to_reject": "- It is not explicitly stated how the findings discussed in Section 4 are utilized for deeper understanding of a model for factuality.\n- Since analysis was done over only People biographies and Wikipedia for prompts and a knowledge source respectively, it is not clear how universal the findings are, which is also pointed out by the authors.",
      "questions_for_the_authors": "- In Page 3, line 484, \"and ChatGPT for PerplexityAI.\"  In Table 3, LLAMA is the best for PerplexityAI with ER=0.1, so my understanding is that the authors choose ChatGPT (with ER=0.8) because it preserves the ranking while LLAMA does not. Is it correct?\n- I wonder if the model size is dominant in factual precision because ChatGPT (GPT-3.5 or InstructGPT) and GPT-4 are known to be much larger than the other models discussed in the paper though no exact information on model sizes is available for them. Could you elaborate more about this if you hit upon anything? I read through the paper including Appendix and could find relevant discussions only in 4.3.2, but may have missed something important about the model size.",
      "typos_grammar_style_and_presentation_improvements": "- Page 3, line 217: I think that [a is supported by C] can be written by the indicator function https://en.wikipedia.org/wiki/Indicator_function  . Or simply f(y) can be written as |{a \\in A_y | a is supported by C}| / |A_y| .\n- Page 6, line 406: across five variants -> \"four\" variants?",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  },
  {
    "rid": "1ccBpvtcUy",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper proposes an evaluation method of factual precision called FACTSCORE, the percentage of pieces of information as atomic facts supported by a given knowledge source.  Since it is expensive to calculate this evaluation metric manually, this paper also proposes a method for automatic evaluation.\nThe evaluations of several commercial language models showed that   1. infrequent entities tend to have lower  FACTSCORE,   1. FACTSCORE of facts mentioned later in generated text tends to be lower, and   1. even PerplexityAI, which uses search results, has a low FACTSCORE.\nThe experiment results also showed that the proposed automatic evaluation method can estimate FACTSCORE with a relatively low error rate for human evaluation. \nIn addition, the automatic evaluation of FACTSCORE for 12 large language models showed several interesting tendencies such that   1.  none of the models is very high,   1.  GPT-4 and ChatGPT are comparable, and   1. GPT-4 and ChatGPT are better than other published models.",
      "reasons_to_accept": "- This paper proposes a new score for evaluating factual precision and a method for automatically estimating the score without manual labor.\n- The paper reports some interesting results by evaluating the scores of various large language models.\n- There are plans to release it as open source.",
      "reasons_to_reject": "- The generality needs to be clarified because the experiment was conducted only on Biographies.\n- The method's validity needs to be clarified because the error compared to the case where the correct facts are considered, which is not written in knowledge sources, has not been evaluated.\n- Since it is difficult for LLMs to generate facts without referring to any sources in principle, evaluating it in a setting that generates text including facts from given relevant sources may be more valuable.",
      "questions_for_the_authors": "- A: What kind and size of the corpus were the NP models trained?\n- B: Is there any reason there are no NP results for ChatGPT in Table 3?\n- C: The reviewer thinks it is difficult for LLMs to generate facts without referring to any sources. \nConsidering that, what do the authors think is more valuable to evaluate it in a setting that generates text including facts from given relevant sources? \n  - This setting would also have the advantage of evaluating \"factual recall\" from the facts mentioned in given relevant sources.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  },
  {
    "rid": "kITYZq5iG6",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper investigates hallucinations in generated biographies. The main contribution is in the in depth analysis of hallucinations in the generated biographies, with the added benefit of considering SOTA language models. FACTScore, the metric proposed by the authors, is simply the average truth score of a sentence composed of several facts, where each fact gets 0 (not true) or 1 (true) score. This score is computed by humans in a large scale evaluation, but the authors also propose a few models that can achieve good results.",
      "reasons_to_accept": "I found the paper well written and easy to follow. In my opinion, the work is sound and the experimental evaluation rigorous. In addition, the problem studied is interesting. The authors invested a lot of money and effort in the experimental evaluation of hallucinations, which is the main strength.  In particular, they do a human evaluation of biography generation for three models: InstructGPT, ChatGPT, PerplexityAI on 183 generations (biographies) from each model. The humans split the generations in smaller units of content that can be easily verified by checking the Wikipedia page of the person.",
      "reasons_to_reject": "The FactScore metric is a very simple metric that cannot be counted as a contribution, as it hides a complex discussion of what constitutes a fact and when a fact is truly supported by an underlying KB/document. For example, if a sentences is not correctly split in the underlying parts, evaluating its truth value is non trivial, for example: President Trump no longer lives in the White House. Here we have 2 facts (Trump, is, President), (Trump, no longer lives in, White House). Also negations are an added layer of difficulty as often KBs or documents do not have all possible negations if any. Hence, the true contribution of the paper is more in the human analysis of the hallucinations of current models. \nThe human analysis of hallucinations has also the drawback that it does not considers that some facts might be true, but not present in the Wikipedia page. This can be more serious for entities with shorter Wikipedia pages, for which the authors find that models tend to produce less accurate results. \nFinally, models might produce much more accurate results if given in input the Wikipedia page - an hypothesis which is very interesting to test.",
      "questions_for_the_authors": "A. In which measure Figure 2 (rare entities has less correct biography) could be explained through the fact that the human annotators did not find those particularly facts on the Wikipedia page? Did you look if there is a correlation between the type of unsupported fact (page level contradiction or annotation level) and the popularity/completeness of the page of entities? Is it possible that for the rare entities the fact is true, just not present on Wikipedia?\nB. Did you consider the setting in which the LM receives in the prompt the Wikipedia page of the entity and has the task of creating a biography? The model is very likely to produce more accurate results, which will be consistent with the findings you have when you create the automatic measure.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  }
]