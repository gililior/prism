[
  {
    "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback. We appreciate the time and effort invested in reviewing our paper. We address each point below:\n\n**Weaknesses:**\n\n*   **Variance and Random Seeds:** The reviewer points out the lack of information about result variance and random seeds (Sec 4, Sec 6). We acknowledge this omission and will address it in the revised version. We will include the random seeds used for all experiments and report the standard deviation of the results, as well as the number of runs, in the relevant tables (Tables 1, 2, 3, 4, 5). We will also clarify this in the experimental setup sections (4.1 and 6.1).\n\n*   **Novelty and Comparison to Abdaoui et al. (2020):** The reviewer questions the novelty and the comparison to Abdaoui et al. (2020). We respectfully disagree that the novelty is limited. While the core idea of vocabulary trimming is present in Abdaoui et al. (2020), our work significantly extends their approach. Our study compares two VT strategies (pre-FT and post-FT), while Abdaoui et al. (2020) only consider pre-FT VT. Furthermore, we extend the experiments to generation tasks (QA and QG) and evaluate more recent LMs (mBART and mT5). We will strengthen the discussion in the Related Work section to explicitly highlight these differences and quantify the improvements, as suggested. We will also add a direct comparison table to the revised version.\n\n*   **Comparison with Other Compression Techniques:** The reviewer suggests a comparison with other model compression techniques (Intro, Sec 2). We acknowledge this is a valid point. While a comprehensive comparison is beyond the scope of this paper, we will add a paragraph in the Introduction and Related Work sections discussing the relative advantages and disadvantages of VT compared to pruning, quantization, and knowledge distillation. We will also cite relevant papers on these techniques.\n\n*   **Figure Clarity:** The reviewer notes the lack of clear axis labels and units in the figures (Fig 3, Fig 4, Fig 5, Fig 6). We agree and will revise all figures to include clear axis labels, units where applicable, and legends to clarify the different lines or bars.\n\n*   **Definition of 'Irrelevant Tokens':** The reviewer requests an explicit definition of 'irrelevant tokens' (Abstract, Section 1). We will clarify this in the Introduction and Section 3. In essence, irrelevant tokens are those that are not present in the target language corpus. We identify language-specific tokens from a language-specific corpus and remove all tokens along with their embeddings except for those that appear in the target language corpus.\n\n*   **Detailed Explanation of VT Method:** The reviewer requests a dedicated section or detailed explanation of the VT method (Section 3). The reviewer appears to have overlooked that Section 3, \"Vocabulary Trimming,\" already provides a detailed explanation of the VT method, including the identification of language-specific tokens and the two VT strategies. We will enhance this section by adding a table summarizing the notation used throughout the paper and providing more details on the criteria for token selection.\n\n*   **Statistical Information in Tables:** The reviewer points out the lack of statistical information in the tables (Tables 1, 2, 4, 5). We acknowledge this and will include standard deviations and/or confidence intervals for all reported metrics in the revised version. We will also add p-values to indicate the statistical significance of performance differences between models and configurations. We will ensure all table headers are clear and concise and provide a brief description for each table.\n\n*   **Misuse Scenarios and Fairness Concerns:** The reviewer raises concerns about potential misuse scenarios and fairness (Insufficient evidence). We will add a section to the revised paper discussing potential biases introduced or amplified by the VT method and propose mitigation strategies. We will also expand the discussion of broader impacts to include potential negative societal consequences and mitigation strategies.\n\n*   **Comparison with Original Multilingual LMs:** The reviewer requests an explicit comparison of VT performance against the original multilingual LMs on the same tasks and datasets (Experiments). We will add a new experiment comparing the proposed VT method with the original multilingual LMs (e.g., mBERT, mBART, mT5) on the same four NLP tasks (two generative and two classification) and seven languages. We will evaluate the performance using the same metrics as in the paper. This will provide a direct comparison of the performance and size reduction achieved by VT.\n\n**Suggestions:**\n\n*   **Code, Data, and Software Environment:** We will provide a link to the code repository and the data used, and specify the software environment (e.g., Python packages, versions) used for the experiments (Sec 4, Sec 6).\n\n*   **Low-Resource Languages:** We will expand the discussion in Section 8 to include a more detailed analysis of the behavior of VT in low-resource languages.\n\n*   **Comprehensive Social Bias Evaluations:** We will expand the social bias evaluations across different languages in the revised version.\n\nWe believe these revisions will significantly improve the clarity, rigor, and impact of our paper. Thank you again for your valuable feedback."
  }
]