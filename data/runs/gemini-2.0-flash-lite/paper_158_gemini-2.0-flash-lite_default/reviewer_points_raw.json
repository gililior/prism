[
  {
    "kind": "strength",
    "text": "The paper describes the datasets, evaluation metrics, and models used in detail.",
    "grounding": "Sec 4.1, Sec 6.1",
    "facet": "code/data availability"
  },
  {
    "kind": "weakness",
    "text": "No mention of seeds used for experiments.",
    "grounding": "Sec 4, Sec 6",
    "facet": "seeds/variance"
  },
  {
    "kind": "weakness",
    "text": "No information about the variance of the results is provided.",
    "grounding": "Sec 4, Sec 6",
    "facet": "seeds/variance"
  },
  {
    "kind": "suggestion",
    "text": "Include the random seeds used for all experiments.",
    "grounding": "Sec 4, Sec 6",
    "facet": "seeds/variance"
  },
  {
    "kind": "suggestion",
    "text": "Report the variance (e.g., standard deviation) of the results, or at least the number of runs.",
    "grounding": "Sec 4, Sec 6",
    "facet": "seeds/variance"
  },
  {
    "kind": "suggestion",
    "text": "Provide a link to the code repository and the data used.",
    "grounding": "Sec 4, Sec 6",
    "facet": "code/data availability"
  },
  {
    "kind": "suggestion",
    "text": "Specify the software environment (e.g., Python packages, versions) used for the experiments.",
    "grounding": "Sec 4, Sec 6",
    "facet": "environment reproducibility"
  },
  {
    "kind": "question",
    "text": "Are the datasets used publicly available or will they be released?",
    "grounding": "Sec 4.1",
    "facet": "code/data availability"
  },
  {
    "kind": "question",
    "text": "Will the code be released?",
    "grounding": "Sec 4, Sec 6",
    "facet": "code/data availability"
  },
  {
    "kind": "question",
    "text": "Are the hyperparameter settings and training details fully described?",
    "grounding": "Sec 4, Sec 6",
    "facet": "reproducibility"
  },
  {
    "kind": "limitations",
    "text": "The paper does not provide information on the computational resources used.",
    "grounding": "Sec 4, Sec 6",
    "facet": "reproducibility"
  },
  {
    "kind": "ethics flag",
    "text": "Yes, the use of sentiment analysis datasets may raise ethical concerns regarding bias and fairness.",
    "grounding": "Sec 4.1, Sec 6.1",
    "facet": "ethics"
  },
  {
    "kind": "summary",
    "text": "The paper proposes a vocabulary trimming (VT) method to reduce the vocabulary of multilingual language models, creating language-specific monolingual models. Experiments show VT maintains the performance of the original multilingual LM while reducing model size. The paper also analyzes social biases in the models.",
    "grounding": null,
    "facet": "summary"
  },
  {
    "kind": "strength",
    "text": "The paper demonstrates that VT can retain the high performance of the original multilingual LM while reducing model size. (Sec 8)",
    "grounding": "Sec 8",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "strength",
    "text": "Experiments show that VT models can achieve better results than the original larger model when trimmed before fine-tuning. (Sec 8)",
    "grounding": "Sec 8",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "strength",
    "text": "The paper shows that monolingual models exhibit larger social biases than VT-induced multilingual LMs. (Sec 6)",
    "grounding": "Sec 6",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "weakness",
    "text": "The paper acknowledges that the analysis comparing multilingual and monolingual models in terms of social bias is focused on English only, and the results could differ for other languages. (Sec 7)",
    "grounding": "Sec 7",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "weakness",
    "text": "The paper acknowledges limitations in testing the methodology in truly low-resource languages. (Sec 8)",
    "grounding": "Sec 8",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "suggestion",
    "text": "Further analysis is needed to explore the behavior of VT in low-resource languages.",
    "grounding": "Sec 8",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "suggestion",
    "text": "More comprehensive social bias evaluations across different languages are needed.",
    "grounding": "Sec 7",
    "facet": "claims_vs_evidence"
  },
  {
    "kind": "question",
    "text": "How does the performance of VT compare to monolingual LMs trained from scratch?",
    "grounding": null,
    "facet": "questions"
  },
  {
    "kind": "question",
    "text": "What is the impact of different compression rates on model performance?",
    "grounding": null,
    "facet": "questions"
  },
  {
    "kind": "limitations",
    "text": "The authors adequately discuss the limitations of their analysis, including the focus on English for social bias evaluation and the lack of testing in low-resource languages. (Sec 7, Sec 8)",
    "grounding": "Sec 7, Sec 8",
    "facet": "limitations"
  },
  {
    "kind": "ethics_flag",
    "text": "yes",
    "grounding": null,
    "facet": "ethics"
  },
  {
    "kind": "summary",
    "text": "The paper proposes a vocabulary trimming (VT) method to reduce the size of multilingual language models (LMs) by removing irrelevant tokens for a target language. The method is evaluated on various NLP tasks and languages, demonstrating the ability to retain performance while reducing model size and potentially mitigating social biases. The work is positioned as an extension of prior work on vocabulary trimming.",
    "grounding": "Abstract, Intro",
    "facet": null
  },
  {
    "kind": "strength",
    "text": "The paper clearly identifies the problem of large model sizes in multilingual LMs and proposes a practical solution.",
    "grounding": "Intro",
    "facet": null
  },
  {
    "kind": "strength",
    "text": "The paper explores two VT strategies (pre- and post-fine-tuning) and evaluates them across multiple tasks and languages, providing a comprehensive analysis.",
    "grounding": "Intro, Sec 1.2, Sec 4",
    "facet": null
  },
  {
    "kind": "weakness",
    "text": "The novelty is limited as the core idea of vocabulary trimming has been explored before. The paper needs to better highlight the delta compared to Abdaoui et al. (2020).",
    "grounding": "Related Work",
    "facet": null
  },
  {
    "kind": "weakness",
    "text": "The paper lacks a detailed comparison with other model compression techniques beyond vocabulary trimming. It is unclear how VT compares to other methods in terms of performance and efficiency.",
    "grounding": "Intro, Sec 2",
    "facet": null
  },
  {
    "kind": "suggestion",
    "text": "Include a direct comparison with Abdaoui et al. (2020) on the same tasks and datasets to quantify the improvements of the proposed method.",
    "grounding": "Related Work",
    "facet": null
  },
  {
    "kind": "suggestion",
    "text": "Compare the proposed VT method with other model compression techniques such as pruning, quantization, or knowledge distillation to establish its relative advantages and disadvantages.",
    "grounding": "Intro, Sec 2",
    "facet": null
  },
  {
    "kind": "suggestion",
    "text": "Provide more details on the methodology used to identify and remove irrelevant tokens from the vocabulary.",
    "grounding": "Intro",
    "facet": null
  },
  {
    "kind": "questions",
    "text": "How does the choice of the corpus used for identifying language-specific tokens impact the performance of VT?",
    "grounding": "Intro",
    "facet": null
  },
  {
    "kind": "questions",
    "text": "What is the computational cost of applying VT (both pre- and post-fine-tuning) compared to the original training and fine-tuning processes?",
    "grounding": "Sec 4",
    "facet": null
  },
  {
    "kind": "questions",
    "text": "Are there any specific types of tokens that are more likely to be removed by VT, and how does this affect the model's performance on different tasks?",
    "grounding": "Sec 4",
    "facet": null
  },
  {
    "kind": "limitations",
    "text": "The effectiveness of VT may depend on the quality and diversity of the corpus used to identify language-specific tokens.",
    "grounding": "Intro",
    "facet": null
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "Abstract, Intro",
    "facet": null
  },
  {
    "kind": "provisional_rating",
    "text": "3",
    "grounding": "Based on the current analysis, the paper has some novelty but needs more comparative evidence.",
    "facet": null
  },
  {
    "kind": "summary",
    "text": "The figures present results of the proposed VT methodology. The clarity and effectiveness of the figures vary.",
    "grounding": "Figures 1-6",
    "facet": "figures"
  },
  {
    "kind": "strength",
    "text": "Figure 2 provides a clear illustration of vocabulary trimming.",
    "grounding": "Fig 2",
    "facet": "figures"
  },
  {
    "kind": "weakness",
    "text": "Figures lack clear axis labels and units, making it difficult to interpret the results.",
    "grounding": "Fig 3, Fig 4, Fig 5, Fig 6",
    "facet": "figures"
  },
  {
    "kind": "suggestion",
    "text": "Add clear labels to the axes, including units where applicable. Include a legend to clarify the different lines or bars.",
    "grounding": "Fig 3, Fig 4, Fig 5, Fig 6",
    "facet": "figures"
  },
  {
    "kind": "questions",
    "text": "What is the specific task being performed in Figure 3? What do the different vocabulary sizes represent in Figures 4, 5, and 6?",
    "grounding": "Fig 3, Fig 4, Fig 5, Fig 6",
    "facet": "figures"
  },
  {
    "kind": "limitations",
    "text": "The figures focus on overall performance metrics and do not provide detailed insights into the underlying mechanisms.",
    "grounding": "All figures",
    "facet": "figures"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "N/A",
    "facet": "figures"
  },
  {
    "kind": "weakness",
    "text": "No information on dataset consent or privacy is provided.",
    "grounding": "Insufficient evidence",
    "facet": "ethics_privacy"
  },
  {
    "kind": "weakness",
    "text": "No information on dataset usage terms is provided.",
    "grounding": "Insufficient evidence",
    "facet": "ethics_usage_terms"
  },
  {
    "kind": "suggestion",
    "text": "Include a section detailing the datasets used, their licenses, and any relevant privacy policies or consent mechanisms.",
    "grounding": "Insufficient evidence",
    "facet": "ethics_licensing"
  },
  {
    "kind": "suggestion",
    "text": "Explicitly state the usage terms for the models created using the VT method.",
    "grounding": "Insufficient evidence",
    "facet": "ethics_usage_terms"
  },
  {
    "kind": "strength",
    "text": "The paper is well-structured, with a clear introduction, related work, and methods section.",
    "grounding": "Abstract, Sections 1-3",
    "facet": "organization"
  },
  {
    "kind": "strength",
    "text": "The abstract clearly outlines the problem, proposed solution (VT), experimental setup, and key findings.",
    "grounding": "Abstract",
    "facet": "clarity_presentation"
  },
  {
    "kind": "weakness",
    "text": "The definition of 'irrelevant tokens' in the context of VT is not explicitly defined. What criteria are used to determine if a token is irrelevant?",
    "grounding": "Abstract, Section 1",
    "facet": "clarity_presentation"
  },
  {
    "kind": "weakness",
    "text": "The paper lacks a dedicated section or detailed explanation of the VT method itself. The current description is brief.",
    "grounding": "Section 3",
    "facet": "clarity_presentation"
  },
  {
    "kind": "suggestion",
    "text": "Provide a more detailed explanation of the VT method, including the criteria for token selection and the process of removing tokens.",
    "grounding": "Section 3",
    "facet": "clarity_presentation"
  },
  {
    "kind": "suggestion",
    "text": "Include a table summarizing the notation used throughout the paper, especially for the VT method.",
    "grounding": "Section 3",
    "facet": "clarity_presentation"
  },
  {
    "kind": "questions",
    "text": "How is the 'language-specific token' identified from the underlying text corpus?",
    "grounding": "Section 1",
    "facet": "clarity_presentation"
  },
  {
    "kind": "questions",
    "text": "What is the computational cost of VT, and how does it compare to the cost of training monolingual models?",
    "grounding": "Section 1",
    "facet": "clarity_presentation"
  },
  {
    "kind": "limitations",
    "text": "The paper's description of the VT method is not detailed enough to reproduce the results.",
    "grounding": "Section 3",
    "facet": "reproducibility"
  },
  {
    "kind": "ethics_flag",
    "text": "No ethical concerns are apparent in the provided text.",
    "grounding": "N/A",
    "facet": "ethics"
  },
  {
    "kind": "ratings",
    "text": "Overall, the paper is well-organized but needs more clarity in the description of the VT method.",
    "grounding": "Sections 1-3",
    "facet": "overall"
  },
  {
    "kind": "summary",
    "text": "The tables present experimental results evaluating the proposed VT methodology across various NLP tasks. The tables include performance metrics, vocabulary sizes, and parameter counts. However, the presentation of statistical information is inconsistent across tables.",
    "grounding": "Tables 1-7",
    "facet": "tables"
  },
  {
    "kind": "strength",
    "text": "Table 3 provides a clear comparison of pre/post-FT VT models against original monolingual and multilingual models across multiple benchmarks. The use of color-coding to highlight social bias scores is helpful.",
    "grounding": "Table 3",
    "facet": "tables"
  },
  {
    "kind": "weakness",
    "text": "Several tables lack crucial statistical information such as standard deviations, confidence intervals, or p-values, making it difficult to assess the significance of the reported results. Some tables also lack clear headers and descriptions.",
    "grounding": "Tables 1, 2, 4, 5",
    "facet": "tables"
  },
  {
    "kind": "suggestion",
    "text": "Include standard deviations and/or confidence intervals for all reported metrics. Add p-values to indicate statistical significance of performance differences between models and configurations. Ensure all table headers are clear and concise. Provide a brief description for each table.",
    "grounding": "Tables 1-7",
    "facet": "tables"
  },
  {
    "kind": "questions",
    "text": "1. What statistical tests were used to determine the significance of the differences in performance reported in the tables? 2. Are the results presented in Tables 1, 2, 4, and 5 averaged over multiple runs, and if so, what is the standard deviation? 3. What is the rationale behind choosing the specific vocabulary sizes (EN, 50K, etc.) used in the VT models?",
    "grounding": "All tables",
    "facet": "tables"
  },
  {
    "kind": "limitations",
    "text": "The conclusions drawn from the tables are limited by the specific datasets and tasks used for evaluation. Generalizability to other NLP tasks and datasets is not directly addressed.",
    "grounding": "All tables",
    "facet": "tables"
  },
  {
    "kind": "ethics_flag",
    "text": "no",
    "grounding": "N/A",
    "facet": "tables"
  },
  {
    "kind": "weakness",
    "text": "The paper does not discuss potential misuse scenarios, such as generating biased or harmful content.",
    "grounding": "Insufficient evidence",
    "facet": "risks"
  },
  {
    "kind": "weakness",
    "text": "The paper does not adequately address fairness concerns related to the VT method, particularly its impact on low-resource languages or underrepresented groups.",
    "grounding": "Insufficient evidence",
    "facet": "fairness"
  },
  {
    "kind": "suggestion",
    "text": "Include a discussion of potential biases introduced or amplified by the VT method and propose mitigation strategies.",
    "grounding": "Section 6, 7",
    "facet": "fairness"
  },
  {
    "kind": "suggestion",
    "text": "Expand the discussion of broader impacts to include potential negative societal consequences and mitigation strategies.",
    "grounding": "Section 1, 7, 8",
    "facet": "broader_impacts"
  },
  {
    "kind": "suggestion",
    "text": "Provide more details on the limitations of the VT method, including its applicability to different languages and tasks.",
    "grounding": "Section 1, 8",
    "facet": "limitations"
  },
  {
    "kind": "weakness",
    "text": "The paper does not discuss the potential for the technology to be used for malicious purposes, such as generating disinformation or spreading hate speech.",
    "grounding": "Insufficient evidence",
    "facet": "misuse"
  },
  {
    "kind": "suggestion",
    "text": "Add a section on potential misuse cases and propose mitigation strategies, such as content moderation and bias detection.",
    "grounding": "Insufficient evidence",
    "facet": "misuse"
  },
  {
    "kind": "strength",
    "text": "The paper clearly differentiates its approach (Vocabulary Trimming - VT) from prior work by Abdaoui et al. (2020), highlighting the novel contributions of exploring VT strategies before and after fine-tuning, and extending the evaluation to generation tasks and more recent multilingual language models (LMs) like mBART and mT5.",
    "grounding": "Intro/Related Work",
    "facet": "novelty"
  },
  {
    "kind": "weakness",
    "text": "The paper does not explicitly compare the performance of VT against the original multilingual LMs on the same tasks and datasets used in the evaluation. This comparison is crucial to demonstrate the effectiveness of VT in retaining performance while reducing model size.",
    "grounding": "Experiments",
    "facet": "missing_comparison"
  },
  {
    "kind": "suggestion",
    "text": "Conduct an experiment comparing the proposed VT method with the original multilingual LMs (e.g., mBERT, mBART, mT5) on the same four NLP tasks (two generative and two classification) and seven languages. Evaluate the performance using the same metrics as in the paper. This will provide a direct comparison of the performance and size reduction achieved by VT.",
    "grounding": "Experiments",
    "facet": "experiment_suggestion"
  }
]