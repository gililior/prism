[
  {
    "rid": "l1wmwaRRVf",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper proposes to use policies like wait-k for simultaneous MT on word level rather than on subword level. The authors additionally suggest to use a language model (possibly with a different subword segmentation than the translation model) to further improve the quality of the translation; operating on word level makes it possible to apply the LM despite the differences in subword segmentation.",
      "reasons_to_accept": "One of the main claims of the paper that so far, the average lagging, one of the main ways of evaluating simultaneous MT systems, has happened on subword level. This claim is not true, and the idea to go from subword level to word level when computing the policy is not new at all either. The remaining part about using the LM is not enough for publication.\nSo overall, I see no reasons to accept the paper in its current form.",
      "reasons_to_reject": "The main claim of the paper that so far the average lagging was computed on subword level and not on word level is simply not true. In fact, SimEval by default uses word level.  This is absolutely necessary to do to have fair comparison between submissions to competitive evaluations/shared tasks like Workshop on Simultaneous Translation, and also Simultaneous speech translation track at the IWSLT conference.  Also, prior research definitely used word-level adaptive policies, like the phrases/chunks in one of the papers that I listed below - these were defined on the word level.  Your use of word-level \"chunk attention\" is misleading - you seem to just attend to multiple subwords of the same word - this is not a chunk in any of the established meaning of this word.  Overall, it is clear that the output of words, not subwords is expected in simultaneous MT, as they have to be presented to a user in form of words. How you read the input, in subwords, words, or directly from speech, or in characters - this is up to the architecture of the system, so there is nothing novel about going from subwords to words there.  So overall, I see no merit in the paper, as its main claims have no ground.",
      "missing_references": "@inproceedings{papi2022does, title={Does Simultaneous Speech Translation need Simultaneous Models?}, author={Papi, Sara and Gaido, Marco and Negri, Matteo and Turchi, Marco}, booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022}, pages={141--153}, year={2022} } @article{wilken2020neural, title={Neural Simultaneous Speech Translation Using Alignment-Based Chunking}, author={Wilken, Patrick and Alkhouli, Tamer and Matusov, Evgeny and Golik, Pavel}, journal={IWSLT 2020}, pages={237}, year={2020} }",
      "typos_grammar_style_and_presentation_improvements": "Line 493: \"all tested latency settings\": according to Fig. 6b, this is not true: in the lowest latency setting of 2, the system without \"chunk attention\" is better.",
      "ethical_concerns": "No",
      "justification_for_ethical_concerns": "From Line 42: \"The performance analysis and implementation of most SiMT systems, to the best of our knowledge, have been carried out on encoded sequences of source and target subwords, rather than on the original source and target sentences. This has led to two critical issues that need to be addressed.\"\nThis is simply not true! And because of this, the paper has no value! The authors did add \"to the best of our knowledge\", but actually, this is a known fact for everyone working on simultaneous MT.  === After the author's rebuttal, I acknowledge that some papers still use BPE-level evaluation of simultaneous MT systems. Still, I believe that it is clear that this is not the right thing to do - yet I don't have any ethical concerns anymore after the authors re-worded the paper."
    },
    "scores": {
      "soundness": "3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",
      "excitement": "3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "Vx1614fGcC",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper shows that moving from token-based to word-level policies in Simultaneous Machine Translation (SiMT) brings about improvements in translation quality leveraging all subwords that are part of the word being read and not writing in the middle of a word. In addition, and more importantly, it eases the integration of external LMs in order to improve SiMT models, and this is shown applying LM-fused attention previously applied in NMT. Finally, word-level policies allows for a fair comparative evaluation independently of the subword algorithm being applied. The results show a consistent improvements up to approximately 15%.",
      "reasons_to_accept": "The improvements in translation quality are consistent and significant across k values in wait-k policies.",
      "reasons_to_reject": "The presentation and discussion of results need to be improved to put into perspective what contributions are having a more significant impact than others and under which conditions. For example, Figure 5 shows too many plots and too many curves, I would discard Transformer base plots and I would select those curves that clearly show the main conclusions that can be extracted from the experiments: word-level policies improve the results and adding a LM does much more. Those conditions that do not help much can be mentioned in the discussion. However, it would be nice to know what the contribution is for the word-level policies and the LMs and under which conditions one may contribute more than the other, if this is the case.\nWord-level chunk attention is not clear to me since little detail is provided about how chunks are defined.\nTo sum up, the results are good and extensive, but are not easy to read in order to draw the main conclusions. The authors should have devoted more time to prepare the presentation of the results to ease the task of the reader.",
      "questions_for_the_authors": "Question A: How are the chunk boundaries defined? what are the parameters governing the model that takes the splitting decision? Obviously, the chunk size affects the quality of the bidirectional encoding, have you explored this point in your experiments? I guess bidirectional encoding is applied to all chunks, except for the last one in which unidirectional encoding would be applied, am I right?",
      "missing_references": "When discussing the encoding of the input sequence in lines 227-232, I think it is worth mentioning two additional references: @inproceedings{iranzo-sanchez-etal-2022-simultaneous,     title = \"From Simultaneous to Streaming Machine Translation by Leveraging Streaming History\",     author = \"Iranzo Sanchez, Javier  and       Civera, Jorge  and       Juan-C{\\'\\i}scar, Alfons\",     booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",     month = may,     year = \"2022\",     address = \"Dublin, Ireland\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2022.acl-long.480\",     doi = \"10.18653/v1/2022.acl-long.480\",     pages = \"6972--6985\",     abstract = \"Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed. However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario. Indeed, these sentence-level latency measures are not well suited for continuous stream translation, resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed. This work proposes a stream-level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation, that is successfully evaluated on streaming conditions for a reference IWSLT task\", } @inproceedings{kahardipraja-etal-2021-towards,     title = \"Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental {NLU}\",     author = \"Kahardipraja, Patrick  and       Madureira, Brielen  and       Schlangen, David\",     booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",     month = nov,     year = \"2021\",     address = \"Online and Punta Cana, Dominican Republic\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2021.emnlp-main.90\",     doi = \"10.18653/v1/2021.emnlp-main.90\",     pages = \"1178--1189\",     abstract = \"Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.\", \n}",
      "typos_grammar_style_and_presentation_improvements": "Typos: L. 142: attetnion -> attention L. 216: , The goal -> , the goal L. 232: unidirectionally**.** (Elbayad et al., 2020) L. 147: missing words in \"Zhang and Feng (2022a) model to predict the alignment...\" L.422: BLEU calculation**.** (Post, 2018) Table 2: An Eexample case -> An example case Presentation improvements: Tables 4-12 in the appendix could be more compacted and elaborated to ease the comparison across k values (since explored k values are different), relative improvements over the baseline would help to clearly see the impact of the different factors.\nFigure 6 (a) and its description in the text need to be improved to make the description more consistent, you are using WW, TW, WT, but Read-k-Write-k to refer to TT policy.\nI would discard Figure 6 (b), since it can be described with a sentence. In addition, this contribution is not clear to me since little detail is provided about how chunks are defined.\nFigure 8 (a) contains too many curves that cannot be read properly. I would reduce the number of curves discarding those less significant in terms of impact in the results and describe these minor effects in the text. In addition, from an aesthetic viewpoint, I would try to use same font size across figures in page8 and shorthen legend description to not occlude the curves (see Figure 8 (a)).",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  },
  {
    "rid": "gy961FhTpg",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper addresses the problem of the assumption in many existing studies that operations are carried out at the subword level, even though the standard unit for input and output in most practical scenarios is typically at the word level. The paper demonstrates that policies devised and validated at the subword level are surpassed by those operating at the word level, which process multiple subwords to form a complete word in a single step. The main contributions of this paper are the demonstration of the superiority of word-level policies over subword-level policies in SiMT and the proposal of a method to boost SiMT models using language models.",
      "reasons_to_accept": "Strengths of the paper: - Proposes a novel method of integrating language models (LM) into Simultaneous Machine Translation (SiMT) systems - Offers a more versatile solution that can be integrated into most existing neural SiMT models - LM-fused attention proves effective in enhancing translation quality across all latency levels - Proposed word-level policy plays a crucial role in effectively managing the vocabulary mismatch between the LM and model",
      "reasons_to_reject": "Weaknesses of the paper: - The proposed method may not be applicable to languages with a writing style that lacks spaces or other delimiters between words or sentences, such as Chinese. Experiments like translation direction of En-Zh should be included in this paper.\n- Integrating a large LM may require a faster compute capability to fulfill the low-latency demands of the SiMT task. Speed experiments should be involved in this paper.",
      "questions_for_the_authors": "1. What are some potential solutions or alternative approaches that could be explored to address the limitation of the proposed method in SiMT systems for languages with a writing style that lacks spaces or other delimiters between words or sentences, such as Chinese?",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  }
]