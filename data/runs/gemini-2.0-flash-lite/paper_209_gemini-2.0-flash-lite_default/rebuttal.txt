[
  {
    "rebuttal": "We thank the reviewer for their detailed and insightful feedback. We address each point below:\n\n**Weaknesses:**\n\n*   **Weakness 1:** \"The paper lacks sufficient comparative evidence to demonstrate a clear advantage over existing multilingual models, especially those designed for long sequences; the performance delta is unclear.\" (Abstract, Related Work) - Partially addressed. The abstract and related work sections do mention comparisons to mBART and M-BERT. However, we acknowledge that the performance delta could be clearer. In the next version, we will include a more detailed analysis of the performance gains, specifically highlighting the advantages of mLongT5 in handling longer sequences compared to models like mT5, which, while multilingual, are not optimized for long inputs. We will also include a more detailed discussion of the limitations of existing models in the related work section.\n\n*   **Weakness 2:** \"The paper claims mLongT5 handles multilingual inputs and outputs and performs well on a variety of tasks, but the evidence is not fully provided, and only one dataset is shown.\" (Sec 5) - Partially addressed. Section 4 presents results on MLSUM, XL-Sum, and WikiLingua. We will clarify this in the revised version and ensure the variety of tasks is more explicitly highlighted in the abstract and conclusion. We will also add a more detailed discussion of the multilingual capabilities, including the datasets used and the metrics, and include results on summarization tasks.\n\n*   **Weakness 3:** \"The paper lacks a clear definition of 'efficient attention mechanism' and how it is implemented in LongT5 and mLongT5.\" (Section 3) - Partially addressed. Section 3 mentions that mLongT5 builds upon the architecture of LongT5, which utilizes a more efficient attention mechanism. We will expand on this in the revised version by providing a more detailed explanation of the efficient attention mechanism, including specific architectural details, such as the use of sparse attention patterns or other techniques employed by LongT5.\n\n*   **Weakness 4:** \"The tables do not explicitly mention the inclusion of standard deviations or confidence intervals, making it difficult to assess the statistical significance of the results. The absence of p-values makes it hard to determine if the observed differences are statistically significant.\" (Tables 1-5) - Acknowledged. We agree that including statistical significance measures would strengthen the paper. In the next version, we will include standard deviations or confidence intervals for all reported metrics and add p-values to indicate the statistical significance of the differences between the proposed model and the baselines.\n\n*   **Weakness 5:** \"The paper lacks a discussion of potential misuse or failure modes.\" (Insufficient evidence) - Acknowledged. We will add a Broader Impact section with mitigation strategies in the next version.\n\n*   **Weakness 6:** \"The paper does not explicitly compare mLongT5 with LongT5 on multilingual tasks, nor with other multilingual models like mT5 and umT5 on long-input tasks.\" (Related Work) - Partially addressed. The paper compares mLongT5 with mT5 in the results section. We will clarify this in the revised version and add a more direct comparison with LongT5 on multilingual tasks, and with umT5 on long-input tasks.\n\n*   **Weakness 7:** \"Dataset license and usage restrictions are not stated.\" (Insufficient evidence) - Acknowledged. We will add explicit license and consent statements for datasets used in the next version.\n\n*   **Weakness 8:** \"Some figures lack clear axis labels and units, and the absence of legends in certain figures makes it difficult to interpret the data.\" (Fig 3) - Acknowledged. We will ensure all figures have clear axis labels, units, and legends in the next version.\n\n*   **Weakness 9:** \"The paper does not provide a table of notation, making it difficult to follow the equations and formulas.\" (Throughout the paper) - Acknowledged. We will include a table of notation to define all symbols and abbreviations used in the paper in the next version.\n\n**Suggestions:**\n\n*   **Suggestion 1:** \"Include a direct comparison with a state-of-the-art multilingual model that also handles long sequences.\" (Evaluation section) - Addressed. We will include a more direct comparison with umT5, which is a state-of-the-art multilingual model, in the evaluation section.\n\n*   **Suggestion 2:** \"Provide more details on the multilingual capabilities of mLongT5, including the datasets used and the metrics, and include results on summarization tasks.\" (Sec 5, Sec 4) - Addressed. We will expand on the details of the multilingual capabilities and include more results on summarization tasks.\n\n*   **Suggestion 3:** \"Provide a more detailed explanation of the efficient attention mechanism used in LongT5 and mLongT5, including specific architectural details.\" (Section 3) - Addressed. We will provide a more detailed explanation of the efficient attention mechanism.\n\n*   **Suggestion 4:** \"Include a table of notation to define all symbols and abbreviations used in the paper.\" (Throughout the paper) - Addressed. We will include a table of notation.\n\n*   **Suggestion 5:** \"In performance plots, include error bars to indicate confidence intervals, ensure all axes are clearly labeled with units, and add legends to clarify the meaning of different visual elements.\" (Fig 2, Fig 3) - Addressed. We will ensure all plots have error bars, axis labels, units, and legends.\n\n*   **Suggestion 6:** \"Include standard deviations or confidence intervals for all reported metrics, add p-values to indicate the statistical significance of the differences between the proposed model and the baselines, and clearly define all acronyms and abbreviations used in the table headers and captions.\" (Tables 1-5) - Addressed. We will include these statistical measures and define all acronyms and abbreviations.\n\n*   **Suggestion 7:** \"Add a Broader Impact section with mitigation strategies.\" (Conclusion) - Addressed. We will add a Broader Impact section.\n\n*   **Suggestion 8:** \"Conduct experiments on the TyDi QA, WikiLingua, and XLSum datasets to evaluate the performance of mLongT5 and compare it with existing multilingual models.\" (Sec 4.1) - Partially addressed. We have already conducted experiments on these datasets, as shown in Section 4. We will clarify this and ensure the comparisons are more explicit.\n\n*   **Suggestion 9:** \"Add explicit license and consent statements for datasets used.\" (Insufficient evidence) - Addressed. We will add explicit license and consent statements."
  }
]