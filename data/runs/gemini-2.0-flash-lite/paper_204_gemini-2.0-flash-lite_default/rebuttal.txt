{
  "rebuttal": "We thank the reviewer for their thorough and insightful feedback. We appreciate the time and effort invested in reviewing our paper. We address each point below:\n\n**Weaknesses:**\n\n*   **Reproducibility (Seeds):** The reviewer points out the lack of specific seeds used for training models (Sec 3.1). We acknowledge this omission. While we detailed the model architectures, datasets, and training procedures, we did not explicitly state the random seeds. We will include the specific seeds used for each experiment in the next version of the paper and in the code repository to ensure full reproducibility. (Proposed Change)\n\n*   **Novelty of Independents:** The reviewer suggests we overclaim the novelty of including independent political identities (Limitations). We respectfully disagree. While we acknowledge that prior work has studied political polarization, the inclusion of Independents alongside Democrats and Republicans allows for a more nuanced understanding of offense perception. As shown in Figure 10 and the associated discussion in Section C.2, the distribution of Independents is a crucial aspect of the US political landscape. We will strengthen this point by adding a comparative analysis, as suggested, in the Limitations section, comparing our findings with existing work on political polarization to highlight the unique contribution of including Independents. (Partially Addressed, Proposed Change)\n\n*   **Overclaiming Dataset Impact:** The reviewer suggests that our claim about the dataset opening gates for modeling ideas is an overstatement (Introduction). We acknowledge that we did not provide specific modeling ideas or preliminary results in this version. However, we believe that the introduction of the vicarious offense dataset, as well as the comprehensive noise audit, provides a valuable resource for future research. We will revise the language in the Introduction to be more precise and less assertive, focusing on the dataset's potential for future modeling efforts. (Partially Addressed)\n\n*   **Figure Clarity:** The reviewer notes the lack of clear axis labels, legends, and units in several figures (Figures 3, 5-7, 9, 11-14). We agree that improving figure clarity is crucial. We will add clear labels to all axes, including units where applicable, and provide legends to explain the meaning of colors, symbols, and abbreviations used in all figures. (Proposed Change)\n\n*   **Statistical Analysis in Tables:** The reviewer points out the lack of quantitative statistical analysis in several tables (Tables 1, 2, 6, 7, 8). We will add standard deviations, confidence intervals, and p-values to these tables to quantify the significance of the observed differences and performance. (Proposed Change)\n\n*   **Vicarious Offense Definition:** The reviewer suggests that the definition of 'vicarious offense' could be more precise (1.1 Definitions). We will refine the definition of 'vicarious offense' in Section 1.1 to provide a clearer and more concise explanation of this concept. (Proposed Change)\n\n*   **Misuse/Failure Modes:** The reviewer requests a discussion of potential misuse or failure modes. We will add a Broader Impact section to the Conclusion, as suggested, discussing potential misuse and mitigation strategies. (Proposed Change)\n\n*   **Comparison with Related Work:** The reviewer suggests explicitly comparing our findings with those of [1] Dallas Card et al. (2022) and [2] Ranasinghe and Zampieri (2020). We will add a more detailed comparison of our work with these papers in the Related Work section, highlighting the key differences and contributions. (Proposed Change)\n\n**Suggestions:**\n\n*   **Computational Resources:** The reviewer suggests including a section detailing the computational resources used for training and evaluation (Sec 3.1). This information is partially addressed in Section B, where we mention the use of a GeForce RTX 3090 GPU. We will expand on this by including specific hardware and software versions in the next version. (Partially Addressed, Proposed Change)\n\n*   **Dataset Details:** The reviewer requests more details on the 'vicarious offense' dataset (Intro ยง1.2). The dataset size, annotation guidelines, and inter-annotator agreement are already discussed in Section 3.2 and Appendix C. We will ensure that these details are clearly highlighted in the Introduction and Section 1.2 to avoid any confusion. (Addressed)\n\n*   **Code Repository:** The reviewer suggests providing a link to the code repository with training scripts and environment setup (Code/Data Availability, Sec 3.1). We will provide a link to our code repository, including the training scripts, specific versions of the libraries used, and a requirements.txt file for environment setup. We will also specify the hardware used for training and report the seed used for each experiment and the variance of the results (e.g., standard deviation) across multiple runs. (Proposed Change)\n\n*   **Experiment with Chen et al. (2021):** The reviewer suggests conducting an experiment comparing the performance of the proposed method with the methods used in [1] on a shared dataset of political discourse and evaluating the performance of the proposed method on the dataset from [3] Chen et al. (2021) and compare it with existing offensive language identification methods. We appreciate this suggestion. However, due to the scope of this work, we will prioritize the improvements mentioned above. We will consider this suggestion for future work. (Not Applicable in Current Scope)"
}