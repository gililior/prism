{
  "summary": "This paper introduces APROMPT, a novel approach to prompt tuning that incorporates prompts into the attention mechanism of Transformer models. The paper demonstrates APROMPT's effectiveness through comprehensive experiments, but requires more rigorous statistical analysis and comparisons to fully assess its impact. The authors have addressed some of the weaknesses raised in the initial review, including plans to incorporate statistical significance and clarify figures.",
  "strengths": [
    {
      "kind": "strength",
      "text": "The paper provides a comprehensive evaluation of APROMPT by comparing it with several parameter-efficient methods, including Partial tuning, Adapter, BitFit, and LoRA, as shown in Table 7. (Table 7) [experimental_design]",
      "grounding": "(Table 7)",
      "facet": "experimental_design"
    },
    {
      "kind": "strength",
      "text": "The paper clearly positions APROMPT within the context of prompt tuning and parameter-efficient fine-tuning methods, highlighting the limitations of existing approaches and the potential for improvement by incorporating prompts into the attention mechanism (Intro, Related Work). (Intro, Related Work) [positioning]",
      "grounding": "(Intro, Related Work)",
      "facet": "positioning"
    },
    {
      "kind": "strength",
      "text": "The paper claims that existing prompt tuning can be considered a special case of attention prompt tuning, which provides a valuable theoretical connection and enhances the understanding of prompt tuning techniques (Intro). (Intro) [originality]",
      "grounding": "(Intro)",
      "facet": "originality"
    },
    {
      "kind": "strength",
      "text": "Figure 1 provides a clear illustration of the proposed APROMPT method and compares it with existing methods (Fig 1). (Fig 1) [figures]",
      "grounding": "(Fig 1)",
      "facet": "figures"
    },
    {
      "kind": "strength",
      "text": "The abstract clearly outlines the problem, proposed solution (APROMPT), and key findings (Abstract). (Abstract) [clarity_presentation]",
      "grounding": "(Abstract)",
      "facet": "clarity_presentation"
    },
    {
      "kind": "strength",
      "text": "Table 2 includes the number of trainable parameters ('Para') and indicates statistical significance with respect to all baselines (p-value < 0.005) (Table 2). (Table 2) [tables]",
      "grounding": "(Table 2)",
      "facet": "tables"
    },
    {
      "kind": "strength",
      "text": "The paper clearly differentiates its approach, APROMPT, from existing prompt tuning methods by incorporating prompts into the attention layers, which is a novel contribution (Intro, Related Work). (Intro, Related Work) [novelty]",
      "grounding": "(Intro, Related Work)",
      "facet": "novelty"
    }
  ],
  "weaknesses": [
    {
      "kind": "weakness",
      "text": "The paper lacks details on the statistical significance of the performance gains, making it difficult to assess the reliability of the results (Table 7). (Table 7) [analysis]",
      "grounding": "(Table 7)",
      "facet": "analysis"
    },
    {
      "kind": "weakness",
      "text": "The paper needs to provide more detailed comparisons with the closest baselines, especially those that also explore attention mechanisms or modifications to the Transformer architecture. The delta needs to be clearer (Related Work, Sec 4.1). (Related Work, Sec 4.1) [comparative_evidence]",
      "grounding": "(Related Work, Sec 4.1)",
      "facet": "comparative_evidence"
    },
    {
      "kind": "weakness",
      "text": "Figures 5, 6, and 7 lack clear axis labels and units, making it difficult to interpret the results quantitatively. The legends are missing in some figures (Figures 5, 6, 7). (Figures 5, 6, 7) [figures]",
      "grounding": "(Figures 5, 6, 7)",
      "facet": "figures"
    },
    {
      "kind": "weakness",
      "text": "The paper lacks a clear definition of 'attention prompt tuning' beyond the high-level description in the abstract and introduction. A more formal definition, including mathematical notation, is needed (Abstract, §1). (Abstract, §1) [clarity_presentation]",
      "grounding": "(Abstract, §1)",
      "facet": "clarity_presentation"
    },
    {
      "kind": "weakness",
      "text": "The paper does not provide a detailed explanation of the self-attention mechanism or how APROMPT modifies it. Readers unfamiliar with Transformers may struggle to understand the approach (§1). (§1) [clarity_presentation]",
      "grounding": "(§1)",
      "facet": "clarity_presentation"
    },
    {
      "kind": "weakness",
      "text": "Table 1 lacks standard deviations or confidence intervals, making it difficult to assess the reliability of the reported accuracy scores. The table also lacks p-values to indicate statistical significance (Table 1). (Table 1) [tables]",
      "grounding": "(Table 1)",
      "facet": "tables"
    },
    {
      "kind": "weakness",
      "text": "The paper does not address consent or privacy considerations related to the datasets used, nor does it discuss potential ethical risks associated with the datasets or the model. (Insufficient evidence) [ethics_privacy]",
      "grounding": "(Insufficient evidence)",
      "facet": "ethics_privacy"
    },
    {
      "kind": "weakness",
      "text": "The paper does not provide information on the licenses of the datasets used. (Insufficient evidence) [ethics_licensing]",
      "grounding": "(Insufficient evidence)",
      "facet": "ethics_licensing"
    },
    {
      "kind": "weakness",
      "text": "No discussion of potential biases or fairness issues or broader impacts. (Insufficient evidence) [fairness]",
      "grounding": "(Insufficient evidence)",
      "facet": "fairness"
    },
    {
      "kind": "weakness",
      "text": "While the paper mentions UniPELT [1], a direct comparison in terms of performance and parameter efficiency is missing. This is a key baseline for parameter-efficient tuning (Related Work). (Related Work) [comparison]",
      "grounding": "(Related Work)",
      "facet": "comparison"
    },
    {
      "kind": "weakness",
      "text": "The paper should compare against SideTuning [2] as it is also an extra module method (Related Work). (Related Work) [comparison]",
      "grounding": "(Related Work)",
      "facet": "comparison"
    }
  ],
  "suggestions": [
    {
      "kind": "suggestion",
      "text": "Provide the code, including the training scripts, and a requirements file (e.g., `environment.yml` or `requirements.txt`) to specify the software environment (All sections). (All sections) [code/data availability]",
      "grounding": "(All sections)",
      "facet": "code/data availability"
    },
    {
      "kind": "suggestion",
      "text": "Include a direct comparison with a recent method that also modifies the attention mechanism or incorporates prompts in a similar way. This could be a contemporaneous method (Sec 4.1). (Sec 4.1) [comparative_evidence]",
      "grounding": "(Sec 4.1)",
      "facet": "comparative_evidence"
    },
    {
      "kind": "suggestion",
      "text": "Provide a more detailed ablation study to analyze the impact of each type of prompt (query, key, value) and their combinations (Sec 4.2). (Sec 4.2) [originality]",
      "grounding": "(Sec 4.2)",
      "facet": "originality"
    },
    {
      "kind": "suggestion",
      "text": "Analyze the computational cost and memory footprint of APROMPT compared to other prompt tuning and fine-tuning methods (Intro). (Intro) [comparative_evidence]",
      "grounding": "(Intro)",
      "facet": "comparative_evidence"
    },
    {
      "kind": "suggestion",
      "text": "Include a section detailing the datasets used, their licenses, and any relevant usage restrictions (Insufficient evidence). (Insufficient evidence) [ethics_licensing]",
      "grounding": "(Insufficient evidence)",
      "facet": "ethics_licensing"
    },
    {
      "kind": "suggestion",
      "text": "Address consent and privacy considerations, especially if the datasets contain personal information (Insufficient evidence). (Insufficient evidence) [ethics_privacy]",
      "grounding": "(Insufficient evidence)",
      "facet": "ethics_privacy"
    },
    {
      "kind": "suggestion",
      "text": "Add a section on ethical risks and mitigations (Insufficient evidence). (Insufficient evidence) [ethics_risks]",
      "grounding": "(Insufficient evidence)",
      "facet": "ethics_risks"
    },
    {
      "kind": "suggestion",
      "text": "Add a Broader Impact section with mitigation strategies (Conclusion). (Conclusion) [societal_impact]",
      "grounding": "(Conclusion)",
      "facet": "societal_impact"
    },
    {
      "kind": "suggestion",
      "text": "Conduct an experiment comparing APROMPT with UniPELT [1] on the SuperGLUE benchmark, measuring both performance and the number of trainable parameters. This will highlight the efficiency gains (Sec 4.1). (Sec 4.1) [experiment]",
      "grounding": "(Sec 4.1)",
      "facet": "experiment"
    },
    {
      "kind": "suggestion",
      "text": "Include an ablation study to isolate the impact of incorporating prompts into the query, key, and value components of the attention mechanism, compared to only using input prompts (Sec 4.1). (Sec 4.1) [experiment]",
      "grounding": "(Sec 4.1)",
      "facet": "experiment"
    }
  ],
  "scores": null,
  "overall": null,
  "confidence": null
}