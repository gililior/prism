[
  {
    "rebuttal": "We sincerely thank the reviewer for their insightful and detailed feedback. We appreciate the time and effort invested in reviewing our paper. We address each point below:\n\n**Weaknesses:**\n\n*   **Statistical Significance (Table 7):** The reviewer points out the lack of statistical significance in Table 7. We acknowledge this omission and will address it in the revised manuscript. We will include standard deviations and p-values to indicate statistical significance in Table 7 and all other tables reporting performance comparisons. We will use a t-test to compare the performance of APROMPT with the baselines.\n\n*   **Detailed Comparisons with Baselines (Related Work, Sec 4.1):** The reviewer requests more detailed comparisons with baselines, especially those exploring attention mechanisms. We believe that Section 4.1 already provides a good overview of the related work. However, we will expand the discussion in Section 2 (Related Work) to include a more in-depth comparison with methods that modify the attention mechanism or incorporate prompts in a similar way. We will also add a more detailed analysis of the performance deltas in Section 5.4.\n\n*   **Figure Clarity (Figures 5, 6, 7):** The reviewer notes the lack of axis labels, units, and legends in Figures 5, 6, and 7. We acknowledge this and will revise these figures to include clear axis labels (with units where applicable), legends, and error bars representing standard deviations to improve clarity and quantitative interpretation.\n\n*   **Formal Definition of APROMPT (Abstract, ยง1):** The reviewer requests a more formal definition of 'attention prompt tuning.' We will add a dedicated section (likely in Section 3.1 or 4) to formally define APROMPT, including mathematical notation for the query, key, and value prompts and their integration into the attention mechanism. We will also include a table of notation to clarify the symbols used throughout the paper.\n\n*   **Explanation of Self-Attention (Section 1):** The reviewer suggests that readers unfamiliar with Transformers may struggle to understand our approach. We will add a brief, more detailed explanation of the self-attention mechanism in Section 1, possibly with a figure illustrating how APROMPT modifies it. This will provide necessary context for readers.\n\n*   **Table 1 Statistics (Table 1):** The reviewer points out the lack of standard deviations, confidence intervals, and p-values in Table 1. We will include standard deviations or confidence intervals for all reported accuracy scores in all tables. We will add p-values to indicate statistical significance of performance differences between APROMPT and the baselines in all tables. We will also clearly define the tasks and metrics used in each table.\n\n*   **Consent, Privacy, and Licenses (Insufficient evidence):** We acknowledge the lack of discussion regarding consent, privacy, and licenses. We will add a section detailing the datasets used, their licenses, and any relevant usage restrictions. We will also address consent and privacy considerations, especially if the datasets contain personal information.\n\n*   **Ethical Risks and Biases (Insufficient evidence):** We will add a section on ethical risks and mitigation strategies, including potential biases and fairness issues.\n\n*   **Comparison with UniPELT (Related Work):** The reviewer requests a direct comparison with UniPELT. We will add a comparison with UniPELT in the revised manuscript, measuring both performance and the number of trainable parameters. This will highlight the efficiency gains of our approach.\n\n*   **Comparison with SideTuning (Related Work):** We will include a comparison with SideTuning in the related work section.\n\n**Suggestions:**\n\n*   **Code and Environment (All sections):** We plan to release the code, including the training scripts, and a requirements file (e.g., `environment.yml` or `requirements.txt`) to specify the software environment. We will include this information in the supplementary material.\n\n*   **Ablation Study (Sec 4.2):** We will provide a more detailed ablation study to analyze the impact of each type of prompt (query, key, value) and their combinations. We will also include an ablation study to isolate the impact of incorporating prompts into the query, key, and value components of the attention mechanism, compared to only using input prompts.\n\n*   **Computational Cost and Memory Footprint (Intro):** We will add an analysis of the computational cost and memory footprint of APROMPT compared to other prompt tuning and fine-tuning methods.\n\n*   **Broader Impact Section (Conclusion):** We will add a Broader Impact section with mitigation strategies.\n\n*   **Prompt Positions (Table 4):** The reviewer's suggestion to analyze prompt positions is already addressed in Table 4 and the related discussion in Section 6.\n\nWe believe these revisions will significantly improve the clarity, rigor, and impact of our work. Thank you again for the valuable feedback."
  }
]