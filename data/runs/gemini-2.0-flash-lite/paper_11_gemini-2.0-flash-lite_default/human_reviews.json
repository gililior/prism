[
  {
    "rid": "nDCeASmceL",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper introduces a new Twitter benchmark for sentiment analysis in african languages, and provide experimental results with few baseline models.",
      "reasons_to_accept": "- A new useful dataset that can be used for sentiment analysis in Afriacan languages.\n- Extensive experimental results on the benchmark.",
      "reasons_to_reject": "- Limited explanation of imbalanced data distribution in Table 6. Why there is no training samples in 'orm' and 'tir'? I see that there was a difficulty in collecting data from annotators, but it does not explain why we don't have training samples. For example, we can have 1500 training samples and 600 test samples in 'orm' language.\n- More detailed experimental results are required. In Table 8, only F1 score is reported, but it is better to give per-label precision and recall as well.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",
      "excitement": "3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  },
  {
    "rid": "pKSBS3XZlG",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The authors present a dataset of 12 African languages, annotated for sentiment by native (and usually local) speakers. They describe challenges in collection and annotation. They also present some results from pre-trained language models, discussing which models do best with which languages and which source languages do the best in zero-shot learning for Oromo and Tigrinya.",
      "reasons_to_accept": "A very good reason to accept this paper is the dataset itself and the ability to talk about collection and annotation outside the parts of the world NLP researchers typically focus on. The authors share specific problems and their solutions that should also be good for discussion and help researchers who are interested in working on other lower resource languages.  They are also releasing the annotations, not just the majority-vote label, which should be useful for researchers interested in methodologies and inter-annotator agreement topics.",
      "reasons_to_reject": "There are no risks for having this paper presented at the conference. In the next section, you'll see me ask a bunch of questions of the authors but I don't really consider these weaknesses, certainly not enough to jump up to a reason to reject.",
      "questions_for_the_authors": "I'm going to ask my questions in the order they pop up in the text, but I'll try to give a sense of which ones I think are more important to address: 1) You have picked sentiment and in lines 47-56, you mention a number of reasons that sentiment may be useful (literary analysis, culturonomics, commerce, psychology, social science). One that I don't think those quite capture would be \"linguistic\". Here, I'm thinking about how, for example, there's a malefactive particle in at least one Ethiopian language that lets a speaker/writer give the sense that someone did something AGAINST someone else, in Dutch there's \"tet\" which marks surprise/irritation, in Czech diphthongs can mean affection (or pejoration), in Cantonese, throwing a -k at the end of particles can intensify the emotion being expressed, in Tongan there are determiners that express sympathy, in Manambu there is a \"frustrative\" case marking for when things are done in vain, etc.  In my heart of hearts, my hope is that your dataset could help people find affective markers/word-order/morphemes but...is that plausible, do you think? Or is your expectation that what your annotators picked up on were louder words like adjectives (\"awful\", \"wonderful\", etc). I don't think this particularly needs to be added to your paper, but I am curious about how you'd look at the dataset being used for more linguistic/sociolinguistic/pragmatics research questions.\n2) You mentioned using Saif Mohammad's sentiment guidelines, I think you mean the \"base\" questions he has (not the ones that try to get at what the sentiment is direct towards). It probably would be good to share the explicit instructions for annotators, probably in an appendix. This I do think is probably a minor weakness and worth addressing, btw. It also connects to some of your comments on code-switching texts\u2014you have some annotators who don't know parts of those mixed language tweets and it feels like they should have a \"I don't understand this\" option (which would also be relevant for everyone else since I am certain as a native English speaker you could give me completely English tweets that I could not understand to give a sentiment to).  2b) Part of this question comes from looking at your examples in Table 1. The Amharic example kicks off with the very powerful bigram \"\u1328\u12ab\u129d \u12a0\u1228\u1218\u1294\" (~brutal barbarian, pitiless pagan) but the rest of it has a much lighter tone as far as I can tell. I don't know the referent but it strikes me that this is fairly sarcastic and has an intent to be humorous which is \"negative\" in a way that is less clear than \"I hate X\". You mention sarcasm a number of times\u2014as does Mohammad 2016\u2014I'm wondering what you might say about that topic more generally.  This also feels related to the annotation problem you describe in Twi with \u201cTweaa\u201d being potentially offensive depending on context\u2014many annotators have a hard time distinguishing a speaker/writer\u2019s sentiment when they use taboo terms (the terms that one community of speakers might think is unexceptional or just expressive may be no-go\u2019s for other speakers who ignore the speaker intention). Do you have any thoughts about your annotators\u2019 predilections on these things?  If I gave you a magic button to press that would replace positive/negative/neutral with any categories of your choosing with high accuracy, which categories would you pick? That is, is valence the real thing you most want or is it a stand-in/approximation/first-step for something you actually would rather collect but is harder?\n3) You briefly mention some of the ways African languages present challenges for sentiment analysis that English and other languages may not. You mention tone, code-switching, and digraphia in lines 127-128 but I don't think you actually talk about tone. Here, I assume you mean phonemic tone since lots of African languages are tonal in nature. You discuss code-switching and digraphia but do you have anything to say about phonemic tone and your dataset/annotations? About half your languages use tone but I think only Yor\u00f9b\u00e1 marks them in writing...but maybe the lack of marking them in tweets causes some extra homographs but...I'm not sure how that would be different than other languages which are replete with homographs (English lead=metal, lead=guide).  4) You have to do a bunch of things to get a meaningful (not-all-neutral) set of tweets. I think you do a pretty good job but I wonder if you could address what you see as the consequences of these choices for possible uses of the dataset. This is a suggestion I think would help the paper.  4b) A minor thing but why are the Nigerian language test sets lower-cased (you reference Muhammad et al 2022, but it\u2019d be nice if you give the reason so readers don\u2019t have to track that reference down).  4c) You discard \u201cfull disagreement\u201d tweets\u2014are those going to be in the dataset? I think that may help with the use of your dataset, provided it\u2019s easy for people to see that these are more for agreement/annotator kinds of questions. I think your mention of Prabhakaran et al (2021) means that you probably do release these but it\u2019s not completely clear to me.  4d) What do you think are the consequences of removing offensive tweets in Algerian Arabic? And was your intention more to produce a non-offensive dataset or to not disturb your annotators? I think it is relevant to share your intentions for this.  5) The idea that Yor\u00f9b\u00e1 is a generally good source language strikes me as very strange. I mean, you are empirically showing it to be the case and mention Adelani et al\u2019s similar finding in NER but...this strikes me as \u201csurely a coincidence\u201d or perhaps because there are some proper names with similar sentiment in Ethiopia and Nigeria\u2026what do you think the most reasonable explanation of this is?\n6) Your plan is to keep releasing more languages (and enable others to follow your lead), which is great. I wonder which languages are at the top of your list. Do you want coverage in terms of speaker populations? Coverage over countries that yet covered? Languages that are outside the language families you\u2019ve already covered? I\u2019m interested in your actual \u201cTop Five\u201d but even moreso for what leads you to that Top Five instead of another Top Five. I think being explicit about this helps readers understand where you're headed/why and what they should keep an eye out for or help with.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "xSHspiDcCx",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper describes a sentiment dataset creation for 14 African languages and performed several experiments to classify the tweets into sentiment categories.",
      "reasons_to_accept": "1. This dataset with 14 African languages benefits the NLP community to develop sentiment classification tools. \n2. The data collection challenges were described in detail which is useful for NLP researchers working for low-resource languages.",
      "reasons_to_reject": "1. There is no innovation in implemetation for NLP development.",
      "questions_for_the_authors": "1. Sentiment categories are not explicitly mentioned in the text. I can guess from Table 1 that there are three categories (positive, negative and neutral). The authors should provide the categories in the text.  2. More details on the models will be helpful. How the fine tunning was performed? What are the tunning parameters? The architecture of the system will be helpful.  3. As the F1-score for the system are no great, what can be done to improve the system performances?  4. What was the reason to test the cross-lingual system only on orm and tir languages?  5. Is there any statistics avaialble on percentage of code-switching and code-mixing? It will be great to have these information in the paper.  6. Are these trained models will also be avialble for the researchers?",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  }
]