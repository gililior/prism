[
  {
    "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback. We appreciate the time and effort invested in reviewing our work. We address each concern below:\n\n**Weaknesses:**\n\n*   **Overclaiming Generalizability:** We acknowledge the reviewer's concern regarding the generalizability of our prompt engineering recommendations in the Ethics & broader impact section. We respectfully disagree that the paper overclaims. We explicitly state that \"providing recommendations for prompt engineering from a linguistic point of view is not the decided aim of this study and indeed any recommendations that could be derived from our results appear localised to the context of a particular models or dataset.\" (Section 8). We will revise this section to further emphasize the context-specific nature of our findings and avoid any potential misinterpretations.\n\n*   **Lack of Reproducibility Information:** The reviewer is correct that we did not explicitly state the seeds used for our experiments and environment details. We acknowledge this omission and will rectify it in the next version. We will include the seeds used for each experiment in Section 4.2 and provide a link to a code repository (e.g., GitHub) with the code, data, and a detailed README file, including the environment setup (e.g., Conda environment.yml or Dockerfile). This will significantly improve reproducibility.\n\n*   **Interchangeable Use of 'Prompt' and 'Instruction':** The reviewer is correct. We use the terms 'prompt' and 'instruction' interchangeably. We will define these terms clearly in the Abstract and Section 1 and consistently use them throughout the paper to avoid confusion.\n\n*   **Lack of Notation Table:** We acknowledge the lack of a notation table. We will create a table or list in Section 1 to clarify the different LLMs, datasets, and tasks used in the experiments, improving readability and clarity.\n\n*   **Lack of Statistical Rigor in Result Presentation:** The reviewer is correct that Table 1 lacks statistical information and Tables 9-20 lack standard deviations or confidence intervals. We will address this in the next version. We will add p-values to Tables 2, 3, and 4 to clarify the significance of the differences. For Tables 9-14, we will include standard deviations or confidence intervals for the accuracy results to provide a measure of result variability. For Tables 15-20, we will provide context by including the mean and standard deviation of the perplexity scores.\n\n*   **Dataset Licensing and Usage Restrictions:** We will add explicit license and consent statements for datasets used in Appendix D.\n\n*   **Lack of Discussion on Misuse/Failure Modes:** We will add a Broader Impact section with mitigation strategies in the Conclusion.\n\n**Suggestions:**\n\n*   **Quantitative Comparison with Prompt Optimization:** We respectfully disagree with the suggestion to include a quantitative comparison with a state-of-the-art prompt optimization method. Our focus is on understanding the impact of linguistic variations in prompts, not on proposing a new prompt optimization method. We believe that comparing our work with prompt optimization methods would shift the focus of our study. However, we will include a discussion of how our findings relate to prompt optimization in the related work section.\n\n*   **Detailed Analysis of Linguistic Features:** We agree with the suggestion to provide a more detailed analysis of the specific linguistic features that correlate with performance variations. We already provide this in Section 6, where we analyze the correlation between accuracy and perplexity, word sense ambiguity, frequency of synonyms, and prompt length. We will expand this section to include more detailed statistical analysis and potentially case studies to further clarify the relationship between linguistic features and performance.\n\n*   **Code Repository:** We will provide a link to the code repository (e.g., GitHub) with the code, data, and a detailed README file, including the environment setup (e.g., Conda environment.yml or Dockerfile). (Code/Data Availability)\n\n*   **Report Seeds and Variance:** We will report the seeds used for all experiments and the variance (e.g., standard deviation) across multiple runs. (Section 4.2)\n\n*   **Standard Deviations/Confidence Intervals and P-values:** We will include standard deviations or confidence intervals for the accuracy results in Tables 9-14 to provide a measure of result variability. Add p-values to Tables 2, 3, and 4 to clarify the significance of the differences. For Tables 15-20, provide context by including the mean and standard deviation of the perplexity scores. (Tables 2-4, Tables 9-20)\n\n*   **Define 'Prompt' and 'Instruction':** We will define 'prompt' and 'instruction' early on and consistently use the terms. (Abstract, ยง1)\n\n*   **Create a Table or List:** We will create a table or list to clarify the different LLMs, datasets, and tasks used in the experiments. (ยง1, ยง2)\n\n*   **Add a Broader Impact Section:** We will add a Broader Impact section with mitigation strategies. (Conclusion)\n\n*   **Experiment with Multi-task Prompted Training:** We respectfully disagree with the suggestion to conduct an experiment comparing the performance of the proposed linguistic variations of prompts with the multi-task prompted training approach of [1] (Sanh et al., 2022). Our focus is on understanding the impact of linguistic variations in prompts, not on proposing a new prompt optimization method. We believe that comparing our work with prompt optimization methods would shift the focus of our study. However, we will include a discussion of how our findings relate to prompt optimization in the related work section.\n\n*   **Discussion of Potential Biases:** We will include a discussion of potential biases in the datasets used for evaluation. (Insufficient evidence)\n\n*   **Add Explicit License and Consent Statements:** We will add explicit license and consent statements for datasets used. (Appendix D)"
  }
]