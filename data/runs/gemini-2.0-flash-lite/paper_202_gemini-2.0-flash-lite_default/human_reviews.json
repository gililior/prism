[
  {
    "rid": "9eEVDw2FHU",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This is an experimental paper that aims to investigate the following research question:  Do linguistic variations in prompts have an effect in performance over final NLP tasks for LLMs? \nTo pursue its goal, the paper examined the performance of LLMs on different classification tasks mainly contained in GLUE and SuperGLUE. Linguistic variations are characterized by different features: mood, aspect, tense, modality, and synonymy.",
      "reasons_to_accept": "- The paper aims to make a significant contribution to the understanding of how prompts should be phrased in order to obtain the best results over linguistic tasks.  - The paper performs extensive experimental analysis",
      "reasons_to_reject": "- The large experimental analysis does not reveal any clear pattern and does not help in choosing what are the best ways to write a prompt for a classification task - The analysis is confined to classification tasks - It is even questionable if it is important to perform a linguistic classification task with LLMs. Indeed, solving many of the tasks is useless since we have LLMs that make decisions and generate responses without using explicit solvers for these tasks.",
      "questions_for_the_authors": "- Can you please elaborate on the reason why you have decided to not apply synonymy to the main verb of the prompt?",
      "missing_references": "No missing references",
      "typos_grammar_style_and_presentation_improvements": "Writing the introduction: - please reorganize the introduction such that there is a clear paragraph describing the work of your paper, which is: studying the effect on performances of syntactic variations on semantically equivalent prompts.\n- \"To test ***this hypothesis***, we examine prompting ... \" (line 056). It is weird to start a paragraph with an anaphoric word referring to an idea expressed in the previous paragraph. This sentence needs rephrasing.  - Make clear that the \"hypothesis\" is expressed in lines 051-055 (that is **this hypothesis***) and it is part of your intuition in building the paper",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",
      "excitement": "2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "396OXbrxNc",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper conducts comprehensive experiments to show the prompt instability of LLMs. Unlike previous work, this paper investigates the prompt robustness of the model from a lexical perspective, where the authors slightly shift the linguistic properties (e.g., mood, tense) of prompt and find a significant performance variation of LLMs, including instruction-tuned LLMs. Some results are also different from the previous works, suggesting the vulnerability of numerous research findings in current prompt-related research. Thus, the author also concludes with some empirical advice for future research.",
      "reasons_to_accept": "1. ** A novel perspective**. The author utilizes a lexico-level perturbation and linguistic perspective to investigate the prompt robustness, which differs from the previous sentence-level paraphrasing or simple synonymy replacing. \n2. ** Useful proposal**. The proposal concluded by this paper might also benefit future research.",
      "reasons_to_reject": "1. ** Limited contribution**. Though this paper provides some useful suggestions, most conclusions have already been proposed by a bunch of previous works or are just commonsense for the community (e.g., prompt transfer). During the whole section 5, the authors simply introduce the results and compare scores, but there are no more insights, analysis, or explanations. After reading the whole paper, I didn't seem to learn much from it.\n2. ** Concern about the experiment setting.** In the experiment, the authors adopt five models, but essentially, there are only two model categories --- vanilla LMs and instruction-tuned LMs. Similarly, all of these models (OPT, LLaMA) are decoder-only LMs. I am worried about the expandability of the experimental conclusion. Similarly, the datasets seem only cover the cross-dataset generalization setting; a better choice is to adopt a more challenging cross-task setting [1], which is a popular trend in the current community.\n3. ** Some viewpoints are subjective.** For example, in line 537, the authors suggest \"include estimates of performance mean and variance based on a large set of prompts\". Personally, I think the prompt is just a fundamental \"feature\" for LLMs to deal with a specific task, \"prompt engineering\" is just similar to \"feature engineering\"; we have to tune an optimal (or sub-optimal) prompt for one specific task, but we cannot anticipate the LMs will be robust to the change of the prompt. So reporting statistic scores on large-set prompts for one task seems unrealistic. But I agree with \"treating prompts as hyperparameters\" --- tune the prompt on dev set, fix it, and then repot scores on the test set.\n4. ** Writing.** I found it hard to follow some specific sections of this paper. For example, the title of section 5.3 --- \"Robustness and instruction-tuning\" --- sounds a little bit confusing.\n--- References: [1]. Mishra S, Khashabi D, Baral C, et al. Cross-task generalization via natural language crowdsourcing instructions[J]. arXiv preprint arXiv:2104.08773, 2021.",
      "questions_for_the_authors": "In your experiments, did you design all the prompts by yourself? Since you want to investigate the prompt robustness of instruction-tuned LMs, it should be better to choose the prompts used in tuning OPT-IML (seen by LMs during training), which can more effectively demonstrate your motivations.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "ZqwCNyulIu",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper presents a large-scale systematic analysis of how the variations in prompts influence downstream performances. The authors conduct empirical studies on multiple tasks using different LMs under various settings. In these studies, the authors control (i) grammatical properties such as mood, tense and modality; and (ii)  lexico-semantic variation by replacing a word with its synonyms. Experimental results show that prompts transfer rather poorly across datasets and LMs.",
      "reasons_to_accept": "- A very comprehensive and systematic empirical study of the instability of prompt choices. Those experimental results could potentially be utilized in many future studies.\n- The authors present several interesting conclusions, such as the existence of the correlation between LM perplexity of prompts and performance.",
      "reasons_to_reject": "- Though it is not a serious flaw, it would make this paper stronger if the authors could give some explanations on why they choose certain types of linguistic variations, such as mood and tense. What if the prompts are paraphrased to a larger extent, e.g., using a constituency parser to generate sub-tree structures and making changes on the sub-tree level?",
      "missing_references": "- Do Prompts Solve NLP Tasks Using Natural Language? Sen Yang, Yunchen Zhang, Leyang Cui and Yue Zhang. ArXiv 2022.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  }
]