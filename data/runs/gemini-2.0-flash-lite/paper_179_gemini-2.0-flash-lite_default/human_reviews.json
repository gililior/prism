[
  {
    "rid": "n6blR5W7cv",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper constructs an open-domain QA dataset (CAMBIGNQ) with clarification questions for resolve the ambiguity. The dataset is built upon an existing dataset (AMBIGNQ). The only difference between these two datasets are that the newly constructed one asks clarification question while the existing one asks disambiguated questions for each possible interpretation. This paper reports the human evaluation on the preference between these two ways for resolving ambiguity in QAs. The results support their arguments that this new setting is more preferable.  In addition to the dataset, this work establishes benchmark performance on three tasks: ambiguity detection, clarification generation and clarification-based question answering. The experimental results reveal the difficulty of these tasks, thus show the need of this new dataset  for future development.",
      "reasons_to_accept": "- Construct a new open-domain QA dataset for resolving ambiguity by asking clarification questions. This dataset can be used for following work to improve the modeling performance.\n- Demonstrate the advantage of asking clarification questions than disambiguated questions. This effort encourages future work towards generating clarification question.\n- Report solid benchmark performance. These experiments reveal the challenge of these tasks.\n- Evaluate InstructGPT's ability on generate clarification questions from disambiguated questions through in-context few-shot learning. The insufficient performance demands better methods.",
      "reasons_to_reject": "- Some experimental design is not convincing. For example, in Sec 6.1, they use BERT model for one setting and BART model for the other, so the difference between these two results may not come from whether the predicted answers are useful or not, since it may come from the different capacity of these two models. Another one is that in Table 5, the experiment with ground truth CQ is required, because this will identify whether the low performance in Table 5 is caused by the QA modeling or the generated CQ quality.\n- The benchmark experiments are conducted with BERT, BART which don't represent a SOTA performance. These experimental results are not convincing that these tasks are difficult for more SOTA models such as XLNET, GPT-3, GPT-3.5, GPT-4.  - They only consider one fixed template for asking clarification question, which limits the scope of this work.",
      "questions_for_the_authors": "- One key contribution is to generate clarification questions from disambiguated questions. The InstructGPT with few-shot learning shows limited performance. Performance of some intuitive models would be necessary to be reported, such as rule-based approach which extracts new words in disambiguated questions compared to the ambiguous questions and then consider them as \"options\". Other approaches to be considered is to fine-tune BERT for example. Did you conduct them?\n- The clarification question follows a fixed template, i.e. \u201cWhich [category]: [option1], [option2], ..., or [optionn]?\u201d In human communication, there could be other ways which may be more natural and fluent given a certain context. Why did you choose this template, and how to compare with other possibilities?\n- What do you mean by \"reranked related paragraphs\" in line 242 and 504? ( Reranked by what?)",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "JKArhgoYSJ",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper proposes an innovative approach to handle ambiguous questions (AQ) in open-domain QA by introducing clarifying ambiguous natural questions (CAmbigNQ), a dataset consisting of 5,653 AQs with relevant passages, possible answers, and clarification questions (CQ). Unlike previous methods that resolved ambiguities by **directly** generating disambiguated questions (DQs), the authors present a novel method that first ask CQ before generating an answer. The pipeline consists of three tasks: ambiguity detection, clarification question generation, and clarification-based QA. Experimental results are presented, emphasizing the need for further enhancements.",
      "reasons_to_accept": "**-- Novel Approach:** The authors introduce a new method by shifting from conventional disambiguated question generation to a clarification questions-based method, aligning more closely with real-world applications.\n**-- New Dataset:** The creation of the CAmbigNQ dataset to support research in ambiguity in QA represents a valuable asset for the community.\n**-- Writing Quality:** The paper is clear, well-written, and effectively uses examples to explain the concepts, enhancing readability and understanding.",
      "reasons_to_reject": "**-- Doubtful Motivation:** While CQ improves user preference (as shown in Experiment 1), 33% of people preferred not to generate a CQ, which raises questions about the necessity and effectiveness of producing these CQs.\n**-- Insufficient Comparison with SoTA:** The paper lacks comparison with state-of-the-art methods, which is essential to gauge the true effectiveness of the proposed approach.\n**-- Questions about Methodology:** Several questions arise concerning the methodology, including the choice of evaluation metrics, the contradiction in observations, and the real necessity of CQ generation for end-to-end performance. These need to be clearly addressed to strengthen the paper (see questions for more details).",
      "questions_for_the_authors": "1. While CQ improves user preference (as shown in experiment 1), 33% of people preferred not to generate a CQ. Could this indicate the generated CQs could be misleading or incorrect? Why do human annotators do not prefer those CQs? It would be beneficial to understand the underlying reasons for this preference.\nAnd at the very least, generating CQs should ideally (at least) not alter human preference, as more information is provided compared to only AQ and DQ provided. However, the fact that human preference decreases by 33% when generating CQs raises questions about the necessity and effectiveness of producing these CQs.\n2. The paper relies on large language models such as ChatGPT to generate CQs. I am curious about the ability of a model like ChatGPT to **directly** solve AQ, instead of using generated CQs to train small language model? Can ChatGPT solve AQ in a form similar to chain-of-thought, for example, a chain consisting of (1) ask CQ (2) generate DQ (3) answer the AQ. To summarize, what is the ChatGPT performance on such task? Could ChatGPT perform better than this complex pipeline?\n3. line 394 stated *\u201csince predicted answers for AQ have been shown to be helpful for DQ based approaches\u201d*, however, as shown in Table  3 and Table 6, it seems *No Answers for AQ* makes better performance than *Predicted Answers for AQ* on the ambiguity detection and end-QA task. Does it mean your observation contradict to the observation from existing literature? Can you explain this discrepancy? Or please correct me if I misunderstand this.  4. Since *category* name are mostly short, so, does the use of BLEU or EM score as evaluation metrics reflect the correctness of generated CQs accurately? How does it align with human evaluations? And also same question as the evaluation of CQs, did authors perform human evaluation or semantic-based evaluation on CQs, since the CQs is much longer than *category*?\n5. As shown in Table 5, it seems adding this CQ generation, as an intermediate process, does not help much on end-to-end QA performance, i.e., predicting answers for AQ. So, why do authors think generating CQ is necessary if people still mostly care about the end-to-end QA performance?  6. The proposed method makes good ablation study on using or not using CQ in the question answering generation process. However, the paper does not compare with SoTA performance on AmbigQA.  Leaderboard: https://nlp.cs.washington.edu/ambigqa/leaderboard.html Minor:  7. For CQ generation evaluation, how many references are provided? Do authors construct multi-reference for evaluation to improve the robustness?  8. Do authors try different prompts (as shown in line 239 and line 289) to generate CQ using ChatGPT?",
      "missing_references": "n/a",
      "typos_grammar_style_and_presentation_improvements": "n/a",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.",
      "excitement": "2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "bitOc3EZcv",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "When confronted with an ambiguous question, rather than have the alternatives spelled out as a list of separate questions, it is more natural to produce a clarification question with concisely expressed options. In this paper the authors describe the addition to an existing dataset of ambiguous questions, relevant passages, and possible answers, a clarification question of the preferred type. This was done using InstructGPT with some few-shot examples to generate possible candidates which were then manually checked and revised.\nThey distinguish two subtasks relevant to the creation of this dataset: ambiguity detection and clarification question generation, and one which uses the results of these subtasks, namely finding answers to the clarification question.\nAnnotators were asked to state a preference between a number of the original verbose individual questions and the more compact alternative version. This latter was usually preferred, and was invariably preferred when more than three interpretations were possible.\nThe paper presents preliminary results for ambiguity detection: under one condition, a BERT-based classifier is trained to distinguish ambiguous from non-ambiguous questions (trained and tested on the original dataset) and under another a BART-based model is used to predict answers, with the input classified as ambiguous if more than one is predicted. The first classifier achieved 61.3 F1, the second 34.3.\nThere are also preliminary results for clarification question generation. Various BART models are trained to predict the clarification question given (1) the ambiguous one and relevant passages, or (2) the ambiguous question, answers as predicted by one of the models just described as well as the relevant passages. For comparison, in one condition (3) the predicted answers were replaced by the actual answers, as a kind of best case. Naturally, the latter case produced the best results, with no significant variation between 1 and 2.\nThe final set of experiments tests QA performance on clarification questions, testing four conditions: ambiguity detection with and without predicted answers and clarification questions created with or without the predicted answers. Perhaps surprisingly the best results were obtained with no answers for either stage.\nThe main contributions of the paper are (1) a clear description of the problem (2) provision (public?) of a dataset and (3) initial results for the accuracy of automation of some of the stages, and utility in a QA setting.",
      "reasons_to_accept": "This is an interesting set of problems, which the paper presents one type of solution to.",
      "reasons_to_reject": "I can't see any reason to reject this paper.",
      "typos_grammar_style_and_presentation_improvements": "I could not see any typos. The paper is clear and well written.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  }
]