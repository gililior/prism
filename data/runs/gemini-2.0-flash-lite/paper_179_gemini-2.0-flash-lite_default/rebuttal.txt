[
  {
    "rebuttal": "We sincerely thank the reviewer for their detailed and insightful feedback. We have carefully considered each point and provide the following responses:\n\n**Weaknesses:**\n\n*   **Weakness 1:** \"The paper does not provide sufficient information on the statistical analysis performed on the human preference data. (Section C.1)\" \n    *   **Response:** We respectfully disagree. Section C.1, titled \"Details of test setup,\" explicitly states the use of Amazon Mechanical Turk (MTurk) for the human preference test and details the restrictions placed on workers to ensure quality (e.g., country of origin, HIT approval rate). Section C.2, \"Detailed analysis on test result,\" provides a breakdown of the results. We believe the reviewer may have overlooked these details.\n\n*   **Weakness 2:** \"The paper overclaims the performance of the generated clarification questions due to various factors. (Conclusion section)\" \n    *   **Response:** We acknowledge this point. The Conclusion section already highlights the limitations of our evaluation methods, including the fact that reference CQs are not the only correct answer and that intrinsic evaluation may not capture semantic similarity. We will strengthen this section by explicitly stating that our results should be interpreted cautiously, given these limitations.\n\n*   **Weakness 3:** \"The paper lacks crucial information regarding the environment used for the human preference test (Section C.1) and the seeds used for random selection of examples or batch creation. (Section C.1)\" \n    *   **Response:** We acknowledge the need for more detail. While Section C.1 mentions MTurk, we will add a sentence specifying the MTurk platform used and any relevant libraries. We will also include a statement in the Appendix clarifying that the random seeds used for example selection and batch creation will be made available in the code release.\n\n*   **Weakness 4:** \"Dataset license and usage restrictions are not stated. (Insufficient evidence)\" \n    *   **Response:** We will add an explicit statement regarding the dataset license and usage restrictions, clarifying that our dataset is built upon AMBIGNQ, which is derived from Natural Questions, and therefore subject to the same licensing terms.\n\n*   **Weakness 5:** \"Figure 3 lacks clear axis labels and units, making it difficult to interpret the results. (Fig 3)\" \n    *   **Response:** We will add axis labels and units to Figure 3 to improve clarity.\n\n*   **Weakness 6:** \"The definition of 'clarification question' (CQ) could be more precise, and the paper lacks a clear notation table. (Abstract, ยง1)\" \n    *   **Response:** We will add a concise definition of 'clarification question' early in the introduction and include a notation table in the appendix.\n\n*   **Weakness 7:** \"Several tables lack crucial statistical information such as standard deviations, confidence intervals, or p-values, making it difficult to assess the significance of the results. (Tables 1, 3, 4, 5, 6, 7, 8, 9)\" \n    *   **Response:** We will add standard deviations or confidence intervals to the tables where appropriate. However, due to the nature of some metrics (e.g., EM score in Table 1), standard deviations may not be directly applicable. We will also add p-values to indicate statistical significance where applicable. We will also clarify acronyms and abbreviations in table headers and captions.\n\n*   **Weakness 8:** \"The paper lacks a discussion of potential misuse or failure modes. (Insufficient evidence)\" \n    *   **Response:** We will add a Broader Impact section, discussing potential misuse and failure modes, and outlining mitigation strategies.\n\n*   **Weakness 9:** \"The paper is missing a head-to-head comparison with a top cited baseline. (Related Work)\" \n    *   **Response:** We will add a discussion of a top-cited baseline in the Related Work section.\n\n**Suggestions:**\n\n*   **Suggestion 1:** \"Include a discussion of inter-annotator agreement and its impact on the results (Section C.1). (C.1)\" \n    *   **Response:** We believe this is already addressed. Section 4.1 and B.3 already discuss inter-annotator agreement, including the Kappa coefficient. We will add a sentence to Section C.1 to explicitly refer to this discussion.\n\n*   **Suggestion 2:** \"Further research is needed to improve evaluation methods for clarification question generation tasks (Conclusion section). (Conclusion section)\" \n    *   **Response:** We agree, and the Conclusion section already acknowledges this point.\n\n*   **Suggestion 3:** \"Provide the code used for the experiment, including the MTurk setup and data processing scripts, and specify the random seeds used for example selection and batch creation (Section C.1). (Section C.1)\" \n    *   **Response:** We will provide the code, including the MTurk setup and data processing scripts, and specify the random seeds used for example selection and batch creation.\n\n*   **Suggestion 4:** \"Document the environment used for the human preference test, including the MTurk platform and any relevant libraries (Section C.1). (Section C.1)\" \n    *   **Response:** We will add this information to Section C.1.\n\n*   **Suggestion 5:** \"Add explicit license and consent statements for datasets used. (Insufficient evidence)\" \n    *   **Response:** We will add explicit license and consent statements.\n\n*   **Suggestion 6:** \"In Figure 3, add labels to the axes to clarify what is being measured and the units used. Consider adding a legend if different colors are used. (Fig 3)\" \n    *   **Response:** We will add labels to the axes of Figure 3.\n\n*   **Suggestion 7:** \"Provide a concise definition of 'clarification question' early in the introduction and include a notation table (or list of abbreviations) in the appendix or after the introduction. (Abstract, ยง1)\" \n    *   **Response:** We will add a concise definition and a notation table.\n\n*   **Suggestion 8:** \"Include standard deviations or confidence intervals to indicate the variability of the results. Add p-values to indicate statistical significance of performance differences. Clearly define all acronyms and abbreviations used in table headers and captions. Provide more context for the metrics used, such as the number of samples or trials (Tables 1-9). (Tables 1-9)\" \n    *   **Response:** We will add standard deviations or confidence intervals, p-values, and clarify acronyms and abbreviations.\n\n*   **Suggestion 9:** \"Add a Broader Impact section with mitigation strategies (Conclusion). (Conclusion)\" \n    *   **Response:** We will add a Broader Impact section.\n\n*   **Suggestion 10:** \"Add ablation isolating the delta vs. cited method [1] (what the new module adds) (Sec 4.1). (Sec 4.1)\" \n    *   **Response:** We will add an ablation study in Section 4.1."
  }
]