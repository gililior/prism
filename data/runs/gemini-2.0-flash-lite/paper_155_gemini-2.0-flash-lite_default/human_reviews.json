[
  {
    "rid": "yDIC18bpq3",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This review/position paper identifies common pitfalls in fact checking papers, e.g., vagueness about who should use the proposed systems and how. A comprehensive categorization of such issues is proposed and applied to the analysis of the 100 most highly cited papers in the area, using a content analysis approach conducted by the authors. Finally, recommendations are provided for improving narratives in fact checking contributions.\nWhile I have not worked on fact checking myself, I am familiar with the literature and the overall rationale.",
      "reasons_to_accept": "Important examination of the very basic goals of automated fact checking research and the extent to which they are expressed in influencial publications in the area, finding low coherence in many cases.  Clear, well motivated and systematic argumentation that uses just the terminology and reasoning strategies common in the fact checking literature, e.g., lacking evidence, making it well positioned toward this community.\nRobust and thorough annotation and analysis procedure following best practices in content analysis, resulting in both a novel categorization scheme and annotated corpus used to quantitatively back up the arguments in the paper. Exceptionally detailed account of the annotation guidelines and the meaning of each label, including examples for each single case.\nBroad and informative perspective on related work on the goals of fact checking from various disciplines.",
      "reasons_to_reject": "Seems to be implicitly making a strong assumption, that (influencial) papers in automated fact checking always contribute an artifact positioned within some (potential) application. Contributions in NLP, on the other hand, often focus on improving or investigating specific aspects of components that can be internal to such a system, leaving out a broader discussion on the overall system, especially given space limitations. It is therefore not entirely clear whether the normative statement the authors are making, that a comprehensive narrative is always warranted, is a subjective preference or if there is evidence that it is necessary in all cases. For example, arguably it is not the researcher's job to discuss what to do with the identified misinformation after it was found by the system.  No information on the papers themselves is given in the paper, such as distribution over publication years or number of citations. It is also not specified how the number was obtained.",
      "questions_for_the_authors": "A: For unsupported ends, the existence of counter evidence is given as a sufficient criterion. Is it, in general, a criterion for unsupported ends? What if there is both evidence and counter evidence? This black and white view of fact checking may be an oversimplification. Have you considered a more nuanced categorization of faithfulness to evidence, e.g. claim strength as in https://aclanthology.org/2021.emnlp-main.845/ ?",
      "typos_grammar_style_and_presentation_improvements": "Feasibility support does not seem to appear in figure 1. This is fine, but may be worth mentioning so the reader does not look for it.\nCiting Spinoza and Heidegger seems somewhat out of context given the subject of technological artifacts, which is implicitly about information technology. Expanding the discourse more explicitly may help.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "5: Transformative: This paper is likely to change its subfield or computational linguistics broadly. It should be considered for a best paper award. This paper changes the current understanding of some phenomenon, shows a widely held practice to be erroneous in someway, enables a promising direction of research for a (broad or narrow) topic, or creates an exciting new technique.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  },
  {
    "rid": "W8FuydY6jk",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The authors investigate the usefulness of research papers about automated fact-checking. They argue that their usefulness decreases without considering who will be using the developed fact-checking artifacts, and without specifying how they should be used. Therefore, they identify relevant components of a fact-checking system and document their degree of consideration in 100 popular fact-checking papers. \nThey contribute a holistic perspective on fact-checking systems, critically analyze a high number of existing papers, define influential components for the usefulness of fact-checking systems, and derive recommendations for future research. They document the results of the analyzed papers in the form of annotations, which is provided in a json file for further analysis.",
      "reasons_to_accept": "The paper is well written and structured. The research was conducted systematically and logically, and the arguments made are thorough and valid. When accepted, the paper will draw researchers' attention towards the usability of fact-checking systems. It might especially influence those who \"only\" develop a technological solution without considering its practical and societal benefit. The paper would also introduce a new paradigm for evaluating the usefulness of fact-checking papers and systems. It would raise awareness of important aspects to consider for all those who work on fact-checking systems and care about its value for the community.",
      "reasons_to_reject": "There are many categories and new terms introduced which are hard to distinguish without concrete examples. The wording of those terms is also somewhat confusing, so that it is hard to read the paper at the first time. Especially when there are terms with subtle differences, like many of the sub-categories for epistemic elements and narrative types in the paper, combining a few of the terms would have been a good idea in order to make those rememberable for the reader. \nOne might argue that many of the analysis results and recommendations are quite simple and intuitive. It might have been more useful to find out why the analyzed papers have those shortcomings of usefulness instead of supporting the fact that they do have them. In my opinion the authors should have addressed the \"why\" and at least come up with ideas for follow-up research.",
      "questions_for_the_authors": "The numbers in \"()\" refer to the page numbers in the paper.\n(1) Where does the term \"artifacts\" come from?\n(3) Put the epistemic elements and narrative types in a table along with short descriptions and simple examples. This can also be part of the appendix if too large for the body.\n(3) There are many percentages given for epistemic elements and the narrative types, and many of them do not make sense to me. E.g., the given categories for \"Model Owners\" total in 8% - what about the other 92%? And the categories for \"Modeling means\" exceed 100% - how can that be? And the \"Vague narrative\" types should make up 56% (as stated in the paper (on page 4), but the percentages add up to 64%? There are more examples like this in the paper - make sure to correct them, or to clarify how the percentages relate.\n(4) What's the discourse level?\n(4)  What is the difference between vague debunking and vague opposition?\n(6)  Chapter \"Evidence is not a silver bullet\" is well researched and the arguments made in it are all valid. However, the paper is about improving the usefulness of fact-checking related research, and stating these general psychological phenomena which cannot be prevented by fact-checking systems is misplaced here, in my opinion. You should at least give suggestions on how researchers can consider these points in their work.\n(7) One point in your recommendations is to include the data actor in fact-checking papers. It totally makes sense to think from the perspective of the target group when developing a tool, but do you have any suggestions of how to involve them? Are you suggesting to actually invite a sample from the target group as evaluators? How are you going to know what they need if they have a special role which you are not familiar with?\n(15) \"Social Media Moderators (0%)\" ??",
      "typos_grammar_style_and_presentation_improvements": "(1) \"Connecting research to potential use allows researchers to shape their work taking into account the expressed needs of key stakeholders, such as professional fact-checkers (Nakov et al., 2021).\" -- > Overcomplicated sentence:  (2) \"In this paper, we investigate narratives about intended use in automated [...]\" --> \"uses\" or \"the intended use\" (2) \"Based on this, we give recommendations to clarify discussions of intended use:\" --> weird style (2) \"[...] time, rendering resulting artefacts diffi- 122 cult to use in real-world scenarios.\" -- > style (8) \"We investigate narratives of intended use in fact-checking papers, finding that the majority describe how this tool will function in vague or unrealistic terms\" --> \"[...] papers, and find that [...] how a tool [...].\"\n(9) \"We chose to understand automated fact-checking artefacts through their intended uses. This is only one way understand technologies.\" -- > \"[...] one way to understand technologies.\"\n(23) Figure 7 should be formatted in a cleaner way.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "5: Could easily reproduce the results."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "kZtCRKktuR",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper is an analysis of 100 \"highly cited\" papers on the topic of automated fact-finding regarding the questions of (i) the motivation for the paper? Why was the solution being discussed? ( ii) the means or mechanisms for doing this; the 'how', and (iii) who this proposed solution is intended for -- the participants in the scenarios being looked at.\nThe analysis was conducted by two annotators (presumably the authors) manually going through the abstracts and introductory sections of these 100 papers and marking passages/text-spans with a series of tags based on a set of guidelines that the paper proposes. The paper presents some of the statistics from this analysis.",
      "reasons_to_accept": "This is a discussion of a proposed structure for analyzing fact-checker algorithms, along with an overview of how this framework may be used to understand some of the more highly cited recent papers proposing solutions; as such, this is both interesting and important. Especially as we enter a time of increasingly easy generation of realistic looking narratives using AI, fact-checking, whether automated or not, will become increasingly critical. Having a consistent framework and baseline will make it easier to compare these systems against one another. As a starting point, future work in this area can extend the framework in interesting ways, stimulate discussions on how some of these metrics may be automated and how to compare or weight one dimension over another.",
      "reasons_to_reject": "I was hoping that this would be an automated scoring algorithm for fact-checkers -- taking as input algorithms and datasets, and outputting a completeness and correctness score for each (essentially an equivalent of Precision and Recall in information retrieval). Perhaps even a machine learning based analysis of language and communication cues that may, in conjunction with background information, may be used to find potential candidates for further checking.  Instead, this is more of a thoughtful discussion or guidelines for how reviewers (or professional fact-checkers) might want to think about reading papers while evaluating them for their claims on fact-checking. The analysis of the 100 papers that the paper presents is somewhat underspecified, leading to ambiguity: for instance, while the authors point out that evidence is often lacking for claims being made in narratives (and missing entirely for 'vague narratives'), they also acknowledge that even though citations are included, they need not always be relevant. Checking a fact, even if there is a citation, requires chasing down the references, understanding them, and making a determination about the validity of the current 'fact'. The paper points out that citations/evidence in itself is not a silver bullet, since there are a number of ways in which the citations or evidence presented can be cause problems for the reader.\nThe paper is an interesting and very timely reminder of the problems that we should be thinking about, but EMNLP is not (in my view) the best forum for this work, given the focus of this work. An ethics or social science conference would be a better venue.",
      "questions_for_the_authors": "- Are there scoring frameworks that could be designed to assign a numerical value to find potential candidates for fact-checking?\n- What are good datasets for training and testing that might be assembled, either real or synthetic? ( Generating synthetic datasets should be an interesting problem since the proposed framework could be inverted) Could some of these be done without doing deep NLP? ( for instance, by doing purely propositional theorem proving, or by solving for evidence graphs, etc)",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "1: Poor: This study is not yet sufficiently thorough to warrant publication or is not relevant to EMNLP.",
      "excitement": "2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  }
]