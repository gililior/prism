[
  {
    "rid": "s1wsuqNeS1",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper is centrally interested in structured analogical reasoning in large language models. Existing work has focused on word analogies. This work suggests that results from that paradigm overestimate the overlap between neural models and human cognition. They introduce a task of analogical structure abduction that evaluates whether models can systemically associate the components of an analogy. The results suggest that models often miss the underlying relations, and thus, differ from humans. The main contributions are: i) a benchmark called ScAR and ii) a evaluation of LLMs on this task highlighting the need for future work to improve models.",
      "reasons_to_accept": "The paper is well motivated and its aim clear. The benchmark will be of use to community and appears to address a gap in existing resources. The experiments are reasonably thorough and expose limitations with existing models, while suggesting ways they might be improved.",
      "reasons_to_reject": "The problem identified by the paper is interesting and well articulated. There are a few methodological issues that weaken the rating (included under questions for the authors). Additionally, the linkage to human cognition (which appears to be a central aim of the paper) is a bit unclear. It is stated that the Structure Mapping Theory (SMT) has been proposed as a mechanism for analogical reasoning. Other times, it is stated that SMT suggests that new knowledge is gained by system level analogies. Put another way, there are two uses of SMT in the paper, (i) as the account for analogical reasoning broadly construed, and (ii) as the account for system level analogical reasoning (a more narrow form of analogy). If (ii) is the correct interpretation, then it\u2019s possible models can do simple word analogies in a human-like fashion without the ability to do system level analogies. The lack of clarity around these possibilities makes it difficult to interpret what these results say about comparisons between models and humans.  Ultimately, the paper proposes a potentially useful benchmark for evaluating the underlying structural reasoning abilities of large-language models. However, some issues with the methodology and the underlying arguments motivating the conclusion depress my rating.",
      "questions_for_the_authors": "A. Can you elaborate on the reasonableness of using the E-KAR dataset heavily in this work? The main concern is that it is a dataset taken from China\u2019s Civil Service Examinations. The description provided in Appendix A suggests that \u201ccultural knowledge\u201d is part of the exam. Is it reasonable to expect that a model trained largely on English data and evaluated on English data will be able to answer these questions? I have in mind work like Santurkar et al. (2023). Whose Opinions Do Language Models Reflect. https://arxiv.org/abs/2303.17548., which demonstrate that models encode only some groups well.  B. Why was GPT-4 used so extensively in the creation of the dataset (e.g., for the explanation generation)? Wouldn\u2019t higher quality data come from having the annotators do all of this (they are already reading through them after all)?  C. In the Ethics Statement it is stated that \u201cas described in our paper, all annotators are compensated\u201d, where is this stated in the body of the paper? It would be helpful if more details about the annotators (e.g., what are the 5 annotators backgrounds on lines 229-231) and their task were included (e.g., the instructions they were provided). This would facilitate both replication and extension of this work.  D. Technique for embeddings was unclear in section 2. It is stated that the method proposed by Ushio et al. (2021) was used (line 139). However, that paper used cosine similarity in evaluating embeddings, as far as I can tell. Moreover, it is stated that the winning answer candidate was determined with the marginal likelihood biased perplexity measure which uses probability of the token in context, not the embedding. Could this method be clarified in the paper?   MINOR: E. In section 4, a few more models were evaluated. Why weren\u2019t these models included in the other experiments?  F. The role of RSI in the paper is a bit unclear. Is it part of the new proposed benchmark as well? Or is it just used as an additional source of evidence that models fail to capture the underlying structural relations in an analogy (similar to how the ability of models to fully associate the set of concepts in the benchmark evidences they systematically understand the relations)?",
      "typos_grammar_style_and_presentation_improvements": "Figure 3d is used to argue that models are more robust in the CoT prompting scenario. The inclusion of error bars would facilitate more informed comparisons between the variation in a given condition and across conditions.  Consistent y-axis bounds in Figures 3, 5, and 6 would facilitate comparisons.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "hzWaFpE9ty",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "This paper is about analogical reasoning in language models supported by structure abduction. Its contributions are as follows: 1. The authors present a study on past analogical reasoning benchmarks to show that while LLMs can achieve high performance on simpler word analogy benchmarks, this is not always supported by an understanding of the underlying structure for the analogy (unlike humans). This motivates them to create a stronger evaluation for analogical reasoning that considers structure abduction jointly with making analogies.\n2. The authors create a benchmark called Scientific Analogical Reasoning (ScAR) which provides 400 analogies across systems in 13 different scientific and other domains, including 1600 possible mappings between concepts within systems. They additionally collect background knowledge and explanations to support the analogies in the dataset, making it a possibly valuable and high-quality resource for studying analogy-making in LLMs.\n3. The authors evaluate contemporary LLMs, both open-source and closed API-based, on ScAR under several prompting settings. Further, they perform analysis on the impact of various LLM-based aspects in the experiment and the domain. Lastly, they apply earlier word embedding-based methods for analogy as a reference, and perform a preliminary study on open analogical structure abduction, where concepts from the two analogous systems are not provided directly, and rather must be retrieved from a large list.\nWhile the paper does not present new methods for tackling analogical reasoning with LLMs and perhaps lacks some consideration of the impact of LLM pre-training data on this problem, it still constitutes a substantial contribution that could be valuable to the NLP community.",
      "reasons_to_accept": "**Strength A:** The paper compiles a (seemingly) high-quality benchmark dataset of interesting analogies across scientific and other domains, including consistent details about the underlying structure of analogies, and background knowledge and explanations for analogies. This resource could be useful to others in the community.\n**Strength B:** The creation of the benchmark is justified by a preliminary study on how word analogy tasks do not actually represent analogical reasoning capabilities. I have not seen another work demonstrate this before, and this can be motivation for the community to look at analogical reasoning more holistically than prior work has.\n**Strength C:** The analysis of benchmark results is thorough and fairly comprehensive, covering a lot of variations and details on the prompt engineering/LLM interface side, as well as including a thoughtful study on how analogies can be made across an open domain of concepts (which I believe is the hardest challenge in analogical reasoning). The authors also explore earlier representation/embedding-based approaches to analogy. It feels like the authors have considered a lot of questions in performing these experiments.",
      "reasons_to_reject": "**Weakness A:** This paper doesn\u2019t propose any new modeling techniques or methods to apply LLMs to analogy tasks. This causes the main benchmark results on the paper to not be very inspiring, as they don\u2019t show much significant variation across existing models and prompting techniques that are explored.\n**Weakness B:** The paper does not rigorously explore the impact of pre-training data on LLMs\u2019 analogical reasoning. In natural domains like this that may be largely supported by memorization of training data, some analysis for this is important.",
      "questions_for_the_authors": "**Question A:** To what extent can contamination of LLM pre-training data impact the observed abilities of LLMs in analogical reasoning? For example, I can Google about the aperture of a camera and pupil of a human eye and find plenty of web content talking about how they\u2019re analogous. Does the LLM gain this capability just by memorizing the pre-training data?  I would be curious to see how thoroughly the system understands common systems or concepts across examples from ScAR - if the system can correctly answer the *aperture -> pupil* example, but can\u2019t understand some other example about *aperture* that\u2019s less common, this may suggest a lack of deep understanding of the concept of *aperture.* Figure 4 gives some hints about this possibility across domains, but doing such an analysis could also be a valuable addition to the work! * As a side note, this is one reason why Mitchell (2021) listed later in my review argues that we shouldn\u2019t focus on natural language analogy problems, but I think this issue can also be explored by more thorough evaluations.*",
      "missing_references": "ACL 2023 has at least one paper on LLMs and analogical reasoning, which looks at abstract analogical reasoning: https://aclanthology.org/2023.acl-long.109/  This paper by Melanie Mitchell would be important to cite: https://arxiv.org/abs/2102.10717",
      "typos_grammar_style_and_presentation_improvements": "**Organization/presentation suggestions:** L091: If accepted, you may want to use the extra space to break this paragraph out into a numbered list - may make it easier to read. There are some other paragraphs where you do this too, and I would recommend the same.\nSection 3.4 presents a formal problem definition, which might make more sense to introduce before your dataset. This would help give the reader a foundation on what task you\u2019re looking at, and then the data collection would be easier to understand and map each step to components of the task. I\u2019m not totally sure - would leave it up to you.\nI\u2019m not sure if it\u2019s fair to characterize Webb et al. (2022) as just looking at analogy generation with word analogies or simple sentence analogies as your Section 5 says around L529. This paper looks at abstract language-based analogy tasks and uses them to study how well LLMs can perform analogical reasoning. I might suggest adding a separate paragraph in Section 5 looking at abstract analogical reasoning rather than lumping it into the paragraph about analogies between situations. You may also think about organizing Section 5 based on how these different works focus on different parts of analogy-making, i.e., representation, mapping, and evaluation, and how your work contrasts from it.\n**Formatting/typo corrections:** L036, L046, L055, L060, L090, L110, L135, L344-348\u2026: putting two sets of parentheses next to each other or nesting parentheses is improper format and a bit confusing to read. You can use \\citealp in LaTeX to combine citations with other content in parentheses, or find another way to remove one set of parentheses.\nL124: missing a space after period Did you use ChatGPT or GPT-4 to revise background information about systems? The paper says ChatGPT, but Table 10 says GPT-4.",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "yuHFUXJjjQ",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper proposes to move beyond word analogies for LLMs and to enable \"analogical structure abduction\".  To this end a new data set is constructed based on the efforts of human annotators paid a local minimum wage, wherever local may be and however much that works out to be.  The results are mixed on a closed list of analogies, quite bad on an open list.  The main contributions are a move towards going beyond word analogies and the creation of a new data set.",
      "reasons_to_accept": "The topic of the paper is interesting given that, as the authors point out, analogy is a very basic human skill that is applied across many different domains. It is as such interesting to extend LLMs capabilities to include analogical reasoning.",
      "reasons_to_reject": "Within linguistics, language change based on analogy is pervasive as is language acquisition based on rule formation via analogy.  Analogy is also a powerful tool for story-telling, writing and argumentation.  Yet the authors chose none of these domains for their experiments.  Rather, they construct an artificial task of a slightly more complex relationship between words that looking at word similarity in that the relationship involves pairs of two words rather than a single pair of words.  It is not clear how this really reflects the human process of analogy or how it is more generally of interest beyond this rather artificially constructed sets of relations between pairs of words.  This is also presumably why the attempts at having LLMs work with the open list of \"analogies\" do so badly: the task is artificially defined and does not tap into higher principles/generalizations that the LLMs might have learned.",
      "questions_for_the_authors": "It is not clear to me how the classic \"queen is to woman as king is man\" is really an instance of analogy since basically only the similarity of two vectors to one another  is being compared: how is that really analogical reasoning?   Do the authors equate semantic similarity with analogy?  This point needs to be brought out more strongly in the paper.",
      "ethical_concerns": "No",
      "justification_for_ethical_concerns": "The annotation was done by human annotators at \"local\" minimum wage.  It is not said where those annotators are from or under what conditions they are performing the annotation.  This concern was addressed sufficiently by the authors.   They used undergraduates who were trained in this particular task."
    },
    "scores": {
      "soundness": "2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems",
      "excitement": "2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty."
    }
  }
]