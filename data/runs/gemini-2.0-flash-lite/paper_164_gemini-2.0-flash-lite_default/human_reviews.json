[
  {
    "rid": "8cVjdhciaA",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "In this paper, GPT models are used to modify prompts for text-to-image generation. The authors show that, in the generation process, GPT modified prompts can reduce the number of edits by around 20%. A human study is conducted to compare the type of GPT generated edits and human edits, as well as the practicality of the usefulness of the generated prompts.",
      "reasons_to_accept": "The paper is generally well written with clear motivation, solid experiments, and presented with concrete examples. It is interesting to see such analysis and how GPT models can contribute to the prompting process of text-to-image generation.",
      "reasons_to_reject": "Some of the qualitative examples are a bit confusing without detailed analysis, please see the questions.",
      "questions_for_the_authors": "In Figure 7 in the appendix, it was quite surprising to see that Edit2 and the initial prompt are the same but ended up with both failure generations, though totally different, have you analyzed factors of such failure cases/prompts/edits? Or if any filtering has been done from the traces before performing in-context learning/prompting?",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "ksL6z0NIr1",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper investigates the benefits of incorporating GPT-k into the text2image pipeline by using GPT-k to edit the initial t2i prompt. They then investigate how these edits perform compared to human edits. They also investigate what common edit types are made by humans compared with GPT-k and how edit types and # of edits affect the results.",
      "reasons_to_accept": "1. The paper provides interesting and detailed experiments on the benefits and drawbacks of incorporating GPT-k models into the text-to-image pipeline. Some recent work was focusing on incorporating LLMs and diffusion models (see missing references) but this study provides more formal empirical investigation of this phenomenon.  2. The experiments are well-defined and executed. While additional models and settings are possible, we can already draw some preliminary conclusions from this study about the usefulness of GPT models to improve t2i generation.",
      "reasons_to_reject": "1. Authors create a dataset of t2i prompt edits by clustering a dataset of t2i prompts into 100K clusters using sentence embeddings. However, no evaluation or verification of edit clusters is provided. Thus, it is hard to judge if the clusters make sense or not or if they even refer to the same image. Taking the clusters with more than just a few instructions (authors select clusters of up to 20 instructions) seems like an unrealistic setting to me and potentially increasing the chance that some of the clusters will have unrelated prompts. It would be interesting to see how smaller clusters of e.g. under 5 instructions would affect the results.\n2. Each edit cluster has 1 to n edits. I find problematic how only the n-th edit of the trace was used for fine-tuning and few-shot prompts. First, as there was no guarantee that the edits were about the same image, but also the first and last edits will be very different and skipping multiple steps in the edit trace. This potentially teaches the model to hallucinate edits as it has no way to understand the intermediary steps. For example, steps in the middle might have have had different edit types completely altering the final prompt. There needs to be control for semantic/lexical similarity of first and last prompt. The experiment could also include models fine-tuned on a single edit step for a single or multiple edit types. Another way would be to vary n-th edit e.g. from 1 k=n to see the impact on i'-i_MS.\n3. The human evaluation is not very clear to me. First, gpt-3-curie was chosen even though babbage has better CLIP scores, RNE (Table 2), and closer distribution to human edits (Table 4). Second, the framing of the task as choosing between prompts is very vague. For example in Figure 6 it is very hard to choose what edit would be best. It would be more clear to frame it explicitly as an image selection task (which I suspect crowdworkers did anyway).\n4. In the effects of edit types experiment, the CLIP score reported is only between generated image and the last image in the edit cluster. Since each edit cluster can contain multiple types of edits, it would be more appropriate to compare to the first image on which the edit type was applied.",
      "questions_for_the_authors": "A: What are hyperparameters for clustering prompts into traces of edits and how were they chosen? I was surprised to see that you have clusters with such a huge number of edits, did you inspect them to see that the clusters make sense? How many clusters in absolute numbers do you have after selecting only those that have at most 20 edits?\nB: 131-135: How is the ordering for the traces determined?\nC: 150-152 this does not make sense to me. Before you mentioned you only focused on 95% of the traces. How come now it is the full 100K?\nD: 225-227 How exactly did you identify edit type using sequenceMatcher?\nE: 233: gpt-3 are also autoregressive but you say their distribution is more similar. so the claim \"GPT-2 models, due to their autoregressive training nature, have a tendency towards continual generation, resulting in a majority of edits being insert\" does not hold.",
      "missing_references": "Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, and Smaranda Muresan. 2023. I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. In Findings of the Association for Computational Linguistics: ACL 2023, pages 7370\u20137388, Toronto, Canada. Association for Computational Linguistics.",
      "typos_grammar_style_and_presentation_improvements": "023-024 \"predicting spontaneous changes in the primary subject matters\"  not very clear what are primary subject matters 049-053 Figure 1 and description is very unclear. What is a trace and how did you define an edit? the division symbol is also confusing as it seems # of edits /trace is simply the size of a cluster of prompts, probably just keeping # edits is simpler 059-062 \"LLMs and T2I models are trained on different modalities\" that's not entirely true as many multimodal models are being released recently (including announced GPT-4) 094 adjusting -> adjustment 152 holdout -> heldout",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  },
  {
    "rid": "XaFIfc96tf",
    "reviewer": null,
    "report": {
      "paper_topic_and_main_contributions": "The paper investigates the concept of \"prompt editing\" associated with text-to-image generation, in which the users apply successive edits to their prompts in order to generate their desired image. It does so by analyzing a dataset of user prompts scraped from the official Stable Diffusion Discord server and employing various GPT-based models to improve the prompt editing process. In their experiments the authors observe that while the GPT-based models seem to mostly insert modifiers, human users tend replace words and phrases, yielding a significantly different image in the end. Furthermore, it also seems that editing the prompt via GPT-based models results in images more similar to intermediate edits made by the users rather than the final image, suggesting that the GPT-based models may be more effectively used to reduce the number of edits necessary to arrive at the final image.",
      "reasons_to_accept": "- An analysis of the prompt edits made by users at the Stable Diffusion Discord server - Exploration of the difference between the images generated by user-edited vs. GPT-generated prompts - Evaluation of various GPT-based models on this task, with the GPT-2 model family being finetuned and GPT-3 model family being prompted - Human evaluation of the prompts generated/edited by the GPT-based models, showing them to be better or comparable to user edited prompts - An ablation study on the edit types (insert/delete/swap/replace) on the generated output image",
      "reasons_to_reject": "The paper is well written and hence there are only a few reasons why this paper might be rejected.\nPerhaps the strongest one is the fact that while the GPT-based models are a key part of the analysis, except for the prompt the authors used the paper doesn't contain any further details on how it was used as a metric. Furthermore, the OpenAI's APIs provide various parameters such as `temperature`, `top_p`, `presence_penalty` and `frequency_penalty`, which are not specified in the paper, further complicating potential future replication efforts. This should also be applicable to GPT-2 models that were publicly released and are not available only via an API (e.g. the HuggingFace Transformers provide APIs for specifying similar parameters).\nA similar issue can be found with the Stable Diffusion model that was used for image generation. It is not obvious what parameters, except for the prompts, were varied when the images were generated, and to what extent would changing these have an impact on the presented results.\nThere were also two relatively minor inconsistencies we found in the paper which unfortunately detract from the quality of the paper.  The first can be found on line 151 when they authors mention \"We split the 100k trace of edits into two halves,\" but the next part of the sentence says \"with 30k traces used for evaluations and the remaining 70k serving as holdout set\". When splitting something into two halves, it would be reasonable to expect a 50:50 split but that is not what we observe here. It is therefore unclear which of the potential interpretations the authors meant in this sentence.\nThe second can be found at the discussion of edit types which starts on line 217. When using SequenceMatcher, it is unclear how are the edits classified. If for instance one generated prompt both swaps the order of words, inserts a few new words towards the beginning and removes a few words from the end of the prompt, which category would such a change fall into? Would it be all of them or just one of them or neither?",
      "questions_for_the_authors": "**Question A**: Would you mind providing some more details on how you used the GPT-3 models in the evaluation, especially what parameters were provided to the API and also how expensive was it to execute the experiments described in the paper?\n**Question B**: Is there any specific reason why the `gpt-3.5-turbo` model wasn't used in the experiments as opposed to `text-davinci-003`, which is both more expensive and which the OpenAI's docs now consider legacy?\n**Question C**:  Would you be able to provide any intuition as to what extent are the GPT-3 results robust to changes of the prompt shown in Table 8?\n**Question D**: Similarly to the previous question, how did you arrive at choosing 16 shots for the in-context learning and to what extent would varying this number change the results?",
      "typos_grammar_style_and_presentation_improvements": "- line 051: \"issue\" -- it would be best to replace this word with say \"phenomenon\" or \"figure\" - line 060: \"LLMs and T2I models are trained on different modalities and architectures\" -- although the models do use different architectures, it is not clear what training on architectures would refer to - line 138: \"denotes\" -> \"denote\"",
      "ethical_concerns": "No"
    },
    "scores": {
      "soundness": "4: Strong: This study provides sufficient support for all of its claims/arguments. ",
      "excitement": "4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.",
      "reproducibility": "3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available."
    },
    "meta": {
      "license": "CC BY 4.0",
      "reviewer_confidence": "4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
    }
  }
]