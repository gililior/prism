[
  {
    "rebuttal": "We sincerely appreciate the reviewer's thorough and insightful feedback. We address each point below:\n\n**Weaknesses:**\n\n*   **Lack of detailed comparison with Bražinskas et al. (2021) and TransSum [1] and unclear delta:** We believe the paper *does* provide a direct comparison with Bražinskas et al. (2021) (SELSUM). Table 1 and Table 2 present the results of SELSUM and our model on AmaSum and RT datasets, respectively. We retrained SELSUM* on pros, cons, and verdict separately for a fair comparison, as stated in the caption of Table 1. We also compare with SELSUM in the discussion of the results in Section 4.2. We will enhance the discussion to more clearly articulate the performance gains of SUBSUMM over SELSUM, including a more direct comparison of ROUGE scores and a clearer explanation of the advantages of our review sampling and two-stage training scheme. We did not include TransSum [1] as a baseline because it was not available at the time of submission. We will consider including it in future work.\n\n*   **Overclaiming superiority and speculative reasons for underperforming SELSUM on ROUGE-2:** We respectfully disagree that we overclaim superiority. Section 4.2 clearly states that SUBSUMM outperforms the state-of-the-art models and LLM-related systems. We acknowledge the ROUGE-2 performance in the verdict partition of AmaSum is slightly lower than SELSUM in Section 5 (Conclusion). We provide a plausible explanation for this, suggesting that the absence of explicit designs for aspect learning in SUBSUMM may cause the model to miss more 2-gram aspect terms. We will refine this discussion to avoid any impression of overclaiming and to more clearly state the limitations of our model.\n\n*   **Unclear definition of \"contrastive information valuation\" and two-stage training scheme:** The term \"contrastive information valuation\" is introduced in Section 3.1. The methodology section provides a detailed explanation of how we estimate the information value of each review using a contrastive margin loss. The two-stage training scheme is explained in Section 3.2. We will revise the abstract and introduction to provide a more concise overview of these concepts, and we will clarify the relationship between the two stages.\n\n*   **Insufficient information on figures and statistical significance:** We will add axis labels, legends, and improve the visual design of the figures in the next version. We will also include p-values or confidence intervals in Table 3 and Table 4 to provide a more rigorous assessment of the results.\n\n*   **Lack of information on consent, privacy, and bias:** We used publicly available datasets. We will add a section detailing the dataset licenses and the steps taken to mitigate the risk of generating fabricated or harmful content. We will also discuss potential biases in the datasets and the model and how we address them.\n\n*   **Lack of discussion on misinformation and societal impacts:** We will add a Broader Impact section to discuss the potential societal benefits and drawbacks of the proposed framework, including the potential for misuse and mitigation strategies.\n\n**Suggestions:**\n\n*   **Direct comparison with Bražinskas et al. (2021) and TransSum [1]:** Addressed above.\n\n*   **Detailed ablation studies:** We have already provided ablation studies in Table 5 to isolate the impact of each component of SUBSUMM. We will consider adding more detailed ablation studies in future work.\n\n*   **Detailed explanation of \"contrastive information valuation\" and two-stage training scheme:** Addressed above.\n\n*   **More details about the figures and statistical significance:** Addressed above.\n\n*   **Dataset licenses, consent procedures, and privacy measures:** Addressed above.\n\n*   **Discussion of potential biases:** Addressed above.\n\n*   **Discussion of misinformation and societal impacts:** Addressed above.\n\nWe believe that these revisions will significantly improve the clarity and impact of our paper. Thank you again for your valuable feedback."
  }
]